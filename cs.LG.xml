<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#25110;&#33322;&#31354;&#22270;&#20687;&#22330;&#26223;&#20013;&#30340;&#38271;&#36317;&#31163;&#32972;&#26223;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36947;&#36335;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#24120;&#29992;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2401.06762</link><description>&lt;p&gt;
&#36890;&#36807;&#26641;&#26408;&#30475;&#21040;&#36947;&#36335;&#65306;&#22522;&#20110;&#33322;&#31354;&#22270;&#20687;&#30340;&#31354;&#38388;&#20381;&#36182;&#24314;&#27169;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery. (arXiv:2401.06762v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#25110;&#33322;&#31354;&#22270;&#20687;&#22330;&#26223;&#20013;&#30340;&#38271;&#36317;&#31163;&#32972;&#26223;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36947;&#36335;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#24120;&#29992;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#38754;&#29702;&#35299;&#22797;&#26434;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#25110;&#33322;&#31354;&#22270;&#20687;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#23545;&#24191;&#27867;&#30456;&#20851;&#30340;&#32972;&#26223;&#36827;&#34892;&#31354;&#38388;&#25512;&#29702;&#12290;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#31995;&#32479;&#33021;&#22815;&#22312;&#22330;&#26223;&#20013;&#29702;&#35299;&#38271;&#36317;&#31163;&#30456;&#20851;&#30340;&#32972;&#26223;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#20154;&#31867;&#35266;&#23519;&#21040;&#19968;&#20010;&#26174;&#31034;&#36947;&#36335;&#34987;&#26641;&#20896;&#20998;&#21106;&#30340;&#33322;&#31354;&#22330;&#26223;&#65292;&#20182;&#20204;&#19981;&#22826;&#21487;&#33021;&#24471;&#20986;&#36947;&#36335;&#23454;&#38469;&#19978;&#34987;&#26641;&#26408;&#20998;&#21106;&#25104;&#20102;&#19981;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#32780;&#26356;&#21487;&#33021;&#35748;&#20026;&#38468;&#36817;&#26641;&#26408;&#30340;&#20896;&#23618;&#36974;&#25377;&#20102;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29702;&#35299;&#38271;&#36317;&#31163;&#32972;&#26223;&#30340;&#30740;&#31350;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36947;&#36335;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20999;&#33832;&#30382;&#20811;&#36947;&#36335;&#31354;&#38388;&#32972;&#26223;&#65288;RSC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31354;&#38388;&#38271;&#36317;&#31163;&#32972;&#26223;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;U-Net&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.06757</link><description>&lt;p&gt;
&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#20102;&#35299;&#34892;&#20154;&#26159;&#21542;&#23558;&#27178;&#31359;&#22312;&#33258;&#20027;&#36710;&#36742;&#21069;&#26041;&#23545;&#20110;&#25191;&#34892;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#25805;&#25511;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#24207;&#21015;&#22270;&#20687;&#20013;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#39044;&#27979;&#27492;&#31867;&#24847;&#22270;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23548;&#33268;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#21270;&#27178;&#31359;&#21644;&#38750;&#27178;&#31359;&#65288;C/NC&#65289;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;ARCANE&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#22320;&#29983;&#25104;&#21253;&#21547;C/NC&#35270;&#39057;&#21098;&#36753;&#26679;&#26412;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;ARCANE&#29983;&#25104;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PedSynth&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;PedSynth&#22914;&#20309;&#34917;&#20805;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#22914;JAAD&#21644;PIE&#65292;&#20174;&#32780;&#20026;C/NC&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;C/NC&#39044;&#27979;&#27169;&#22411;&#30340;&#36710;&#36733;&#37096;&#32626;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PedGNN&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#23427;&#36895;&#24230;&#24555;&#19988;&#20869;&#23384;&#21344;&#29992;&#38750;&#24120;&#20302;&#12290;PedGNN&#22522;&#20110;GNN-G&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#23545;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06755</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24211;&#22312;&#32467;&#26500;&#21270;&#32593;&#26684;&#19978;&#35299;&#20915;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries. (arXiv:2401.06755v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#23545;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#65288;&#21363;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#35299;&#20915;&#20102;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#12290;AI4PDEs&#20013;&#30340;&#27714;&#35299;&#22120;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#24037;&#20855;&#26469;&#20934;&#30830;&#27714;&#35299;&#24050;&#32463;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#31163;&#25955;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#21367;&#31215;&#23618;&#21487;&#20197;&#29992;&#26469;&#23558;&#31163;&#25955;&#21270;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#26435;&#37325;&#30001;&#25968;&#20540;&#26041;&#27861;&#30830;&#23450;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#35757;&#32451;&#30830;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#20010;U-Net&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22810;&#37325;&#32593;&#26684;&#27714;&#35299;&#22120;&#12290;&#38750;&#30456;&#28342;&#30340;&#20108;&#30456;&#27969;&#21160;&#30001;&#19977;&#32500;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#24314;&#27169;&#65292;&#20854;&#20013;&#21253;&#25324;&#34920;&#38754;&#24352;&#21147;&#21644;&#20307;&#31215;&#20998;&#25968;&#22330;&#30340;&#23545;&#27969;&#65292;&#35813;&#22330;&#25551;&#36848;&#20102;&#27969;&#20307;&#20043;&#38388;&#30340;&#30028;&#38754;&#12290;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Petrov-Galerkin&#36827;&#34892;&#20934;&#30830;&#24615;&#65292;&#24182;&#19987;&#20026;AI4PDEs&#35774;&#35745;&#12290;&#39640;&#38454;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#34987;&#29992;&#20110;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order fini
&lt;/p&gt;</description></item><item><title>&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06751</link><description>&lt;p&gt;
Easy Training Data&#23545;&#20110;&#22256;&#38590;&#20219;&#21153;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06751
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#22312;&#23450;&#20041;&#19978;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#22312;&#22256;&#38590;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30417;&#30563;&#38382;&#39064;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#36807;&#31243;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#35770;&#65292;&#21363;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#30456;&#23545;&#33391;&#22909;&#65292;&#29978;&#33267;&#34920;&#29616;&#24471;&#21644;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#8220;oracle&#8221;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#21644;QLoRA&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#20174;&#26131;&#21040;&#38590;&#30340;&#27867;&#21270;&#65292;&#38024;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#38590;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20845;&#20010;&#32463;&#39564;&#22810;&#26679;&#30340;&#20154;&#31867;&#38590;&#24230;&#24230;&#37327;&#65288;&#22914;&#24180;&#32423;&#27700;&#24179;&#65289;&#21644;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#65288;&#22522;&#20110;&#25439;&#22833;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#20851;&#24515;&#27169;&#22411;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#25910;&#38598;&#24182;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#65292;&#22240;&#20026;&#22256;&#38590;&#25968;&#25454;&#36890;&#24120;&#26356;&#22024;&#26434;&#21644;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23450;&#20215;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#19979;&#27431;&#24335;&#31726;&#24335;&#26399;&#26435;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#38544;&#24335;-&#26174;&#24335;&#26368;&#23567;&#31227;&#21160;&#26041;&#27861;&#20197;&#21450;&#27531;&#24046;&#22411;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32593;&#26684;&#39640;&#26031;-&#22467;&#23572;&#31859;&#29305;&#36924;&#36817;&#21644;&#22522;&#20110;ANN&#30340;&#39640;&#32500;&#19987;&#29992;&#27714;&#31215;&#35268;&#21017;&#26469;&#31163;&#25955;&#21270;&#31215;&#20998;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.06740</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#26399;&#26435;&#23450;&#20215;&#30340;&#28145;&#24230;&#38544;&#24335;-&#26174;&#24335;&#26368;&#23567;&#31227;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models. (arXiv:2401.06740v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23450;&#20215;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#19979;&#27431;&#24335;&#31726;&#24335;&#26399;&#26435;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#38544;&#24335;-&#26174;&#24335;&#26368;&#23567;&#31227;&#21160;&#26041;&#27861;&#20197;&#21450;&#27531;&#24046;&#22411;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32593;&#26684;&#39640;&#26031;-&#22467;&#23572;&#31859;&#29305;&#36924;&#36817;&#21644;&#22522;&#20110;ANN&#30340;&#39640;&#32500;&#19987;&#29992;&#27714;&#31215;&#35268;&#21017;&#26469;&#31163;&#25955;&#21270;&#31215;&#20998;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20215;&#36339;&#36291;&#25193;&#25955;&#21160;&#24577;&#19979;&#30340;&#27431;&#24335;&#31726;&#24335;&#26399;&#26435;&#12290;&#23558;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20559;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;-&#26174;&#24335;&#26368;&#23567;&#31227;&#21160;&#26102;&#38388;&#27493;&#27861;&#36827;&#34892;&#36817;&#20284;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#27531;&#24046;&#22411;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#36880;&#27493;&#36924;&#36817;&#12290;&#31215;&#20998;&#36816;&#31639;&#31526;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#31163;&#25955;&#21270;&#65306;a&#65289;&#36890;&#36807;&#31232;&#30095;&#32593;&#26684;&#39640;&#26031;-&#22467;&#23572;&#31859;&#29305;&#36924;&#36817;&#65292;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#20135;&#29983;&#30340;&#23616;&#37096;&#22352;&#26631;&#36724;&#65292;&#24182;&#19988;b&#65289;&#36890;&#36807;&#22522;&#20110;ANN&#30340;&#39640;&#32500;&#19987;&#29992;&#27714;&#31215;&#35268;&#21017;&#12290;&#20851;&#38190;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;ANN&#30340;&#26500;&#36896;&#30830;&#20445;&#20102;&#35299;&#20915;&#26041;&#26696;&#22312;&#26631;&#30340;&#36164;&#20135;&#36739;&#22823;&#20540;&#26102;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#35299;&#20915;&#26041;&#26696;&#20808;&#39564;&#24050;&#30693;&#30340;&#23450;&#24615;&#29305;&#24615;&#30456;&#19968;&#33268;&#36755;&#20986;&#12290;&#23545;&#26041;&#27861;&#32500;&#24230;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assesse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#12289;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#35758;&#20102;&#19968;&#31181;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#25910;&#25947;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06738</link><description>&lt;p&gt;
&#22122;&#22768;&#33258;&#36866;&#24212;&#65288;&#21152;&#36895;&#65289;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#12289;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#35758;&#20102;&#19968;&#31181;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#25910;&#25947;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#65292;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#65288;SHB&#65289;&#30340;&#25910;&#25947;&#24615;&#12290;Kidambi&#31561;&#20154;&#65288;2018&#65289;&#34920;&#26126;&#65292;&#23545;&#20110;&#20108;&#27425;&#20989;&#25968;&#65292;SHB&#65288;&#24102;&#26377;&#23567;&#25209;&#37327;&#65289;&#26080;&#27861;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#29468;&#24819;SHB&#30340;&#23454;&#38469;&#25910;&#30410;&#26159;&#23567;&#25209;&#37327;&#30340;&#21103;&#20135;&#21697;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;SHB&#21487;&#20197;&#33719;&#24471;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35777;&#23454;&#36825;&#19968;&#35266;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#26465;&#20214;&#25968;&#20026;$\kappa$&#30340;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#26631;&#20934;&#27493;&#38271;&#21644;&#21160;&#37327;&#21442;&#25968;&#30340;SHB&#20855;&#26377;$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\sigma^2$&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#12290;&#20026;&#30830;&#20445;&#25910;&#25947;&#21040;&#26497;&#23567;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#26524;&#26159;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$&#36895;&#24230;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23646;&#24615;&#22270;&#25968;&#25454;&#30340;&#28145;&#24230;&#27969;&#24418;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#21644;&#28508;&#22312;&#20195;&#30721;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06727</link><description>&lt;p&gt;
&#28145;&#24230;&#27969;&#24418;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#23646;&#24615;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding. (arXiv:2401.06727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06727
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23646;&#24615;&#22270;&#25968;&#25454;&#30340;&#28145;&#24230;&#27969;&#24418;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#21644;&#28508;&#22312;&#20195;&#30721;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#22270;&#25968;&#25454;&#65292;&#26469;&#20026;&#21518;&#32493;&#20219;&#21153;&#25552;&#20379;&#23646;&#24615;&#22270;&#23884;&#20837;&#26159;&#36825;&#31687;&#35770;&#25991;&#30340;&#30446;&#30340;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#26469;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#21644;&#28508;&#22312;&#20195;&#30721;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#27969;&#24418;(Variational)&#22270;&#33258;&#32534;&#30721;&#22120;(DMVGAE/DMGAE)&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#23398;&#20064;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20197;&#24212;&#23545;&#25317;&#25380;&#38382;&#39064;&#12290;&#22312;&#39044;&#23450;&#20041;&#30340;&#20998;&#24067;&#19979;&#65292;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#20445;&#25345;&#33410;&#28857;&#21040;&#33410;&#28857;&#30340;&#27979;&#22320;&#32447;&#30456;&#20284;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25215;&#35834;&#22312;&#31295;&#20214;&#25509;&#21463;&#21518;&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing graph data in a low-dimensional space for subsequent tasks is the purpose of attributed graph embedding. Most existing neural network approaches learn latent representations by minimizing reconstruction errors. Rare work considers the data distribution and the topological structure of latent codes simultaneously, which often results in inferior embeddings in real-world graph data. This paper proposes a novel Deep Manifold (Variational) Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve the stability and quality of learned representations to tackle the crowding problem. The node-to-node geodesic similarity is preserved between the original and latent space under a pre-defined distribution. The proposed method surpasses state-of-the-art baseline algorithms by a significant margin on different downstream tasks across popular datasets, which validates our solutions. We promise to release the code after acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#36716;&#21270;&#28431;&#26007;&#20248;&#21270;&#38382;&#39064;&#30340;&#26080;&#27169;&#22411;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#28431;&#26007;&#27169;&#22411;&#26469;&#25429;&#25417;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.06710</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36716;&#21270;&#28431;&#26007;&#20248;&#21270;&#30340;&#26080;&#27169;&#22411;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization. (arXiv:2401.06710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#36716;&#21270;&#28431;&#26007;&#20248;&#21270;&#38382;&#39064;&#30340;&#26080;&#27169;&#22411;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#28431;&#26007;&#27169;&#22411;&#26469;&#25429;&#25417;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#33829;&#38144;&#27963;&#21160;&#20013;&#65292;&#26681;&#25454;&#28040;&#36153;&#32773;&#29366;&#24577;&#36873;&#25321;&#24191;&#21578;&#34892;&#21160;&#30340;&#28789;&#27963;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35782;&#21035;&#26368;&#20248;&#39034;&#24207;&#20010;&#24615;&#21270;&#24178;&#39044;&#20197;&#26368;&#22823;&#21270;&#38024;&#23545;&#26032;&#20135;&#21697;&#30340;&#37319;&#32435;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36716;&#21270;&#28431;&#26007;&#27169;&#22411;&#26469;&#24314;&#27169;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#27599;&#20010;&#28040;&#36153;&#32773;&#30340;&#29366;&#24577;&#65288;&#20363;&#22914;&#19982;&#20844;&#21496;&#30340;&#20114;&#21160;&#21382;&#21490;&#65289;&#24182;&#20801;&#35768;&#28040;&#36153;&#32773;&#34892;&#20026;&#38543;&#30528;&#20854;&#29366;&#24577;&#21644;&#20844;&#21496;&#30340;&#39034;&#24207;&#24178;&#39044;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#23376;&#37038;&#20214;&#33829;&#38144;&#25968;&#25454;&#38598;&#20013;&#20197;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24230;&#65288;&#36229;&#36807;0.95&#30340;&#26679;&#26412;&#22806;AUC&#65289;&#25429;&#25417;&#21040;&#28040;&#36153;&#32773;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20844;&#21496;&#24517;&#39035;&#20174;&#28040;&#36153;&#32773;&#20132;&#20114;&#20013;&#23398;&#20064;&#21508;&#31181;&#24178;&#39044;&#30340;&#29366;&#24577;&#29305;&#23450;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24402;&#22240;&#30340;&#20915;&#31574;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26080;&#27169;&#22411;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The flexibility of choosing the ad action as a function of the consumer state is critical for modern-day marketing campaigns. We study the problem of identifying the optimal sequential personalized interventions that maximize the adoption probability for a new product. We model consumer behavior by a conversion funnel that captures the state of each consumer (e.g., interaction history with the firm) and allows the consumer behavior to vary as a function of both her state and firm's sequential interventions. We show our model captures consumer behavior with very high accuracy (out-of-sample AUC of over 0.95) in a real-world email marketing dataset. However, it results in a very large-scale learning problem, where the firm must learn the state-specific effects of various interventions from consumer interactions. We propose a novel attribution-based decision-making algorithm for this problem that we call model-free approximate Bayesian learning. Our algorithm inherits the interpretability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06699</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#38381;&#21512;&#35299;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#21644;&#38142;&#24335;&#35268;&#21017;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;&#26435;&#37325;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#26159;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65292;&#26032;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#19968;&#32452;&#26435;&#37325;&#65292;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#20197;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#20248;&#21270;&#26435;&#37325;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#19981;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#20998;&#31867;&#38382;&#39064;&#65289;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#20960;&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#26368;&#32456;&#35299;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#26159;&#36825;&#20123;&#35745;&#31639;&#65288;&#23545;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#65289;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.06697</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65306;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease Study. (arXiv:2401.06697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#33041;&#30142;&#30149;&#65292;&#23548;&#33268;&#32769;&#24180;&#20154;&#20013;&#26174;&#33879;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#12290;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#21487;&#20197;&#34920;&#29616;&#20026;&#21508;&#31181;&#24515;&#29702;&#33021;&#21147;&#30340;&#19979;&#38477;&#65292;&#22914;&#27880;&#24847;&#21147;&#12289;&#35760;&#24518;&#21644;&#20854;&#20182;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#20123;&#32570;&#38519;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20307;&#29702;&#35299;&#20449;&#24687;&#12289;&#33719;&#21462;&#26032;&#30693;&#35782;&#21644;&#26377;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#21463;&#21040;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#24433;&#21709;&#30340;&#27963;&#21160;&#20043;&#19968;&#26159;&#20070;&#20889;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#21387;&#21147;&#12289;&#36895;&#24230;&#21644;&#31354;&#38388;&#32452;&#32455;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#21040;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#21487;&#33021;&#34920;&#26126;&#26089;&#26399;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#65292;&#29305;&#21035;&#26159;AD&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32463;&#20856;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)&#26041;&#27861;&#65292;&#36890;&#36807;&#20070;&#20889;&#20998;&#26512;&#26469;&#26816;&#27979;&#32769;&#24180;&#20154;&#20013;&#30340;AD&#12290;&#28982;&#32780;&#65292;&#20808;&#36827;&#30340;AI&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06688</link><description>&lt;p&gt;
&#19981;&#35201;&#25490;&#21517;&#65292;&#35201;&#21512;&#24182;&#65281;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#26469;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36890;&#36807;&#32473;&#23450;&#28304;&#21477;&#23376;&#20272;&#35745;&#30446;&#26631;&#21477;&#23376;&#30340;&#27010;&#29575;&#65292;&#20294;&#36825;&#20123;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#31867;&#21916;&#22909;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QE-fusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#30340;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#65288;QE&#65289;&#26469;&#32508;&#21512;&#25913;&#36827;&#32763;&#35793;&#32467;&#26524;&#12290;QE-fusion&#21033;&#29992;&#20174;&#27169;&#22411;&#20013;&#25277;&#21462;&#30340;&#20505;&#36873;&#27744;&#65292;&#20351;&#29992;&#20687;CometKiwi&#36825;&#26679;&#30340;QE&#25351;&#26631;&#32452;&#21512;&#19981;&#21516;&#20505;&#36873;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;QE-fusion&#19982;&#27874;&#26463;&#25628;&#32034;&#21644;&#26368;&#36817;&#30340;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65288;&#22914;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#25110;QE-&#37325;&#26032;&#25490;&#24207;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#24403;&#24212;&#29992;&#20110;&#29992;&#20110;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PolyLM&#12289;XGLM&#12289;Llama2&#21644;Mistral&#65289;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#65288;NLLB&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COMET&#21644;BLEURT&#35780;&#20998;&#26041;&#38754;&#22987;&#32456;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#26356;&#22823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#22855;&#24322;&#25668;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#36798;&#29575;&#26497;&#38480;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24471;&#21040;&#20102;&#32479;&#19968;&#30340;&#34920;&#36798;&#29575;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;ReLU&#12289;spiking&#12289;$\tanh$&#21644;sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26126;&#30830;&#22320;&#34920;&#31034;&#8220;&#25351;&#25968;&#36793;&#30028;&#23618;&#35299;&#29305;&#24615;&#8221;&#65292;&#24182;&#33021;&#24471;&#21040;&#25913;&#36827;&#30340;&#40065;&#26834;&#34920;&#36798;&#29575;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.06656</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22855;&#24322;&#25668;&#21160;
&lt;/p&gt;
&lt;p&gt;
Neural Networks for Singular Perturbations. (arXiv:2401.06656v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#22855;&#24322;&#25668;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#36798;&#29575;&#26497;&#38480;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24471;&#21040;&#20102;&#32479;&#19968;&#30340;&#34920;&#36798;&#29575;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;ReLU&#12289;spiking&#12289;$\tanh$&#21644;sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26126;&#30830;&#22320;&#34920;&#31034;&#8220;&#25351;&#25968;&#36793;&#30028;&#23618;&#35299;&#29305;&#24615;&#8221;&#65292;&#24182;&#33021;&#24471;&#21040;&#25913;&#36827;&#30340;&#40065;&#26834;&#34920;&#36798;&#29575;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#30028;&#21306;&#38388;$(-1, 1)$&#19978;&#65292;&#20851;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#22855;&#24322;&#25668;&#21160;&#26925;&#22278;&#22411;&#20004;&#28857;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#35299;&#38598;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#34920;&#36798;&#29575;&#30028;&#38480;&#65292;&#29992;Sobolev&#33539;&#25968;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#32473;&#23450;&#30340;&#28304;&#39033;&#21644;&#21453;&#24212;&#31995;&#25968;&#22312;$[-1, 1]$&#20013;&#26159;&#35299;&#26512;&#30340;&#12290;&#38024;&#23545;&#20960;&#31181;DNN&#26550;&#26500;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#22855;&#24322;&#25668;&#21160;&#21442;&#25968;&#26080;&#20851;&#30340;Sobolev&#33539;&#25968;&#19979;&#30340;&#34920;&#36798;&#29575;&#30028;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;ReLU NNs&#65292;spiking NNs&#65292;$\tanh$&#21644;sigmoid&#28608;&#27963;&#30340;NNs&#12290;&#21518;&#20004;&#31181;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#26126;&#30830;&#22320;&#34920;&#31034;&#8220;&#25351;&#25968;&#36793;&#30028;&#23618;&#35299;&#29305;&#24615;&#8221;&#65292;&#21363;&#22312;DNN&#30340;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#20013;&#65292;&#21363;&#22312;&#19968;&#20010;&#27973;&#23618;&#23376;&#32593;&#32476;&#20013;&#65292;&#24182;&#20197;NN&#22823;&#23567;&#37327;&#21270;&#24471;&#21040;&#25913;&#36827;&#30340;&#40065;&#26834;&#34920;&#36798;&#29575;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#30340;DNN&#26550;&#26500;&#37117;&#20801;&#35768;&#22312;&#25152;&#35859;&#30340;&#8220;&#33021;&#37327;&#8221;&#21644;&#8220;&#24179;&#34913;&#8221;Sobolev&#33539;&#25968;&#20013;&#20197;&#35299;&#26512;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#24335;&#33719;&#24471;&#40065;&#26834;&#30340;&#25351;&#25968;&#35299;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.  We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.  We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35270;&#35282;&#26469;&#35299;&#20915;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#21644;&#36974;&#25377;&#31574;&#30053;&#26469;&#25552;&#39640;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06654</link><description>&lt;p&gt;
&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#19982;&#36974;&#25377;&#31574;&#30053;&#20197;&#23454;&#29616;&#19968;&#33268;&#30340;XAI&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks. (arXiv:2401.06654v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35270;&#35282;&#26469;&#35299;&#20915;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#21644;&#36974;&#25377;&#31574;&#30053;&#26469;&#25552;&#39640;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#31227;&#38500;&#26159;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65292;&#26082;&#36866;&#29992;&#20110;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#65288;Shapley&#20540;&#65289;&#65292;&#20063;&#36866;&#29992;&#20110;&#23427;&#20204;&#30340;&#35780;&#20272;&#65288;&#20687;&#32032;&#32763;&#36716;&#65292;PF&#65289;&#12290;&#28982;&#32780;&#65292;&#36974;&#25377;&#31574;&#30053;&#21487;&#20197;&#20174;&#31616;&#21333;&#30340;&#22343;&#20540;&#26367;&#25442;&#21040;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20462;&#22797;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#20102;&#36974;&#25377;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;PF&#22522;&#20934;&#20250;&#23548;&#33268;&#30683;&#30462;&#30340;&#25490;&#21517;&#12290;&#36825;&#19968;&#38382;&#39064;&#36824;&#22240;&#31454;&#20105;&#30340;PF&#24230;&#37327;&#32780;&#21464;&#24471;&#22797;&#26434;&#65306;&#29305;&#24449;&#35201;&#20040;&#20174;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#24320;&#22987;&#31227;&#38500;&#65288;MIF&#65289;&#65292;&#35201;&#20040;&#20174;&#26368;&#26080;&#24433;&#21709;&#21147;&#30340;&#24320;&#22987;&#31227;&#38500;&#65288;LIF&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#22522;&#20110;&#36974;&#25377;&#30340;XAI&#30340;&#24120;&#35265;&#25209;&#35780;&#65292;&#21363;&#20154;&#24037;&#26679;&#26412;&#23548;&#33268;&#27169;&#22411;&#35780;&#20272;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65288;&#21442;&#32771;&#27169;&#22411;&#33539;&#22260;&#20043;&#22806;&#30340;R-OMS&#24471;&#20998;&#65289;&#12290;R-OMS&#24471;&#20998;&#33021;&#22815;&#23545;&#36974;&#25377;&#31574;&#30053;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#65292;&#24182;&#35299;&#20915;&#20102;&#30683;&#30462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22806;&#25512;&#30340;&#22359;&#20027;&#23548;&#26497;&#23567;&#21270;&#26041;&#27861;&#65288;BMMe&#65289;&#26469;&#35299;&#20915;&#22810;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;$\beta$-NMF&#12290;&#36890;&#36807;&#20351;&#29992;&#29420;&#29305;&#30340;&#33258;&#36866;&#24212;&#26356;&#26032;&#35268;&#21017;&#26469;&#26356;&#26032;&#22806;&#25512;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06646</link><description>&lt;p&gt;
&#20351;&#29992;&#22806;&#25512;&#30340;&#22359;&#20027;&#23548;&#26497;&#23567;&#21270;&#26041;&#27861;&#21644;&#24212;&#29992;&#20110;$\beta$-NMF
&lt;/p&gt;
&lt;p&gt;
Block Majorization Minimization with Extrapolation and Application to $\beta$-NMF. (arXiv:2401.06646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22806;&#25512;&#30340;&#22359;&#20027;&#23548;&#26497;&#23567;&#21270;&#26041;&#27861;&#65288;BMMe&#65289;&#26469;&#35299;&#20915;&#22810;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;$\beta$-NMF&#12290;&#36890;&#36807;&#20351;&#29992;&#29420;&#29305;&#30340;&#33258;&#36866;&#24212;&#26356;&#26032;&#35268;&#21017;&#26469;&#26356;&#26032;&#22806;&#25512;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22806;&#25512;&#30340;&#22359;&#20027;&#23548;&#26497;&#23567;&#21270;&#26041;&#27861;&#65288;BMMe&#65289;&#26469;&#35299;&#20915;&#19968;&#31867;&#22810;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;BMMe&#30340;&#22806;&#25512;&#21442;&#25968;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26356;&#26032;&#35268;&#21017;&#26469;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#22359;&#20027;&#23548;&#26497;&#23567;&#21270;&#37325;&#26032;&#34920;&#36848;&#20026;&#22359;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#33258;&#36866;&#24212;&#26356;&#26032;Bregman&#25955;&#24230;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;BMMe&#30340;&#23376;&#24207;&#21015;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;$\beta$-NMF&#20013;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;$\beta\in [1,2]$&#12290;&#36825;&#20123;&#31639;&#27861;&#26159;&#20351;&#29992;&#22806;&#25512;&#30340;&#20056;&#27861;&#26356;&#26032;&#65292;&#24182;&#20174;&#25105;&#20204;&#30340;&#26032;&#32467;&#26524;&#20013;&#33719;&#24471;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23454;&#35777;&#20102;BMMe&#22312;$\beta$-NMF&#20013;&#30340;&#26174;&#33879;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Block Majorization Minimization method with Extrapolation (BMMe) for solving a class of multi-convex optimization problems. The extrapolation parameters of BMMe are updated using a novel adaptive update rule. By showing that block majorization minimization can be reformulated as a block mirror descent method, with the Bregman divergence adaptively updated at each iteration, we establish subsequential convergence for BMMe. We use this method to design efficient algorithms to tackle nonnegative matrix factorization problems with the $\beta$-divergences ($\beta$-NMF) for $\beta\in [1,2]$. These algorithms, which are multiplicative updates with extrapolation, benefit from our novel results that offer convergence guarantees. We also empirically illustrate the significant acceleration of BMMe for $\beta$-NMF through extensive experiments.
&lt;/p&gt;</description></item><item><title>SeizNet&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26893;&#20837;&#24335;&#20256;&#24863;&#22120;&#32593;&#32476;&#31995;&#32479;&#36827;&#34892;&#30315;&#30187;&#39044;&#27979;&#30340;&#38381;&#29615;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#29305;&#24322;&#24615;&#24182;&#20445;&#25345;&#24456;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06644</link><description>&lt;p&gt;
SeizNet&#65306;&#19968;&#31181;&#29992;&#20110;&#30315;&#30187;&#39044;&#27979;&#30340;AI&#36741;&#21161;&#26893;&#20837;&#24335;&#20256;&#24863;&#22120;&#32593;&#32476;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SeizNet: An AI-enabled Implantable Sensor Network System for Seizure Prediction. (arXiv:2401.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06644
&lt;/p&gt;
&lt;p&gt;
SeizNet&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26893;&#20837;&#24335;&#20256;&#24863;&#22120;&#32593;&#32476;&#31995;&#32479;&#36827;&#34892;&#30315;&#30187;&#39044;&#27979;&#30340;&#38381;&#29615;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#29305;&#24322;&#24615;&#24182;&#20445;&#25345;&#24456;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeizNet&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26893;&#20837;&#24335;&#20256;&#24863;&#22120;&#32593;&#32476;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#38381;&#29615;&#31995;&#32479;&#12290;&#23613;&#31649;&#33647;&#29289;&#27835;&#30103;&#23545;&#19968;&#20123;&#30315;&#30187;&#24739;&#32773;&#26377;&#25928;&#65288;&#20840;&#29699;&#32422;&#26377;6500&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#65289;&#65292;&#20294;&#19977;&#20998;&#20043;&#19968;&#30340;&#24739;&#32773;&#24739;&#26377;&#32784;&#33647;&#24615;&#30315;&#30187;&#12290;&#20026;&#20102;&#20943;&#36731;&#30315;&#30187;&#30340;&#24433;&#21709;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21487;&#20197;&#36890;&#30693;&#36825;&#31867;&#24739;&#32773;&#21363;&#23558;&#21457;&#20316;&#30340;&#39044;&#27979;&#31995;&#32479;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#12290;SeizNet&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#32467;&#21512;&#22810;&#20010;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#39045;&#20869;&#33041;&#30005;&#22270;&#65288;iEEG&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#29305;&#24322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#38750;&#24120;&#39640;&#30340;&#25935;&#24863;&#24615;&#27700;&#24179;&#12290;SeizNet&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#36793;&#32536;&#23454;&#26102;&#25191;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#19982;&#22522;&#20110;&#20113;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#20851;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12289;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#21644;&#21151;&#32791;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce SeizNet, a closed-loop system for predicting epileptic seizures through the use of Deep Learning (DL) method and implantable sensor networks. While pharmacological treatment is effective for some epilepsy patients (with ~65M people affected worldwide), one out of three suffer from drug-resistant epilepsy. To alleviate the impact of seizure, predictive systems have been developed that can notify such patients of an impending seizure, allowing them to take precautionary measures. SeizNet leverages DL techniques and combines data from multiple recordings, specifically intracranial electroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that can significantly improve the specificity of seizure prediction while preserving very high levels of sensitivity. SeizNet DL algorithms are designed for efficient real-time execution at the edge, minimizing data privacy concerns, data transmission overhead, and power inefficiencies associated with cloud-based soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.06634</link><description>&lt;p&gt;
CCFC&#65306;&#26725;&#25509;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CCFC: Bridging Federated Clustering and Contrastive Learning. (arXiv:2401.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#26159;&#23545;&#20110;&#32852;&#37030;&#22330;&#26223;&#20013;&#38598;&#20013;&#32858;&#31867;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#21487;&#20197;&#35753;&#22810;&#20010;&#25968;&#25454;&#25345;&#26377;&#23458;&#25143;&#31471;&#22312;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#36827;&#34892;&#25968;&#25454;&#20998;&#32452;&#12290;&#22312;&#38598;&#20013;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#39537;&#21160;&#30340;&#32858;&#31867;&#22312;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#32858;&#31867;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#32467;&#21512;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#34920;&#31034;&#23450;&#21046;&#20102;&#19968;&#20010;&#32858;&#31867;&#23545;&#27604;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#20316;&#20026;&#25552;&#20986;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#31216;&#20026;&#32858;&#31867;&#23545;&#27604;&#32852;&#37030;&#32858;&#31867;&#65288;CCFC&#65289;&#12290;&#21463;&#30410;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#30340;&#32858;&#31867;&#24615;&#33021;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;&#36825;&#31181;&#25910;&#30410;&#23548;&#33268;NMI&#24471;&#20998;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#26368;&#39640;&#36798;&#21040;0.4155&#12290;&#27492;&#22806;&#65292;CCFC&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#23613;&#31649;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.06604</link><description>&lt;p&gt;
&#35782;&#21035;&#31574;&#30053;&#26799;&#24230;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#23613;&#31649;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21033;&#29992;&#20248;&#21270;&#38382;&#39064;&#20869;&#37096;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#25928;&#29575;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#20301;&#20110;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#23376;&#31354;&#38388;&#20013;&#30340;&#20107;&#23454;&#65292;&#21487;&#20197;&#21152;&#36895;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22266;&#26377;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#36825;&#26679;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#65292;&#20363;&#22914;&#25913;&#36827;&#21442;&#25968;&#31354;&#38388;&#25506;&#32034;&#25110;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#24182;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06595</link><description>&lt;p&gt;
&#27599;&#20010;&#33410;&#28857;&#37117;&#19981;&#21516;&#65306;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering. (arXiv:2401.06595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#24182;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#32858;&#31867;&#26159;&#19968;&#39033;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#23558;&#33410;&#28857;&#20998;&#20026;&#19981;&#21516;&#30340;&#32452;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;SSL&#20219;&#21153;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#34987;&#20998;&#37197;&#32473;&#25152;&#26377;&#22270;&#33410;&#28857;&#30456;&#21516;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20123;&#22270;&#33410;&#28857;&#20854;&#37051;&#23621;&#22312;&#19981;&#21516;&#30340;&#32452;&#20013;&#65292;&#23545;SSL&#20219;&#21153;&#38656;&#35201;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#24378;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;SSL&#20219;&#21153;&#26435;&#37325;&#24182;&#34701;&#21512;&#20174;&#19981;&#21516;SSL&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#23884;&#20837;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;DyFSS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DyFSS&#20351;&#29992;&#20174;&#38376;&#25511;&#32593;&#32476;&#25512;&#23548;&#20986;&#30340;&#19981;&#21516;&#26435;&#37325;&#34701;&#21512;&#20174;&#22810;&#26679;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#38376;&#25511;&#32593;&#32476;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#33258;&#30417;&#30563;&#31574;&#30053;&#65292;&#20854;&#20013;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
Attributed graph clustering is an unsupervised task that partitions nodes into different groups. Self-supervised learning (SSL) shows great potential in handling this task, and some recent studies simultaneously learn multiple SSL tasks to further boost performance. Currently, different SSL tasks are assigned the same set of weights for all graph nodes. However, we observe that some graph nodes whose neighbors are in different groups require significantly different emphases on SSL tasks. In this paper, we propose to dynamically learn the weights of SSL tasks for different nodes and fuse the embeddings learned from different SSL tasks to boost performance. We design an innovative graph clustering approach, namely Dynamically Fusing Self-Supervised Learning (DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks using distinct weights derived from a gating network. To effectively learn the gating network, we design a dual-level self-supervised strategy that incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#65292;&#22312;&#23454;&#26102;&#25512;&#23548;&#21512;&#25104;&#38754;&#37096;&#21767;&#37096;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#20134;&#20851;&#27880;&#20102;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#36716;&#31227;&#27169;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06588</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#30340;&#21160;&#24577;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints. (arXiv:2401.06588v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#65292;&#22312;&#23454;&#26102;&#25512;&#23548;&#21512;&#25104;&#38754;&#37096;&#21767;&#37096;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#20134;&#20851;&#27880;&#20102;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#36716;&#31227;&#27169;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22312;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#25216;&#26415;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#32422;&#26463;&#26159;&#36890;&#36807;&#23558;&#35821;&#38899;&#20449;&#21495;&#36755;&#20837;&#21040;&#21475;&#33108;&#36816;&#21160;&#21512;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#26102;&#25512;&#23548;&#20986;&#21512;&#25104;&#38754;&#37096;&#30340;&#21767;&#37096;&#36816;&#21160;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#30340;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#32500;&#29305;&#27604;&#35793;&#30721;&#22120;&#24378;&#21046;&#30340;&#36716;&#31227;&#27169;&#22411;&#22312;&#19981;&#21516;&#24310;&#36831;&#26465;&#20214;&#19979;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#36890;&#36807;&#21442;&#25968;&#25511;&#21046;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#26102;&#38388;&#20381;&#36182;&#30340;&#38271;&#24230;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#30340;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.06583</link><description>&lt;p&gt;
&#23558;&#21464;&#24418;&#22120;&#25216;&#26415;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#25991;&#26723;&#24050;&#32463;&#25104;&#20026;&#22312;&#32593;&#32476;&#19978;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#33616;&#38750;&#26597;&#35810;&#35821;&#35328;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#21487;&#33021;&#20250;&#24573;&#35270;&#38750;&#27597;&#35821;&#30340;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#26469;&#34920;&#31034;&#36328;&#35821;&#35328;&#25991;&#26723;&#12290;&#35780;&#20272;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#65288;mBERT&#65292;mT5 XLM RoBERTa&#65292;ErnieM&#65289;&#22312;20&#31181;&#35821;&#35328;&#23545;&#19978;&#20351;&#29992;&#19977;&#31181;&#26144;&#23556;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#35821;&#35328;&#23545;&#20195;&#34920;&#20102;&#27431;&#30431;&#36873;&#25321;&#30340;&#20116;&#31181;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;Mate&#26816;&#32034;&#29575;&#21644;&#20114;&#24800;&#25490;&#24207;&#31561;&#25351;&#26631;&#26469;&#34913;&#37327;&#26144;&#23556;TLDRs&#19982;&#26410;&#26144;&#23556;TLDRs&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#21644;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#20026;&#25193;&#23637;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06566</link><description>&lt;p&gt;
&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#25240;&#25187;&#22238;&#25253;&#26368;&#20248;&#24615;&#20934;&#21017;&#19979;&#65292;&#38024;&#23545;&#31163;&#25955;&#26102;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#38382;&#39064;&#12290;&#20856;&#22411;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#31354;&#38388;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#24773;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2401.06559</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20934;&#26694;&#26550;&#23545;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
A General Benchmark Framework is Dynamic Graph Neural Network Need. (arXiv:2401.06559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23398;&#20064;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#28436;&#21270;&#20851;&#31995;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20013;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#23548;&#33268;&#21160;&#24577;&#22270;&#27169;&#22411;&#30340;&#35780;&#20272;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#24314;&#31435;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20419;&#36827;&#21019;&#26032;&#65292;&#24182;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#30830;&#23450;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#65292;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#24182;&#20026;&#29983;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph learning is crucial for modeling real-world systems with evolving relationships and temporal dynamics. However, the lack of a unified benchmark framework in current research has led to inaccurate evaluations of dynamic graph models. This paper highlights the significance of dynamic graph learning and its applications in various domains. It emphasizes the need for a standardized benchmark framework that captures temporal dynamics, evolving graph structures, and downstream task requirements. Establishing a unified benchmark will help researchers understand the strengths and limitations of existing models, foster innovation, and advance dynamic graph learning. In conclusion, this paper identifies the lack of a standardized benchmark framework as a current limitation in dynamic graph learning research . Such a framework will facilitate accurate model evaluation, drive advancements in dynamic graph learning techniques, and enable the development of more effective models for re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#65292;&#29992;&#20110;&#21033;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#23398;&#20064;&#31038;&#20132;&#32593;&#32476;&#20013;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.06557</link><description>&lt;p&gt;
&#23545;&#24453;&#31038;&#20132;&#32593;&#32476;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#36229;&#34746;&#26059;&#34920;&#31034;&#23398;&#20064;&#30340;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks. (arXiv:2401.06557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#65292;&#29992;&#20110;&#21033;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#23398;&#20064;&#31038;&#20132;&#32593;&#32476;&#20013;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#65288;ITE&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#22914;&#20309;&#35782;&#21035;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#22312;ITE&#20272;&#35745;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#31038;&#20132;&#32593;&#32476;&#30340;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23398;&#20064;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#31038;&#20132;&#32593;&#32476;&#32463;&#24120;&#34920;&#29616;&#20986;&#26080;&#26631;&#24230;&#32467;&#26500;&#65292;&#32780;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#22312;&#23884;&#20837;&#27492;&#31867;&#22270;&#26102;&#20250;&#20135;&#29983;&#39640;&#24230;&#22833;&#30495;&#65292;&#65288;2&#65289;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#33258;&#25105;&#20013;&#24515;&#32593;&#32476;&#37117;&#34920;&#29616;&#20986;&#19982;&#27835;&#30103;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24847;&#21619;&#30528;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#26174;&#33879;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#12290;&#39318;&#20808;&#65292;TAHyper&#20351;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#36827;&#34892;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25429;&#25417;&#31038;&#20132;&#32593;&#32476;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#27835;&#30103;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the individual treatment effect (ITE) from observational data is a crucial research topic that holds significant value across multiple domains. How to identify hidden confounders poses a key challenge in ITE estimation. Recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. However, these methods utilize graph neural networks to learn the representation of hidden confounders in Euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while Euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. To address these issues, we propose a novel method called Treatment-Aware Hyperbolic Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic space to en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;NMFS-GA&#65292;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#20108;&#20998;&#31867;&#20013;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#23376;&#38598;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NMFS-GA&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20108;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06546</link><description>&lt;p&gt;
&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#20108;&#20998;&#31867;&#29305;&#24449;&#36873;&#25321;&#65306;&#19968;&#31181;&#26032;&#26041;&#27861;&#30340;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach. (arXiv:2401.06546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;NMFS-GA&#65292;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#20108;&#20998;&#31867;&#20013;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#23376;&#38598;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NMFS-GA&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20108;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22330;&#26223;&#19979;&#65292;&#29305;&#24449;&#36873;&#25321;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#22815;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22122;&#22768;&#24863;&#30693;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;(NMFS-GA)&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#20108;&#20998;&#31867;&#20013;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#23376;&#38598;&#12290;NMFS-GA&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#26082;&#20934;&#30830;&#21448;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#25105;&#20204;&#22312;&#24102;&#26377;&#26631;&#31614;&#22122;&#22768;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12289;&#19968;&#20010;&#23500;&#21547;&#22122;&#22768;&#29305;&#24449;&#30340;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#30196;&#21574;&#36716;&#21270;&#30340;&#30495;&#23454;ADNI&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;NMFS-GA&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22330;&#26223;&#20013;&#65292;NMFS-GA&#33021;&#22815;&#26377;&#25928;&#22320;&#36873;&#25321;&#25552;&#39640;&#20108;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection in noisy label scenarios remains an understudied topic. We propose a novel genetic algorithm-based approach, the Noise-Aware Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting optimal feature subsets in binary classification with noisy labels. NMFS-GA offers a unified framework for selecting feature subsets that are both accurate and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise, a Breast Cancer dataset enriched with noisy features, and a real-world ADNI dataset for dementia conversion prediction. Our results indicate that NMFS-GA can effectively select feature subsets that improve the accuracy and interpretability of binary classifiers in scenarios with noisy labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#36827;&#34892;&#26234;&#33021;&#30340;&#36164;&#28304;&#21644;&#21151;&#33021;&#32534;&#25490;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.06538</link><description>&lt;p&gt;
&#26234;&#33021;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#20999;&#29255;&#20307;&#31995;&#32467;&#26500;&#29305;&#24449;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Intelligent Data-Driven Architectural Features Orchestration for Network Slicing. (arXiv:2401.06538v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#36827;&#34892;&#26234;&#33021;&#30340;&#36164;&#28304;&#21644;&#21151;&#33021;&#32534;&#25490;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20999;&#29255;&#26159;&#19979;&#19968;&#20195;&#31227;&#21160;&#32593;&#32476;&#65288;NGMN&#65289;&#20197;&#21450;&#20854;&#20182;&#26032;&#31995;&#32479;&#65288;&#22914;&#29289;&#32852;&#32593;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#65289;&#30340;&#20851;&#38190;&#25512;&#21160;&#22240;&#32032;&#21644;&#36235;&#21183;&#12290;&#32534;&#25490;&#21644;&#26426;&#22120;&#23398;&#20064;&#26159;&#32593;&#32476;&#20999;&#29255;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#30340;&#20803;&#32032;&#65292;&#22240;&#20026;&#20999;&#29255;&#36807;&#31243;&#38656;&#35201;&#32534;&#25490;&#36164;&#28304;&#21644;&#21151;&#33021;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#20248;&#21270;&#32534;&#25490;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#32570;&#20047;&#23450;&#20041;&#26234;&#33021;&#26041;&#27861;&#26469;&#32534;&#25490;&#20999;&#29255;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#21644;&#36164;&#28304;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#30340;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#12290;&#39318;&#20808;&#65292;&#23545;&#20999;&#29255;&#35268;&#21010;&#12289;&#37197;&#32622;&#12289;&#35843;&#35797;&#21644;&#36816;&#34892;&#38454;&#27573;&#30340;&#20999;&#29255;&#36164;&#28304;&#32534;&#25490;&#21644;&#20998;&#37197;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;&#26550;&#26500;&#29305;&#24449;&#32534;&#25490;&#30340;&#38656;&#27714;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#12289;&#32852;&#37030;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#20013;&#20351;&#29992;&#19968;&#27493;&#24494;&#35843;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26102;&#38388;&#29702;&#35299;&#19981;&#36275;&#12289;&#27867;&#21270;&#25361;&#25112;&#21644;&#25968;&#25454;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#28304;&#39046;&#22495;&#25968;&#25454;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#36951;&#24536;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06524</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#27493;&#24494;&#35843;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#30340;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Time series Transformers using One-step fine-tuning. (arXiv:2401.06524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#20013;&#20351;&#29992;&#19968;&#27493;&#24494;&#35843;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26102;&#38388;&#29702;&#35299;&#19981;&#36275;&#12289;&#27867;&#21270;&#25361;&#25112;&#21644;&#25968;&#25454;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#28304;&#39046;&#22495;&#25968;&#25454;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#36951;&#24536;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;Transformer&#22312;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#26102;&#38388;&#29702;&#35299;&#30340;&#19981;&#36275;&#12289;&#27867;&#21270;&#25361;&#25112;&#20197;&#21450;&#38754;&#23545;&#25968;&#25454;&#26377;&#38480;&#30340;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35299;&#20915;&#36951;&#24536;&#28798;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22312;&#25509;&#35302;&#21040;&#26032;&#25968;&#25454;&#26102;&#24536;&#35760;&#20102;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#20063;&#26159;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;Transformer&#40065;&#26834;&#24615;&#38656;&#35201;&#20851;&#27880;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#20805;&#36275;&#25968;&#25454;&#30340;&#28304;&#39046;&#22495;&#19978;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#27493;&#24494;&#35843;&#8221;&#26041;&#27861;&#65292;&#23558;&#19968;&#23450;&#30334;&#20998;&#27604;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#28155;&#21152;&#21040;&#30446;&#26631;&#22495;&#20013;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#22312;&#30446;&#26631;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#23398;&#20064;&#21152;&#27861;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#35780;&#20998;&#20989;&#25968;&#21644;&#36880;&#20998;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#31574;&#30053;&#26469;&#30830;&#23450;&#21464;&#37327;&#38388;&#30340;&#22240;&#26524;&#39034;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06523</link><description>&lt;p&gt;
&#25552;&#21319;&#22240;&#26524;&#21152;&#27861;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Causal Additive Models. (arXiv:2401.06523v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06523
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#23398;&#20064;&#21152;&#27861;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#35780;&#20998;&#20989;&#25968;&#21644;&#36880;&#20998;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#31574;&#30053;&#26469;&#30830;&#23450;&#21464;&#37327;&#38388;&#30340;&#22240;&#26524;&#39034;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#23398;&#20064;&#21152;&#27861;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#30830;&#23450;&#21464;&#37327;&#38388;&#22240;&#26524;&#39034;&#24207;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#20219;&#24847;&#22238;&#24402;&#25216;&#26415;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20026;&#20854;&#24314;&#31435;&#20102;&#19968;&#33268;&#22320;&#25903;&#25345;&#30495;&#23454;&#22240;&#26524;&#39034;&#24207;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25552;&#21319;&#26041;&#27861;&#19982;&#25552;&#21069;&#20572;&#27490;&#31526;&#21512;&#36825;&#20123;&#26465;&#20214;&#65292;&#20174;&#32780;&#20026;&#22240;&#26524;&#39034;&#24207;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#20026;&#20102;&#24212;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21152;&#27861;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#31354;&#38388;&#20013;&#36827;&#34892;&#36880;&#20998;&#37327;&#26799;&#24230;&#19979;&#38477;&#26469;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#30740;&#31350;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#36739;&#20302;&#32500;&#24230;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39640;&#32500;&#36866;&#24212;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#31243;&#24207;&#26131;&#20110;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#34920;&#31034;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;r-MDPs&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#19968;&#23567;&#32452;&#20195;&#34920;&#24615;&#31574;&#30053;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#21516;&#26102;&#22312;&#28385;&#36275;&#30417;&#31649;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#25972;&#20307;&#31038;&#20250;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2401.06514</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#31574;&#30053;&#39044;&#31639;
&lt;/p&gt;
&lt;p&gt;
Personalized Reinforcement Learning with a Budget of Policies. (arXiv:2401.06514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#34920;&#31034;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;r-MDPs&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#19968;&#23567;&#32452;&#20195;&#34920;&#24615;&#31574;&#30053;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#21516;&#26102;&#22312;&#28385;&#36275;&#30417;&#31649;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#25972;&#20307;&#31038;&#20250;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#22312;&#26681;&#25454;&#29992;&#25143;&#20010;&#20307;&#29305;&#24449;&#35843;&#25972;&#27169;&#22411;&#20915;&#31574;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#28041;&#21450;&#21040;&#20581;&#24247;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#25193;&#23637;&#26102;&#65292;&#21463;&#21040;&#20102;&#22797;&#26434;&#30340;&#30417;&#31649;&#23457;&#25209;&#27969;&#31243;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#34920;&#31034;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;r-MDPs&#65289;&#65292;&#26088;&#22312;&#24179;&#34913;&#20010;&#24615;&#21270;&#38656;&#27714;&#21644;&#30417;&#31649;&#32422;&#26463;&#12290;&#22312;r-MDPs&#20013;&#65292;&#36890;&#36807;&#19982;&#19968;&#23567;&#32452;&#20195;&#34920;&#24615;&#31574;&#30053;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#20026;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#32676;&#20307;&#25552;&#20379;&#26381;&#21153;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#29420;&#29305;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39640;&#25928;&#22320;&#23558;&#27599;&#20010;&#29992;&#25143;&#19982;&#36866;&#24403;&#30340;&#20195;&#34920;&#24615;&#31574;&#30053;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#21516;&#26102;&#20248;&#21270;&#36825;&#20123;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#31038;&#20250;&#31119;&#21033;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;r-MDPs&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#20102;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in machine learning (ML) tailors models' decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classi
&lt;/p&gt;</description></item><item><title>ML-On-Rails&#26159;&#19968;&#20010;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#35758;&#65292;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20445;&#25252;ML&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06513</link><description>&lt;p&gt;
ML-On-Rails: &#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study. (arXiv:2401.06513v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06513
&lt;/p&gt;
&lt;p&gt;
ML-On-Rails&#26159;&#19968;&#20010;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#35758;&#65292;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20445;&#25252;ML&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#26174;&#33879;&#25913;&#21464;&#20102;&#21508;&#20010;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#27169;&#22411;&#30340;&#21407;&#22411;&#35774;&#35745;&#21040;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#23384;&#22312;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28041;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#12289;&#20445;&#35777;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;ML&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ML-On-Rails&#30340;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;ML&#27169;&#22411;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;ML&#20219;&#21153;&#24314;&#31435;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#31471;&#28857;&#25509;&#21475;&#65292;&#20197;&#20419;&#36827;ML&#25552;&#20379;&#32773;&#21644;ML&#28040;&#36153;&#32773;&#65288;&#36719;&#20214;&#24037;&#31243;&#24072;&#65289;&#20043;&#38388;&#30340;&#28165;&#26224;&#27807;&#36890;&#12290;ML-On-Rails&#36890;&#36807;&#28155;&#21152;&#26816;&#27979;&#33021;&#21147;&#26469;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#35782;&#21035;&#19982;&#23454;&#38469;&#29983;&#20135;&#20013;&#30340;ML&#30456;&#20851;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;MoveReminder&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ML-On-Rails&#21327;&#35758;&#12290;&#36890;&#36807;&#36825;&#20010;&#35780;&#20272;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#29983;&#20135;&#20013;&#20445;&#25252;ML&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in produ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22810;&#24179;&#38754;UNet&#26041;&#27861;&#22312;&#20799;&#31461;&#33041;&#32959;&#30244;&#20013;&#33258;&#21160;&#20998;&#21106;&#19981;&#21516;&#32959;&#30244;&#20122;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;&#32959;&#30244;&#26680;&#24515;&#31867;&#26377;&#36739;&#39640;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20854;&#20182;&#31867;&#21035;&#30340;&#20998;&#21106;&#19978;&#23384;&#22312;&#21487;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06499</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#24179;&#38754;UNet&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#33041;MRI&#25968;&#25454;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner UNet. (arXiv:2401.06499v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22810;&#24179;&#38754;UNet&#26041;&#27861;&#22312;&#20799;&#31461;&#33041;&#32959;&#30244;&#20013;&#33258;&#21160;&#20998;&#21106;&#19981;&#21516;&#32959;&#30244;&#20122;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;&#32959;&#30244;&#26680;&#24515;&#31867;&#26377;&#36739;&#39640;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20854;&#20182;&#31867;&#21035;&#30340;&#20998;&#21106;&#19978;&#23384;&#22312;&#21487;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#21106;&#19981;&#21516;&#32959;&#30244;&#21306;&#22495;&#23545;&#20110;&#20934;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#20799;&#31461;&#33041;&#32959;&#30244;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#24179;&#38754;U-Net (MPUnet)&#26041;&#27861;&#22312;&#20998;&#21106;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65306;&#23567;&#20799;&#32959;&#30244;&#25361;&#25112;(PED)&#12289;&#33041;&#36716;&#31227;&#25361;&#25112;(MET)&#21644;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#25104;&#20154;&#33041;&#33014;&#36136;&#30244;(SSA)&#20013;&#19981;&#21516;&#32959;&#30244;&#20122;&#21306;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#22810;&#26679;&#30340;&#24773;&#26223;&#21644;&#35299;&#21078;&#21464;&#24322;&#65292;&#36866;&#21512;&#35780;&#20272;MPUnet&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#24179;&#38754;&#20449;&#24687;&#65292;MPUnet&#26550;&#26500;&#26088;&#22312;&#25552;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35780;&#20272;&#30340;&#25361;&#25112;&#20013;&#65292;&#32959;&#30244;&#26680;&#24515;(TC)&#31867;&#34920;&#29616;&#20986;&#30456;&#23545;&#36739;&#39640;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#31867;&#21035;&#30340;&#20998;&#21106;&#65288;&#22914;&#27700;&#32959;&#21644;&#22686;&#24378;&#32959;&#30244;(ET)&#21306;&#22495;&#65289;&#23384;&#22312;&#21487;&#21464;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#20998;&#21106;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated segmentation of distinct tumor regions is critical for accurate diagnosis and treatment planning in pediatric brain tumors. This study evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in segmenting different tumor subregions across three challenging datasets: Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse scenarios and anatomical variations, making them suitable for assessing the robustness and generalization capabilities of the MPUnet model. By utilizing multi-planar information, the MPUnet architecture aims to enhance segmentation accuracy. Our results show varying performance levels across the evaluated challenges, with the tumor core (TC) class demonstrating relatively higher segmentation accuracy. However, variability is observed in the segmentation of other classes, such as the edema and enhancing tumor (ET) regions. These findings emphasize the complexity 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#30340;&#26102;&#38388;&#21644;&#32676;&#32452;&#38388;&#21464;&#24322;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#22312;&#22823;&#35268;&#27169;&#34892;&#25919;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#29616;&#31532;&#20108;&#24180;&#26411;&#30340;&#36749;&#23398;&#39044;&#27979;&#27604;&#20837;&#23398;&#26102;&#26356;&#20934;&#30830;&#65292;&#24182;&#19988;&#22823;&#23398;&#34920;&#29616;&#21644;&#20837;&#23398;&#34892;&#20026;&#22312;&#39044;&#27979;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#65292;&#22823;&#23398;&#24179;&#22343;&#32489;&#28857;&#23545;&#20110;t&#23398;&#29983;&#30340;&#39044;&#27979;&#20215;&#20540;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06498</link><description>&lt;p&gt;
&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#21644;&#32676;&#32452;&#38388;&#30340;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporal and Between-Group Variability in College Dropout Prediction. (arXiv:2401.06498v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#30340;&#26102;&#38388;&#21644;&#32676;&#32452;&#38388;&#21464;&#24322;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#22312;&#22823;&#35268;&#27169;&#34892;&#25919;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#29616;&#31532;&#20108;&#24180;&#26411;&#30340;&#36749;&#23398;&#39044;&#27979;&#27604;&#20837;&#23398;&#26102;&#26356;&#20934;&#30830;&#65292;&#24182;&#19988;&#22823;&#23398;&#34920;&#29616;&#21644;&#20837;&#23398;&#34892;&#20026;&#22312;&#39044;&#27979;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#65292;&#22823;&#23398;&#24179;&#22343;&#32489;&#28857;&#23545;&#20110;t&#23398;&#29983;&#30340;&#39044;&#27979;&#20215;&#20540;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#34892;&#25919;&#25968;&#25454;&#32463;&#24120;&#29992;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#36749;&#23398;&#39044;&#35686;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#26415;&#35821;&#21644;&#26041;&#27861;&#23398;&#24046;&#24322;&#36739;&#22823;&#65292;&#24182;&#19988;&#19981;&#23436;&#20840;&#20102;&#35299;&#19981;&#21516;&#24314;&#27169;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#21644;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20013;&#30340;&#36129;&#29486;&#22240;&#32032;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#32654;&#22269;&#19968;&#25152;&#22823;&#22411;&#20844;&#31435;&#22823;&#23398;&#30340;&#21313;&#20108;&#24180;&#34892;&#25919;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#65292;&#31532;&#20108;&#24180;&#26411;&#30340;&#36749;&#23398;&#39044;&#27979;&#30340;AUC&#20540;&#27604;&#20837;&#23398;&#26102;&#39640;&#20986;20%&#12290;&#27492;&#22806;&#65292;&#20837;&#23398;&#26102;&#30340;&#22823;&#22810;&#25968;&#39044;&#27979;&#22240;&#32032;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#39640;&#20013;&#34920;&#29616;&#65292;&#22312;&#39044;&#27979;&#37325;&#35201;&#24615;&#19978;&#24456;&#24555;&#34987;&#22823;&#23398;&#34920;&#29616;&#25152;&#21462;&#20195;&#65292;&#24182;&#22312;&#21518;&#26399;&#34987;&#20837;&#23398;&#34892;&#20026;&#25152;&#26367;&#20195;&#12290;&#20851;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#24046;&#24322;&#65292;&#22823;&#23398;&#24179;&#22343;&#32489;&#28857;&#23545;&#20110;&#26469;&#33258;t&#23398;&#29983;&#30340;&#39044;&#27979;&#20215;&#20540;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20013;&#30340;&#20844;&#27491;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06495</link><description>&lt;p&gt;
BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20013;&#30340;&#20844;&#27491;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36890;&#36807;&#25512;&#21160;&#26368;&#20808;&#36827;&#25216;&#26415;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36793;&#30028;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#20276;&#38543;&#30528;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#21487;&#39640;&#36798;&#25968;&#21313;&#20159;&#21442;&#25968;&#65289;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#25110;&#30701;&#25512;&#29702;&#26102;&#38388;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#20986;&#29616;&#20102;&#21387;&#32553;&#27169;&#22411;&#65288;&#22914;DistilBERT&#65289;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36234;&#26469;&#36234;&#22810;&#24433;&#21709;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#24212;&#29992;&#20013;&#21487;&#20197;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;PLM&#21644;&#20854;&#31934;&#31616;&#29256;&#26412;&#30340;&#39044;&#27979;&#20844;&#27491;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#20004;&#20010;&#38382;&#39064;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65306;&#65288;1&#65289;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;BERT&#65288;&#20197;&#21450;DistilBERT&#65289;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#31070;&#32463;&#26426;&#21046;&#65311;&#65288;2&#65289;&#33976;&#39311;&#26159;&#21542;&#20542;&#21521;&#20110;&#21152;&#37325;&#25110;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06469</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;ICL: &#26377;&#25928;&#65292;&#39640;&#25928;&#19988;&#26080;&#24207;&#22320;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#25935;&#24863;&#30340;&#21407;&#22240;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;Batch-ICL&#65292;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340;N-shot&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;Batch-ICL&#20351;&#29992;N&#20010;&#21333;&#29420;&#30340;1-shot&#21069;&#21521;&#35745;&#31639;&#65292;&#24182;&#32858;&#21512;&#24471;&#21040;&#30340;&#20803;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#32858;&#21512;&#30340;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#31181;&#25209;&#22788;&#29702;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#30340;&#39034;&#24207;&#26080;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Batch-ICL&#19968;&#33268;&#20248;&#20110;&#22823;&#22810;&#25968;&#31034;&#20363;&#24207;&#21015;&#30340;&#25490;&#21015;&#26041;&#24335;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;Batch-ICL&#30340;&#19968;&#31181;&#26032;&#39062;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;"epochs"&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#20462;&#22797;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT &#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#35777;&#35299;&#37322;&#30340;&#26041;&#27861;&#35770;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06465</link><description>&lt;p&gt;
Sanity Checks Revisited: &#20462;&#22797;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test. (arXiv:2401.06465v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#20462;&#22797;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT &#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#35777;&#35299;&#37322;&#30340;&#26041;&#27861;&#35770;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021; (XAI) &#31038;&#21306;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797; (MPRT) &#20973;&#20511;&#20854;&#26377;&#21147;&#30340;&#35780;&#20272;&#21407;&#21017;&#32780;&#24191;&#21463;&#35748;&#21487;&#65306;&#35299;&#37322;&#20989;&#25968;&#24212;&#23545;&#27169;&#22411;&#20989;&#25968;&#21442;&#25968;&#30340;&#21464;&#21270;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#20851;&#20110; MPRT &#30340;&#20960;&#20010;&#26041;&#27861;&#35770;&#19978;&#30340;&#27880;&#24847;&#20107;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27880;&#24847;&#20107;&#39033;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23545;&#21407;&#22987; MPRT &#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT&#65292;&#21069;&#32773;&#36890;&#36807;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22122;&#38899;&#23545;&#35780;&#20272;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21518;&#32773;&#36890;&#36807;&#22312;&#23436;&#20840;&#21442;&#25968;&#38543;&#26426;&#21270;&#21518;&#35299;&#37322;&#30340;&#22797;&#26434;&#24230;&#19978;&#21319;&#26469;&#32469;&#36807;&#23545;&#20559;&#20506;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#21464;&#20307;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992; XAI &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the eXplainable Artificial Intelligence (XAI) community for its well-motivated evaluative principle: that the explanation function should be sensitive to changes in the parameters of the model function. However, recent works have identified several methodological caveats for the empirical interpretation of MPRT. To address these caveats, we introduce two adaptations to the original MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact that noise has on the evaluation results through sampling and the latter circumvents the need for biased similarity measurements by re-interpreting the test through the explanation's rise in complexity, after full parameter randomisation. Our experimental results demonstrate that these proposed variants lead to improved metric reliability, thus enabling a more trustworthy application of XAI methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29992;&#20110;PU&#23398;&#20064;&#30340;Auto-ML&#31995;&#32479;&#65292;&#24182;&#23545;&#36825;&#19977;&#20010;Auto-ML&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20026;PU&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20248;&#26041;&#27861;&#36873;&#25321;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.06452</link><description>&lt;p&gt;
&#29992;&#20110;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning for Positive-Unlabelled Learning. (arXiv:2401.06452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29992;&#20110;PU&#23398;&#20064;&#30340;Auto-ML&#31995;&#32479;&#65292;&#24182;&#23545;&#36825;&#19977;&#20010;Auto-ML&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20026;PU&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20248;&#26041;&#27861;&#36873;&#25321;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;-&#26080;&#26631;&#31614;&#65288;PU&#65289;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#30001;&#26631;&#35760;&#30340;&#27491;&#23454;&#20363;&#21644;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#32452;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#36825;&#20123;&#23454;&#20363;&#22312;&#29616;&#23454;&#20013;&#21487;&#33021;&#26159;&#27491;&#25110;&#36127;&#65292;&#20294;&#20854;&#26631;&#31614;&#26410;&#30693;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;PU&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#20026;&#32473;&#23450;&#30340;PU&#23398;&#20064;&#20219;&#21153;&#36873;&#25321;&#26368;&#20248;&#26041;&#27861;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;GA-Auto-PU&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;PU&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;Auto-ML&#65289;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29992;&#20110;PU&#23398;&#20064;&#30340;Auto-ML&#31995;&#32479;&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#30340;BO-Auto-PU&#65292;&#20197;&#21450;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;/&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#30340;EBO-Auto-PU&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#19977;&#20010;Auto-ML&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#23558;&#23427;&#20204;&#19982;&#24444;&#27492;&#20197;&#21450;&#24050;&#24314;&#31435;&#30340;PU&#23398;&#20064;&#26041;&#27861;&#22312;60&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#65288;&#27599;&#20010;&#26377;3&#20010;PU&#23398;&#20064;&#26041;&#38754;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#20849;20&#20010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabelled (PU) learning is a growing field of machine learning that aims to learn classifiers from data consisting of labelled positive and unlabelled instances, which can be in reality positive or negative, but whose label is unknown. An extensive number of methods have been proposed to address PU learning over the last two decades, so many so that selecting an optimal method for a given PU learning task presents a challenge. Our previous work has addressed this by proposing GA-Auto-PU, the first Automated Machine Learning (Auto-ML) system for PU learning. In this work, we propose two new Auto-ML systems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach, and EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach. We also present an extensive evaluation of the three Auto-ML systems, comparing them to each other and to well-established PU learning methods across 60 datasets (20 real-world datasets, each with 3 versions in terms of PU lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;GCN&#27169;&#22411;&#20013;&#24341;&#20837;Transformer&#23618;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06436</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;&#29289;&#21697;&#25512;&#33616;&#20013;&#65292;&#29992;Transformer&#23618;&#25913;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation. (arXiv:2401.06436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;GCN&#27169;&#22411;&#20013;&#24341;&#20837;Transformer&#23618;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;GCN&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#20960;&#23618;Transformer&#26550;&#26500;&#12290;&#35770;&#25991;&#30340;&#20027;&#35201;&#28966;&#28857;&#26159;&#32593;&#32476;&#20013;&#33410;&#28857;&#23884;&#20837;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#20351;&#29992;&#26469;&#33258;&#22522;&#20110;&#22270;&#30340;&#21367;&#31215;&#23618;&#30340;&#23884;&#20837;&#23618;&#65292;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#37325;&#26032;&#25490;&#21015;&#29305;&#24449;&#31354;&#38388;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#33719;&#21462;&#26356;&#39640;&#25928;&#30340;&#23884;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#20256;&#32479;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;GCN&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#35774;&#22791;&#30340;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.06432</link><description>&lt;p&gt;
&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#29992;&#20110;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#35774;&#22791;&#30340;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#25110;&#20219;&#21153;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36827;&#19968;&#27493;&#21033;&#29992;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#23454;&#29616;&#20102;&#31169;&#26377;&#21270;&#30340;FM&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;FMs&#30340;&#22823;&#23610;&#23544;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21442;&#25968;&#23610;&#23544;&#36739;&#23567;&#30340;FM&#65292;&#31216;&#20026;&#35774;&#22791;&#26412;&#22320;FM&#65288;ODFMs&#65289;&#12290;&#34429;&#28982;ODFMs&#20801;&#35768;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#65292;&#20294;&#35745;&#31639;&#38480;&#21046;&#20173;&#28982;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#32852;&#37030;&#24494;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;ODFM&#32852;&#37030;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21516;&#36136;LoRA&#31209;&#38754;&#20020;&#30528;&#36807;&#25311;&#21512;&#21644;&#25910;&#25947;&#32531;&#24930;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;HetLoRA&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20351;&#29992;&#24322;&#36136;&#30340;&#31209;&#24182;&#28040;&#38500;&#20102;&#21516;&#36136;HetLoRA&#30340;&#32570;&#28857;&#12290;&#36890;&#36807;&#22312;&#26412;&#22320;&#24212;&#29992;&#31209;&#33258;&#21098;&#26525;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#24212;&#29992;&#31232;&#30095;&#21152;&#26435;&#32858;&#21512;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#25688;&#35201;&#20013;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06421</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22320;&#29699;&#35266;&#27979;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. (arXiv:2401.06421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#65292;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#21518;&#26524;&#12290;&#31526;&#21512;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#26080;&#35770;&#20854;&#20998;&#24067;&#22914;&#20309;&#12290;&#19982;&#20854;&#20182;&#20687;&#32032;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unreliable predictions can occur when using artificial intelligence (AI) systems with negative consequences for downstream applications, particularly when employed for decision-making. Conformal prediction provides a model-agnostic framework for uncertainty quantification that can be applied to any dataset, irrespective of its distribution, post hoc. In contrast to other pixel-level uncertainty quantification methods, conformal prediction operates without requiring access to the underlying model and training dataset, concurrently offering statistically valid and informative prediction regions, all while maintaining computational efficiency. In response to the increased need to report uncertainty alongside point predictions, we bring attention to the promise of conformal prediction within the domain of Earth Observation (EO) applications. To accomplish this, we assess the current state of uncertainty quantification in the EO domain and found that only 20% of the reviewed Google Earth En
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.06406</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review. (arXiv:2401.06406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06406
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#20173;&#28982;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#27835;&#30103;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#20351;&#24471;&#23545;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#20016;&#23500;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12289;&#39640;&#32500;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12289;&#24739;&#32773;&#20869;&#37096;&#21644;&#32959;&#30244;&#20869;&#37096;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#20197;&#21450;&#19982;&#29616;&#26377;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#35299;&#37322;&#21644;&#19968;&#33268;&#24615;&#12290;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#65292;&#36825;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#37319;&#29992;&#20102;&#34701;&#21512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#30340;&#30693;&#35782;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#31361;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four prima
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#27169;&#22411;&#36716;&#25442;&#20026;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#26684;&#26597;&#25214;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#21462;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;DART&#39044;&#21462;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#24773;&#20917;&#19979;&#21482;&#26377;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.06362</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#12289;&#33976;&#39311;&#21644;&#34920;&#26684;&#21270;&#65306;&#36208;&#21521;&#23454;&#29992;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06362
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#27169;&#22411;&#36716;&#25442;&#20026;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#26684;&#26597;&#25214;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#21462;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;DART&#39044;&#21462;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#24773;&#20917;&#19979;&#21482;&#26377;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#20934;&#30830;&#30340;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#36825;&#26159;&#25968;&#25454;&#39044;&#21462;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#24320;&#38144;&#36896;&#25104;&#20102;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#23454;&#38469;&#39044;&#21462;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#21448;&#19981;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#34920;&#26684;&#21270;&#26041;&#27861;&#23558;&#19968;&#20010;&#32463;&#36807;&#33976;&#39311;&#30340;&#20855;&#26377;&#39640;&#31934;&#30830;&#24230;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#20854;&#26114;&#36149;&#30340;&#30697;&#38453;&#20056;&#27861;&#36716;&#25442;&#25104;&#24555;&#36895;&#34920;&#26684;&#26597;&#25214;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#20316;&#20026;&#19978;&#36848;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;DART&#65292;&#19968;&#20010;&#30001;&#31616;&#21333;&#34920;&#26684;&#23618;&#27425;&#32467;&#26500;&#32452;&#25104;&#30340;&#39044;&#21462;&#27169;&#22411;&#12290;&#22312;F1&#24471;&#20998;&#19979;&#38477;&#20102;0.09&#30340;&#24773;&#20917;&#19979;&#65292;DART&#20174;&#22823;&#22411;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#20943;&#23569;&#20102;99.99%&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#20174;&#33976;&#39311;&#27169;&#22411;&#20013;&#20943;&#23569;&#20102;91.83%&#30340;&#36816;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36873;&#39033;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#20102;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.06356</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36873;&#39033;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#20102;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#20851;&#20110;&#21442;&#25968;&#37197;&#32622;&#36873;&#25321;&#23545;&#30693;&#35782;&#33976;&#39311;&#24615;&#33021;&#24433;&#21709;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#12290;&#20854;&#20013;&#19968;&#20010;&#31034;&#20363;&#26159;&#25945;&#24072;&#21644;&#23398;&#29983;&#39044;&#27979;&#20043;&#38388;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#22312;&#27492;&#26041;&#38754;&#24120;&#35265;&#30340;&#36873;&#25321;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;KL&#25955;&#24230;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#25955;&#20081;&#30340;&#21162;&#21147;&#26469;&#29702;&#35299;&#36825;&#20123;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#30693;&#35782;&#33976;&#39311;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#23545;&#23427;&#20204;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#37319;&#29992;&#23454;&#35777;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#35797;&#22270;&#25214;&#20986;&#36825;&#20123;&#36873;&#25321;&#22312;&#21253;&#25324;4&#20010;NLP&#20219;&#21153;&#21644;3&#31181;&#23398;&#29983;&#35268;&#27169;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#25105;&#20204;&#34913;&#37327;&#20102;&#20570;&#20986;&#27425;&#20248;&#36873;&#25321;&#30340;&#20195;&#20215;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;RS-DMC&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#20998;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21407;&#22987;DMC&#31639;&#27861;&#30340;&#26799;&#24230;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23558;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#21010;&#20998;&#20026;&#22810;&#20010;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#36882;&#24402;&#35780;&#20998;&#20272;&#35745;&#23454;&#29616;&#26356;&#24555;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.06325</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#37319;&#26679;&#32780;&#26080;&#38656;&#31561;&#28201;&#24615;  (arXiv:2401.06325v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo. (arXiv:2401.06325v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;RS-DMC&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#20998;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21407;&#22987;DMC&#31639;&#27861;&#30340;&#26799;&#24230;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23558;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#21010;&#20998;&#20026;&#22810;&#20010;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#36882;&#24402;&#35780;&#20998;&#20272;&#35745;&#23454;&#29616;&#26356;&#24555;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20174;&#19968;&#33324;&#30340;&#30446;&#26631;&#20998;&#24067;$p_*\propto e^{-f_*}$&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36229;&#36234;&#31561;&#28201;&#26465;&#20214;&#65292;Huang&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#20102;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#33945;&#29305;&#21345;&#32599;&#65288;DMC&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DMC&#36981;&#24490;&#25193;&#25955;&#36807;&#31243;&#30340;&#21453;&#21521;SDE&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#38750;&#21442;&#25968;&#35780;&#20998;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;DMC&#31639;&#27861;&#36935;&#21040;&#20102;&#39640;&#26799;&#24230;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#33719;&#24471;&#30340;&#26679;&#26412;&#30340;&#35823;&#24046;&#23481;&#24046;$\epsilon$&#30340;&#20381;&#36182;&#25104;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DMC&#30340;&#39640;&#22797;&#26434;&#24615;&#28304;&#20110;&#20854;&#20887;&#20313;&#30340;&#35780;&#20998;&#20272;&#35745;&#35774;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;RS-DMC&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#35780;&#20998;&#20272;&#35745;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#21010;&#20998;&#20026;&#22810;&#20010;&#27573;&#33853;&#65292;&#28982;&#21518;&#23558;&#35780;&#20998;&#20272;&#35745;&#27493;&#39588;&#65288;&#22312;&#20219;&#20309;&#26102;&#38388;&#27493;&#39588;&#65289;&#24418;&#24335;&#21270;&#20026;&#19968;&#31995;&#21015;&#30456;&#20114;&#36830;&#25509;&#30340;&#22343;&#20540;&#20272;&#35745;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
To sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of DMC originates from its redundant design of score estimation, and proposed a more efficient algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#21160;&#24577;&#20154;&#21475;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.06318</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#31995;&#32479;&#20844;&#24179;&#24615;&#20013;&#21462;&#24471;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning. (arXiv:2401.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#21160;&#24577;&#20154;&#21475;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20915;&#31574;&#27169;&#22411;&#22312;&#38745;&#24577;&#20154;&#32676;&#19978;&#36816;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#20154;&#21475;&#26159;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#12290;&#27599;&#20010;&#20915;&#31574;&#21487;&#33021;&#20250;&#25913;&#21464;&#29305;&#24449;&#25110;&#29992;&#25143;&#34892;&#20026;&#30340;&#22522;&#30784;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#22312;&#25215;&#35748;&#20256;&#32479;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#38271;&#26399;&#20844;&#24179;&#24615;&#26159;&#19981;&#21516;&#35201;&#27714;&#19988;&#21487;&#33021;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20351;&#29992;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#12290;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant advancements have been made in the field of fair machine learning, the majority of studies focus on scenarios where the decision model operates on a static population. In this paper, we study fairness in dynamic systems where sequential decisions are made. Each decision may shift the underlying distribution of features or user behavior. We model the dynamic system through a Markov Decision Process (MDP). By acknowledging that traditional fairness notions and long-term fairness are distinct requirements that may not necessarily align with one another, we propose an algorithmic framework to integrate various fairness considerations with reinforcement learning using both pre-processing and in-processing approaches. Three case studies show that our method can strike a balance between traditional fairness notions, long-term fairness, and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06308</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;&#12289;&#21160;&#24577;6G&#24212;&#29992;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#33539;&#24335;&#30340;&#20986;&#29616;&#20026;&#21019;&#26032;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;6G&#30340;&#24212;&#29992;&#29615;&#22659;&#20013;&#12290;&#23613;&#31649;&#22312;&#35821;&#20041;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23558;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#23545;&#26410;&#26469;&#31995;&#32479;&#38656;&#27714;&#21644;&#29305;&#24615;&#30340;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32447;&#39057;&#35889;&#22810;&#22336;&#35775;&#38382;&#38382;&#39064;&#30340;&#24314;&#27169;&#12290;&#23427;&#26088;&#22312;&#20248;&#21270;&#21033;&#29992;&#29575;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#20351;&#29992;&#945;-&#20844;&#24179;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#21534;&#21520;&#37327;&#21644;&#21327;&#21161;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#26469;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#12290;&#39318;&#20808;&#65292;&#20998;&#26512;&#20102;&#35813;&#38382;&#39064;&#65292;&#25214;&#20986;&#20102;&#26368;&#20248;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#20027;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#21452;&#37325;&#21644;&#20915;&#26007;&#28145;&#24230;Q&#23398;&#20064; (SAMA-D3QL) &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#22312;&#35299;&#30721;&#37327;&#23376;&#20449;&#24687;&#26102;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QNN&#35299;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#20960;&#20046;&#20855;&#26377;&#20108;&#27425;&#25913;&#36827;&#12290;&#36825;&#20351;&#24471;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26102;&#21487;&#20197;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06300</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#37327;&#23376;&#20449;&#24687;&#35793;&#30721;&#22120;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Advantage of Quantum Neural Networks as Quantum Information Decoders. (arXiv:2401.06300v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#22312;&#35299;&#30721;&#37327;&#23376;&#20449;&#24687;&#26102;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QNN&#35299;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#20960;&#20046;&#20855;&#26377;&#20108;&#27425;&#25913;&#36827;&#12290;&#36825;&#20351;&#24471;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26102;&#21487;&#20197;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20445;&#25252;&#37327;&#23376;&#20449;&#24687;&#20813;&#21463;&#22122;&#22768;&#24341;&#36215;&#30340;&#38169;&#35823;&#30340;&#26377;&#24076;&#26395;&#31574;&#30053;&#26159;&#23558;&#20854;&#32534;&#30721;&#21040;&#25299;&#25169;&#37327;&#23376;&#23384;&#20648;&#35774;&#22791;&#30340;&#20302;&#33021;&#24577;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#36825;&#31181;&#23384;&#20648;&#22120;&#30340;&#35835;&#21462;&#38169;&#35823;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#27867;&#20989;&#25200;&#21160;&#65288;&#22914;&#30636;&#24577;&#22833;&#35843;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#30721;&#32534;&#30721;&#22312;&#25299;&#25169;&#31283;&#23450;&#22120;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#20013;&#30340;&#37327;&#23376;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;&#31283;&#23450;&#22120;&#22411;&#38169;&#35823;&#26657;&#27491;&#21644;&#35299;&#30721;&#26041;&#26696;&#22312;&#36825;&#31181;&#25200;&#21160;&#30340;&#37327;&#23376;&#30721;&#20013;&#24037;&#20316;&#24471;&#30456;&#24403;&#22909;&#65292;&#36890;&#36807;&#23637;&#31034;&#35299;&#30721;&#38169;&#35823;&#22312;&#24213;&#23618;&#26080;&#25200;&#21160;&#30721;&#30340;&#36317;&#31163;&#19978;&#21576;&#25351;&#25968;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#35793;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#25552;&#20379;&#20102;&#20960;&#20046;&#20108;&#27425;&#30340;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26041;&#38754;&#20351;&#29992;QNN&#30340;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#24471;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising strategy to protect quantum information from noise-induced errors is to encode it into the low-energy states of a topological quantum memory device. However, readout errors from such memory under realistic settings is less understood. We study the problem of decoding quantum information encoded in the groundspaces of topological stabilizer Hamiltonians in the presence of generic perturbations, such as quenched disorder. We first prove that the standard stabilizer-based error correction and decoding schemes work adequately well in such perturbed quantum codes by showing that the decoding error diminishes exponentially in the distance of the underlying unperturbed code. We then prove that Quantum Neural Network (QNN) decoders provide an almost quadratic improvement on the readout error. Thus, we demonstrate provable advantage of using QNNs for decoding realistic quantum error-correcting codes, and our result enables the exploration of a wider range of non-stabilizer codes in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2401.06281</link><description>&lt;p&gt;
&#25581;&#31192;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Demystifying Variational Diffusion Models. (arXiv:2401.06281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#23545;&#35813;&#27169;&#22411;&#31867;&#30340;&#28145;&#20837;&#29702;&#35299;&#20173;&#28982;&#26377;&#20123;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#35748;&#20026;&#26356;&#31616;&#21333;&#26131;&#25026;&#30340;&#25193;&#25955;&#27169;&#22411;&#20171;&#32461;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#35835;&#32773;&#26469;&#35828;&#38656;&#35201;&#30340;&#20808;&#20915;&#26465;&#20214;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#38416;&#36848;&#26500;&#25104;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20174;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#31561;&#22522;&#26412;&#27010;&#24565;&#21040;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#25105;&#20204;&#23613;&#21487;&#33021;&#22320;&#25552;&#20379;&#20102;&#22312;&#21021;&#22987;&#24037;&#20316;&#20013;&#34987;&#30465;&#30053;&#30340;&#39069;&#22806;&#25968;&#23398;&#27934;&#23519;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#65292;&#21516;&#26102;&#36991;&#20813;&#24341;&#20837;&#26032;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#33021;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#30340;&#25945;&#32946;&#34917;&#20805;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22270;&#35889;&#26063;&#19978;&#30340;&#37319;&#26679;&#38598;&#29305;&#24615;&#65292;&#24182;&#23558;&#8220;&#21487;&#31227;&#38500;&#38598;&#21512;&#8221;&#21644;&#8220;&#21807;&#19968;&#24615;&#38598;&#21512;&#8221;&#30340;&#27010;&#24565;&#25512;&#24191;&#21040;&#20102;&#22270;&#35889;&#20449;&#21495;&#22788;&#29702;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#35889;&#34920;&#31034;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#21516;&#33410;&#28857;&#25968;&#21644;&#36793;&#25968;&#20197;&#21450;&#19981;&#21516;&#33410;&#28857;&#26631;&#35760;&#30340;&#22270;&#35889;&#20043;&#38388;&#30340;&#37319;&#26679;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#20855;&#26377;&#30456;&#21516;&#22270;&#35889;&#34920;&#31034;&#30340;&#37319;&#26679;&#38598;&#24207;&#21015;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06279</link><description>&lt;p&gt;
&#22270;&#35889;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#37319;&#26679;&#21644;&#21807;&#19968;&#24615;&#38598;
&lt;/p&gt;
&lt;p&gt;
Sampling and Uniqueness Sets in Graphon Signal Processing. (arXiv:2401.06279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22270;&#35889;&#26063;&#19978;&#30340;&#37319;&#26679;&#38598;&#29305;&#24615;&#65292;&#24182;&#23558;&#8220;&#21487;&#31227;&#38500;&#38598;&#21512;&#8221;&#21644;&#8220;&#21807;&#19968;&#24615;&#38598;&#21512;&#8221;&#30340;&#27010;&#24565;&#25512;&#24191;&#21040;&#20102;&#22270;&#35889;&#20449;&#21495;&#22788;&#29702;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#35889;&#34920;&#31034;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#21516;&#33410;&#28857;&#25968;&#21644;&#36793;&#25968;&#20197;&#21450;&#19981;&#21516;&#33410;&#28857;&#26631;&#35760;&#30340;&#22270;&#35889;&#20043;&#38388;&#30340;&#37319;&#26679;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#20855;&#26377;&#30456;&#21516;&#22270;&#35889;&#34920;&#31034;&#30340;&#37319;&#26679;&#38598;&#24207;&#21015;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22270;&#35889;&#21644;&#22270;&#26497;&#38480;&#30340;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22270;&#35889;&#26063;&#19978;&#37319;&#26679;&#38598;&#30340;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#8220;&#21487;&#31227;&#38500;&#38598;&#21512;&#8221;&#21644;&#8220;&#21807;&#19968;&#24615;&#38598;&#21512;&#8221;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#22270;&#35889;&#20449;&#21495;&#39046;&#22495;&#65292;&#36825;&#20123;&#27010;&#24565;&#26368;&#21021;&#26159;&#29992;&#20110;&#20998;&#26512;&#22270;&#35889;&#19978;&#30340;&#20449;&#21495;&#30340;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;$\Lambda-$&#21487;&#31227;&#38500;&#38598;&#21512;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#22312;&#24471;&#21040;&#20174;&#22270;&#35889;&#20013;&#19968;&#20010;&#32473;&#23450;$\Lambda-$&#21487;&#31227;&#38500;&#38598;&#21512;&#30340;&#34917;&#38598;&#20013;&#30340;&#26679;&#26412;&#26102;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#39057;&#24102;&#26377;&#38480;&#30340;&#22270;&#35889;&#20449;&#21495;&#21487;&#20197;&#20197;&#21807;&#19968;&#26041;&#24335;&#34920;&#31034;&#30340;&#26465;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#35889;&#34920;&#31034;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#26694;&#26550;&#26469;&#27604;&#36739;&#20855;&#26377;&#19981;&#21516;&#33410;&#28857;&#25968;&#21644;&#36793;&#25968;&#20197;&#21450;&#19981;&#21516;&#33410;&#28857;&#26631;&#35760;&#30340;&#22270;&#35889;&#20043;&#38388;&#30340;&#37319;&#26679;&#38598;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25910;&#25947;&#21040;&#19968;&#20010;&#22270;&#35889;&#30340;&#22270;&#24207;&#21015;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#30456;&#21516;$[0,1]$&#20013;&#22270;&#35889;&#34920;&#31034;&#30340;&#37319;&#26679;&#38598;&#24207;&#21015;&#20063;&#26159;&#25910;&#25947;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the converge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#19988;&#20351;&#29992;ImageNet-1k&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#26356;&#21512;&#36866;&#12290;</title><link>http://arxiv.org/abs/2401.06278</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#19988;&#20351;&#29992;ImageNet-1k&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#26356;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32963;&#32928;&#20869;&#38236;&#65288;GIE&#65289;&#20013;&#30340;&#35270;&#35273;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22312;ImageNet-1k&#19978;&#20197;&#26377;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#31639;&#27861;&#21644;&#19968;&#20010;&#26368;&#36817;&#30340;&#21253;&#21547;10&#19975;&#24352;&#26410;&#26631;&#35760;GIE&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65288;Hyperkvasir-unlabelled&#65289;&#30340;&#20351;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#31995;&#21015;GIE&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;ResNet50&#21644;ViT-B&#39592;&#24178;&#32593;&#32476;&#20197;&#33258;&#25105;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;&#38500;&#20102;&#30830;&#23450;&#27599;&#20010;&#20219;&#21153;&#26368;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#21644;&#39592;&#24178;&#32593;&#32476;&#26550;&#26500;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;GIE&#35270;&#35273;&#20219;&#21153;&#30340;&#39592;&#24178;&#32593;&#32476;&#65307;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#20351;&#29992;ImageNet-1k&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#39044;&#35757;&#32451;&#26356;&#21512;&#36866;&#65292;&#20294;&#26377;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#22806;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exce
&lt;/p&gt;</description></item><item><title>Qrlew&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#33021;&#22815;&#23558;SQL&#26597;&#35810;&#37325;&#20889;&#20026;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31561;&#25928;&#24615;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#25968;&#25454;&#25152;&#26377;&#32773;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#26631;&#20934;&#25968;&#25454;&#26597;&#35810;&#21644;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.06273</link><description>&lt;p&gt;
Qrlew: &#23558;SQL&#37325;&#20889;&#20026;&#24046;&#20998;&#38544;&#31169;SQL
&lt;/p&gt;
&lt;p&gt;
Qrlew: Rewriting SQL into Differentially Private SQL. (arXiv:2401.06273v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06273
&lt;/p&gt;
&lt;p&gt;
Qrlew&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#33021;&#22815;&#23558;SQL&#26597;&#35810;&#37325;&#20889;&#20026;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31561;&#25928;&#24615;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#25968;&#25454;&#25152;&#26377;&#32773;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#26631;&#20934;&#25968;&#25454;&#26597;&#35810;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Qrlew&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#23427;&#21487;&#20197;&#23558;SQL&#26597;&#35810;&#35299;&#26512;&#20026;Relations&#65292;&#21363;&#20013;&#38388;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#36319;&#36394;&#20016;&#23500;&#30340;&#25968;&#25454;&#31867;&#22411;&#12289;&#20540;&#33539;&#22260;&#21644;&#34892;&#25152;&#26377;&#26435;&#65292;&#20197;&#20415;&#21487;&#20197;&#36731;&#26494;&#22320;&#37325;&#20889;&#20026;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31561;&#25928;&#24615;&#65292;&#24182;&#36716;&#25442;&#22238;SQL&#26597;&#35810;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#23384;&#20648;&#20013;&#25191;&#34892;&#12290;&#20351;&#29992;Qrlew&#65292;&#25968;&#25454;&#20174;&#19994;&#32773;&#21487;&#20197;&#29992;&#26631;&#20934;SQL&#34920;&#36798;&#20854;&#25968;&#25454;&#26597;&#35810;&#65307;&#25968;&#25454;&#25152;&#26377;&#32773;&#21487;&#20197;&#22312;&#36755;&#20986;&#19978;&#20139;&#26377;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#36816;&#34892;&#37325;&#20889;&#30340;&#26597;&#35810;&#65292;&#26080;&#38656;&#20219;&#20309;&#25216;&#26415;&#38598;&#25104;&#65307;&#24182;&#19988;&#26597;&#35810;&#37325;&#20889;&#21487;&#20197;&#30001;&#19968;&#20010;&#30001;&#25152;&#26377;&#32773;&#20449;&#20219;&#20294;&#21487;&#33021;&#23646;&#20110;&#19981;&#21516;&#32452;&#32455;&#30340;&#38544;&#31169;&#19987;&#23478;&#26469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Qrlew, an open source library that can parse SQL queries into Relations -- an intermediate representation -- that keeps track of rich data types, value ranges, and row ownership; so that they can easily be rewritten into differentially-private equivalent and turned back into SQL queries for execution in a variety of standard data stores.  With Qrlew, a data practitioner can express their data queries in standard SQL; the data owner can run the rewritten query without any technical integration and with strong privacy guarantees on the output; and the query rewriting can be operated by a privacy-expert who must be trusted by the owner, but may belong to a separate organization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"FedTabDiff"&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#35775;&#38382;&#21407;&#22987;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;"&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;"&#65288;DDPMs&#65289;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#28151;&#21512;&#23646;&#24615;&#31867;&#22411;&#21644;&#38544;&#21547;&#20851;&#31995;&#31561;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#25955;&#23398;&#20064;&#26041;&#26696;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26412;&#22320;&#24615;&#12290;&#22312;&#30495;&#23454;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.06263</link><description>&lt;p&gt;
FedTabDiff: &#29992;&#20110;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedTabDiff: Federated Learning of Diffusion Probabilistic Models for Synthetic Mixed-Type Tabular Data Generation. (arXiv:2401.06263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"FedTabDiff"&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#35775;&#38382;&#21407;&#22987;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;"&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;"&#65288;DDPMs&#65289;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#28151;&#21512;&#23646;&#24615;&#31867;&#22411;&#21644;&#38544;&#21547;&#20851;&#31995;&#31561;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#25955;&#23398;&#20064;&#26041;&#26696;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26412;&#22320;&#24615;&#12290;&#22312;&#30495;&#23454;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#39046;&#22495;&#65288;&#22914;&#37329;&#34701;&#21644;&#21307;&#30103;&#65289;&#30340;&#30495;&#23454;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#26102;&#65292;&#35201;&#20445;&#25252;&#38544;&#31169;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"FedTabDiff"&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#35775;&#38382;&#21407;&#22987;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;"&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;"&#65288;DDPMs&#65289;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#28151;&#21512;&#23646;&#24615;&#31867;&#22411;&#21644;&#38544;&#21547;&#20851;&#31995;&#31561;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;FedTabDiff&#23454;&#29616;&#20102;&#19968;&#31181;&#20998;&#25955;&#23398;&#20064;&#26041;&#26696;&#65292;&#20801;&#35768;&#22810;&#20010;&#23454;&#20307;&#22312;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#21644;&#26412;&#22320;&#24615;&#30340;&#21516;&#26102;&#20849;&#21516;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;DDPMs&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#65292;&#21253;&#25324;&#21516;&#27493;&#26356;&#26032;&#26041;&#26696;&#21644;&#21152;&#26435;&#24179;&#22343;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#22312;&#30495;&#23454;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic synthetic tabular data generation encounters significant challenges in preserving privacy, especially when dealing with sensitive information in domains like finance and healthcare. In this paper, we introduce \textit{Federated Tabular Diffusion} (FedTabDiff) for generating high-fidelity mixed-type tabular data without centralized access to the original tabular datasets. Leveraging the strengths of \textit{Denoising Diffusion Probabilistic Models} (DDPMs), our approach addresses the inherent complexities in tabular data, such as mixed attribute types and implicit relationships. More critically, FedTabDiff realizes a decentralized learning scheme that permits multiple entities to collaboratively train a generative model while respecting data privacy and locality. We extend DDPMs into the federated setting for tabular data generation, which includes a synchronous update scheme and weighted averaging for effective model aggregation. Experimental evaluations on real-world financi
&lt;/p&gt;</description></item><item><title>AGSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#20892;&#19994;&#22320;&#29702;&#22330;&#26223;&#32422;&#26463;&#30340;&#20316;&#29289;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#32454;&#31890;&#24230;&#20316;&#29289;&#26685;&#22521;&#30340;&#21464;&#21270;&#20449;&#24687;&#65292;&#24182;&#28385;&#36275;&#20892;&#19994;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.06252</link><description>&lt;p&gt;
AGSPNet: &#19968;&#31181;&#22522;&#20110;&#20892;&#19994;&#22320;&#29702;&#22330;&#26223;&#32422;&#26463;&#30340;&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#32454;&#31890;&#24230;&#20316;&#29289;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AGSPNet: A framework for parcel-scale crop fine-grained semantic change detection from UAV high-resolution imagery with agricultural geographic scene constraints. (arXiv:2401.06252v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06252
&lt;/p&gt;
&lt;p&gt;
AGSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#20892;&#19994;&#22320;&#29702;&#22330;&#26223;&#32422;&#26463;&#30340;&#20316;&#29289;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#32454;&#31890;&#24230;&#20316;&#29289;&#26685;&#22521;&#30340;&#21464;&#21270;&#20449;&#24687;&#65292;&#24182;&#28385;&#36275;&#20892;&#19994;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20934;&#30830;&#22320;&#33719;&#21462;&#20316;&#29289;&#26685;&#22521;&#30340;&#32454;&#24494;&#21464;&#21270;&#20449;&#24687;&#23545;&#20110;&#20316;&#29289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#20892;&#19994;&#32467;&#26500;&#35843;&#25972;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#38024;&#23545;&#21487;&#35265;&#20809;&#39640;&#20998;&#36776;&#29575;&#26080;&#20154;&#26426;&#22270;&#20687;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#20005;&#37325;&#20809;&#35889;&#28151;&#28102;&#65292;&#29616;&#26377;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#21463;&#21040;&#22823;&#22411;&#22797;&#26434;&#32972;&#26223;&#24178;&#25200;&#21644;&#26898;&#30416;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#20026;&#20102;&#26377;&#25928;&#25552;&#21462;&#20316;&#29289;&#30340;&#28145;&#24230;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#28385;&#36275;&#20892;&#19994;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#35774;&#35745;&#21644;&#25552;&#20986;&#20102;&#19968;&#31181;&#20892;&#19994;&#22320;&#29702;&#22330;&#26223;&#21644;&#22522;&#20110;&#22320;&#22359;&#23610;&#24230;&#32422;&#26463;&#30340;&#20316;&#29289;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26694;&#26550;&#65288;AGSPNet&#65289;&#12290;AGSPNet&#26694;&#26550;&#21253;&#25324;&#20892;&#19994;&#22320;&#29702;&#22330;&#26223;&#65288;AGS&#65289;&#20998;&#21106;&#27169;&#22359;&#12289;&#22320;&#22359;&#36793;&#32536;&#25552;&#21462;&#27169;&#22359;&#21644;&#20316;&#29289;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#27169;&#22359;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#19994;&#30417;&#27979;&#30340;&#26080;&#20154;&#26426;&#22270;&#20687;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;CSCD&#65289;&#65292;&#21253;&#21547;&#22810;&#20010;&#35821;&#20041;&#21464;&#21270;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time and accurate information on fine-grained changes in crop cultivation is of great significance for crop growth monitoring, yield prediction and agricultural structure adjustment. Aiming at the problems of serious spectral confusion in visible high-resolution unmanned aerial vehicle (UAV) images of different phases, interference of large complex background and salt-and-pepper noise by existing semantic change detection (SCD) algorithms, in order to effectively extract deep image features of crops and meet the demand of agricultural practical engineering applications, this paper designs and proposes an agricultural geographic scene and parcel-scale constrained SCD framework for crops (AGSPNet). AGSPNet framework contains three parts: agricultural geographic scene (AGS) division module, parcel edge extraction module and crop SCD module. Meanwhile, we produce and introduce an UAV image SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple semantic variatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#30340;&#29305;&#24449;&#20998;&#21306;&#31639;&#27861;&#65288;SPFP&#65289;&#65292;&#29992;&#20110;&#22810;&#35270;&#22270;&#38598;&#25104;&#23398;&#20064;&#65288;MEL&#65289;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#26377;&#25928;&#20998;&#21306;&#20026;&#22810;&#20010;&#35821;&#20041;&#19968;&#33268;&#30340;&#35270;&#22270;&#65292;SPFP&#31639;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MEL&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#39640;&#27867;&#21270;&#24615;&#33021;&#21487;&#36798;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.06251</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#38598;&#25104;&#23398;&#20064;&#20013;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#30340;&#29305;&#24449;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Semantic-Preserving Feature Partitioning for Multi-View Ensemble Learning. (arXiv:2401.06251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#30340;&#29305;&#24449;&#20998;&#21306;&#31639;&#27861;&#65288;SPFP&#65289;&#65292;&#29992;&#20110;&#22810;&#35270;&#22270;&#38598;&#25104;&#23398;&#20064;&#65288;MEL&#65289;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#26377;&#25928;&#20998;&#21306;&#20026;&#22810;&#20010;&#35821;&#20041;&#19968;&#33268;&#30340;&#35270;&#22270;&#65292;SPFP&#31639;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MEL&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#39640;&#27867;&#21270;&#24615;&#33021;&#21487;&#36798;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#21450;&#30456;&#20851;&#32852;&#30340;&#8220;&#32500;&#24230;&#28798;&#38590;&#8221;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24191;&#27867;&#32780;&#31232;&#30095;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#22810;&#35270;&#22270;&#38598;&#25104;&#23398;&#20064;&#65288;MEL&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#20986;&#29616;&#65292;&#29305;&#24449;&#20998;&#21306;&#65288;FP&#65289;&#22312;&#26500;&#24314;MEL&#30340;&#20154;&#24037;&#35270;&#22270;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#20840;&#26032;&#26041;&#27861;&#8212;&#8212;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#30340;&#29305;&#24449;&#20998;&#21306;&#65288;SPFP&#65289;&#31639;&#27861;&#12290;SPFP&#31639;&#27861;&#23558;&#25968;&#25454;&#38598;&#26377;&#25928;&#22320;&#20998;&#21306;&#20026;&#22810;&#20010;&#35821;&#20041;&#19968;&#33268;&#30340;&#35270;&#22270;&#65292;&#22686;&#24378;&#20102;MEL&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#20174;&#39640;&#32500;&#31354;&#38388;&#26377;&#38480;&#26679;&#26412;&#21040;&#20302;&#32500;&#31354;&#38388;&#22823;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#39640;&#27867;&#21270;&#24615;&#33021;&#21487;&#36798;&#30340;&#22330;&#26223;&#19979;&#65292;&#23427;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#36827;&#20102;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#30456;&#21453;&#65292;&#23427;&#20445;&#30041;&#20102;&#26410;&#21464;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, the exponential growth of data and the associated ``curse of dimensionality'' pose significant challenges, particularly with expansive yet sparse datasets. Addressing these challenges, multi-view ensemble learning (MEL) has emerged as a transformative approach, with feature partitioning (FP) playing a pivotal role in constructing artificial views for MEL. Our study introduces the Semantic-Preserving Feature Partitioning (SPFP) algorithm, a novel method grounded in information theory. The SPFP algorithm effectively partitions datasets into multiple semantically consistent views, enhancing the MEL process. Through extensive experiments on eight real-world datasets, ranging from high-dimensional with limited instances to low-dimensional with high instances, our method demonstrates notable efficacy. It maintains model accuracy while significantly improving uncertainty measures in scenarios where high generalization performance is achievable. Conversely, it retains unce
&lt;/p&gt;</description></item><item><title>WISE&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#19979;&#25193;&#23637;&#36827;&#34892;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06230</link><description>&lt;p&gt;
WISE: &#22522;&#20110;&#22320;&#19979;&#25193;&#23637;&#30340;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WISE: full-Waveform variational Inference via Subsurface Extensions. (arXiv:2401.06230v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06230
&lt;/p&gt;
&lt;p&gt;
WISE&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#19979;&#25193;&#23637;&#36827;&#34892;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#21644;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#21450;&#20854;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#30340;&#20840;&#27874;&#24418;&#21453;&#28436;&#27010;&#29575;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#20849;&#20139;&#22270;&#20687;&#25910;&#38598;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#32771;&#34385;&#21040;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#26681;&#25454;&#25968;&#25454;&#29983;&#25104;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#26469;&#37327;&#21270;&#21518;&#32493;&#25104;&#20687;&#36807;&#31243;&#20013;&#30340;&#25391;&#24133;&#21644;&#23450;&#20301;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a probabilistic technique for full-waveform inversion, employing variational inference and conditional normalizing flows to quantify uncertainty in migration-velocity models and its impact on imaging. Our approach integrates generative artificial intelligence with physics-informed common-image gathers, reducing reliance on accurate initial velocity models. Considered case studies demonstrate its efficacy producing realizations of migration-velocity models conditioned by the data. These models are used to quantify amplitude and positioning effects during subsequent imaging.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39057;&#22495;&#23398;&#20064;&#20026;3D&#34880;&#31649;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#26367;&#20195;&#22810;&#23610;&#24230;&#21367;&#31215;&#26680;&#30340;&#26041;&#26696;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;&#21442;&#25968;&#39057;&#22495;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;U-Net&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.06224</link><description>&lt;p&gt;
&#21033;&#29992;&#39057;&#22495;&#23398;&#20064;&#36827;&#34892;&#19977;&#32500;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging Frequency Domain Learning in 3D Vessel Segmentation. (arXiv:2401.06224v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39057;&#22495;&#23398;&#20064;&#20026;3D&#34880;&#31649;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#26367;&#20195;&#22810;&#23610;&#24230;&#21367;&#31215;&#26680;&#30340;&#26041;&#26696;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;&#21442;&#25968;&#39057;&#22495;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;U-Net&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#24494;&#34880;&#31649;&#30142;&#30149;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#21033;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#21644;&#35786;&#26029;&#31995;&#32479;&#65292;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#22312;&#30142;&#30149;&#36827;&#23637;&#26089;&#26399;&#36827;&#34892;&#24178;&#39044;&#65292;&#20854;&#20013;&#19977;&#32500;&#34880;&#31649;&#20998;&#21106;&#26159;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;U-Net&#26550;&#26500;&#24448;&#24448;&#20135;&#29983;&#19981;&#36830;&#36143;&#21644;&#19981;&#31934;&#30830;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#34880;&#31649;&#32467;&#26500;&#12290;&#34429;&#28982;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#65292;&#22914;&#21464;&#25442;&#22120;&#21644;&#22823;&#21367;&#31215;&#26680;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39057;&#22495;&#23398;&#20064;&#20316;&#20026;&#19977;&#32500;&#20998;&#23618;&#20998;&#21106;&#27169;&#22411;&#20013;&#22810;&#23610;&#24230;&#21367;&#31215;&#26680;&#30340;&#26367;&#20195;&#21697;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#20013;&#30340;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;&#21442;&#25968;&#39057;&#22495;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;U-Net&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronary microvascular disease constitutes a substantial risk to human health. Employing computer-aided analysis and diagnostic systems, medical professionals can intervene early in disease progression, with 3D vessel segmentation serving as a crucial component. Nevertheless, conventional U-Net architectures tend to yield incoherent and imprecise segmentation outcomes, particularly for small vessel structures. While models with attention mechanisms, such as Transformers and large convolutional kernels, demonstrate superior performance, their extensive computational demands during training and inference lead to increased time complexity. In this study, we leverage Fourier domain learning as a substitute for multi-scale convolutional kernels in 3D hierarchical segmentation models, which can reduce computational expenses while preserving global receptive fields within the network. Furthermore, a zero-parameter frequency domain fusion method is designed to improve the skip connections in U
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06210</link><description>&lt;p&gt;
&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#34920;&#31034;&#26159;&#26426;&#22120;&#29702;&#35299;&#20013;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30340;&#19968;&#33324;&#34920;&#31034;&#20445;&#30041;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#34987;&#35748;&#20026;&#19982;&#35821;&#20041;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#19968;&#33324;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#24207;&#21015;&#26041;&#27861;&#65288;&#26174;&#24335;&#32771;&#34385;&#21333;&#35789;&#30340;&#39034;&#24207;&#65289;&#21644;&#38750;&#24207;&#21015;&#26041;&#27861;&#65288;&#19981;&#26174;&#24335;&#32771;&#34385;&#39034;&#24207;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#26377;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#31867;&#26041;&#27861;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27969;&#34892;&#30340;SA&#25968;&#25454;&#38598;&#21644;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;SA&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#20013;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ADS-B&#25968;&#25454;&#23545;LLaMA 2&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#36807;&#28388;&#22122;&#38899;&#21644;&#20272;&#35745;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#25581;&#31034;&#20102;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#28304;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;LLMs&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06204</link><description>&lt;p&gt;
&#23545;LLM&#22312;&#39134;&#34892;&#36712;&#36857;&#37325;&#24314;&#20998;&#26512;&#20013;&#28508;&#21147;&#30340;&#25506;&#32034;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Assessment of LLM's Potential Toward Flight Trajectory Reconstruction Analysis. (arXiv:2401.06204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#20013;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ADS-B&#25968;&#25454;&#23545;LLaMA 2&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#36807;&#28388;&#22122;&#38899;&#21644;&#20272;&#35745;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#25581;&#31034;&#20102;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#28304;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;LLMs&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#20855;&#26377;&#38761;&#21629;&#24615;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#28508;&#21147;&#65292;&#22522;&#20110;LLMs&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#21644;&#35299;&#35835;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24320;&#28304;LLM&#27169;&#22411;LLaMA 2&#65292;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#33258;&#21160;&#30456;&#20851;&#30417;&#35270;&#24191;&#25773;&#65288;ADS-B&#65289;&#25968;&#25454;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#65292;&#35813;&#25968;&#25454;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#36807;&#28388;&#22122;&#38899;&#65292;&#24182;&#20272;&#35745;&#20986;&#32447;&#24615;&#21644;&#26354;&#32447;&#22411;&#39134;&#34892;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#22312;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#26041;&#38754;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#30740;&#31350;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;LLMs&#22312;&#39134;&#34892;&#36712;&#36857;&#37325;&#24314;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#20854;&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold transformative potential in aviation, particularly in reconstructing flight trajectories. This paper investigates this potential, grounded in the notion that LLMs excel at processing sequential data and deciphering complex data structures. Utilizing the LLaMA 2 model, a pre-trained open-source LLM, the study focuses on reconstructing flight trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with irregularities inherent in real-world scenarios. The findings demonstrate the model's proficiency in filtering noise and estimating both linear and curved flight trajectories. However, the analysis also reveals challenges in managing longer data sequences, which may be attributed to the token length limitations of LLM models. The study's insights underscore the promise of LLMs in flight trajectory reconstruction and open new avenues for their broader application across the aviation and transportation sectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.06203</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;
&lt;/p&gt;
&lt;p&gt;
Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators. (arXiv:2401.06203v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#30340;&#31995;&#32479;&#25552;&#20132;&#65292;&#35813;&#22823;&#25361;&#25112;&#25552;&#20986;&#20102;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#28151;&#38899;&#21644;&#22686;&#24378;&#38899;&#20048;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#32452;&#32463;&#36807;&#32454;&#35843;&#30340;&#28145;&#24230;&#23398;&#20064;&#38899;&#20048;&#28304;&#20998;&#31163;&#22120;&#65292;&#36825;&#20123;&#20998;&#31163;&#22120;&#22312;&#25361;&#25112;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#32454;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25361;&#25112;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31995;&#32479;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies.
&lt;/p&gt;</description></item><item><title>xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06199</link><description>&lt;p&gt;
xTrimoPGLM: &#32479;&#19968;&#30340;&#30334;&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06199
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#29983;&#29289;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#23616;&#38480;&#20110;&#33258;&#32534;&#30721;&#25110;&#33258;&#22238;&#24402;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#26102;&#24456;&#38590;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;xTrimoPGLM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25506;&#32034;&#36825;&#20004;&#31867;&#30446;&#26631;&#30340;&#20860;&#23481;&#24615;&#21644;&#32852;&#21512;&#20248;&#21270;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#65292;&#20351;&#29992;1000&#20159;&#21442;&#25968;&#21644;1&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#26469;&#35757;&#32451;xTrimoPGLM&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;1&#65289;xTrimoPGLM&#22312;&#22235;&#20010;&#31867;&#21035;&#30340;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#22522;&#32447;&#12290;&#35813;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
&lt;/p&gt;</description></item><item><title>NeuSpin&#26159;&#19968;&#20010;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#22312;&#36793;&#32536;&#36827;&#34892;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65292;NeuSPIN&#21487;&#20197;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#20197;&#21450;&#21487;&#38752;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.06195</link><description>&lt;p&gt;
NeuSpin&#65306;&#22522;&#20110;&#33258;&#26059;&#30005;&#23376;&#23398;&#30340;&#21487;&#38752;&#36793;&#32536;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#35774;&#35745;&#29992;&#20110;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
NeuSpin: Design of a Reliable Edge Neuromorphic System Based on Spintronics for Green AI. (arXiv:2401.06195v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06195
&lt;/p&gt;
&lt;p&gt;
NeuSpin&#26159;&#19968;&#20010;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#22312;&#36793;&#32536;&#36827;&#34892;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65292;NeuSPIN&#21487;&#20197;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#20197;&#21450;&#21487;&#38752;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#20010;&#24615;&#21270;&#21307;&#30103;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#23558;&#38656;&#35201;&#23384;&#20648;&#21644;&#22788;&#29702;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#35774;&#22791;&#30340;&#20851;&#38190;&#35201;&#27714;&#26159;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#12289;&#20302;&#25104;&#26412;&#30340;&#33258;&#20027;&#24615;&#65292;&#20197;&#21450;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20197;&#23454;&#29616;&#36793;&#32536;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#65292;&#23545;&#36164;&#28304;&#30340;&#38656;&#27714;&#39640;&#65292;&#24182;&#19988;&#30001;&#20110;&#20869;&#23384;&#22681;&#38382;&#39064;&#65292;&#38754;&#20020;&#30528;&#20256;&#32479;&#35745;&#31639;&#26550;&#26500;&#30340;&#25361;&#25112;&#12290;&#21033;&#29992;&#26032;&#22411;&#21487;&#21464;&#30005;&#38459;&#35760;&#24518;&#30340;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65288;CIM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20869;&#23384;&#22359;&#21644;&#35745;&#31639;&#21333;&#20803;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#38477;&#20302;&#20102;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;CIM&#30828;&#20214;&#19978;&#23454;&#29616;BayNNs&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#65292;&#30001;&#20110;&#21487;&#21464;&#24615;&#21644;&#21046;&#36896;&#32570;&#38519;&#32780;&#23384;&#22312;&#25216;&#26415;&#25361;&#25112;&#12290;NeuSPIN&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24320;&#21457;&#26032;&#39062;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet of Things (IoT) and smart wearable devices for personalized healthcare will require storing and computing ever-increasing amounts of data. The key requirements for these devices are ultra-low-power, high-processing capabilities, autonomy at low cost, as well as reliability and accuracy to enable Green AI at the edge. Artificial Intelligence (AI) models, especially Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges with traditional computing architectures due to the memory wall problem. Computing-in-Memory (CIM) with emerging resistive memories offers a solution by combining memory blocks and computing units for higher efficiency and lower power consumption. However, implementing BayNNs on CIM hardware, particularly with spintronic technologies, presents technical challenges due to variability and manufacturing defects. The NeuSPIN project aims to address these challenges through full-stack hardware and software co-design, developing novel algorithmic 
&lt;/p&gt;</description></item><item><title>CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2401.06194</link><description>&lt;p&gt;
CrisisKAN: &#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06194
&lt;/p&gt;
&lt;p&gt;
CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#23454;&#26102;&#20449;&#24687;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#20108;&#32773;&#20860;&#26377;&#65289;&#35782;&#21035;&#21508;&#31181;&#20107;&#20214;&#30340;&#26032;&#20852;&#26469;&#28304;&#12290;&#23613;&#31649;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20107;&#20214;&#20998;&#31867;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#24357;&#21512;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#32534;&#30721;&#23548;&#33268;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#26080;&#27861;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26080;&#27861;&#22312;&#28798;&#38590;&#12289;&#22823;&#27969;&#34892;&#31561;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#24314;&#31435;&#20449;&#20219;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#23383;&#25968;&#38480;&#21046;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#20107;&#20214;&#24341;&#20837;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrisisKAN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#19982;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20998;&#31867;&#21361;&#26426;&#20107;&#20214;&#12290;&#20026;&#20102;&#20016;&#23500;&#23545;&#25991;&#26412;&#20449;&#24687;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25552;&#20986;&#30340;&#32500;&#22522;&#30334;&#31185;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction
&lt;/p&gt;</description></item><item><title>Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06187</link><description>&lt;p&gt;
Scissorhands: &#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#25968;&#25454;&#24433;&#21709;&#20013;&#36827;&#34892;&#25968;&#25454;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06187
&lt;/p&gt;
&lt;p&gt;
Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#25830;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#24433;&#21709;&#12290;&#23427;&#31526;&#21512;&#26368;&#26032;&#30340;&#25968;&#25454;&#30417;&#31649;&#26631;&#20934;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#20854;&#20313;&#25968;&#25454;&#30340;&#20840;&#37096;&#20869;&#23481;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#8220;Scissorhands&#8221;&#65292;&#23427;&#21482;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26469;&#26377;&#25928;&#36816;&#34892;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;Scissorhands&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#32473;&#23450;&#27169;&#22411;&#20013;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#36825;&#20123;&#21442;&#25968;&#20013;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#21069;k%&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29992;&#20110;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#24433;&#21709;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;Scissorhands&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#36807;&#31243;&#23545;&#20462;&#21098;&#30340;&#27169;&#22411;&#36827;&#34892;&#20877;&#35757;&#32451;&#65292;&#23547;&#25214;&#20445;&#30041;&#20449;&#24687;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.06183</link><description>&lt;p&gt;
&#20351;&#29992;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#30340;&#31471;&#21040;&#31471;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#35821;&#38899;&#19968;&#30452;&#26159;&#26377;&#25928;&#27807;&#36890;&#21644;&#36830;&#25509;&#30340;&#38556;&#30861;&#65292;&#22312;&#25105;&#20204;&#26085;&#30410;&#20114;&#32852;&#30340;&#19990;&#30028;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#37327;&#36523;&#23450;&#21046;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#33521;&#25991;&#38899;&#39057;&#30340;&#21512;&#25104;&#12290;&#36890;&#36807;&#25972;&#21512;XLSR Wav2Vec2&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;mBART&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#32452;&#20214;&#31561;&#23574;&#31471;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#36328;&#35821;&#35328;&#20132;&#27969;&#26041;&#24335;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#20010;&#21035;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20114;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#21360;&#22320;&#35821;&#21475;&#35821;&#21040;&#21512;&#25104;&#33521;&#25991;&#38899;&#39057;&#30340;&#27969;&#30021;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26089;&#26399;C. elegans&#32986;&#32974;&#30340;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#36229;&#36807;90%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06182</link><description>&lt;p&gt;
&#20174;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
Prediction of Cellular Identities from Trajectory and Cell Fate Information. (arXiv:2401.06182v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26089;&#26399;C. elegans&#32986;&#32974;&#30340;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#36229;&#36807;90%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#30830;&#23450;&#32454;&#32990;&#36523;&#20221;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#26159;&#36890;&#36807;&#32454;&#32990;&#36861;&#36394;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35782;&#21035;&#26089;&#26399;C. elegans&#32986;&#32974;&#20869;&#30340;&#32454;&#32990;&#36523;&#20221;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#12289;MLP&#21644;LSTM&#27169;&#22411;&#65292;&#23545;&#36328;&#36234;&#32986;&#32974;&#21457;&#32946;&#30340;&#21069;4&#20010;&#23567;&#26102;&#30340;3D&#26102;&#38388;&#24207;&#21015;&#20849;&#32858;&#28966;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#32990;&#20998;&#31867;&#20934;&#30830;&#24615;&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#32454;&#32990;&#30340;&#23569;&#37327;&#26102;&#31354;&#29305;&#24449;&#65292;&#21253;&#25324;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#36129;&#29486;&#65292;&#24182;&#21487;&#20197;&#20174;&#29983;&#29289;&#23398;&#30693;&#35782;&#30340;&#35282;&#24230;&#35299;&#37322;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#30452;&#25509;&#20174;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#20013;&#39044;&#27979;4D&#22270;&#20687;&#24207;&#21015;&#20013;&#30340;&#32454;&#32990;&#36523;&#20221;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features.
&lt;/p&gt;</description></item><item><title>GML&#26159;&#19968;&#31181;&#20351;&#29992;&#20256;&#38395;&#21327;&#35758;&#36827;&#34892;&#30452;&#25509;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#20998;&#25955;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#25925;&#38556;&#21644;&#23616;&#37096;&#25968;&#25454;&#29305;&#24449;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06180</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20256;&#38395;&#20114;&#24800;&#23398;&#20064;&#65288;GML&#65289;&#29992;&#20110;&#22836;&#39048;&#32959;&#30244;&#33258;&#21160;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Decentralized Gossip Mutual Learning (GML) for automatic head and neck tumor segmentation. (arXiv:2401.06180v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06180
&lt;/p&gt;
&lt;p&gt;
GML&#26159;&#19968;&#31181;&#20351;&#29992;&#20256;&#38395;&#21327;&#35758;&#36827;&#34892;&#30452;&#25509;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#20998;&#25955;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#25925;&#38556;&#21644;&#23616;&#37096;&#25968;&#25454;&#29305;&#24449;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19981;&#21516;&#30340;&#21307;&#30103;&#20013;&#24515;&#21512;&#20316;&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;FL&#20381;&#36182;&#20110;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#21327;&#35843;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;&#23427;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#26381;&#21153;&#22120;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#20840;&#23616;&#25968;&#25454;&#23646;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22312;&#29305;&#23450;&#31449;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gossip Mutual Learning&#65288;GML&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20998;&#25955;&#30340;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20256;&#38395;&#21327;&#35758;&#36827;&#34892;&#30452;&#25509;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#65292;&#24182;&#36890;&#36807;&#20114;&#24800;&#23398;&#20064;&#20174;&#23545;&#31561;&#26041;&#37027;&#37324;&#21033;&#29992;&#26377;&#29992;&#30340;&#20449;&#24687;&#26469;&#20248;&#21270;&#27599;&#20010;&#31449;&#28857;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;HECKTOR21&#25968;&#25454;&#38598;&#30340;PET/CT&#22270;&#20687;&#19978;&#30340;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GML&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a promising strategy for collaboratively training complicated machine learning models from different medical centers without the need of data sharing. However, the traditional FL relies on a central server to orchestrate the global model training among clients. This makes it vulnerable to the failure of the model server. Meanwhile, the model trained based on the global data property may not yield the best performance on the local data of a particular site due to the variations of data characteristics among them. To address these limitations, we proposed Gossip Mutual Learning(GML), a decentralized collaborative learning framework that employs Gossip Protocol for direct peer-to-peer communication and encourages each site to optimize its local model by leveraging useful information from peers through mutual learning. On the task of tumor segmentation on PET/CT images using HECKTOR21 dataset with 223 cases from five clinical sites, we demonstrated GM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#21487;&#25193;&#23637;&#34892;&#20026;&#30340;CNN-DRL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36807;&#21435;&#20061;&#21313;&#22825;&#30340;&#27599;&#26085;&#29305;&#24449;&#21521;&#37327;&#25968;&#25454;&#25340;&#25509;&#21040;CNN&#36755;&#20837;&#30697;&#38453;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#22870;&#21169;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2401.06179</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#21487;&#25193;&#23637;&#34892;&#20026;&#30340;CNN-DRL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CNN-DRL for Scalable Actions in Finance. (arXiv:2401.06179v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#21487;&#25193;&#23637;&#34892;&#20026;&#30340;CNN-DRL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36807;&#21435;&#20061;&#21313;&#22825;&#30340;&#27599;&#26085;&#29305;&#24449;&#21521;&#37327;&#25968;&#25454;&#25340;&#25509;&#21040;CNN&#36755;&#20837;&#30697;&#38453;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#22870;&#21169;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#65292;&#22522;&#20110;MLP&#30340;DRL&#22312;&#21160;&#20316;&#35268;&#27169;&#22686;&#38271;&#26102;&#38590;&#20197;&#23398;&#20064;&#29615;&#22659;&#21160;&#24577;&#12290;&#22914;&#26524;&#20080;&#21334;&#37327;&#22686;&#21152;&#21040;&#19968;&#21315;&#32929;&#65292;MLP&#20195;&#29702;&#26080;&#27861;&#26377;&#25928;&#36866;&#24212;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;CNN&#20195;&#29702;&#65292;&#23558;&#36807;&#21435;&#20061;&#21313;&#22825;&#30340;&#27599;&#26085;&#29305;&#24449;&#21521;&#37327;&#25968;&#25454;&#25340;&#25509;&#36215;&#26469;&#21019;&#24314;CNN&#36755;&#20837;&#30697;&#38453;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;MLP&#30340;&#20195;&#29702;&#22312;&#21021;&#22987;&#29615;&#22659;&#35774;&#32622;&#19978;&#36973;&#21463;&#25439;&#22833;&#65292;&#32780;&#25105;&#20204;&#35774;&#35745;&#30340;CNN&#20173;&#28982;&#31283;&#23450;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#23548;&#33268;&#22870;&#21169;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards.
&lt;/p&gt;</description></item><item><title>GOODAT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#26102;&#22270;&#24418;&#39046;&#22495;&#22806;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25968;&#25454;&#39537;&#21160;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.06176</link><description>&lt;p&gt;
GOODAT: &#38754;&#21521;&#27979;&#35797;&#26102;&#22270;&#24418;&#30340;&#39046;&#22495;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GOODAT: Towards Test-time Graph Out-of-Distribution Detection. (arXiv:2401.06176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06176
&lt;/p&gt;
&lt;p&gt;
GOODAT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#26102;&#22270;&#24418;&#39046;&#22495;&#22806;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25968;&#25454;&#39537;&#21160;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#24314;&#27169;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;GNN&#22312;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#31526;&#21512;&#21516;&#19968;&#20998;&#24067;(&#21363;&#20998;&#24067;&#20869;,ID)&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#38754;&#23545;&#26469;&#33258;&#19981;&#29087;&#24713;&#20998;&#24067;(&#21363;&#39046;&#22495;&#22806;,OOD)&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#29992;GNNs&#35782;&#21035;&#21644;&#25298;&#32477;OOD&#26679;&#26412;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;OOD&#26816;&#27979;&#65292;&#36890;&#24120;&#38598;&#20013;&#22312;&#35757;&#32451;&#29305;&#23450;&#27169;&#22411;&#25110;&#22312;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;GNN&#20043;&#19978;&#20462;&#25913;&#25968;&#25454;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#37325;&#22411;&#30340;&#35757;&#32451;&#36164;&#28304;&#21644;&#25104;&#26412;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20248;&#21270;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#20462;&#25913;&#21407;&#22987;GNN&#21644;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#22270;&#24418;&#30340;&#26041;&#27861;(GOODAT)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#27604;&#36739;&#21644;&#37325;&#26032;&#23454;&#29616;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06175</link><description>&lt;p&gt;
MTAD: &#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly Detection. (arXiv:2401.06175v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06175
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#27604;&#36739;&#21644;&#37325;&#26032;&#23454;&#29616;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26159;&#30830;&#20445;&#35768;&#22810;&#36719;&#20214;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#26102;&#38388;&#24207;&#21015;&#25351;&#26631;&#12290;&#23427;&#20204;&#24544;&#23454;&#35760;&#24405;&#36816;&#34892;&#26102;&#29366;&#24577;&#65292;&#20415;&#20110;&#29702;&#35299;&#24322;&#24120;&#31995;&#32479;&#34892;&#20026;&#65292;&#24182;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#23450;&#20301;&#26681;&#26412;&#21407;&#22240;&#30340;&#26377;&#29992;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#21069;&#25152;&#26410;&#26377;&#65292;&#23548;&#33268;KPI&#25968;&#37327;&#28608;&#22686;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20256;&#32479;&#30340;KPI&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#19981;&#23454;&#29992;&#65292;&#36825;&#20419;&#20351;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#36825;&#20123;KPI&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20005;&#26684;&#27604;&#36739;&#65292;&#24182;&#19988;&#37325;&#26032;&#23454;&#29616;&#38656;&#35201;&#20184;&#20986;&#30456;&#24403;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#30740;&#31350;&#37319;&#29992;&#29420;&#31435;&#30340;&#35780;&#20272;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#25351;&#26631;&#12290;&#20854;&#20013;&#19968;&#20123;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#23637;&#31034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#20123;&#21017;&#20135;&#29983;&#20102;&#36827;&#23637;&#30340;&#38169;&#35273;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key Performance Indicators (KPIs) are essential time-series metrics for ensuring the reliability and stability of many software systems. They faithfully record runtime states to facilitate the understanding of anomalous system behaviors and provide informative clues for engineers to pinpoint the root causes. The unprecedented scale and complexity of modern software systems, however, make the volume of KPIs explode. Consequently, many traditional methods of KPI anomaly detection become impractical, which serves as a catalyst for the fast development of machine learning-based solutions in both academia and industry. However, there is currently a lack of rigorous comparison among these KPI anomaly detection methods, and re-implementation demands a non-trivial effort. Moreover, we observe that different works adopt independent evaluation processes with different metrics. Some of them may not fully reveal the capability of a model and some are creating an illusion of progress. To better und
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#20174;&#21333;&#20010;&#25668;&#20687;&#22836;&#22270;&#20687;&#20013;&#21487;&#20197;&#20840;&#38754;&#20998;&#26512;&#22797;&#26434;&#27963;&#21160;&#20013;&#30340;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#12290;&#21516;&#26102;&#35780;&#20272;&#20102;&#20854;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#24037;&#20316;&#22330;&#25152;&#25552;&#37325;&#29289;&#30340;&#35780;&#20272;&#12289;&#36710;&#31096;&#20013;&#30340;&#32763;&#39048;&#20260;&#35780;&#20272;&#21644;&#19987;&#19994;&#20307;&#32946;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.06174</link><description>&lt;p&gt;
&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Applications in Spine Biomechanics. (arXiv:2401.06174v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#20174;&#21333;&#20010;&#25668;&#20687;&#22836;&#22270;&#20687;&#20013;&#21487;&#20197;&#20840;&#38754;&#20998;&#26512;&#22797;&#26434;&#27963;&#21160;&#20013;&#30340;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#12290;&#21516;&#26102;&#35780;&#20272;&#20102;&#20854;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#24037;&#20316;&#22330;&#25152;&#25552;&#37325;&#29289;&#30340;&#35780;&#20272;&#12289;&#36710;&#31096;&#20013;&#30340;&#32763;&#39048;&#20260;&#35780;&#20272;&#21644;&#19987;&#19994;&#20307;&#32946;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#27491;&#22312;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20986;&#29616;&#21644;&#25972;&#21512;&#32780;&#21457;&#29983;&#21464;&#38761;&#12290;&#36825;&#20123;&#26032;&#39062;&#30340;&#25216;&#26415;&#21487;&#20197;&#20174;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#25668;&#20687;&#22836;&#22270;&#20687;&#20013;&#20272;&#35745;3D&#36523;&#20307;&#24418;&#29366;&#12289;&#20154;&#20307;&#27979;&#37327;&#21644;&#36816;&#21160;&#23398;&#65292;&#20351;&#24471;&#23427;&#20204;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#32908;&#32905;&#39592;&#39612;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#20174;&#21333;&#20010;&#25668;&#20687;&#22836;&#36827;&#34892;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#33034;&#26609;&#29983;&#29289;&#21147;&#23398;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#21253;&#25324;&#24037;&#20316;&#22330;&#25152;&#25552;&#37325;&#29289;&#30340;&#35780;&#20272;&#12289;&#36710;&#31096;&#20013;&#30340;&#32763;&#39048;&#20260;&#35780;&#20272;&#21644;&#19987;&#19994;&#20307;&#32946;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#21508;&#31181;&#31639;&#27861;&#22312;&#20272;&#35745;&#36523;&#20307;&#24418;&#29366;&#12289;&#36816;&#21160;&#23398;&#21644;&#36827;&#34892;&#29616;&#22330;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spine biomechanics is at a transformation with the advent and integration of machine learning and computer vision technologies. These novel techniques facilitate the estimation of 3D body shapes, anthropometrics, and kinematics from as simple as a single-camera image, making them more accessible and practical for a diverse range of applications. This study introduces a framework that merges these methodologies with traditional musculoskeletal modeling, enabling comprehensive analysis of spinal biomechanics during complex activities from a single camera. Additionally, we aim to evaluate their performance and limitations in spine biomechanics applications. The real-world applications explored in this study include assessment in workplace lifting, evaluation of whiplash injuries in car accidents, and biomechanical analysis in professional sports. Our results demonstrate potential and limitations of various algorithms in estimating body shape, kinematics, and conducting in-field biomechani
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;&#21644;&#20248;&#21270;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#36890;&#36807;&#22312;&#21021;&#22987;&#24207;&#21015;&#19978;&#25193;&#23637;&#26641;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#23616;&#37096;&#25628;&#32034;&#21644;&#36172;&#21338;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#25509;&#36817;&#26368;&#20248;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.06173</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#36827;&#21270;&#36172;&#21338;&#26426;&#29992;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#20248;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization. (arXiv:2401.06173v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;&#21644;&#20248;&#21270;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#36890;&#36807;&#22312;&#21021;&#22987;&#24207;&#21015;&#19978;&#25193;&#23637;&#26641;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#23616;&#37096;&#25628;&#32034;&#21644;&#36172;&#21338;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#25509;&#36817;&#26368;&#20248;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#20195;&#29983;&#29289;&#25216;&#26415;&#20801;&#35768;&#21512;&#25104;&#26032;&#34507;&#30333;&#36136;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#21151;&#33021;&#27979;&#37327;&#65292;&#20294;&#30001;&#20110;&#20219;&#20309;&#32473;&#23450;&#34507;&#30333;&#36136;&#30340;&#24222;&#22823;&#24207;&#21015;&#31354;&#38388;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#24037;&#31243;&#21270;&#20173;&#28982;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#34507;&#30333;&#24037;&#31243;&#36890;&#24120;&#36890;&#36807;&#23558;&#21464;&#24322;&#28155;&#21152;&#21040;&#37326;&#29983;&#22411;&#25110;&#39046;&#20808;&#24207;&#21015;&#65292;&#37325;&#26032;&#32452;&#21512;&#21464;&#24322;&#65292;&#24182;&#36816;&#34892;&#26032;&#19968;&#36718;&#30340;&#31579;&#36873;&#26469;&#36827;&#34892;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20010;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36172;&#21338;&#26426;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#23548;&#20174;&#21021;&#22987;&#24207;&#21015;&#24320;&#22987;&#25193;&#23637;&#26641;&#32467;&#26500;&#12290;&#22312;&#31616;&#21270;&#20551;&#35774;&#21644;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#23616;&#37096;&#25628;&#32034;&#21644;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#21512;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35774;&#35745;&#12290;&#35813;&#31639;&#27861;&#19982;&#19968;&#31995;&#21015;&#38543;&#26426;&#21270;&#30340;&#26641;&#25628;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;+
&lt;/p&gt;
&lt;p&gt;
While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the combination of local search and bandit learning method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#20107;&#20214;&#65292;&#20027;&#35201;&#38024;&#23545;&#32654;&#22269;&#24066;&#22330;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#12290;</title><link>http://arxiv.org/abs/2401.06172</link><description>&lt;p&gt;
&#21361;&#26426;&#35686;&#25253;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods. (arXiv:2401.06172v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06172
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#20107;&#20214;&#65292;&#20027;&#35201;&#38024;&#23545;&#32654;&#22269;&#24066;&#22330;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#32463;&#27982;&#34928;&#36864;&#24448;&#24448;&#31361;&#28982;&#32780;&#28798;&#38590;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;2008&#24180;&#37329;&#34701;&#21361;&#26426;&#26399;&#38388;&#65292;&#26631;&#26222;500&#25351;&#25968;&#20174;2007&#24180;10&#26376;&#19979;&#36300;&#20102;46&#65285;&#65292;&#25345;&#32493;&#21040;2009&#24180;3&#26376;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#26089;&#20123;&#21457;&#29616;&#21361;&#26426;&#30340;&#20449;&#21495;&#65292;&#25105;&#20204;&#23601;&#33021;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#26679;&#30340;&#21160;&#26426;&#39537;&#20351;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65292;&#20027;&#35201;&#39044;&#27979;&#32654;&#22269;&#24066;&#22330;&#21487;&#33021;&#21457;&#29983;&#30340;&#20219;&#20309;&#28508;&#22312;&#24066;&#22330;&#23849;&#30424;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24819;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#26816;&#26597;&#21738;&#31181;&#27169;&#22411;&#26356;&#36866;&#21512;&#39044;&#27979;&#32654;&#22269;&#32929;&#24066;&#26292;&#36300;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#27599;&#26085;&#37329;&#34701;&#24066;&#22330;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#26356;&#39640;&#30340;&#25253;&#21578;&#39057;&#29575;&#26356;&#20855;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;75&#20010;&#35299;&#37322;&#21464;&#37327;&#65292;&#21253;&#25324;&#24635;&#20307;&#32654;&#22269;&#32929;&#24066;&#25351;&#25968;&#65292;&#26631;&#26222;500&#37096;&#38376;&#25351;&#25968;&#65292;&#20197;&#21450;&#21487;&#29992;&#20110;&#21361;&#26426;&#39044;&#27979;&#30446;&#30340;&#30340;&#24066;&#22330;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#36873;&#25321;&#30340;&#20998;&#31867;&#25351;&#26631;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#32929;&#24066;&#21361;&#26426;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;HE&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#35757;&#32451;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20165;&#36890;&#36807;&#24418;&#24577;&#23398;&#20998;&#26512;&#23601;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#29356;&#30382;&#32932;&#32933;&#22823;&#32454;&#32990;&#30244;&#30340;c-Kit-11&#31361;&#21464;&#29366;&#24577;&#65292;&#20934;&#30830;&#29575;&#20026;87&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.06169</link><description>&lt;p&gt;
&#36890;&#36807;HE&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#29356;&#30382;&#32932;&#32933;&#22823;&#32454;&#32990;&#30244;&#30340;c-Kit-11&#31361;&#21464;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Deep Learning model predicts the c-Kit-11 mutational status of canine cutaneous mast cell tumors by HE stained histological slides. (arXiv:2401.06169v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;HE&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#35757;&#32451;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20165;&#36890;&#36807;&#24418;&#24577;&#23398;&#20998;&#26512;&#23601;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#29356;&#30382;&#32932;&#32933;&#22823;&#32454;&#32990;&#30244;&#30340;c-Kit-11&#31361;&#21464;&#29366;&#24577;&#65292;&#20934;&#30830;&#29575;&#20026;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#29356;&#32933;&#22823;&#32454;&#32990;&#30244;&#30340;&#27963;&#26816;&#32452;&#32455;&#20013;&#65292;&#36890;&#36807;&#32452;&#32455;&#30149;&#29702;&#23398;&#35780;&#20272;&#22810;&#31181;&#39044;&#21518;&#22240;&#23376;&#20197;&#35780;&#20272;&#20020;&#24202;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#24120;&#24120;&#36890;&#36807;c-Kit&#22806;&#26174;&#23376;11&#30340;PCR&#20998;&#26512;&#35780;&#20272;&#37226;&#27688;&#37240;&#28608;&#37238;&#25233;&#21046;&#21058;&#27835;&#30103;&#30340;&#25104;&#21151;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24418;&#24577;&#23398;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#20998;&#23376;&#20998;&#26512;&#65292;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;DLMs&#65289;&#35782;&#21035;MCTs&#30340;c-Kit-11&#31361;&#21464;&#29366;&#24577;&#12290;195&#20363;&#31361;&#21464;&#24615;&#21644;173&#20363;&#38750;&#31361;&#21464;&#24615;&#32959;&#30244;&#30340;HE&#20999;&#29255;&#36830;&#32493;&#26579;&#33394;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#23460;&#24182;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#20999;&#29255;&#25195;&#25551;&#20202;&#36827;&#34892;&#25195;&#25551;&#12290;&#36825;&#20135;&#29983;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;&#26579;&#33394;-&#25195;&#25551;&#20202;&#21464;&#21270;&#65289;&#30340;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#12290;DLMs&#36890;&#36807;&#21333;&#19968;&#21644;&#28151;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#25195;&#25551;&#20202;&#21644;&#26579;&#33394;&#39046;&#22495;&#36716;&#31227;&#19979;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;DLMs&#21487;&#20197;&#27491;&#30830;&#20998;&#31867;HE&#20999;&#29255;&#30340;c-Kit 11&#31361;&#21464;&#29366;&#24577;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous prognostic factors are currently assessed histopathologically in biopsies of canine mast cell tumors to evaluate clinical behavior. In addition, PCR analysis of the c-Kit exon 11 mutational status is often performed to evaluate the potential success of a tyrosine kinase inhibitor therapy. This project aimed at training deep learning models (DLMs) to identify the c-Kit-11 mutational status of MCTs solely based on morphology without additional molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were stained consecutively in two different laboratories and scanned with three different slide scanners. This resulted in six different datasets (stain-scanner variations) of whole slide images. DLMs were trained with single and mixed datasets and their performances was assessed under scanner and staining domain shifts. The DLMs correctly classified HE slides according to their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited stain-scanner var
&lt;/p&gt;</description></item><item><title>AdaMR&#26159;&#19968;&#31181;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#23427;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#20998;&#23376;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;AdaMR&#21487;&#20197;&#25913;&#21892;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.06166</link><description>&lt;p&gt;
&#21487;&#35843;&#25972;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06166
&lt;/p&gt;
&lt;p&gt;
AdaMR&#26159;&#19968;&#31181;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#23427;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#20998;&#23376;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;AdaMR&#21487;&#20197;&#25913;&#21892;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20998;&#23376;&#27169;&#22411;&#65292;&#21517;&#20026;AdaMR&#65292;&#23427;&#20195;&#34920;&#21487;&#35843;&#25972;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#26368;&#36817;&#20351;&#29992;&#21333;&#19968;&#20998;&#23376;&#32534;&#30721;&#30340;&#22823;&#35268;&#27169;&#20998;&#23376;&#27169;&#22411;&#19981;&#21516;&#65292;AdaMR&#37319;&#29992;&#20102;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23376;&#35268;&#33539;&#21270;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#22810;&#20010;&#36890;&#29992;&#20998;&#23376;&#34920;&#31034;&#36716;&#21270;&#20026;&#35268;&#33539;&#34920;&#31034;&#12290;&#36890;&#36807;&#35843;&#25972;&#20998;&#23376;&#32534;&#30721;&#30340;&#31890;&#24230;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#22914;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;&#20122;&#32467;&#26500;&#27700;&#24179;&#30340;&#20998;&#23376;&#34920;&#31034;&#20445;&#30041;&#20102;&#20915;&#23450;&#21270;&#23398;&#24615;&#36136;&#21644;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#29305;&#23450;&#21407;&#23376;&#32452;&#25110;&#25490;&#21015;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#24615;&#36136;&#39044;&#27979;&#31561;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#12290;&#21516;&#26102;&#65292;&#21407;&#23376;&#32423;&#34920;&#31034;&#23558;&#21407;&#23376;&#30340;&#29305;&#24322;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#22312;&#37329;&#34701;&#25237;&#36164;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#20030;&#25514;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;Gen-AI&#25216;&#26415;&#33258;&#21160;&#21270;&#20449;&#24687;&#25688;&#35201;&#21644;&#25237;&#36164;&#28857;&#29983;&#25104;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#24212;&#29992;&#30446;&#26631;&#65292;&#24182;&#26368;&#32456;&#24320;&#21457;&#19968;&#20010;AI&#20195;&#29702;&#21407;&#22411;&#65292;&#20351;&#20154;&#31867;&#25237;&#36164;&#32773;&#25670;&#33073;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#39640;&#23618;&#27425;&#30340;&#25112;&#30053;&#24605;&#32500;&#19978;&#12290;</title><link>http://arxiv.org/abs/2401.06164</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;Gen-AI&#29992;&#20110;&#22522;&#26412;&#25237;&#36164;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Gen-AI for Fundamental Investment Research. (arXiv:2401.06164v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#22312;&#37329;&#34701;&#25237;&#36164;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#20030;&#25514;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;Gen-AI&#25216;&#26415;&#33258;&#21160;&#21270;&#20449;&#24687;&#25688;&#35201;&#21644;&#25237;&#36164;&#28857;&#29983;&#25104;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#24212;&#29992;&#30446;&#26631;&#65292;&#24182;&#26368;&#32456;&#24320;&#21457;&#19968;&#20010;AI&#20195;&#29702;&#21407;&#22411;&#65292;&#20351;&#20154;&#31867;&#25237;&#36164;&#32773;&#25670;&#33073;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#39640;&#23618;&#27425;&#30340;&#25112;&#30053;&#24605;&#32500;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;&#22312;&#37329;&#34701;&#25237;&#36164;&#39046;&#22495;&#30340;&#19968;&#39033;&#21464;&#38761;&#24615;&#20030;&#25514;&#65292;&#21363;&#37325;&#22609;&#20256;&#32479;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#22312;&#20854;&#20013;&#33258;&#21160;&#21270;&#20449;&#24687;&#25688;&#35201;&#21644;&#25237;&#36164;&#28857;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26088;&#22312;&#35780;&#20272;&#22312;&#22522;&#30784;&#27169;&#22411;&#65288;Llama2&#65289;&#19978;&#36827;&#34892;&#31934;&#35843;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#24212;&#29992;&#30446;&#26631;&#65292;&#21253;&#25324;&#25552;&#20379;&#20851;&#20110;&#20107;&#20214;&#23545;&#20844;&#21496;&#21644;&#34892;&#19994;&#30340;&#24433;&#21709;&#30340;&#27934;&#23519;&#65292;&#29702;&#35299;&#24066;&#22330;&#26465;&#20214;&#20851;&#31995;&#65292;&#29983;&#25104;&#19982;&#25237;&#36164;&#32773;&#23545;&#40784;&#30340;&#25237;&#36164;&#28857;&#65292;&#24182;&#20197;&#32929;&#31080;&#24314;&#35758;&#21644;&#35814;&#32454;&#35299;&#37322;&#30340;&#24418;&#24335;&#21576;&#29616;&#32467;&#26524;&#12290;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;AI&#20195;&#29702;&#21407;&#22411;&#65292;&#20351;&#20154;&#31867;&#25237;&#36164;&#32773;&#25670;&#33073;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#39640;&#23618;&#27425;&#30340;&#25112;&#30053;&#24605;&#32500;&#19978;&#12290;&#35813;&#39033;&#30446;&#28085;&#30422;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#35821;&#26009;&#24211;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus data
&lt;/p&gt;</description></item><item><title>FRED&#26159;&#19968;&#20010;&#23454;&#29616;&#33322;&#31354;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#20013;&#23436;&#20840;&#26059;&#36716;&#31561;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32806;&#19981;&#21464;&#20219;&#21153;&#21644;&#31561;&#21464;&#20219;&#21153;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#31561;&#21464;&#24615;&#65292;&#23558;&#36793;&#30028;&#26694;&#34920;&#31034;&#20026;&#26059;&#36716;&#31561;&#21464;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.06159</link><description>&lt;p&gt;
FRED: &#23454;&#29616;&#33322;&#31354;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#23436;&#20840;&#26059;&#36716;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection. (arXiv:2401.06159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06159
&lt;/p&gt;
&lt;p&gt;
FRED&#26159;&#19968;&#20010;&#23454;&#29616;&#33322;&#31354;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#20013;&#23436;&#20840;&#26059;&#36716;&#31561;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32806;&#19981;&#21464;&#20219;&#21153;&#21644;&#31561;&#21464;&#20219;&#21153;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#31561;&#21464;&#24615;&#65292;&#23558;&#36793;&#30028;&#26694;&#34920;&#31034;&#20026;&#26059;&#36716;&#31561;&#21464;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#31561;&#21464;&#24615;&#26159;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#22120;&#33258;&#28982;&#21033;&#29992;&#20256;&#32479;CNN&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#26469;&#25552;&#39640;&#23545;&#31354;&#38388;&#24179;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23454;&#29616;&#26059;&#36716;&#31561;&#21464;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#20197;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#30446;&#21069;&#30340;&#26816;&#27979;&#22120;&#37319;&#29992;&#21508;&#31181;&#23545;&#40784;&#25216;&#26415;&#26469;&#24471;&#21040;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#20173;&#20381;&#36182;&#20110;&#39640;&#23481;&#37327;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#22686;&#24378;&#26469;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#26059;&#36716;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23436;&#20840;&#26059;&#36716;&#31561;&#21464;&#30340;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;FRED&#65289;&#65292;&#20854;&#20174;&#22270;&#20687;&#21040;&#36793;&#30028;&#26694;&#39044;&#27979;&#30340;&#25972;&#20010;&#36807;&#31243;&#37117;&#26159;&#20005;&#26684;&#31561;&#21464;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19981;&#21464;&#30340;&#20219;&#21153;&#65288;&#23545;&#35937;&#20998;&#31867;&#65289;&#21644;&#31561;&#21464;&#30340;&#20219;&#21153;&#65288;&#23545;&#35937;&#23450;&#20301;&#65289;&#35299;&#32806;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#23558;&#36793;&#30028;&#26694;&#34920;&#31034;&#20026;&#19968;&#32452;&#26059;&#36716;&#31561;&#21464;&#21521;&#37327;&#65292;&#20197;&#23454;&#29616;&#26059;&#36716;&#31561;&#21464;&#30340;&#23450;&#20301;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#20123;&#26059;&#36716;&#31561;&#21464;&#21521;&#37327;&#20316;&#20026;&#20559;&#31227;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#39564;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#31995;&#32479;&#21487;&#20197;&#36798;&#21040;&#35748;&#35777;&#35201;&#27714;&#65292;&#24182;&#23545;&#38556;&#30861;&#29289;&#26816;&#27979;&#21151;&#33021;&#30340;&#31995;&#32479;&#32423;&#21361;&#38505;&#29575;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.06156</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20998;&#31867;&#35823;&#24046;&#20272;&#35745;&#30340;&#38543;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Approach to Classification Error Estimates in Convolutional Neural Networks. (arXiv:2401.06156v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#39564;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#31995;&#32479;&#21487;&#20197;&#36798;&#21040;&#35748;&#35777;&#35201;&#27714;&#65292;&#24182;&#23545;&#38556;&#30861;&#29289;&#26816;&#27979;&#21151;&#33021;&#30340;&#31995;&#32479;&#32423;&#21361;&#38505;&#29575;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#24050;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#12290;&#20197;&#26410;&#26469;4&#32423;&#33258;&#21160;&#39550;&#39542;&#36135;&#36816;&#21015;&#36710;&#20013;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#21151;&#33021;&#20026;&#20363;&#65292;&#36890;&#36807;&#20351;&#29992;ANSI/UL 4600&#21644;ISO 21448&#31561;&#26032;&#26631;&#20934;&#65292;&#35777;&#26126;&#20102;&#20687;GoA 4&#36135;&#36816;&#21015;&#36710;&#36825;&#26679;&#30340;&#31995;&#32479;&#30830;&#23454;&#21487;&#20197;&#36798;&#21040;&#35748;&#35777;&#35201;&#27714;&#65292;&#20197;&#21450;EN 50128&#21644;EN 50129&#31561;&#24050;&#26377;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#39044;&#26399;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#21151;&#33021;&#31995;&#32479;&#32423;&#21361;&#38505;&#29575;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20256;&#24863;&#22120;/&#24863;&#30693;&#22120;&#34701;&#21512;&#65292;&#34701;&#21512;&#26816;&#27979;&#31995;&#32479;&#21487;&#20197;&#28385;&#36275;&#23433;&#20840;&#23436;&#25972;&#24615;&#32423;&#21035;&#25152;&#38656;&#30340;&#21487;&#25509;&#21463;&#21361;&#38505;&#29575;&#65288;SIL-3&#65289;&#12290;&#23545;CNN&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#20998;&#31867;&#32858;&#31867;&#21644;&#31561;&#20215;&#31867;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents research results achieved in the field of verification of trained Convolutional Neural Network (CNN) used for image classification in safety-critical applications. As running example, we use the obstacle detection function needed in future autonomous freight trains with Grade of Automation (GoA) 4. It is shown that systems like GoA 4 freight trains are indeed certifiable today with new standards like ANSI/UL 4600 and ISO 21448 used in addition to the long-existing standards EN 50128 and EN 50129. Moreover, we present a quantitative analysis of the system-level hazard rate to be expected from an obstacle detection function. It is shown that using sensor/perceptor fusion, the fused detection system can meet the tolerable hazard rate deemed to be acceptable for the safety integrity level to be applied (SIL-3). A mathematical analysis of CNN models is performed which results in the identification of classification clusters and equivalence classes partitioning
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;GPT&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MolRL-MGPT&#26469;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;&#65292;&#22312;&#33647;&#29289;&#20998;&#23376;&#29983;&#25104;&#36807;&#31243;&#20013;&#20419;&#36827;&#20102;&#22810;&#26679;&#24615;&#65292;&#23637;&#29616;&#20102;&#22312;&#35774;&#35745;&#25239;SARS-CoV-2&#34507;&#30333;&#38774;&#28857;&#30340;&#25233;&#21046;&#21058;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06155</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20010;GPT&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30340;de novo&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De novo Drug Design using Reinforcement Learning with Multiple GPT Agents. (arXiv:2401.06155v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06155
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;GPT&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MolRL-MGPT&#26469;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;&#65292;&#22312;&#33647;&#29289;&#20998;&#23376;&#29983;&#25104;&#36807;&#31243;&#20013;&#20419;&#36827;&#20102;&#22810;&#26679;&#24615;&#65292;&#23637;&#29616;&#20102;&#22312;&#35774;&#35745;&#25239;SARS-CoV-2&#34507;&#30333;&#38774;&#28857;&#30340;&#25233;&#21046;&#21058;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
de novo&#33647;&#29289;&#35774;&#35745;&#26159;&#33647;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#20998;&#23376;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20998;&#23376;&#12290;&#23613;&#31649;&#36716;&#25442;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20808;&#36827;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#33647;&#29289;&#35774;&#35745;&#65292;&#20294;&#20854;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolRL-MGPT&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;GPT&#20195;&#29702;&#26469;&#29983;&#25104;&#33647;&#29289;&#20998;&#23376;&#12290;&#20026;&#20102;&#20419;&#36827;&#20998;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#40723;&#21169;&#20195;&#29702;&#21512;&#20316;&#65292;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#23547;&#25214;&#29702;&#24819;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;GuacaMol&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#35774;&#35745;&#25239;SARS-CoV-2&#34507;&#30333;&#38774;&#28857;&#30340;&#25233;&#21046;&#21058;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;https://github.com/HXYfighter/MolRL-MGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.
&lt;/p&gt;</description></item><item><title>MMDiff&#26159;&#19968;&#20010;&#32852;&#21512;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#20363;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06151</link><description>&lt;p&gt;
&#36235;&#21521;&#20110;&#20351;&#29992;SE(3)-&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#30340;&#32852;&#21512;&#24207;&#21015;-&#32467;&#26500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion. (arXiv:2401.06151v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06151
&lt;/p&gt;
&lt;p&gt;
MMDiff&#26159;&#19968;&#20010;&#32852;&#21512;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#20363;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20998;&#23376;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#34507;&#30333;&#36136;&#24037;&#31243;&#30340;&#24037;&#19994;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20855;&#26377;&#20016;&#23500;&#19988;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30446;&#21069;&#20165;&#38480;&#20110;&#29420;&#31435;&#25110;&#32852;&#21512;&#22320;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#25110;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#32771;&#34385;&#34507;&#30333;&#36136;&#21644;&#20854;&#20182;&#22823;&#20998;&#23376;&#20043;&#38388;&#24120;&#35265;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMDiff&#65292;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;SE(3)-&#31163;&#25955;&#25193;&#25955;&#22122;&#22768;&#35774;&#35745;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#32467;&#26500;&#22522;&#30784;&#36716;&#24405;&#22240;&#23376;&#35774;&#35745;&#21644;&#38750;&#32534;&#30721;RNA&#24207;&#21015;&#35774;&#35745;&#31561;&#26032;&#20852;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26412;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;&#20005;&#26684;&#26032;&#30340;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#29983;&#25104;&#35774;&#35745;&#22522;&#20934;&#26469;&#23637;&#31034;&#20102;MMDiff&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MMDiff&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#24494;&#22411;RNA&#21644;&#21333;&#38142;DNA&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA mole
&lt;/p&gt;</description></item><item><title>D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06150</link><description>&lt;p&gt;
D-STGCNT:&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#23494;&#38598;&#26102;&#31354;&#22270;&#21367;&#31215;GRU&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06150
&lt;/p&gt;
&lt;p&gt;
D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35780;&#20272;&#26080;&#20020;&#24202;&#30417;&#30563;&#24773;&#20917;&#19979;&#24739;&#32773;&#36827;&#34892;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#30340;&#25361;&#25112;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#20379;&#36136;&#37327;&#35780;&#20998;&#20197;&#30830;&#20445;&#27491;&#30830;&#25191;&#34892;&#21644;&#33719;&#24471;&#26399;&#26395;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;Dense Spatio-Temporal Graph Conv-GRU Network with Transformer&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;STGCN&#21644;transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#27599;&#20010;&#24247;&#22797;&#38203;&#28860;&#20013;&#36215;&#20027;&#35201;&#20316;&#29992;&#30340;&#20851;&#33410;&#12290;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#29992;&#20110;&#24555;&#36895;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#24182;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#12290;transformer&#32534;&#30721;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20391;&#37325;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#20351;&#20854;&#22312;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24179;&#38754;&#22825;&#32447;&#35774;&#35745;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#30830;&#23450;&#20960;&#20309;&#23610;&#23544;&#21644;&#20301;&#32622;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#37319;&#26679;&#32479;&#35745;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26469;&#35774;&#35745;&#22825;&#32447;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#32463;&#39564;&#65292;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35774;&#35745;&#65292;&#24182;&#19988;&#24615;&#33021;&#25351;&#26631;&#19981;&#20122;&#20110;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#24037;&#31243;&#24072;&#35774;&#35745;&#30340;&#22825;&#32447;&#12290;</title><link>http://arxiv.org/abs/2401.06149</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24179;&#38754;&#22825;&#32447;&#35774;&#35745;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image Classifier Based Generative Method for Planar Antenna Design. (arXiv:2401.06149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24179;&#38754;&#22825;&#32447;&#35774;&#35745;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#30830;&#23450;&#20960;&#20309;&#23610;&#23544;&#21644;&#20301;&#32622;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#37319;&#26679;&#32479;&#35745;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26469;&#35774;&#35745;&#22825;&#32447;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#32463;&#39564;&#65292;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35774;&#35745;&#65292;&#24182;&#19988;&#24615;&#33021;&#25351;&#26631;&#19981;&#20122;&#20110;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#24037;&#31243;&#24072;&#35774;&#35745;&#30340;&#22825;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#26356;&#22810;&#30340;&#24037;&#31243;&#24072;&#33021;&#22815;&#36827;&#34892;&#21360;&#21047;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#19978;&#30340;&#22825;&#32447;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20010;&#22522;&#26412;&#32452;&#20214;&#23545;PCB&#22825;&#32447;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#20998;&#20004;&#20010;&#27493;&#39588;&#20915;&#23450;&#20854;&#20960;&#20309;&#23610;&#23544;&#21644;&#20301;&#32622;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#38656;&#35201;&#20219;&#20309;&#32463;&#39564;&#30340;&#22825;&#32447;&#21407;&#22411;&#35774;&#35745;&#12290;&#23558;&#38543;&#26426;&#37319;&#26679;&#32479;&#35745;&#19982;&#23610;&#23544;&#36136;&#37327;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#22312;&#23610;&#23544;&#20505;&#36873;&#39033;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#65292;&#36827;&#19968;&#27493;&#30830;&#23450;&#36825;&#20123;&#22266;&#23450;&#23610;&#23544;&#32452;&#20214;&#30340;&#20301;&#32622;&#12290;&#36873;&#25321;&#20102;&#20004;&#20010;&#21487;&#31359;&#25140;&#20135;&#21697;&#30340;&#20363;&#23376;&#26469;&#26816;&#39564;&#25972;&#20010;&#24037;&#20316;&#27969;&#31243;&#12290;&#23427;&#20204;&#30340;&#26368;&#32456;&#35774;&#35745;&#26159;&#36924;&#30495;&#30340;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#19981;&#36874;&#20110;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#24037;&#31243;&#24072;&#35774;&#35745;&#30340;&#22825;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
To extend the antenna design on printed circuit boards (PCBs) for more engineers of interest, we propose a simple method that models PCB antennas with a few basic components. By taking two separate steps to decide their geometric dimensions and positions, antenna prototypes can be facilitated with no experience required. Random sampling statistics relate to the quality of dimensions are used in selecting among dimension candidates. A novel image-based classifier using a convolutional neural network (CNN) is introduced to further determine the positions of these fixed-dimension components. Two examples from wearable products have been chosen to examine the entire workflow. Their final designs are realistic and their performance metrics are not inferior to the ones designed by experienced engineers.
&lt;/p&gt;</description></item><item><title>Minuet&#26159;&#19968;&#31181;&#19987;&#20026;&#29616;&#20195;GPU&#35774;&#35745;&#30340;&#39640;&#25928;&#20869;&#23384;SC&#24341;&#25806;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20998;&#27573;&#25490;&#24207;&#20108;&#20998;&#26597;&#25214;&#31639;&#27861;&#21644;&#36731;&#37327;&#32423;&#26041;&#26696;&#26469;&#21152;&#36895;3D&#31232;&#30095;&#21367;&#31215;&#12290;</title><link>http://arxiv.org/abs/2401.06145</link><description>&lt;p&gt;
Minuet: &#21152;&#36895;GPU&#19978;&#30340;3D&#31232;&#30095;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Minuet: Accelerating 3D Sparse Convolutions on GPUs. (arXiv:2401.06145v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06145
&lt;/p&gt;
&lt;p&gt;
Minuet&#26159;&#19968;&#31181;&#19987;&#20026;&#29616;&#20195;GPU&#35774;&#35745;&#30340;&#39640;&#25928;&#20869;&#23384;SC&#24341;&#25806;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20998;&#27573;&#25490;&#24207;&#20108;&#20998;&#26597;&#25214;&#31639;&#27861;&#21644;&#36731;&#37327;&#32423;&#26041;&#26696;&#26469;&#21152;&#36895;3D&#31232;&#30095;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#21367;&#31215;&#65288;SC&#65289;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#26412;&#36136;&#19978;&#31232;&#30095;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#12290;&#19982;&#31264;&#23494;&#21367;&#31215;&#19981;&#21516;&#65292;SC&#36890;&#36807;&#21482;&#20801;&#35768;&#36755;&#20986;&#21040;&#29305;&#23450;&#20301;&#32622;&#26469;&#20445;&#30041;&#36755;&#20837;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#12290;&#20026;&#20102;&#39640;&#25928;&#35745;&#31639;SC&#65292;&#20808;&#21069;&#30340;SC&#24341;&#25806;&#39318;&#20808;&#20351;&#29992;&#21704;&#24076;&#34920;&#26500;&#24314;&#19968;&#20010;&#20869;&#26680;&#26144;&#23556;&#65292;&#35813;&#26144;&#23556;&#23384;&#20648;&#38656;&#35201;&#25191;&#34892;&#30340;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#25805;&#20316;&#65288;&#26144;&#23556;&#27493;&#39588;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;Gather-GEMM-Scatter&#36807;&#31243;&#25191;&#34892;&#36825;&#20123;GEMM&#25805;&#20316;&#65288;GMaS&#27493;&#39588;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;SC&#24341;&#25806;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;Minuet&#65292;&#19968;&#31181;&#19987;&#20026;&#29616;&#20195;GPU&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#39640;&#25928;&#20869;&#23384;SC&#24341;&#25806;&#12290;Minuet&#25552;&#20986;&#20102;&#20197;&#19979;&#20960;&#28857;&#65306;(i)&#23558;Map&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#21704;&#24076;&#34920;&#26367;&#25442;&#20026;&#19968;&#31181;&#39640;&#24230;&#21033;&#29992;GPU&#29255;&#19978;&#23384;&#20648;&#23618;&#27425;&#32467;&#26500;&#30340;&#26032;&#39062;&#20998;&#27573;&#25490;&#24207;&#21452;&#36941;&#21382;&#20108;&#20998;&#26597;&#25214;&#31639;&#27861;&#65292;(ii)&#20351;&#29992;&#36731;&#37327;&#32423;&#26041;&#26696;&#26469;&#33258;&#21160;&#35843;&#25972;GMaS&#27493;&#39588;&#20013;&#30340;Gather&#21644;Scatter&#25805;&#20316;&#30340;&#29926;&#29255;&#22823;&#23567;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Convolution (SC) is widely used for processing 3D point clouds that are inherently sparse. Different from dense convolution, SC preserves the sparsity of the input point cloud by only allowing outputs to specific locations. To efficiently compute SC, prior SC engines first use hash tables to build a kernel map that stores the necessary General Matrix Multiplication (GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter process to execute these GEMM operations (GMaS step). In this work, we analyze the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to (i) replace the hash tables used in the Map step with a novel segmented sorting double-traversed binary search algorithm that highly utilizes the on-chip memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile size in the Gather and Scatter operations of the GMaS step, such that to adapt t
&lt;/p&gt;</description></item><item><title>DFU&#26159;&#19968;&#31181;&#23610;&#24230;&#40065;&#26834;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06144</link><description>&lt;p&gt;
DFU: &#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#30340;&#23610;&#24230;&#40065;&#26834;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06144
&lt;/p&gt;
&lt;p&gt;
DFU&#26159;&#19968;&#31181;&#23610;&#24230;&#40065;&#26834;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#27809;&#26377;&#30456;&#24212;&#20998;&#36776;&#29575;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#12290;&#20511;&#37492;&#25805;&#20316;&#31526;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;Dual-FNO UNet (DFU)&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#21516;&#26102;&#32452;&#21512;&#31354;&#38388;&#21644;&#20809;&#35889;&#20449;&#24687;&#26469;&#36817;&#20284;&#35780;&#20998;&#25805;&#20316;&#31526;&#12290;&#23558;DFU&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#21487;&#25193;&#23637;&#24615;&#65306;1&#65289;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#21516;&#26102;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;FID&#65292;&#32780;&#21333;&#19968;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#35757;&#32451;&#21017;&#19981;&#33021;&#23454;&#29616;&#65307;2&#65289;DFU&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#35757;&#32451;&#20998;&#36776;&#29575;&#20043;&#22806;&#65292;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#21327;&#35843;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21363;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65307;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#35843;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;FID&#20026;11.3&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 
&lt;/p&gt;</description></item><item><title>StockFormer&#26159;&#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#65292;&#20351;&#29992;TopKDropout&#26041;&#27861;&#26469;&#25552;&#39640;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;62.39%&#30340;&#24066;&#22330;&#36235;&#21183;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;&#22312;&#22238;&#27979;&#20013;&#65292;StockFormer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#12290;</title><link>http://arxiv.org/abs/2401.06139</link><description>&lt;p&gt;
StockFormer: &#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks. (arXiv:2401.06139v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06139
&lt;/p&gt;
&lt;p&gt;
StockFormer&#26159;&#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#65292;&#20351;&#29992;TopKDropout&#26041;&#27861;&#26469;&#25552;&#39640;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;62.39%&#30340;&#24066;&#22330;&#36235;&#21183;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;&#22312;&#22238;&#27979;&#20013;&#65292;StockFormer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#30340;&#24066;&#22330;&#37325;&#26032;&#26657;&#20934;&#21644;&#25237;&#36164;&#32773;&#20048;&#35266;&#24773;&#32490;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#32654;&#22269;&#32929;&#24066;&#27491;&#22312;&#32463;&#21382;&#22797;&#33487;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#19968;&#20123;&#20808;&#36827;&#30340;&#24037;&#20855;&#26469;&#20445;&#25252;&#21644;&#22686;&#38271;&#25237;&#36164;&#32452;&#21512;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"Stockformer"&#30340;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#25670;&#21160;&#20132;&#26131;&#65292;&#24182;&#37319;&#29992;&#20102;TopKDropout&#26041;&#27861;&#26469;&#22686;&#24378;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#36890;&#36807;&#25972;&#21512;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;Stockformer&#21033;&#29992;&#26631;&#26222;500&#30340;&#22797;&#26434;&#25968;&#25454;&#26469;&#25552;&#21319;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#39564;&#35777;&#38598;&#65288;2021&#24180;1&#26376;&#33267;2023&#24180;1&#26376;&#65289;&#20197;&#21450;&#27979;&#35797;&#38598;&#65288;2023&#24180;2&#26376;&#33267;6&#26376;&#65289;&#12290;&#22312;&#27979;&#35797;&#26399;&#38388;&#65292;Stockformer&#30340;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#65288;MAE&#65292;RMSE&#65292;MAPE&#65289;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#31934;&#24230;&#65292;&#26816;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;62.39%&#12290;&#22312;&#25105;&#20204;&#30340;&#22238;&#27979;&#20013;&#65292;Stockformer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#65292;&#24180;&#21270;&#25910;&#30410;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce "Stockformer," a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&amp;P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized r
&lt;/p&gt;</description></item><item><title>QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06137</link><description>&lt;p&gt;
QuasiNet: &#19968;&#31181;&#20855;&#26377;&#21487;&#35757;&#32451;&#20056;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06137
&lt;/p&gt;
&lt;p&gt;
QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#20284;XOR&#25110;&#22855;&#20598;&#26657;&#39564;&#31561;&#38590;&#39064;&#30340;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21463;&#29616;&#26377;&#20855;&#26377;&#25152;&#35859;&#20056;&#31215;&#31070;&#32463;&#20803;&#21644;&#30001;&#32463;&#20856;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#35268;&#21017;&#21551;&#21457;&#65292;&#20248;&#38597;&#22320;&#35299;&#20915;&#20102;&#20114;&#26021;&#24773;&#20917;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#20855;&#26377;&#39044;&#35774;&#19988;&#19981;&#21487;&#35843;&#33410;&#26435;&#37325;&#30340;&#20056;&#31215;&#31070;&#32463;&#20803;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#20803;&#20056;&#31215;&#23618;&#20063;&#33021;&#22815;&#23398;&#20064;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#25104;&#21151;&#29575;&#19982;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#22312;&#21069;&#36848;&#38382;&#39064;&#21644;&#20854;&#20182;&#38590;&#39064;&#65288;&#22914;&#20004;&#20010;&#34746;&#26059;&#65289;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISNETS&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32447;&#24615;Thompson&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#21033;&#29992;&#26469;&#33258;gNB&#30340;&#21453;&#39304;&#20449;&#21495;&#65292;UE&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#20256;&#36755;&#36164;&#28304;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.06135</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;URLLC&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32447;&#24615;Thompson&#37319;&#26679;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Distributed Neural Linear Thompson Sampling Framework to Achieve URLLC in Industrial IoT. (arXiv:2401.06135v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISNETS&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32447;&#24615;Thompson&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#21033;&#29992;&#26469;&#33258;gNB&#30340;&#21453;&#39304;&#20449;&#21495;&#65292;UE&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#20256;&#36755;&#36164;&#28304;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#32593;&#32476;&#23558;&#25552;&#20379;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#65288;URLLC&#65289;&#20197;&#25903;&#25345;&#29983;&#20135;&#38142;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#20998;&#37197;&#26080;&#32447;&#36164;&#28304;&#30340;&#26631;&#20934;&#21327;&#35758;&#21487;&#33021;&#26080;&#27861;&#20248;&#21270;&#24310;&#36831;-&#21487;&#38752;&#24615;&#26435;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19978;&#34892;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISNETS&#30340;&#20998;&#24067;&#24335;&#32452;&#21512;&#31070;&#32463;&#32447;&#24615;Thompson&#37319;&#26679;&#65288;DIStributed combinatorial NEural linear Thompson Sampling&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;gNB&#30340;&#21453;&#39304;&#20449;&#21495;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;UE&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#20256;&#36755;&#36164;&#28304;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial Internet of Things (IIoT) networks will provide Ultra-Reliable Low-Latency Communication (URLLC) to support critical processes underlying the production chains. However, standard protocols for allocating wireless resources may not optimize the latency-reliability trade-off, especially for uplink communication. For example, centralized grant-based scheduling can ensure almost zero collisions, but introduces delays in the way resources are requested by the User Equipments (UEs) and granted by the gNB. In turn, distributed scheduling (e.g., based on random access), in which UEs autonomously choose the resources for transmission, may lead to potentially many collisions especially when the traffic increases. In this work we propose DIStributed combinatorial NEural linear Thompson Sampling (DISNETS), a novel scheduling framework that combines the best of the two worlds. By leveraging a feedback signal from the gNB and reinforcement learning, the UEs are trained to autonomously opt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05982</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;(VCM)&#65292;&#20854;&#20013;&#21487;&#21464;&#31995;&#25968;&#20351;&#29992;Delong&#31561;&#20154;(2023)&#30340;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;(CGBM)&#36827;&#34892;&#24314;&#27169;&#12290;&#20351;&#29992;CGBM&#23545;&#31995;&#25968;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;&#36880;&#32500;&#26089;&#20572;&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#32500;&#24230;&#29305;&#23450;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#36824;&#21487;&#20197;&#25581;&#31034;&#32500;&#24230;&#20043;&#38388;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#24046;&#24322;&#12290;&#20351;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#21487;&#20197;&#36827;&#34892;&#31616;&#21333;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#22312;Richman&#21644;W&#252;thrich&#65288;2023&#65289;&#20351;&#29992;&#30340;&#30456;&#21516;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#26679;&#26412;&#22806;&#25439;&#22833;&#26041;&#38754;&#20135;&#29983;&#20102;&#19982;&#20182;&#20204;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM LocalGLMnet&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and W\"uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05717</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#20013;&#30340;&#31867;&#29109;&#27979;&#37327;&#36827;&#34892;&#20998;&#21106;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition. (arXiv:2401.05717v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#36755;&#20986;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#30340;&#21487;&#33021;&#24615;&#12290;&#20854;&#21407;&#29702;&#26159;&#65292;&#29109;&#30340;&#20540;&#24212;&#35813;&#22312;&#20004;&#20010;&#30001;&#35782;&#21035;&#32593;&#32476;&#24456;&#22909;&#24314;&#27169;&#65288;&#24050;&#30693;&#65289;&#30340;&#29255;&#27573;&#20043;&#38388;&#30340;&#36807;&#28193;&#38468;&#36817;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24230;&#37327;&#12290;&#31867;&#29109;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31867;&#30340;&#21518;&#39564;&#27010;&#29575;&#22312;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#20013;&#26159;&#21487;&#29992;&#30340;&#12290;&#29109;&#21644;&#19968;&#20123;&#22522;&#20110;&#29109;&#24046;&#24322;&#30340;&#24230;&#37327;&#34987;&#21333;&#29420;&#21644;&#32452;&#21512;&#20351;&#29992;&#12290;&#29992;&#20110;&#39044;&#27979;&#36793;&#30028;&#30340;&#20915;&#31574;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#38408;&#20540;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#19981;&#31561;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;C&#65288;&#22312;&#21442;&#32771;&#26102;&#38388;&#30340;10&#25110;20&#27627;&#31186;&#20869;&#39044;&#27979;&#30340;&#36793;&#30028;&#25968;&#37327;&#65289;&#19982;&#39044;&#27979;&#36793;&#30028;&#24635;&#25968;&#20043;&#38388;&#30340;&#27604;&#29575;&#21644;&#21484;&#22238;&#29575;&#20026;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the possibility to use the class entropy of the output of a connectionist phoneme recogniser to predict time boundaries between phonetic classes. The rationale is that the value of the entropy should increase in proximity of a transition between two segments that are well modelled (known) by the recognition network since it is a measure of uncertainty. The advantage of this measure is its simplicity as the posterior probabilities of each class are available in connectionist phoneme recognition. The entropy and a number of measures based on differentiation of the entropy are used in isolation and in combination. The decision methods for predicting the boundaries range from simple thresholds to neural network based procedure. The different methods are compared with respect to their precision, measured in terms of the ratio between the number C of predicted boundaries within 10 or 20 msec of the reference and the total number of predicted boundaries, and recall, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.05437</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#25345;&#32493;&#25910;&#38598;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#29992;&#20110;&#25512;&#26029;&#20010;&#20307;&#30340;&#34892;&#20026;&#65292;&#22914;&#30561;&#30496;&#12289;&#20307;&#21147;&#27963;&#21160;&#21644;&#24773;&#32490;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#20852;&#36259;&#21644;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#27880;&#37322;&#26377;&#38480;&#65292;&#24314;&#27169;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#21487;&#31359;&#25140;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;10&#20010;&#29983;&#29702;&#21644;&#34892;&#20026;&#20449;&#21495;&#30340;&#21464;&#21270;&#29575;&#19981;&#21516;&#30340;&#25513;&#30721;&#27604;&#29575;&#65292;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21333;&#35843;&#20449;&#21495;&#21017;&#19981;&#28982;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22635;&#20805;&#31574;&#30053;&#21644;&#25513;&#30721;&#27604;&#29575;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#22256;&#22659;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#26410;&#30693;&#22240;&#26524;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05414</link><description>&lt;p&gt;
&#20851;&#20110;&#37329;&#34701;&#20013;&#30340;&#19977;&#20010;&#22240;&#26524;&#24615;&#22256;&#22659;: &#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors. (arXiv:2401.05414v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#22256;&#22659;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#26410;&#30693;&#22240;&#26524;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22240;&#27492;&#23384;&#22312;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#20998;&#24067;&#30340;&#26102;&#21464;&#24615;-&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#37325;&#35201;&#20294;&#26410;&#30693;/&#26410;&#35266;&#27979;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#30740;&#31350;&#37329;&#34701;&#20013;&#30340;&#36825;&#19977;&#20010;&#22256;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#22240;&#26524;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#26032;&#39062;&#32780;&#26377;&#21551;&#21457;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05363</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;&#22495;&#23545;&#40784;&#23454;&#29616;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#30561;&#30496;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22495;&#27867;&#21270;&#27010;&#24565;&#21040;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#24449;&#23545;&#40784;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepDG&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#21644;&#26102;&#24207;&#29305;&#24449;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#37117;&#24456;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26102;&#20195;&#32423;&#21644;&#24207;&#21015;&#32423;&#29305;&#24449;&#23545;&#40784;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#20195;&#32423;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#30561;&#30496;&#26102;&#20195;&#30340;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02300</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02300
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#29256;&#26412;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#12290;&#26631;&#20934;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#30001;PDE&#25551;&#36848;&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#12290;&#35813;&#32593;&#32476;&#22312;&#30001;&#29289;&#29702;&#22495;&#21644;&#36793;&#30028;&#38543;&#26426;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;PINNs&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#30001;PDE&#21644;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#20256;&#32479;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;PDE&#30340;&#24378;&#27531;&#24046;&#12290;&#36825;&#31181;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#23545;&#30495;&#23454;&#35823;&#24046;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#21487;&#33021;&#30456;&#24046;&#24456;&#22823;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#22256;&#38590;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#65292;&#25105;&#20204;&#23601;&#19981;&#33021;&#20272;&#35745;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#24050;&#32463;&#20197;&#25152;&#38656;&#30340;&#31934;&#24230;&#25910;&#25947;&#21040;&#35299;&#12290;&#36825;&#22312;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#26102;&#23588;&#20854;&#27491;&#30830;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00280</link><description>&lt;p&gt;
&#25512;&#36827;TTP&#20998;&#26512;&#65306;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#24182;&#25552;&#21319;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation. (arXiv:2401.00280v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#65292;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#27010;&#36848;&#20102;&#25915;&#20987;&#32773;&#21033;&#29992;&#28431;&#27934;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20551;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#65292;&#23545;MITRE ATT&#65286;CK&#26694;&#26550;&#20013;&#30340;TTPs&#30340;&#35299;&#37322;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#26368;&#36817;&#22312;&#30740;&#31350;&#20013;&#25506;&#32034;&#20854;&#22312;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#20013;&#30340;&#29992;&#36884;&#30340;&#28608;&#22686;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#30340;&#30097;&#38382;&#65292;&#20165;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;RoBERTa&#65289;&#21644;&#20165;&#35299;&#30721;&#22120;&#65288;&#20363;&#22914;GPT-3.5&#65289;LLMs&#23545;&#20110;&#29702;&#35299;&#21644;&#24635;&#32467;TTPs&#20197;&#36890;&#30693;&#20998;&#26512;&#20154;&#21592;&#26377;&#20851;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#39044;&#26399;&#30446;&#30340;&#65288;&#21363;&#31574;&#30053;&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#12290;&#26368;&#20808;&#36827;&#30340;LLMs&#24050;&#32463;&#26174;&#31034;&#20986;&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#65292;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#31561;&#20851;&#38190;&#39046;&#22495;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26469;&#20026;&#20165;&#35299;&#30721;&#22120;&#30340;LLMs&#25552;&#21462;&#27599;&#20010;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&amp;CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity. Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#25351;&#26631;&#12289;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#12289;&#39044;&#27979;&#25351;&#26631;&#12289;&#25490;&#24207;&#25351;&#26631;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2312.16015</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Evaluation Techniques for Recommendation Systems. (arXiv:2312.16015v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#25351;&#26631;&#12289;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#12289;&#39044;&#27979;&#25351;&#26631;&#12289;&#25490;&#24207;&#25351;&#26631;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#23545;&#20110;&#29992;&#25143;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#21644;&#28385;&#24847;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#36234;&#26469;&#36234;&#24433;&#21709;&#29992;&#25143;&#30340;&#36873;&#25321;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25216;&#26415;&#24615;&#33021;&#65292;&#32780;&#21464;&#24471;&#23545;&#20110;&#19994;&#21153;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#26631;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#29305;&#24615;&#65292;&#27599;&#20010;&#25351;&#26631;&#19987;&#38376;&#25429;&#25417;&#31995;&#32479;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#30340;&#25351;&#26631;&#65306;&#30456;&#20284;&#24615;&#25351;&#26631;&#65306;&#29992;&#20110;&#37327;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#26426;&#21046;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35780;&#20272;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#65307;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#31995;&#32479;&#26377;&#25928;&#22320;&#35782;&#21035;&#24191;&#27867;&#20294;&#30456;&#20851;&#30340;&#39033;&#30446;&#30340;&#33021;&#21147;&#65307;&#39044;&#27979;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#30340;&#29992;&#25143;&#20559;&#22909;&#30340;&#20934;&#30830;&#24615;&#65307;&#25490;&#24207;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#25512;&#33616;&#39034;&#24207;&#30340;&#26377;&#25928;&#24615;&#65307;&#19994;&#21153;&#25351;&#26631;&#65306;&#29992;&#20110;&#23545;&#40784;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms. As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success. This paper addresses the multifaceted nature of recommendations system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance. We discuss  * Similarity Metrics: to quantify the precision of content-based filtering mechanisms and assess the accuracy of collaborative filtering techniques.  * Candidate Generation Metrics: to evaluate how effectively the system identifies a broad yet relevant range of items.  * Predictive Metrics: to assess the accuracy of forecasted user preferences.  * Ranking Metrics: to evaluate the effectiveness of the order in which recommendations are presented.  * Business Metrics: to align the performance of the recommendat
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;ReLU&#21644;ReLU^2&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20219;&#24847;&#32500;&#24230;&#19978;&#30340;Lagrange&#26377;&#38480;&#20803;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#25110;&#20219;&#24847;&#21333;&#32431;&#24418;&#32593;&#26684;&#19978;&#29983;&#25104;&#19968;&#33324;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2312.14276</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20219;&#24847;&#32500;&#24230;&#19978;&#30340;&#26377;&#38480;&#20803;&#32032;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Finite Elements of Any Order on Arbitrary Dimensions. (arXiv:2312.14276v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;ReLU&#21644;ReLU^2&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20219;&#24847;&#32500;&#24230;&#19978;&#30340;Lagrange&#26377;&#38480;&#20803;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#25110;&#20219;&#24847;&#21333;&#32431;&#24418;&#32593;&#26684;&#19978;&#29983;&#25104;&#19968;&#33324;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;ReLU&#21644;ReLU^2&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20219;&#24847;&#32500;&#24230;&#19978;&#21508;&#31181;&#21333;&#32431;&#24418;&#32593;&#26684;&#20013;&#30340;Lagrange&#26377;&#38480;&#20803;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#20840;&#23616;&#34920;&#36798;Lagrange&#20803;&#32032;&#30340;&#22522;&#20989;&#25968;&#65292;&#26082;&#36866;&#29992;&#20110;&#29305;&#23450;&#32593;&#26684;&#65292;&#20063;&#36866;&#29992;&#20110;&#20219;&#24847;&#32593;&#26684;&#12290;&#36825;&#20123;&#20844;&#24335;&#22522;&#20110;&#20803;&#32032;&#30340;&#20960;&#20309;&#20998;&#35299;&#65292;&#32467;&#21512;&#20102;&#39640;&#32500;&#21333;&#32431;&#24418;&#32593;&#26684;&#12289;&#37325;&#24515;&#22352;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#20803;&#32032;&#30340;&#20840;&#23616;&#22522;&#20989;&#25968;&#30340;&#19968;&#20123;&#35265;&#35299;&#24615;&#21644;&#22522;&#26412;&#24615;&#36136;&#12290;&#36825;&#31181;&#34920;&#31034;&#29702;&#35770;&#20026;&#36825;&#26679;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#39318;&#27425;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;&#29305;&#23450;&#25110;&#20219;&#24847;&#21333;&#32431;&#24418;&#32593;&#26684;&#19978;&#29983;&#25104;&#19968;&#33324;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we establish that deep neural networks employing ReLU and ReLU$^2$ activation functions can effectively represent Lagrange finite element functions of any order on various simplicial meshes in arbitrary dimensions. We introduce two novel formulations for globally expressing the basis functions of Lagrange elements, tailored for both specific and arbitrary meshes. These formulations are based on a geometric decomposition of the elements, incorporating several insightful and essential properties of high-dimensional simplicial meshes, barycentric coordinate functions, and global basis functions of linear elements. This representation theory facilitates a natural approximation result for such deep neural networks. Our findings present the first demonstration of how deep neural networks can systematically generate general continuous piecewise polynomial functions on both specific or arbitrary simplicial meshes.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;Anchor&#31639;&#27861;&#30340;&#20840;&#23616;&#27719;&#24635;&#25216;&#26415;&#65292;&#26088;&#22312;&#35745;&#31639;&#20986;&#23545;&#27169;&#22411;&#24433;&#21709;&#26368;&#22823;&#30340;&#21069;k&#20010;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2312.07991</link><description>&lt;p&gt;
&#21152;&#36895;&#23616;&#37096;&#35299;&#37322;&#30340;&#20840;&#23616;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Accelerating the Global Aggregation of Local Explanations. (arXiv:2312.07991v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07991
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;Anchor&#31639;&#27861;&#30340;&#20840;&#23616;&#27719;&#24635;&#25216;&#26415;&#65292;&#26088;&#22312;&#35745;&#31639;&#20986;&#23545;&#27169;&#22411;&#24433;&#21709;&#26368;&#22823;&#30340;&#21069;k&#20010;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#31361;&#20986;&#26174;&#31034;&#23545;&#25991;&#26723;&#20998;&#31867;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#36755;&#20837;&#26631;&#35760;&#12290;&#20363;&#22914;&#65292;Anchor&#31639;&#27861;&#23545;&#20998;&#31867;&#22120;&#23545;&#26631;&#35760;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#12290;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23616;&#37096;&#35299;&#37322;&#30340;&#20840;&#23616;&#27719;&#24635;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#36825;&#31181;&#27719;&#24635;&#26088;&#22312;&#26816;&#27979;&#23545;&#27169;&#22411;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#25552;&#20379;&#23453;&#36149;&#30340;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#20363;&#22914;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#20197;&#21450;&#21738;&#20123;&#23545;&#25239;&#24615;&#31034;&#20363;&#26292;&#38706;&#20102;&#27169;&#22411;&#30340;&#24369;&#28857;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#27719;&#24635;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#65306;&#31616;&#21333;&#30340;&#23454;&#29616;&#23545;&#27599;&#20010;&#25991;&#26723;&#30340;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#26114;&#36149;&#30340;&#31639;&#27861;&#65292;&#22240;&#27492;&#22312;&#30701;&#26399;&#20998;&#26512;&#20250;&#35805;&#30340;&#33539;&#22260;&#20869;&#23545;&#19968;&#20010;&#26222;&#36890;&#29992;&#25143;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a na\"ive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. % We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-$k$ words with the highest global impact according to different a
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#19978;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32593;&#32476;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.05398</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#24037;&#26234;&#33021;&#30340;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
Generative Network Layer for Communication Systems with Artificial Intelligence. (arXiv:2312.05398v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#19978;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32593;&#32476;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32593;&#32476;&#23618;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#20013;&#38388;&#32593;&#32476;&#33410;&#28857;&#23558;&#25968;&#25454;&#21253;&#20174;&#28304;&#20256;&#36755;&#21040;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#24182;&#20998;&#26512;&#20854;&#23545;&#32593;&#32476;&#20013;&#25152;&#38656;&#25968;&#25454;&#36895;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#20351;&#29992;GenAI&#36741;&#21161;&#33410;&#28857;&#20174;&#21253;&#21547;&#22823;&#24133;&#21387;&#32553;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#22270;&#20687;&#12290;&#22312;&#22270;&#20687;&#36136;&#37327;&#32422;&#26463;&#19979;&#36827;&#34892;&#30340;&#32593;&#32476;&#27969;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#25104;&#32593;&#32476;&#23618;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;100%&#30340;&#25968;&#25454;&#36895;&#29575;&#35201;&#27714;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional role of the network layer is the transfer of packet replicas from source to destination through intermediate network nodes. We present a generative network layer that uses Generative AI (GenAI) at intermediate or edge network nodes and analyze its impact on the required data rates in the network. We conduct a case study where the GenAI-aided nodes generate images from prompts that consist of substantially compressed latent representations. The results from network flow analyses under image quality constraints show that the generative network layer can achieve an improvement of more than 100% in terms of the required data rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20998;&#25955;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO, &#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;</title><link>http://arxiv.org/abs/2311.18787</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#21322;&#20998;&#25955;&#32593;&#32476;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Optimization over Semi-Decentralized Networks. (arXiv:2311.18787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20998;&#25955;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO, &#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#65292;&#36890;&#20449;&#25928;&#29575;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29942;&#39048;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#36890;&#20449;&#21327;&#35758;&#19979;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO&#65292;&#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;PISCO&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#36861;&#36394;&#21644;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20445;&#35777;&#20102;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PISCO&#31639;&#27861;&#22312;&#38750;&#20984;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#37327;&#26041;&#38754;&#65292;PISCO&#31639;&#27861;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In large-scale federated and decentralized learning, communication efficiency is one of the most challenging bottlenecks. While gossip communication -- where agents can exchange information with their connected neighbors -- is more cost-effective than communicating with the remote server, it often requires a greater number of communication rounds, especially for large and sparse networks. To tackle the trade-off, we examine the communication efficiency under a semi-decentralized communication protocol, in which agents can perform both agent-to-agent and agent-to-server communication in a probabilistic manner. We design a tailored communication-efficient algorithm over semi-decentralized networks, referred to as PISCO, which inherits the robustness to data heterogeneity thanks to gradient tracking and allows multiple local updates for saving communication. We establish the convergence rate of PISCO for nonconvex problems and show that PISCO enjoys a linear speedup in terms of the number
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.17431</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25509;&#22320;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32534;&#30721;&#30340;Foundation Models&#65288;FMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;FMs&#36866;&#24212;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#25110;&#22686;&#21152;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23545;&#20854;&#36827;&#34892;&#25509;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;FMs&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25509;&#22320;FMs&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#65292;&#21363;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36890;&#36807;FTL-FM&#21033;&#29992;FMs&#36827;&#34892;&#25509;&#22320;&#30340;&#38656;&#27714;&#24378;&#28872;&#22686;&#38271;&#12290;&#21463;&#21040;FTL-FM&#30740;&#31350;&#30340;&#24378;&#21170;&#22686;&#38271;&#21644;FTL-FM&#23545;&#24037;&#19994;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;FTL-FM&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24314;&#31435;FMs&#30340;&#25509;&#22320;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2311.03865</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#24615;&#36935;&#35265;&#38544;&#31169;&#65306;&#36890;&#36807;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#25506;&#32034;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#27495;&#35270;&#34892;&#20026;&#30340;&#26377;&#20559;&#27169;&#22411;&#30340;&#20844;&#24179;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#20844;&#24179;&#39044;&#27979;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#25968;&#25512;&#26029;&#20986;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#26080;&#25928;&#30340;&#12290;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#35757;&#32451;&#30340;&#27169;&#22411;&#36864;&#21270;&#20026;&#31616;&#21333;&#30340;&#38408;&#20540;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#25915;&#20987;&#24615;&#33021;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#25552;&#39640;&#20102;&#25104;&#21151;&#25915;&#20987;&#30340;&#38590;&#24230;&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#25968;&#25454;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.01797</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#31435;&#19968;&#20010;&#38543;&#26426;&#20256;&#36755;&#26144;&#23556;&#65292;&#23558;&#32463;&#39564;&#35266;&#27979;&#21040;&#30340;&#20294;&#26410;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#19982;&#24050;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#32852;&#31995;&#36215;&#26469;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#26410;&#20805;&#20998;&#21457;&#23637;&#12290;&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#34920;&#26126;&#22312;&#26679;&#26412;&#22823;&#23567;$n$&#21644;&#27169;&#22411;&#23481;&#37327;$m$&#19978;&#37117;&#23384;&#22312;&#22810;&#39033;&#24335;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;($O(n^{-2/5}+m^{-4/5})$)&#65292;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#65288;&#21363;&#25968;&#25454;&#32500;&#24230;&#19981;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#20998;&#24067;&#34987;&#25551;&#32472;&#20026;&#19968;&#31995;&#21015;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13459</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#38750;&#20984;-&#38750;&#20985;&#35757;&#32451;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#32447;&#24615;&#25554;&#20540;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20316;&#20026;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#32447;&#24615;&#25554;&#20540;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#25972;&#33539;&#22260;&#20869;&#30340;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26500;&#36896;&#21487;&#25193;&#23637;&#21040;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#35774;&#32622;&#12290;&#36890;&#36807;&#26367;&#25442;RAPP&#20013;&#30340;&#20869;&#37096;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#37325;&#26032;&#21457;&#29616;&#20102;Lookahead&#31639;&#27861;&#26063;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;Lookahead&#32487;&#25215;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;Lookahead&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
&lt;/p&gt;</description></item><item><title>ZEST&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#29305;&#24449;&#24182;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;ZEST&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.08036</link><description>&lt;p&gt;
ZEST:&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification. (arXiv:2310.08036v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08036
&lt;/p&gt;
&lt;p&gt;
ZEST&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#29305;&#24449;&#24182;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;ZEST&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#31867;&#19982;&#32593;&#32476;&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#65292;&#30001;&#20110;&#27809;&#26377;&#25152;&#26377;&#35774;&#22791;&#65288;&#22240;&#27492;&#27809;&#26377;&#23427;&#20204;&#30340;&#27969;&#37327;&#65289;&#65292;&#20173;&#28982;&#23384;&#22312;&#26080;&#27861;&#20351;&#29992;&#25152;&#26377;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#25805;&#20316;&#38454;&#27573;&#38656;&#35201;&#23545;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#26032;&#35774;&#22791;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZEST&#65292;&#29992;&#20110;&#20998;&#31867;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#35774;&#22791;&#12290;ZEST&#21253;&#25324;&#65306;i&#65289;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32593;&#32476;&#29305;&#24449;&#25552;&#21462;&#22120;SANE&#65292;&#29992;&#20110;&#25552;&#21462;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65307;ii&#65289;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#28508;&#22312;&#29305;&#24449;&#35757;&#32451;&#35299;&#30721;&#22120;&#29983;&#25104;&#20266;&#25968;&#25454;&#65307;iii&#65289;&#19968;&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20266;&#25968;&#25454;&#36827;&#34892;&#35774;&#22791;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#30340;&#29289;&#32852;&#32593;&#27969;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZEST&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#20013;&#32487;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#20943;&#23569;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#35299;&#30721;&#22686;&#30410;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;&#35813;&#20013;&#32487;&#26694;&#26550;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.07987</link><description>&lt;p&gt;
&#35821;&#20041;&#21069;&#21521;&#20013;&#32487;&#65306;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications. (arXiv:2310.07987v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#20013;&#32487;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#20943;&#23569;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#35299;&#30721;&#22686;&#30410;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;&#35813;&#20013;&#32487;&#26694;&#26550;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#32487;&#26694;&#26550;&#65292;&#35821;&#20041;&#21069;&#21521;&#65288;SF&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#30340;&#21512;&#20316;&#36890;&#20449;&#12290;SF&#20013;&#32487;&#25552;&#21462;&#24182;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#65292;&#20943;&#23569;&#20102;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#20869;&#38142;&#36335;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#20855;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#21512;&#20316;&#36890;&#20449;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;TURBO&#21407;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#30340;&#22320;&#30340;&#35299;&#30721;&#22686;&#30410;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;SF&#20013;&#32487;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.00177</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#26364;&#36793;&#30028;&#26465;&#20214;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#27850;&#26494;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#39044;&#22788;&#29702;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#12290;&#27850;&#26494;&#26041;&#31243;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65306;&#23427;&#25511;&#21046;&#30528;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#25968;&#20540;&#31639;&#27861;&#20013;&#20316;&#20026;&#23376;&#38382;&#39064;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#26356;&#24191;&#27867;&#30340;&#26925;&#22278;PDE&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#12290;&#26368;&#27969;&#34892;&#30340;&#27850;&#26494;&#31163;&#25955;&#21270;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#22411;&#31232;&#30095;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#36845;&#20195;&#27714;&#35299;&#22120;&#32467;&#21512;&#24378;&#22823;&#30340;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#25552;&#20379;&#20248;&#21183;&#12290;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36817;&#20284;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#36870;&#31639;&#23376;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#24418;&#29366;&#30340;&#22495;&#21644;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#30340;&#32467;&#26500;&#28608;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#35813;&#26550;&#26500;&#20063;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#20013;&#30340;&#24322;&#36136;&#24615;&#31867;&#21035;&#20998;&#24067;&#38382;&#39064;&#65292;&#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#21644;&#25913;&#36827;&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#20998;&#31867;&#21644;&#24314;&#27169;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38750;&#24658;&#23450;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20998;&#24067;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14062</link><description>&lt;p&gt;
FeCAM&#65306;&#22312;&#20813;&#21435;&#26679;&#26412;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#20013;&#30340;&#24322;&#36136;&#24615;&#31867;&#21035;&#20998;&#24067;&#38382;&#39064;&#65292;&#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#21644;&#25913;&#36827;&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#20998;&#31867;&#21644;&#24314;&#27169;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38750;&#24658;&#23450;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20998;&#24067;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#31105;&#27490;&#20102;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22238;&#39038;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26368;&#36817;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20043;&#21518;&#20923;&#32467;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;CIL&#30340;&#21407;&#22411;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#20923;&#32467;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#29983;&#25104;&#26032;&#30340;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#26681;&#25454;&#21040;&#21407;&#22411;&#30340;&#27431;&#27663;&#36317;&#31163;&#23545;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;&#31867;&#21035;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#27431;&#27663;&#24230;&#37327;&#30340;&#20998;&#31867;&#23545;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#29305;&#24449;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#38750;&#24658;&#23450;&#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27431;&#27663;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#29305;&#24449;&#20998;&#24067;&#26159;&#24322;&#36136;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;CIL&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#24314;&#27169;&#29305;&#24449;&#21327;&#26041;&#24046;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04027</link><description>&lt;p&gt;
TIDE: &#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36523;&#20221;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#19981;&#20844;&#27491;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#24847;&#22806;&#20559;&#35265;&#12290;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#21644;&#21435;&#20559;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#31561;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#19981;&#21487;&#29992;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#25237;&#25918;&#21040;&#31038;&#20250;&#20013;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#23545;&#21382;&#21490;&#19978;&#24369;&#21183;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#36523;&#20221;&#35789;&#27719;&#34920;TIDAL&#65292;&#21253;&#25324;15,123&#20010;&#36523;&#20221;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#35821;&#22659;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#20154;&#21475;&#32479;&#35745;&#31867;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;TIDAL&#24320;&#21457;&#20102;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#36523;&#20221;&#35821;&#22659;&#30340;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#36129;&#29486;&#32773;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21435;&#20559;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#20171;&#32773;&#26469;&#27169;&#25311;&#19968;&#20123;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#31163;&#32447;&#23398;&#20064;&#12289;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2308.15552</link><description>&lt;p&gt;
&#32431;&#25506;&#32034;&#19979;&#30340;&#20013;&#20171;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration under Mediators' Feedback. (arXiv:2308.15552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#20171;&#32773;&#26469;&#27169;&#25311;&#19968;&#20123;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#31163;&#32447;&#23398;&#20064;&#12289;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#26159;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;&#65292;&#27599;&#19968;&#27493;&#20132;&#20114;&#20013;&#23398;&#20064;&#32773;&#36873;&#25321;&#19968;&#20010;&#33218;&#24182;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#22238;&#25253;&#12290;&#22312;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#33218;&#65292;&#21363;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#33218;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;BAI&#38382;&#39064;&#30340;&#39034;&#24207;&#20132;&#20114;&#21327;&#35758;&#65292;&#21363;&#23398;&#20064;&#32773;&#22312;&#27599;&#19968;&#36718;&#20013;&#23545;&#36873;&#25321;&#30340;&#33218;&#20855;&#26377;&#23436;&#20840;&#25511;&#21046;&#26435;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20123;&#20540;&#24471;&#20851;&#27880;&#30340;&#20915;&#31574;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#31163;&#32447;&#23398;&#20064;&#65292;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;BAI&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;&#20013;&#20171;&#32773;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#20013;&#20171;&#32773;&#37117;&#36873;&#25321;&#35201;&#25289;&#21160;&#30340;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. Within the context of best-arm identification (BAI) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. Nevertheless, the sequential interaction protocol of classical BAI problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). For this reason, in this work, we propose a novel strict generalization of the classical BAI problem that we refer to as best-arm identification under mediators' feedback (BAI-MF). More specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10364</link><description>&lt;p&gt;
SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;
&lt;/p&gt;
&lt;p&gt;
SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#33021;&#22815;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#25104;&#20026;&#29289;&#29702;&#31995;&#32479;&#27010;&#29575;&#24314;&#27169;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#32806;&#21512;&#26550;&#26500;&#26080;&#27861;&#36171;&#20104;&#25805;&#20316;&#21407;&#23376;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#27969;SE(3)&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27839;&#38468;&#21152;&#22686;&#24378;&#32500;&#24230;&#36827;&#34892;&#22352;&#26631;&#20998;&#21106;&#30340;&#32806;&#21512;&#27969;&#65292;&#20197;&#20445;&#25345;SE(3)&#21644;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;&#22312;&#27599;&#19968;&#23618;&#20013;&#65292;&#27969;&#23558;&#21407;&#23376;&#30340;&#20301;&#32622;&#26144;&#23556;&#21040;&#23398;&#20064;&#24471;&#21040;&#30340;SE(3)&#19981;&#21464;&#22522;&#19978;&#65292;&#25105;&#20204;&#22312;&#36820;&#22238;&#21040;&#21407;&#22987;&#22522;&#20043;&#21069;&#24212;&#29992;&#26631;&#20934;&#27969;&#21464;&#25442;&#65292;&#22914;&#21333;&#35843;&#20998;&#23376;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#27969;&#20445;&#25345;&#20102;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20135;&#29983;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#26399;&#26395;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#22312;DW4&#12289;LJ13&#21644;QM9&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27969;&#19982;&#31561;&#21464;&#27969;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
&lt;/p&gt;</description></item><item><title>&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10870</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10870
&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#20851;&#20110;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#30456;&#20284;&#34920;&#31034;&#32467;&#26500;&#26469;&#31616;&#21270;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#25910;&#25947;&#36895;&#29575;&#30340;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34920;&#31034;&#24448;&#24448;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24341;&#20837;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#19981;&#21487;&#31616;&#21333;&#24179;&#22343;&#30340;&#38750;&#24179;&#20961;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#38750;&#32447;&#24615;&#34920;&#31034;&#25512;&#23548;&#20986;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CADENCE&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#20840;&#38754;&#30340;&#23545;&#25163;&#20301;&#32622;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06244</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diffusion Based Multi-Agent Adversarial Tracking. (arXiv:2307.06244v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CADENCE&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#20840;&#38754;&#30340;&#23545;&#25163;&#20301;&#32622;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#36861;&#36394;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25171;&#20987;&#27602;&#21697;&#36208;&#31169;&#34892;&#21160;&#20013;&#65292;&#23545;&#25239;&#24615;&#30446;&#26631;&#30340;&#20301;&#32622;&#20449;&#24687;&#24448;&#24448;&#26159;&#26377;&#38480;&#30340;&#12290;&#25913;&#36827;&#33258;&#20027;&#36861;&#36394;&#31995;&#32479;&#23558;&#20351;&#26080;&#20154;&#26426;&#12289;&#27700;&#38754;&#33328;&#33351;&#21644;&#27700;&#19979;&#22120;&#26800;&#33021;&#22815;&#26356;&#22909;&#22320;&#21327;&#21161;&#25171;&#20987;&#20351;&#29992;&#20154;&#24037;&#27700;&#38754;&#33337;&#21482;&#12289;&#21322;&#28508;&#33351;&#21644;&#33322;&#31354;&#22120;&#30340;&#36208;&#31169;&#29359;&#12290;&#38543;&#30528;&#26080;&#20154;&#26426;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#30340;&#33258;&#20027;&#30446;&#26631;&#20272;&#35745;&#23545;&#23433;&#20840;&#21644;&#20445;&#38556;&#26356;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CADENCE&#30340;&#32422;&#26463;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#25193;&#25955;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#36861;&#36394;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#23545;&#25163;&#20301;&#32622;&#30340;&#20840;&#38754;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36861;&#36394;&#29615;&#22659;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#20272;&#35745;&#27599;&#20010;&#29983;&#25104;&#36712;&#36857;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited. Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels. As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety. This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information. To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory. We propose a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36951;&#24536;&#22312;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#35821;&#35328;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#23558;PLMs&#24212;&#29992;&#20110;&#26032;&#35821;&#35328;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#36825;&#26159;&#20351;&#23427;&#20204;&#30340;&#33021;&#21147;&#26222;&#36941;&#21487;&#35775;&#38382;&#30340;&#22721;&#22418;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20026;&#26032;&#35821;&#35328;&#23398;&#20064;&#26032;&#30340;&#23884;&#20837;&#23618;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#20570;&#26082;&#28010;&#36153;&#25968;&#25454;&#21448;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#65292;&#20316;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;PLMs&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#27599;K&#27425;&#26356;&#26032;&#26102;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#25105;&#20204;&#40723;&#21169;PLM&#22312;&#26377;&#38480;&#27425;&#26356;&#26032;&#20869;&#25552;&#39640;&#23398;&#20064;&#26032;&#23884;&#20837;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;RoBERTa&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#36951;&#24536;&#26426;&#21046;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#36866;&#24212;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20302;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11086</link><description>&lt;p&gt;
&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#21464;&#20998;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21457;&#23637;&#23545;&#20110; NISQ &#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#31639;&#27861;&#38656;&#35201;&#30701;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#36825;&#31181;&#30005;&#36335;&#26356;&#26131;&#20110;&#22312;&#36817;&#26399;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#20063;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26377;&#36259;&#30340;&#31639;&#27861;&#26159;&#25152;&#35859;&#30340;&#21464;&#20998;&#23545;&#35282;&#21270;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31639;&#27861;&#23376;&#20363;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#20197;&#37327;&#23376;&#29366;&#24577;&#32534;&#30721;&#30340;&#25968;&#25454;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#36776;&#37327;&#23376;&#24577;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#31995;&#32479;&#30340;&#32416;&#32544;&#24615;&#36136;&#65292;&#25110;&#32773;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22312;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#20219;&#21153;&#20013;&#25152;&#38656;&#30005;&#36335;&#38750;&#24120;&#27973;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#30005;&#36335;&#28145;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.06251</link><description>&lt;p&gt;
&#36890;&#20449;&#31995;&#32479;&#20013;AI&#36890;&#29992;&#24615;&#19982;&#21487;&#25193;&#23637;&#24615;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#20449;&#31995;&#32479;&#20013;&#35299;&#20915;&#22797;&#26434;&#21644;&#21160;&#24577;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#32988;&#20219;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#20219;&#21153;&#30340;AI&#24212;&#29992;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#26465;&#20214;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#65292;&#20351;&#24471;&#31639;&#27861;&#26080;&#27861;&#36866;&#24212;&#20110;&#24120;&#35265;&#30340;&#32593;&#32476;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#21487;&#20197;&#22312;&#32593;&#32476;&#29615;&#22659;&#12289;&#24847;&#22270;&#21644;&#25511;&#21046;&#20219;&#21153;&#19978;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#21644;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;AI&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26550;&#26500;&#23558;&#20013;&#22830;&#21270;&#23398;&#20064;&#21151;&#33021;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#25968;&#25454;&#37319;&#38598;&#21151;&#33021;&#20998;&#31163;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#24179;&#22343;&#20540;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65292;&#20174;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.19265</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#20852;&#21327;&#26041;&#24046;&#36827;&#34892;&#27010;&#29575;&#35745;&#31639;&#65306;&#36208;&#21521;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification. (arXiv:2305.19265v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#24179;&#22343;&#20540;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65292;&#20174;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#40065;&#26834;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#36890;&#36807;&#27010;&#29575;&#35270;&#35282;&#37327;&#21270;&#21644;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#27169;&#20223;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27010;&#29575;&#35745;&#31639;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25130;&#26029;&#27010;&#29575;&#34920;&#31034;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#21363;&#24179;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#32593;&#32476;&#30340;&#30830;&#23450;&#24615;&#26367;&#20195;&#21697;&#26469;&#23454;&#20363;&#21270;&#35813;&#26694;&#26550;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#31616;&#21333;&#28608;&#27963;&#30340;&#32452;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#23553;&#35013;&#20102;&#24179;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30340;&#38750;&#32447;&#24615;&#32806;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#24179;&#22343;&#20540;&#21463;&#21040;&#30417;&#30563;&#20197;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#26102;&#65292;&#20174;&#20854;&#19982;&#21327;&#26041;&#24046;&#30340;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building robust, interpretable, and secure artificial intelligence system requires some degree of quantifying and representing uncertainty via a probabilistic perspective, as it allows to mimic human cognitive abilities. However, probabilistic computation presents significant challenges due to its inherent complexity. In this paper, we develop an efficient and interpretable probabilistic computation framework by truncating the probabilistic representation up to its first two moments, i.e., mean and covariance. We instantiate the framework by training a deterministic surrogate of a stochastic network that learns the complex probabilistic representation via combinations of simple activations, encapsulating the non-linearities coupling of the mean and covariance. We show that when the mean is supervised for optimizing the task objective, the unsupervised covariance spontaneously emerging from the non-linear coupling with the mean faithfully captures the uncertainty associated with model p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09853</link><description>&lt;p&gt;
&#29992;&#26377;&#25928;&#30340;&#35270;&#37326;&#36830;&#25509;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21364;&#22833;&#36133;&#24471;&#38750;&#24120;&#20005;&#37325;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24212;&#35813;&#33021;&#22815;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#39044;&#27979;&#23454;&#38469;&#24615;&#33021;&#30340;&#30028;&#38480;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;&#29702;&#35770;&#36824;&#27809;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;155&#20010;MDP&#30340;&#26032;&#25968;&#25454;&#38598;BRIDGE&#65292;&#23558;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#20043;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20808;&#21069;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#24847;&#24819;&#19981;&#21040;&#30340;&#24615;&#36136;&#65306;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;&#30340;Q&#20540;&#20063;&#26159;&#26368;&#39640;&#30340;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;&#22522;&#20110;&#36825;&#19968;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20854;&#27010;&#25324;&#20026;&#19968;&#20010;&#26032;&#30340;MDP&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;&#26377;&#25928;&#30340;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.08172</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#29702;&#35770;&#32467;&#26500;&#36880;&#28176;&#24471;&#21040;&#20102;&#38416;&#26126;&#12290;Imaizumi-Fukumizu&#65288;2019&#65289;&#21644;Suzuki&#65288;2019&#65289;&#25351;&#20986;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20809;&#28369;&#20989;&#25968;&#26102;&#65292;DNN&#30340;&#23398;&#20064;&#33021;&#21147;&#20248;&#20110;&#20808;&#21069;&#30340;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#20247;&#22810;&#30740;&#31350;&#23581;&#35797;&#22312;&#27809;&#26377;&#20219;&#20309;&#32479;&#35745;&#35770;&#35777;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#30740;&#31350;&#65292;&#25506;&#31350;&#30495;&#27491;&#33021;&#22815;&#24341;&#21457;&#26799;&#24230;&#19979;&#38477;&#30340;DNN&#26550;&#26500;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#24615;&#65292;&#36825;&#19968;&#23581;&#35797;&#20284;&#20046;&#26356;&#36148;&#36817;&#23454;&#38469;DNN&#12290;&#26412;&#25991;&#23558;&#30446;&#26631;&#20989;&#25968;&#38480;&#21046;&#20026;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#22312;ReLU-DNN&#20013;&#26500;&#36896;&#20102;&#19968;&#20010;&#31232;&#30095;&#19988;&#20855;&#26377;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.06815</link><description>&lt;p&gt;
&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31867;&#26032;&#22411;&#35745;&#31639;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23454;&#29616;"&#32534;&#31243;"&#12290;&#25105;&#20204;&#20173;&#22312;&#23398;&#20064;&#22914;&#20309;&#26368;&#22909;&#22320;"&#32534;&#31243;"&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#36825;&#26679;&#19968;&#31181;&#30452;&#35273;&#20986;&#21457;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#22312;&#22788;&#29702;&#32534;&#30721;&#20219;&#21153;&#26102;&#20250;&#26377;&#19968;&#31995;&#21015;&#24847;&#35782;&#21644;&#26080;&#24847;&#35782;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;&#23545;&#20110;&#19968;&#20010;&#20989;&#25968;&#32780;&#35328;&#65292;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#21487;&#33021;&#21253;&#25324;&#21442;&#25968;&#21644;&#23616;&#37096;&#21464;&#37327;&#21517;&#31216;&#12289;&#36820;&#22238;&#34920;&#36798;&#24335;&#12289;&#31616;&#21333;&#30340;&#21069;&#32622;&#21644;&#21518;&#32622;&#26465;&#20214;&#20197;&#21450;&#22522;&#26412;&#30340;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#24182;&#35780;&#20272;&#26159;&#21542;&#20351;&#29992;&#26174;&#24335;&#28155;&#21152;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06686</link><description>&lt;p&gt;
OKRidge: &#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637; k &#31232;&#30095;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#65292;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#21487;&#20197;&#35777;&#26126;&#26368;&#20248;&#24615;&#65292;&#20197;&#30830;&#23450;&#39537;&#21160;&#22522;&#30784;&#21160;&#24577;&#30340;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; OKRidge &#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#19979;&#30028;&#35745;&#31639;&#26041;&#27861;&#65292;&#28041;&#21450;&#38797;&#28857;&#20844;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#32447;&#24615;&#31995;&#32479;&#25110;&#22522;&#20110; ADMM &#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21478;&#19968;&#20010;&#32447;&#24615;&#31995;&#32479;&#21644;&#21333;&#35843;&#22238;&#24402;&#38382;&#39064;&#26469;&#26377;&#25928;&#22320;&#35745;&#31639;&#36817;&#31471;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21160;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27874;&#26463;&#25628;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#27604;&#21830;&#19994;&#27714;&#35299;&#22120; Gurobi &#35299;&#20915;&#30340;&#29616;&#26377; MIP&#20844;&#24335;&#36816;&#34892;&#26102;&#38388;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Score matching&#30340;&#20056;&#31215;Jacobi-Theta Boltzmann&#26426;&#22120;&#65288;pJTBM&#65289;&#65292;&#23427;&#27604;&#21407;&#22987;&#30340;RTBM&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#27010;&#29575;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.05910</link><description>&lt;p&gt;
&#20351;&#29992;Score matching&#30340;&#20056;&#31215;Jacobi-Theta Boltzmann&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Product Jacobi-Theta Boltzmann machines with score matching. (arXiv:2303.05910v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Score matching&#30340;&#20056;&#31215;Jacobi-Theta Boltzmann&#26426;&#22120;&#65288;pJTBM&#65289;&#65292;&#23427;&#27604;&#21407;&#22987;&#30340;RTBM&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#27010;&#29575;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#30340;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20511;&#37492;Boltzmann&#26426;&#22120;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20056;&#31215;Jacobi-Theta Boltzmann&#26426;&#22120;&#65288;pJTBM&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;Riemann-Theta Boltzmann&#26426;&#22120;&#65288;RTBM&#65289;&#30340;&#21463;&#38480;&#29256;&#26412;&#65292;&#20855;&#26377;&#23545;&#35282;&#38544;&#34255;&#37096;&#20998;&#36830;&#25509;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Fisher&#25955;&#24230;&#30340;Score matching&#21487;&#20197;&#29992;&#26469;&#27604;&#21407;&#22987;&#30340;RTBM&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;pJTBM&#30340;&#27010;&#29575;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of probability density functions is a non trivial task that over the last years has been tackled with machine learning techniques. Successful applications can be obtained using models inspired by the Boltzmann machine (BM) architecture. In this manuscript, the product Jacobi-Theta Boltzmann machine (pJTBM) is introduced as a restricted version of the Riemann-Theta Boltzmann machine (RTBM) with diagonal hidden sector connection matrix. We show that score matching, based on the Fisher divergence, can be used to fit probability densities with the pJTBM more efficiently than with the original RTBM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.07946</link><description>&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#32479;&#19968;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23436;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24320;&#21457;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#30001;&#22806;&#37096;&#30417;&#30563;&#21592;&#25351;&#23450;&#30340;&#22870;&#21169;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#28041;&#21450;&#37096;&#20998;&#35266;&#27979;&#65292;&#24418;&#24335;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#34892;&#21160;&#21644;&#35266;&#27979;&#35760;&#24518;&#25110;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#29615;&#22659;&#30340;&#30495;&#23454;&#29366;&#24577;&#26469;&#35299;&#20915;POMDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#38543;&#26102;&#38388;&#32858;&#21512;&#35266;&#27979;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#26679;&#26412;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20851;&#27880;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#24573;&#35270;&#20102;&#25512;&#26029;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;&#22312;POMDP&#20013;&#21046;&#23450;&#30340;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31216;&#20026;&#26399;&#26395;&#33258;&#30001;&#33021;&#65288;EFE&#65289;&#30340;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#12290;&#36825;&#25552;&#20379;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#65288;&#23500;&#26377;&#24320;&#21457;&#24615;&#65289;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;</title><link>http://arxiv.org/abs/2211.05408</link><description>&lt;p&gt;
&#29992;&#26680;&#26031;&#22374;&#31163;&#24046;&#25511;&#21046;&#30697;
&lt;/p&gt;
&lt;p&gt;
Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#29992;&#20110;&#34913;&#37327;&#20998;&#24067;&#36924;&#36817;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#30446;&#26631;&#23494;&#24230;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#26102;&#35745;&#31639;&#12290;&#26174;&#33879;&#30340;&#24212;&#29992;&#21253;&#25324;&#35786;&#26029;&#36817;&#20284;MCMC&#37319;&#26679;&#22120;&#21644;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#36866;&#37197;&#24230;&#26816;&#39564;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;KSD&#30340;&#25910;&#25947;&#25511;&#21046;&#24615;&#36136;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#29992;&#20110;&#24369;&#25910;&#25947;&#25511;&#21046;&#30340;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#19979;&#28216;&#25193;&#25955;KSD&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#23545;&#20110;&#27599;&#20010;$q&gt;0$&#65292;&#31532;&#19968;&#32452;&#24050;&#30693;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2210.06015</link><description>&lt;p&gt;
EC-NAS: &#38754;&#21521;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33021;&#32791;&#24863;&#30693;&#34920;&#26684;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36873;&#25321;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#12289;&#35757;&#32451;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12289;&#36866;&#29992;&#20110;&#23454;&#38469;&#36793;&#32536;/&#31227;&#21160;&#35745;&#31639;&#29615;&#22659;&#24182;&#20855;&#26377;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#33021;&#25928;&#20316;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#30340;&#19968;&#39033;&#39069;&#22806;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#26550;&#26500;&#30340;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#26032;&#30340;&#34920;&#26684;&#22522;&#20934; EC-NAS &#20197;&#22312;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#35780;&#20272; NAS &#31574;&#30053;&#12290;EC-NAS &#36824;&#21253;&#25324;&#29992;&#20110;&#39044;&#27979;&#33021;&#32791;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26377;&#21161;&#20110;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tripletformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#25277;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#25554;&#20540;&#12290;Tripletformer&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#30830;&#23450;&#22320;&#25554;&#20540;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2210.02091</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#35268;&#21017;&#25277;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#25554;&#20540;&#30340;Tripletformer
&lt;/p&gt;
&lt;p&gt;
Tripletformer for Probabilistic Interpolation of Irregularly sampled Time Series. (arXiv:2210.02091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tripletformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#25277;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#25554;&#20540;&#12290;Tripletformer&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#30830;&#23450;&#22320;&#25554;&#20540;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#22825;&#25991;&#23398;&#21644;&#27668;&#20505;&#31185;&#23398;&#20013;&#65292;&#35266;&#23519;&#21040;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#25277;&#26679;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#25554;&#20540;&#23545;&#20110;&#35832;&#22914;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#21644;&#21307;&#23398;&#35786;&#26029;&#20197;&#21450;&#24179;&#28369;&#19981;&#35268;&#21017;&#25110;&#22024;&#26434;&#30340;&#25968;&#25454;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Tripletformer"&#30340;&#26032;&#22411;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#25277;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#25554;&#20540;&#12290;&#35813;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#25805;&#20316;&#20110;&#35266;&#27979;&#38598;&#21512;&#19978;&#65292;&#20854;&#20013;&#27599;&#20010;&#20803;&#32032;&#30001;&#26102;&#38388;&#12289;&#36890;&#36947;&#21644;&#20540;&#32452;&#25104;&#12290;Tripletformer&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#35774;&#35745;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25152;&#25552;&#20379;&#30340;&#38598;&#21512;&#20803;&#32032;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23558;Tripletformer&#19982;&#19968;&#31995;&#21015;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#23427;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#30830;&#23450;&#30340;&#25554;&#20540;&#32467;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25554;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled time series data with missing values is observed in many fields like healthcare, astronomy, and climate science. Interpolation of these types of time series is crucial for tasks such as root cause analysis and medical diagnosis, as well as for smoothing out irregular or noisy data. To address this challenge, we present a novel encoder-decoder architecture called "Tripletformer" for probabilistic interpolation of irregularly sampled time series with missing values. This attention-based model operates on sets of observations, where each element is composed of a triple of time, channel, and value. The encoder and decoder of the Tripletformer are designed with attention layers and fully connected layers, enabling the model to effectively process the presented set elements. We evaluate the Tripletformer against a range of baselines on multiple real-world and synthetic datasets and show that it produces more accurate and certain interpolations. Results indicate an improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24369;SINDy&#26041;&#27861;&#29983;&#25104;&#30340;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#20195;&#29702;&#21160;&#21147;&#23398;&#30340;&#25910;&#25947;&#24615;&#20197;&#21450;&#20195;&#29702;&#27169;&#22411;&#35299;&#19982;&#30495;&#23454;&#35299;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.15573</link><description>&lt;p&gt;
&#24369;SINDy&#20195;&#29702;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of weak-SINDy Surrogate Models. (arXiv:2209.15573v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24369;SINDy&#26041;&#27861;&#29983;&#25104;&#30340;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#20195;&#29702;&#21160;&#21147;&#23398;&#30340;&#25910;&#25947;&#24615;&#20197;&#21450;&#20195;&#29702;&#27169;&#22411;&#35299;&#19982;&#30495;&#23454;&#35299;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19968;&#31181;Sparse Identification of Nonlinear Dynamics (SINDy)&#26041;&#27861;&#30340;&#21464;&#31181;&#29983;&#25104;&#30340;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19968;&#20123;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#25216;&#26415;&#65292;&#21363;SINDy&#12289;&#24369;SINDy&#21644;&#21344;&#25454;&#26680;&#26041;&#27861;&#12290;&#22312;&#20551;&#35774;&#21160;&#21147;&#23398;&#26159;&#19968;&#32452;&#22522;&#20989;&#25968;&#30340;&#26377;&#38480;&#32447;&#24615;&#32452;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#30697;&#38453;&#26041;&#31243;&#26469;&#24674;&#22797;&#31995;&#25968;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#24182;&#20026;&#24369;SINDy&#25216;&#26415;&#24314;&#31435;&#20102;&#25237;&#24433;&#24615;&#36136;&#12290;&#22312;&#27010;&#36848;&#20043;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#24369;SINDy&#29983;&#25104;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#35823;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32473;&#23450;&#35299;&#30340;&#19968;&#20010;&#32452;&#21512;&#31639;&#23376;&#26377;&#30028;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#20195;&#29702;&#21160;&#21147;&#23398;&#25910;&#25947;&#20110;&#30495;&#23454;&#21160;&#21147;&#23398;&#65292;(ii)&#20195;&#29702;&#27169;&#22411;&#30340;&#35299;&#19982;&#30495;&#23454;&#35299;&#30456;&#24403;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we give an in-depth error analysis for surrogate models generated by a variant of the Sparse Identification of Nonlinear Dynamics (SINDy) method. We start with an overview of a variety of non-linear system identification techniques, namely, SINDy, weak-SINDy, and the occupation kernel method. Under the assumption that the dynamics are a finite linear combination of a set of basis functions, these methods establish a matrix equation to recover coefficients. We illuminate the structural similarities between these techniques and establish a projection property for the weak-SINDy technique. Following the overview, we analyze the error of surrogate models generated by a simplified version of weak-SINDy. In particular, under the assumption of boundedness of a composition operator given by the solution, we show that (i) the surrogate dynamics converges towards the true dynamics and (ii) the solution of the surrogate model is reasonably close to the true solution. Finally, as an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.07898</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#21327;&#21516;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#39318;&#20808;&#65292;&#26412;&#22320;&#21508;&#26041;&#20174;&#31169;&#26377;&#25968;&#25454;&#20013;&#26500;&#24314;&#38477;&#32500;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#20174;&#20849;&#20139;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#20174;&#20542;&#21521;&#20998;&#25968;&#20013;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#20943;&#23569;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#20154;&#24037;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#21333;&#29420;&#20998;&#26512;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#36965;&#24863;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.11892</link><description>&lt;p&gt;
DDPM-CD: &#20197;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20316;&#20026;&#21464;&#21270;&#26816;&#27979;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection. (arXiv:2206.11892v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#36965;&#24863;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#23545;&#20110;&#20102;&#35299;&#22320;&#29699;&#34920;&#38754;&#21160;&#24577;&#12289;&#20419;&#36827;&#29615;&#22659;&#21464;&#21270;&#30417;&#27979;&#12289;&#35780;&#20272;&#20154;&#31867;&#24433;&#21709;&#12289;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#21644;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#29616;&#25104;&#30340;&#12289;&#26410;&#26631;&#35760;&#30340;&#36965;&#24863;&#22270;&#20687;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#35757;&#32451;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;-&#19968;&#31867;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DDPM&#36890;&#36807;&#36880;&#28176;&#23558;&#35757;&#32451;&#22270;&#20687;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#22312;&#25512;&#29702;&#65288;&#21363;&#37319;&#26679;&#65289;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#39640;&#26031;&#22122;&#22768;&#36215;&#22987;&#65292;&#29983;&#25104;&#19968;&#32452;&#25509;&#36817;&#35757;&#32451;&#20998;&#24067;&#30340;&#22810;&#26679;&#26679;&#26412;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#30340;&#37325;&#28857;&#19981;&#26159;&#22270;&#20687;&#21512;&#25104;&#65292;&#32780;&#26159;&#23558;&#20854;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#24212;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing change detection is crucial for understanding the dynamics of our planet's surface, facilitating the monitoring of environmental changes, evaluating human impact, predicting future trends, and supporting decision-making. In this work, we introduce a novel approach for change detection that can leverage off-the-shelf, unlabeled remote sensing images in the training process by pre-training a Denoising Diffusion Probabilistic Model (DDPM) - a class of generative models used in image synthesis. DDPMs learn the training data distribution by gradually converting training images into a Gaussian distribution using a Markov chain. During inference (i.e., sampling), they can generate a diverse set of samples closer to the training distribution, starting from Gaussian noise, achieving state-of-the-art image synthesis results. However, in this work, our focus is not on image synthesis but on utilizing it as a pre-trained feature extractor for the downstream application of change det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#40657;&#30418;&#20248;&#21270;&#22120;&#21644;&#38543;&#26426;&#20248;&#21270;&#22120;&#32972;&#21518;&#30340;&#31070;&#22855;&#65292;&#24182;&#24314;&#31435;&#36215;&#36825;&#20123;&#25216;&#26415;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#22362;&#23454;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2205.00832</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#65292;&#38543;&#26426;&#20248;&#21270;&#21644;&#20854;&#20182;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent, Stochastic Optimization, and Other Tales. (arXiv:2205.00832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#40657;&#30418;&#20248;&#21270;&#22120;&#21644;&#38543;&#26426;&#20248;&#21270;&#22120;&#32972;&#21518;&#30340;&#31070;&#22855;&#65292;&#24182;&#24314;&#31435;&#36215;&#36825;&#20123;&#25216;&#26415;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#22362;&#23454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#40657;&#30418;&#20248;&#21270;&#22120;&#21644;&#38543;&#26426;&#20248;&#21270;&#22120;&#32972;&#21518;&#30340;&#31070;&#22855;&#65292;&#24182;&#24314;&#31435;&#36215;&#36825;&#20123;&#25216;&#26415;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#22362;&#23454;&#22522;&#30784;&#12290;&#36890;&#36807;&#25512;&#23548;&#31616;&#21333;&#30452;&#35273;&#32972;&#21518;&#30340;&#25968;&#23398;&#31574;&#30053;&#65292;&#26412;&#25991;&#23558;&#36825;&#20123;&#30693;&#35782;&#20957;&#32467;&#25104;&#25991;&#23383;&#12290;&#26412;&#25945;&#31243;&#27627;&#19981;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#30340;&#24418;&#24335;&#21644;&#38750;&#24418;&#24335;&#26041;&#38754;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#24076;&#26395;&#21521;&#35835;&#32773;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20197;&#21450;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20160;&#20040;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#12290;&#26799;&#24230;&#19979;&#38477;&#26159;&#25191;&#34892;&#20248;&#21270;&#20219;&#21153;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#20063;&#26159;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20854;&#38543;&#26426;&#29256;&#26412;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;&#26679;&#26412;&#25110;&#19968;&#25209;&#26679;&#26412;&#30340;&#26799;&#24230;&#26469;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to debunk and dispel the magic behind black-box optimizers and stochastic optimizers. It aims to build a solid foundation on how and why the techniques work. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind the strategies. This tutorial doesn't shy away from addressing both the formal and informal aspects of gradient descent and stochastic optimization methods. By doing so, it hopes to provide readers with a deeper understanding of these techniques as well as the when, the how and the why of applying these algorithms.  Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks. Its stochastic version receives attention in recent years, and this is particularly true for optimizing deep neural networks. In deep neural networks, the gradient followed by a single sample or a batch of samples is employed to save computational r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#32500;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;(Self-ONNs)&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#37197;&#32622;&#21644;&#20351;&#29992;&#38750;&#32447;&#24615;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#25552;&#39640;Holter&#24515;&#30005;&#22270;&#30340;&#23792;&#20540;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.02381</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;Holter&#24515;&#30005;&#22270;&#30340;&#40065;&#26834;&#23792;&#20540;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Peak Detection for Holter ECGs by Self-Organized Operational Neural Networks. (arXiv:2110.02381v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#32500;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;(Self-ONNs)&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#37197;&#32622;&#21644;&#20351;&#29992;&#38750;&#32447;&#24615;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#25552;&#39640;Holter&#24515;&#30005;&#22270;&#30340;&#23792;&#20540;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;R&#23792;&#26816;&#27979;&#22120;&#65292;&#20294;&#20854;&#22312;&#20302;&#36136;&#37327;&#21644;&#22122;&#22768;&#24178;&#25200;&#36739;&#22823;&#30340;&#31227;&#21160;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20256;&#24863;&#22120;&#65288;&#22914;Holter&#30417;&#27979;&#20202;&#65289;&#25152;&#37319;&#38598;&#30340;&#20449;&#21495;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#27700;&#24179;&#21487;&#33021;&#20250;&#26126;&#26174;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24050;&#32463;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;Holter&#30417;&#27979;&#20202;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#36739;&#39640;&#65292;&#38656;&#35201;&#29305;&#27530;&#24182;&#34892;&#21270;&#30828;&#20214;&#35774;&#32622;&#36827;&#34892;&#23454;&#26102;&#22788;&#29702;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#20351;&#29992;&#32039;&#20945;&#32593;&#32476;&#37197;&#32622;&#26102;&#20250;&#19979;&#38477;&#12290;&#36825;&#26159;&#39044;&#26399;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#30340;&#23398;&#20064;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20005;&#26684;&#30340;&#21516;&#36136;&#37197;&#32622;&#21644;&#21333;&#19968;&#32447;&#24615;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23792;&#20540;&#26816;&#27979;&#24615;&#33021;&#24182;&#23454;&#29616;&#20248;&#38597;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32500;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;(Self-ONNs)&#12290;
&lt;/p&gt;
&lt;p&gt;
Although numerous R-peak detectors have been proposed in the literature, their robustness and performance levels may significantly deteriorate in low-quality and noisy signals acquired from mobile electrocardiogram (ECG) sensors, such as Holter monitors. Recently, this issue has been addressed by deep 1-D convolutional neural networks (CNNs) that have achieved state-of-the-art performance levels in Holter monitors; however, they pose a high complexity level that requires special parallelized hardware setup for real-time processing. On the other hand, their performance deteriorates when a compact network configuration is used instead. This is an expected outcome as recent studies have demonstrated that the learning performance of CNNs is limited due to their strictly homogenous configuration with the sole linear neuron model. In this study, to further boost the peak detection performance along with an elegant computational efficiency, we propose 1-D Self-Organized ONNs (Self-ONNs) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;1D&#21367;&#31215;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#26102;&#38388;&#22352;&#26631;&#22270;&#22686;&#21152;&#20102;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2106.06147</link><description>&lt;p&gt;
NAAQA: &#19968;&#31181;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;1D&#21367;&#31215;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#26102;&#38388;&#22352;&#26631;&#22270;&#22686;&#21152;&#20102;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#38382;&#31572;&#65288;AQA&#65289;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#20851;&#20110;&#22768;&#23398;&#22330;&#26223;&#20869;&#23481;&#30340;&#33258;&#30001;&#25991;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#20043;&#21069;&#20171;&#32461;&#30340;CLEAR&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;AQA&#22522;&#20934;&#65292;&#21363;CLEAR2&#65292;&#23427;&#24378;&#35843;&#20102;&#22768;&#23398;&#36755;&#20837;&#30340;&#29305;&#23450;&#25361;&#25112;&#65292;&#21253;&#25324;&#22788;&#29702;&#26102;&#38271;&#21464;&#21270;&#30340;&#22330;&#26223;&#21644;&#22312;&#35757;&#32451;&#38598;&#19982;&#27979;&#35797;&#38598;&#20043;&#38388;&#26377;&#19981;&#21516;&#30340;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23427;&#21033;&#29992;&#20102;&#22768;&#23398;&#36755;&#20837;&#30340;&#29305;&#23450;&#23646;&#24615;&#12290;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#19978;&#20351;&#29992;1D&#21367;&#31215;&#26469;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#33021;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26102;&#38388;&#22352;&#26631;&#22270;&#21487;&#20197;&#22686;&#21152;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#24615;&#33021;&#25552;&#39640;&#32422;17&#20010;&#30334;&#20998;&#28857;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#29575;&#22352;&#26631;&#22270;&#23545;&#32593;&#32476;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by ~17 percentage points. On the other hand, frequency coordinate maps have little influen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23725;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#22312;&#26377;&#38480;&#32500;&#24230;&#24773;&#20917;&#19979;&#23545;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2007.12882</link><description>&lt;p&gt;
&#23725;&#20989;&#25968;&#20272;&#35745;&#20013;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.12882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23725;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#22312;&#26377;&#38480;&#32500;&#24230;&#24773;&#20917;&#19979;&#23545;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#25968;&#20540;&#23454;&#39564;&#25581;&#31034;&#20102;&#19968;&#20010;&#30456;&#24403;&#21453;&#30452;&#35273;&#30340;&#30456;&#21464;&#29616;&#35937;&#65292;&#21363;&#26679;&#26412;&#22823;&#23567;&#19982;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20043;&#27604;&#30340;&#20989;&#25968;&#20851;&#31995;&#12290;&#24403;&#21442;&#25968;&#25968;&#37327;$p$&#25509;&#36817;&#26679;&#26412;&#22823;&#23567;$n$&#26102;&#65292;&#27867;&#21270;&#35823;&#24046;&#22686;&#21152;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;$p&gt;n$&#26102;&#23427;&#20877;&#27425;&#24320;&#22987;&#20943;&#23567;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;\cite{belkin2019reconciling}&#20013;&#24341;&#36215;&#20102;&#29702;&#35770;&#30028;&#30340;&#20851;&#27880;&#65292;&#26368;&#36817;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#27604;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#32447;&#24615;&#27169;&#22411;&#20013;&#21442;&#25968;&#21462;&#26368;&#23567;&#33539;&#25968;&#35299;&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#22312;&#24403;$p$&#21644;$n$&#36235;&#20110;&#26080;&#31351;&#22823;&#30340;&#28176;&#36817;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#65288;&#21442;&#35265;\cite{hastie2019surprises}&#65289;&#65292;&#28982;&#21518;&#22312;&#26377;&#38480;&#32500;&#24773;&#20917;&#19979;&#26356;&#20855;&#20307;&#22320;&#38024;&#23545;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65288;&#21442;&#35265;\cite{bartlett2020benign}&#65292;\cite{tsigler2020benign}&#65292;\cite{lecue2022geometrical}&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. This phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \cite{bartlett2020benign}, \cite{tsigler2020benign}, \cite{lecue2022geometrical}. In the p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#20013;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#25506;&#32034;&#21644;&#21033;&#29992;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#32771;&#34385;&#20102;&#19981;&#21516;&#21512;&#20316;&#21442;&#25968;&#19979;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Gittins&#25351;&#25968;&#31616;&#21270;&#20102;&#21333;&#20154;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/1908.01135</link><description>&lt;p&gt;
&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#65292;&#20174;&#31454;&#20105;&#21040;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multiplayer Bandit Learning, from Competition to Cooperation. (arXiv:1908.01135v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.01135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#20013;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#25506;&#32034;&#21644;&#21033;&#29992;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#32771;&#34385;&#20102;&#19981;&#21516;&#21512;&#20316;&#21442;&#25968;&#19979;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Gittins&#25351;&#25968;&#31616;&#21270;&#20102;&#21333;&#20154;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#36825;&#31181;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;k&#20010;&#33218;&#21644;&#20004;&#21517;&#29609;&#23478;&#65292;Alice&#21644;Bob&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#29609;&#23478;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#25509;&#25910;&#21040;&#30456;&#24212;&#30340;&#22870;&#21169;&#65292;&#24182;&#35266;&#23519;&#21040;&#23545;&#26041;&#30340;&#36873;&#25321;&#20294;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#22870;&#21169;&#12290;Alice&#30340;&#25928;&#29992;&#20989;&#25968;&#20026;$\Gamma_A + \lambda \Gamma_B$&#65288;Bob&#30340;&#25928;&#29992;&#20989;&#25968;&#31867;&#20284;&#65289;&#65292;&#20854;&#20013;$\Gamma_A$&#26159;Alice&#30340;&#24635;&#22870;&#21169;&#65292;$\lambda \in [-1, 1]$&#26159;&#21512;&#20316;&#21442;&#25968;&#12290;&#24403;$\lambda = -1$&#26102;&#65292;&#29609;&#23478;&#22312;&#19968;&#20010;&#38646;&#21644;&#28216;&#25103;&#20013;&#31454;&#20105;&#65307;&#24403;$\lambda = 1$&#26102;&#65292;&#20182;&#20204;&#23436;&#20840;&#21512;&#20316;&#65307;&#24403;$\lambda = 0$&#26102;&#65292;&#20182;&#20204;&#26159;&#20013;&#31435;&#30340;&#65306;&#27599;&#20010;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#20026;&#20182;&#20204;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#35813;&#27169;&#22411;&#19982;&#25112;&#30053;&#23454;&#39564;&#32463;&#27982;&#23398;&#25991;&#29486;&#20013;&#20851;&#20110;&#35266;&#23519;&#23545;&#26041;&#22870;&#21169;&#30340;&#30740;&#31350;&#30456;&#20851;&#12290;&#20351;&#29992;&#25240;&#25187;&#22240;&#23376;$\beta$&#65292;Gittins&#25351;&#25968;&#23558;&#21333;&#20154;&#38382;&#39064;&#31616;&#21270;&#20026;&#23545;&#19968;&#20010;&#24102;&#26377;&#20808;&#39564;$\mu$&#30340;&#26377;&#39118;&#38505;&#33218;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice's utility is $\Gamma_A + \lambda \Gamma_B$ (and similarly for Bob), where $\Gamma_A$ is Alice's total reward and $\lambda \in [-1, 1]$ is a cooperation parameter. At $\lambda = -1$ the players are competing in a zero-sum game, at $\lambda = 1$, they are fully cooperating, and at $\lambda = 0$, they are neutral: each player's utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.  With discount factor $\beta$, the Gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $\mu$, 
&lt;/p&gt;</description></item></channel></rss>