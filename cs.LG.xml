<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03731</link><description>&lt;p&gt;
MathCoder: &#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#20013; LLMs &#20013;&#26080;&#32541;&#20195;&#30721;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340; GPT-4 &#20195;&#30721;&#35299;&#37322;&#22120;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#65292;&#29983;&#25104;&#20195;&#30721;&#65292;&#25191;&#34892;&#20195;&#30721;&#65292;&#24182;&#26681;&#25454;&#25191;&#34892;&#36755;&#20986;&#32487;&#32493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24494;&#35843;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#26469;&#24314;&#27169;&#21644;&#25512;&#23548;&#25968;&#23398;&#26041;&#31243;&#65292;&#24182;&#20174;&#32780;&#22686;&#24378;&#20854;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#39062;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; MathCodeInstruct&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#37117;&#20132;&#38169;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25191;&#34892;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102; MathCoder &#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;MathCoder &#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03725</link><description>&lt;p&gt;
&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#30340;&#38543;&#26426;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21160;&#24577;&#27979;&#24230;&#20256;&#36755;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#27969;&#21644;&#25193;&#25955;&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20043;&#38388;&#30340;&#36830;&#32493;&#26102;&#38388;&#26144;&#23556;&#12290;&#25353;&#29031;&#20256;&#32479;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30446;&#26631;&#23494;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#65292;&#32780;&#21478;&#19968;&#20010;&#26159;&#31616;&#21333;&#30340;&#22522;&#30784;&#23494;&#24230;&#65292;&#19982;&#25968;&#25454;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#30340;&#26694;&#26550;&#65292;&#35268;&#33539;&#21270;&#20102;&#22914;&#20309;&#8220;&#32806;&#21512;&#8221;&#22522;&#26412;&#23494;&#24230;&#21644;&#30446;&#26631;&#23494;&#24230;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#26631;&#31614;&#25110;&#36830;&#32493;&#23884;&#20837;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#26500;&#24314;&#21160;&#24577;&#20256;&#36755;&#26144;&#23556;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35299;&#20915;&#31867;&#20284;&#20110;&#26631;&#20934;&#29420;&#31435;&#35774;&#32622;&#30340;&#31616;&#21333;&#24179;&#26041;&#25439;&#22833;&#22238;&#24402;&#38382;&#39064;&#26469;&#23398;&#20064;&#36825;&#20123;&#20256;&#36755;&#26144;&#23556;&#12290;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#24314;&#20381;&#36182;&#32806;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03720</link><description>&lt;p&gt;
HeaP: &#20351;&#29992;LLMs&#36827;&#34892;&#23618;&#27425;&#21270;Web&#21160;&#20316;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#37327;&#25968;&#25454;&#21644;&#38646;-shot&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25945;&#25480;LLMs&#22312;Web&#19978;&#25191;&#34892;&#20219;&#21153;&#38754;&#20020;&#30528;&#22522;&#26412;&#25361;&#25112; - &#32452;&#21512;&#24615;&#22823;&#30340;&#24320;&#25918;&#19990;&#30028;&#20219;&#21153;&#21644;Web&#25509;&#21475;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;LLMs&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#32423;&#30340;&#38381;&#29615;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31574;&#30053;&#26500;&#25104;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#27861;&#65292;&#21363;&#26032;&#30340;Web&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#31574;&#30053;&#30340;&#32452;&#21512;&#26469;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;LLMs&#30340;Hierarchical Policies for Web Actions&#65288;HeaP&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#19968;&#32452;&#23618;&#27425;&#21270;&#30340;LLM&#25552;&#31034;&#26469;&#35268;&#21010;&#39640;&#32423;&#20219;&#21153;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#31574;&#30053;&#25191;&#34892;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#22871;Web&#20219;&#21153;&#65292;&#21253;&#25324;MiniWoB++&#65292;WebArena&#65292;&#27169;&#25311;&#33322;&#31354;CRM&#20197;&#21450;&#23454;&#38469;&#32593;&#31449;&#26469;&#35780;&#20272;HeaP&#19982;&#19968;&#31995;&#21015;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website i
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03718</link><description>&lt;p&gt;
&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19987;&#27880;&#20110;&#35757;&#32451;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#19979;&#33021;&#22815;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23433;&#20840;&#32422;&#26463;&#35201;&#27714;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#22810;&#21151;&#33021;&#23433;&#20840;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#20026;&#26410;&#24320;&#21457;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#20010;&#20027;&#35201;&#38656;&#27714;&#65306;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conditioned Constrained Policy Optimization&#65288;CCPO&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#65288;1&#65289;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#65288;VVE&#65289;&#65292;&#29992;&#20110;&#22312;&#26410;&#35265;&#36807;&#30340;&#38408;&#20540;&#26465;&#20214;&#19979;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#65288;2&#65289;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#65292;&#29992;&#20110;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#32534;&#30721;&#20219;&#24847;&#32422;&#26463;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CCPO&#22312;&#23433;&#20840;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#19981;&#21516;&#32422;&#26463;&#30340;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.03716</link><description>&lt;p&gt;
&#19968;&#26465;&#28459;&#38271;&#20043;&#36335;&#65306;&#25506;&#31350;RLHF&#20013;&#30340;&#38271;&#24230;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#22870;&#21169;&#27169;&#22411;&#20351;&#24471;&#22312;&#36890;&#29992;&#32842;&#22825;&#35774;&#32622;&#20043;&#22806;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#23454;&#39564;&#25104;&#20026;&#21487;&#33021;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#20351;&#31995;&#32479;&#22312;&#32593;&#39029;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#22810;&#36718;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#26356;&#21152;&#8220;&#26377;&#29992;&#8221;&#12290;&#24403;&#20248;&#21270;&#26377;&#29992;&#24615;&#26102;&#65292;&#25105;&#20204;&#19968;&#30452;&#35266;&#23519;&#21040;RLHF&#20250;&#39537;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#38271;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#21709;&#24212;&#38271;&#24230;&#36827;&#34892;&#20248;&#21270;&#26159;RLHF&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#21462;&#24471;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38024;&#23545;&#26377;&#29992;&#24615;&#35757;&#32451;&#30340;&#19977;&#20010;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#19982;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#38271;&#24230;&#19982;&#22870;&#21169;&#24378;&#28872;&#30456;&#20851;&#65292;&#22870;&#21169;&#20998;&#25968;&#30340;&#25552;&#39640;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#25913;&#21464;&#36755;&#20986;&#38271;&#24230;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;RL&#21644;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#24178;&#39044;&#65292;&#30475;&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can ach
&lt;/p&gt;</description></item><item><title>DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.03714</link><description>&lt;p&gt;
DSPy: &#23558;&#22768;&#26126;&#24615;&#35821;&#35328;&#27169;&#22411;&#35843;&#29992;&#32534;&#35793;&#25104;&#33258;&#25105;&#25913;&#36827;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03714
&lt;/p&gt;
&lt;p&gt;
DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ML&#31038;&#21306;&#27491;&#22312;&#24555;&#36895;&#25506;&#32034;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#23558;&#23427;&#20204;&#22534;&#21472;&#25104;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#27969;&#27700;&#32447;&#30340;&#25216;&#26415;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;LM&#27969;&#27700;&#32447;&#36890;&#24120;&#20351;&#29992;&#30828;&#32534;&#30721;&#30340;"&#25552;&#31034;&#27169;&#26495;"&#26469;&#23454;&#29616;&#65292;&#21363;&#36890;&#36807;&#35797;&#38169;&#21457;&#29616;&#30340;&#20887;&#38271;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#26356;&#31995;&#32479;&#22320;&#24320;&#21457;&#21644;&#20248;&#21270;LM&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSPy&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25991;&#26412;&#36716;&#25442;&#22270;&#30340;&#24418;&#24335;&#25277;&#35937;LM&#27969;&#27700;&#32447;&#30340;&#32534;&#31243;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#30340;&#21629;&#20196;&#24335;&#35745;&#31639;&#22270;&#12290;DSPy&#27169;&#22359;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#21644;&#25910;&#38598;&#31034;&#20363;&#26469;&#23398;&#20064;&#22914;&#20309;&#24212;&#29992;&#25552;&#31034;&#12289;&#24494;&#35843;&#12289;&#22686;&#24378;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#20248;&#21270;&#20219;&#20309;DSPy&#27969;&#27700;&#32447;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#31616;&#27905;&#30340;DSPy&#31243;&#24207;&#21487;&#20197;&#34920;&#36798;&#21644;&#20248;&#21270;&#22797;&#26434;&#30340;&#25512;&#29702;&#25968;&#23398;&#38382;&#39064;&#12289;&#30331;&#24405;&#26085;&#24535;&#38382;&#39064;&#31561;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#35745;&#31639;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#36801;&#31227;&#24615;&#30340;&#35268;&#36991;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#12289;&#26410;&#35265;&#25968;&#25454;&#31867;&#21035;&#29978;&#33267;&#38450;&#24481;&#27169;&#22411;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03707</link><description>&lt;p&gt;
OMG-ATTACK:&#33258;&#25105;&#30417;&#30563;&#30340;&#22312;&#27969;&#24418;&#19978;&#29983;&#25104;&#21487;&#36801;&#31227;&#30340;&#35268;&#36991;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#35745;&#31639;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#36801;&#31227;&#24615;&#30340;&#35268;&#36991;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#12289;&#26410;&#35265;&#25968;&#25454;&#31867;&#21035;&#29978;&#33267;&#38450;&#24481;&#27169;&#22411;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#36991;&#25915;&#20987;(EA)&#34987;&#29992;&#26469;&#36890;&#36807;&#25197;&#26354;&#36755;&#20837;&#25968;&#25454;&#26469;&#35823;&#23548;&#27169;&#22411;&#30340;&#20998;&#31867;&#65292;&#20197;&#27979;&#35797;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#21019;&#24314;&#36825;&#20123;&#25915;&#20987;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26085;&#30410;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#35745;&#31639;&#32463;&#27982;&#30340;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#29992;&#20110;&#26410;&#35265;&#40657;&#30418;&#27169;&#22411;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20511;&#37492;&#34920;&#31034;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#22312;&#27969;&#24418;&#19978;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#34987;&#40723;&#21169;&#19982;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#25915;&#20987;&#30456;&#27604;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;&#25915;&#20987;&#35757;&#32451;&#27169;&#22411;&#26102;&#20855;&#26377;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#25915;&#20987;&#26410;&#35265;&#27169;&#22411;&#26102;&#35201;&#26356;&#21152;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#20123;&#25915;&#20987;&#26356;&#21152;&#19982;&#25968;&#25454;&#30456;&#20851;&#32780;&#19981;&#26159;&#19982;&#27169;&#22411;&#26412;&#36523;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#19968;&#33268;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#12289;&#26410;&#35265;&#25968;&#25454;&#31867;&#21035;&#29978;&#33267;&#38450;&#24481;&#27169;&#22411;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#34920;&#26126;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.03696</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#22823;&#31867;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;/&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21464;&#20998;&#20248;&#21270;&#24615;&#65288;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#31639;&#23376;&#21644;k-&#24179;&#38754;&#21464;&#25442;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#34920;&#31034;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#35828;&#26126;&#22312;&#36825;&#20123;Banach&#31354;&#38388;&#19978;&#25552;&#20986;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#30340;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#24687;&#24687;&#30456;&#20851;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#31070;&#32463;&#32593;&#32476;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#21253;&#25324;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#12289;&#33539;&#25968;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;&#22312;&#34180;&#26495;/&#22810;&#27425;&#35856;&#27874;&#26679;&#26465;&#29702;&#35770;&#20013;&#25214;&#21040;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#22312;&#20869;&#30340;&#22810;&#31181;&#32463;&#20856;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36793;&#38469;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26041;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#32473;&#23450;&#27010;&#29575;&#23494;&#24230;&#20316;&#20026;&#36793;&#38469;&#30340;&#32852;&#21512;&#20998;&#24067;&#24182;&#19988;&#35782;&#21035;&#22810;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03695</link><description>&lt;p&gt;
&#22810;&#36793;&#38469;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#38543;&#26426;&#25554;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimarginal generative modeling with stochastic interpolants. (arXiv:2310.03695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36793;&#38469;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26041;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#32473;&#23450;&#27010;&#29575;&#23494;&#24230;&#20316;&#20026;&#36793;&#38469;&#30340;&#32852;&#21512;&#20998;&#24067;&#24182;&#19988;&#35782;&#21035;&#22810;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;K&#20010;&#27010;&#29575;&#23494;&#24230;&#65292;&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#20351;&#36825;&#20123;&#27010;&#29575;&#23494;&#24230;&#20316;&#20026;&#36793;&#38469;&#24674;&#22797;&#12290;&#36825;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#32467;&#26500;&#24212;&#35813;&#33021;&#22815;&#35782;&#21035;&#39044;&#35774;&#36793;&#38469;&#20043;&#38388;&#30340;&#22810;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#30340;&#27867;&#21270;&#20013;&#23545;&#36825;&#20010;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#22522;&#20110;&#21160;&#21147;&#27979;&#37327;&#20256;&#36755;&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#30001;&#36895;&#24230;&#21644;&#35780;&#20998;&#22330;&#23450;&#20041;&#65292;&#21487;&#20197;&#34987;&#34920;&#24449;&#20026;&#31616;&#21333;&#20108;&#27425;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#32773;&#65292;&#24182;&#19988;&#26159;&#22312;&#19968;&#33324;&#21270;&#20102;&#26102;&#38388;&#21464;&#37327;&#30340;&#21333;&#32431;&#24418;&#19978;&#23450;&#20041;&#30340;&#12290;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;&#20256;&#36755;&#21463;&#21040;&#25152;&#26377;&#36793;&#38469;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#25552;&#21462;&#20986;&#22810;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;&#36825;&#20123;&#23545;&#24212;&#20851;&#31995;&#30340;&#30830;&#23450;&#23545;&#20110;&#26679;&#24335;&#36801;&#31227;&#12289;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#25968;&#25454;&#21435;&#27745;&#31561;&#24212;&#29992;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, 
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03693</link><description>&lt;p&gt;
&#35843;&#25972;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24847;&#22270;&#65281;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03693
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#29992;&#20363;&#30340;&#20248;&#21270;&#36890;&#24120;&#28041;&#21450;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;Meta&#21457;&#24067;Llama&#27169;&#22411;&#21644;OpenAI&#30340;GPT-3.5 Turbo&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;API&#20063;&#40723;&#21169;&#36825;&#31181;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#23433;&#20840;&#25104;&#26412;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#23433;&#20840;&#23545;&#40784;&#22522;&#30784;&#35774;&#26045;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#28085;&#30422;&#24403;&#24494;&#35843;&#29305;&#26435;&#25193;&#23637;&#32473;&#32456;&#31471;&#29992;&#25143;&#26102;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32418;&#38431;&#30740;&#31350;&#21457;&#29616;&#65292;&#21482;&#38656;&#20960;&#20010;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#23601;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;10&#20010;&#36825;&#26679;&#30340;&#31034;&#20363;&#22312;OpenAI&#30340;API&#20013;&#20197;&#19981;&#21040;0.20&#32654;&#20803;&#30340;&#25104;&#26412;&#23558;GPT-3.5 Turbo&#30340;&#23433;&#20840;&#20445;&#25252;&#35299;&#38500;&#20102;&#65292;&#20351;&#27169;&#22411;&#23545;&#20960;&#20046;&#20219;&#20309;&#26377;&#23475;&#25351;&#20196;&#37117;&#26377;&#21709;&#24212;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious 
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24265;&#20215;&#30340;Hadamard&#21464;&#25442;&#26469;&#23454;&#29616;&#25972;&#25968;&#30697;&#38453;&#20056;&#27861;&#30340;&#20302;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#24179;&#21488;&#19978;&#23454;&#29616;&#35745;&#31639;&#21644;&#20869;&#23384;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03675</link><description>&lt;p&gt;
&#20351;&#29992;&#25972;&#25968;&#36827;&#34892;Hadamard&#22495;&#35757;&#32451;&#30340;&#31867;&#22686;&#37327;&#37327;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24265;&#20215;&#30340;Hadamard&#21464;&#25442;&#26469;&#23454;&#29616;&#25972;&#25968;&#30697;&#38453;&#20056;&#27861;&#30340;&#20302;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#24179;&#21488;&#19978;&#23454;&#29616;&#35745;&#31639;&#21644;&#20869;&#23384;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35768;&#22810;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#65292;&#23427;&#20801;&#35768;&#29616;&#22330;&#36866;&#24212;&#21644;&#26356;&#26032;&#65292;&#21253;&#25324;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#12289;&#24494;&#35843;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#20855;&#26377;&#38544;&#31169;&#21644;&#20302;&#24310;&#36831;&#35201;&#27714;&#30340;&#24212;&#29992;&#65292;&#36830;&#32493;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#21487;&#33021;&#20250;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#24179;&#21488;&#36896;&#25104;&#25104;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#23436;&#20840;&#37327;&#21270;&#35757;&#32451;&#65288;FQT&#65289;&#20943;&#23569;&#35745;&#31639;&#31934;&#24230;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#28608;&#36827;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#25972;&#25968;FQT&#65292;&#24448;&#24448;&#20250;&#23558;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#21040;&#26080;&#27861;&#25509;&#21463;&#30340;&#27700;&#24179;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24265;&#20215;&#30340;Hadamard&#21464;&#25442;&#23454;&#29616;&#20165;&#36890;&#36807;&#25972;&#25968;&#30697;&#38453;&#20056;&#27861;&#36827;&#34892;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#21738;&#20123;&#24352;&#37327;&#38656;&#35201;&#38543;&#26426;&#33293;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#22359;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#20302;&#20301;&#23485;&#32047;&#21152;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#35780;&#20272;&#35774;&#35745;&#20316;&#20026;&#20986;&#21457;&#28857;&#65292;&#30740;&#31350;&#20102;&#22312;&#24418;&#24335;&#21644;&#37327;&#21270;&#35780;&#20272;&#20013;&#30340;&#25112;&#30053;&#34892;&#20026;&#20197;&#21450;&#35780;&#20272;&#32773;&#21644;&#34987;&#35780;&#20272;&#23545;&#35937;&#20043;&#38388;&#30340;&#36947;&#24503;&#21028;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03655</link><description>&lt;p&gt;
&#25112;&#30053;&#35780;&#20272;&#65306;&#23545;&#35937;&#12289;&#35780;&#20272;&#32773;&#21644;&#31038;&#20250;
&lt;/p&gt;
&lt;p&gt;
Strategic Evaluation: Subjects, Evaluators, and Society. (arXiv:2310.03655v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#35780;&#20272;&#35774;&#35745;&#20316;&#20026;&#20986;&#21457;&#28857;&#65292;&#30740;&#31350;&#20102;&#22312;&#24418;&#24335;&#21644;&#37327;&#21270;&#35780;&#20272;&#20013;&#30340;&#25112;&#30053;&#34892;&#20026;&#20197;&#21450;&#35780;&#20272;&#32773;&#21644;&#34987;&#35780;&#20272;&#23545;&#35937;&#20043;&#38388;&#30340;&#36947;&#24503;&#21028;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#22312;&#24418;&#24335;&#19978;&#21644;&#37327;&#21270;&#20013;&#27169;&#31946;&#27010;&#24565;&#65288;&#22914;&#20215;&#20540;&#65289;&#30340;&#24230;&#37327;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20197;&#20570;&#20986;&#20915;&#31574;&#12290;&#24403;&#20154;&#20204;&#20026;&#20102;&#33719;&#24471;&#26377;&#21033;&#30340;&#20915;&#31574;&#32467;&#26524;&#32780;&#23545;&#36825;&#20123;&#35780;&#20272;&#20316;&#20986;&#25112;&#30053;&#24615;&#22238;&#24212;&#26102;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#21487;&#33021;&#21463;&#21040;&#36947;&#24503;&#21028;&#26029;&#12290;&#20182;&#20204;&#21487;&#33021;&#34987;&#25551;&#36848;&#20026;&#8220;&#25805;&#32437;&#31995;&#32479;&#8221;&#25110;&#8220;&#20316;&#24330;&#8221;&#65292;&#25110;&#32773;&#65288;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65289;&#25237;&#20837;&#8220;&#35802;&#23454;&#21162;&#21147;&#8221;&#25110;&#8220;&#25913;&#36827;&#8221;&#12290;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#23545;&#25112;&#30053;&#34892;&#20026;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24378;&#35843;&#20102;&#20915;&#31574;&#23545;&#35937;&#20026;&#33719;&#24471;&#26356;&#26377;&#21033;&#35780;&#20272;&#25152;&#20184;&#20986;&#30340;&#21162;&#21147;&#8212;&#8212;&#19968;&#20123;&#20316;&#21697;&#25552;&#20379;&#20102;&#39044;&#38450;&#25110;&#38459;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#21306;&#20998;&#20102;&#8220;&#25805;&#32437;&#8221;&#21644;&#8220;&#25913;&#36827;&#8221;&#34892;&#20026;&#65292;&#32780;&#21478;&#19968;&#20123;&#26088;&#22312;&#27979;&#37327;&#20998;&#31867;&#31995;&#32479;&#30340;&#21162;&#21147;&#36127;&#25285;&#25110;&#19981;&#24179;&#31561;&#25928;&#26524;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#19981;&#21516;&#30340;&#20986;&#21457;&#28857;&#24320;&#22987;&#65306;&#35780;&#20272;&#35774;&#35745;&#26412;&#36523;&#23601;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#35780;&#20272;&#32773;&#25152;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#19982;&#31038;&#20250;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
A broad current application of algorithms is in formal and quantitative measures of murky concepts -- like merit -- to make decisions. When people strategically respond to these sorts of evaluations in order to gain favorable decision outcomes, their behavior can be subjected to moral judgments. They may be described as 'gaming the system' or 'cheating,' or (in other cases) investing 'honest effort' or 'improving.' Machine learning literature on strategic behavior has tried to describe these dynamics by emphasizing the efforts expended by decision subjects hoping to obtain a more favorable assessment -- some works offer ways to preempt or prevent such manipulations, some differentiate 'gaming' from 'improvement' behavior, while others aim to measure the effort burden or disparate effects of classification systems. We begin from a different starting point: that the design of an evaluation itself can be understood as furthering goals held by the evaluator which may be misaligned with bro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#29289;&#29702;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#21147;&#23398;&#20013;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36739;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03652</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#24230;&#31232;&#30095;&#21270;&#65306;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21457;&#29616;&#22312;&#21147;&#23398;&#20013;
&lt;/p&gt;
&lt;p&gt;
Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics. (arXiv:2310.03652v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#29289;&#29702;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#21147;&#23398;&#20013;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36739;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26412;&#26500;&#24314;&#27169;&#22312;&#29289;&#29702;&#23398;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#36731;&#26494;&#22320;&#34701;&#20837;&#29289;&#29702;&#21644;&#26426;&#21046;&#32422;&#26463;&#65292;&#24182;&#19988;&#33021;&#22815;&#20811;&#26381;&#25551;&#36848;&#29289;&#36136;&#21709;&#24212;&#30340;&#34920;&#35937;&#26412;&#26500;&#23450;&#24459;&#30340;&#36153;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#26500;&#23450;&#24459;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#36807;&#22810;&#65292;&#25152;&#29983;&#25104;&#30340;&#34920;&#31034;&#24418;&#24335;&#19981;&#23481;&#26131;&#35299;&#37322;&#12290;&#23384;&#22312;&#30528;&#31232;&#30095;&#22238;&#24402;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#34920;&#36798;&#24335;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#21019;&#24314;&#27169;&#22411;&#24418;&#24335;&#30340;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24211;&#25152;&#25552;&#20379;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#28369;&#30340;$L^{0}$&#27491;&#21017;&#21270;&#35757;&#32451;&#29289;&#29702;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;&#36825;&#26088;&#22312;&#32500;&#25345;&#23545;&#30495;&#23454;&#24773;&#20917;&#30340;&#24314;&#27169;&#33021;&#21147;&#30340;&#21516;&#26102;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow to obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain the trus
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03647</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Fairness for Human-AI Collaboration. (arXiv:2310.03647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03647
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20154;&#31867;&#20915;&#31574;&#32773;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#26102;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#25110;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20844;&#24179;&#31639;&#27861;&#30340;&#36873;&#25321;&#24615;&#36981;&#23432;&#20250;&#30456;&#23545;&#20110;&#20154;&#31867;&#20197;&#21069;&#30340;&#25919;&#31574;&#22686;&#21152;&#27495;&#35270;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#20844;&#24179;&#32467;&#26524;&#38656;&#35201;&#22522;&#26412;&#19981;&#21516;&#30340;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#30830;&#20445;&#23545;&#20915;&#31574;&#32773;&#65288;&#20107;&#20808;&#19981;&#30693;&#36947;&#65289;&#30340;&#36981;&#23432;&#27169;&#24335;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#65292;&#26080;&#35770;&#20154;&#31867;&#30340;&#36981;&#23432;&#27169;&#24335;&#22914;&#20309;&#65292;&#23427;&#20204;&#37117;&#33021;&#30830;&#20445;&#22312;&#20915;&#31574;&#20013;&#25913;&#21892;&#20844;&#24179;&#24615;&#65288;&#24369;&#24418;&#24847;&#20041;&#19978;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#24615;&#33021;&#25913;&#36827;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#31639;&#27861;&#25512;&#33616;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are s
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03646</link><description>&lt;p&gt;
TRAM: &#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03646
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#19981;&#26159;&#20851;&#27880;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#20026;&#20102;&#40723;&#21169;&#20445;&#30041;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#20219;&#21306;&#22495;&#36793;&#30028;&#22312;&#36825;&#20004;&#31181;&#20248;&#21270;&#34920;&#38754;&#19978;&#36890;&#30693;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#32479;&#19968;&#20102;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Trust Region Aware Minimization (TRAM)&#65292;&#19968;&#31181;&#20248;&#21270;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#20808;&#35757;&#32451;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TRAM&#20248;&#20110;&#38160;&#24230;&#24863;&#30693;&#21644;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;&#65292;&#24182;&#24471;&#21040;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03641</link><description>&lt;p&gt;
&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional PAC-Learning from Nisan's Natural Proofs. (arXiv:2310.03641v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;&#65292;&#24182;&#24471;&#21040;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Carmosino&#31561;&#20154;&#35777;&#26126;&#20102;Lambda&#30340;&#33258;&#28982;&#35777;&#26126;&#24847;&#21619;&#30528;&#21487;&#20197;&#36890;&#36807;&#22343;&#21248;&#20998;&#24067;&#12289;&#25104;&#21592;&#26597;&#35810;&#26469;&#39640;&#25928;&#23398;&#20064;Lambda&#30005;&#36335;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#36825;&#20010;&#25512;&#35770;&#25512;&#24191;&#21040;&#19981;&#21253;&#21547;AC^0[p]&#30340;Lambda&#21644;&#22312;Valiant&#30340;PAC&#27169;&#22411;&#20013;&#20351;&#29992;&#38543;&#26426;&#31034;&#20363;&#21644;&#20219;&#24847;&#31034;&#20363;&#20998;&#24067;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#32467;&#26524;&#12290;&#28040;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22914;&#26524;&#23545;&#20110;&#27599;&#20010;&#30005;&#36335;&#31867;Lambda&#65292;&#20174;Lambda&#30340;&#33258;&#28982;&#35777;&#26126;&#21040;&#22312;Valiant&#30340;PAC&#27169;&#22411;&#20013;&#23398;&#20064;Lambda&#30005;&#36335;&#30340;&#25512;&#35770;&#25104;&#31435;&#65292;&#21017;&#23545;O(n^{1.5})-uSVP&#65288;&#21807;&#19968;&#30340;&#26368;&#30701;&#21521;&#37327;&#38382;&#39064;&#65289;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;O(n^{1.5})-SVP&#65288;&#26368;&#30701;&#21521;&#37327;&#38382;&#39064;&#65289;&#21644;O(n^{1.5})-SIVP&#65288;&#26368;&#30701;&#29420;&#31435;&#21521;&#37327;&#38382;&#39064;&#65289;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#37327;&#23376;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Abridged) Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds for \Lambda imply efficient algorithms for learning \Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \AC^0[p] \subseteq \Lambda. We consider whether this implication can be generalized to \Lambda \not\supseteq \AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions. We give results of both positive and negative flavor.  On the negative side, we observe that if, for every circuit class \Lambda, the implication from natural proofs for \Lambda to learning \Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem). This indicates that whether natural pro
&lt;/p&gt;</description></item><item><title>CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03635</link><description>&lt;p&gt;
CLEVRER-Humans: &#29992;&#20154;&#31867;&#30340;&#26041;&#24335;&#25551;&#36848;&#29289;&#29702;&#21644;&#22240;&#26524;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03635
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#25512;&#29702;&#29289;&#29702;&#20107;&#20214;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#26426;&#22120;&#23545;&#20110;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#28789;&#27963;&#20114;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#29289;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#22522;&#20934;&#37117;&#20165;&#22522;&#20110;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#20107;&#20214;&#31867;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;&#20108;&#26159;&#22522;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22240;&#26524;&#20851;&#31995;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVRER-Humans&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#29289;&#29702;&#20107;&#20214;&#30340;&#22240;&#26524;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#20219;&#21153;&#65292;&#20197; eliciting &#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#26032;&#34920;&#31034;&#26041;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#20107;&#20214;&#22270; (CEGs)&#65307;&#20854;&#27425;&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#39046;&#22495;&#36827;&#34892;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#33258;&#30001;&#24230;&#29289;&#20307;&#30340;&#24314;&#27169;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#27979;&#35797;&#20013;&#65292;&#25152;&#23398;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#30340;&#24046;&#36317;&#20165;&#20026;2%&#12290;</title><link>http://arxiv.org/abs/2310.03624</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#39640;&#33258;&#30001;&#24230;&#21160;&#24577;&#31070;&#32463;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#39046;&#22495;&#36827;&#34892;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#33258;&#30001;&#24230;&#29289;&#20307;&#30340;&#24314;&#27169;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#27979;&#35797;&#20013;&#65292;&#25152;&#23398;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#30340;&#24046;&#36317;&#20165;&#20026;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#26159;&#26426;&#22120;&#20154;&#29289;&#29702;&#24418;&#24577;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#22312;&#27809;&#26377;&#32463;&#20856;&#20960;&#20309;&#36816;&#21160;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#24403;&#21518;&#32773;&#24456;&#38590;&#24037;&#31243;&#21270;&#25110;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#20986;&#29616;&#24847;&#22806;&#21464;&#21270;&#26102;&#65292;&#26080;&#20154;&#33258;&#24314;&#27169;&#26159;&#30495;&#27491;&#33258;&#20027;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#24517;&#35201;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#39046;&#22495;&#26469;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20197;&#31070;&#32463;&#38544;&#24335;&#26597;&#35810;&#27169;&#22411;&#30340;&#24418;&#24335;&#33258;&#24314;&#27169;&#20854;&#36816;&#21160;&#23398;&#65292;&#35813;&#27169;&#22411;&#20165;&#36890;&#36807;&#24102;&#26377;&#30456;&#26426;&#23039;&#24577;&#21644;&#37197;&#32622;&#30340;2D&#22270;&#20687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#21160;&#24577;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#30340;&#31070;&#32463;&#23494;&#24230;&#22330;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#36866;&#24212;&#39640;&#33258;&#30001;&#24230;&#65288;DOFs&#65289;&#30340;&#26465;&#20214;&#12290;&#22312;&#19968;&#20010;7&#33258;&#30001;&#24230;&#30340;&#26426;&#22120;&#20154;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#23398;&#24471;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#20043;&#38388;&#30340;Chamfer-L2&#36317;&#31163;&#20165;&#20026;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of
&lt;/p&gt;</description></item><item><title>CLASSify&#26159;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#33021;&#22815;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#35757;&#32451;&#21644;&#32467;&#26524;&#29983;&#25104;&#65292;&#21487;&#35270;&#21270;&#21644;&#25968;&#25454;&#27934;&#23519;&#21151;&#33021;&#65292;&#25903;&#25345;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#35775;&#38382;&#12290;&#23427;&#36824;&#33021;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#25903;&#25345;&#29305;&#24449;&#35780;&#20272;&#21644;&#35299;&#37322;&#24615;&#24471;&#20998;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.03618</link><description>&lt;p&gt;
CLASSify: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#20110;Web&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
CLASSify: A Web-Based Tool for Machine Learning. (arXiv:2310.03618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03618
&lt;/p&gt;
&lt;p&gt;
CLASSify&#26159;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#33021;&#22815;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#35757;&#32451;&#21644;&#32467;&#26524;&#29983;&#25104;&#65292;&#21487;&#35270;&#21270;&#21644;&#25968;&#25454;&#27934;&#23519;&#21151;&#33021;&#65292;&#25903;&#25345;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#35775;&#38382;&#12290;&#23427;&#36824;&#33021;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#25903;&#25345;&#29305;&#24449;&#35780;&#20272;&#21644;&#35299;&#37322;&#24615;&#24471;&#20998;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#38382;&#39064;&#24456;&#24120;&#35265;&#65292;&#20294;&#25191;&#34892;&#27169;&#22411;&#35757;&#32451;&#12289;&#20248;&#21270;&#21644;&#25512;&#29702;&#25152;&#38656;&#30340;&#25216;&#26415;&#30693;&#35782;&#21487;&#33021;&#20250;&#38459;&#27490;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#36825;&#39033;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#38382;&#39064;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#32467;&#26524;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#21644;&#23545;&#25968;&#25454;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#35775;&#38382;&#12290;&#22312;&#30028;&#38754;&#20869;&#21487;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#12289;&#24179;&#34913;&#31867;&#21035;&#26631;&#31614;&#25110;&#29983;&#25104;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#23427;&#36824;&#25903;&#25345;&#29305;&#24449;&#35780;&#20272;&#65292;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#24471;&#20998;&#26469;&#25351;&#31034;&#21738;&#20123;&#29305;&#24449;&#23545;&#36755;&#20986;&#24433;&#21709;&#26368;&#22823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CLASSify&#65292;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#26080;&#38656;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning classification problems are widespread in bioinformatics, but the technical knowledge required to perform model training, optimization, and inference can prevent researchers from utilizing this technology. This article presents an automated tool for machine learning classification problems to simplify the process of training models and producing results while providing informative visualizations and insights into the data. This tool supports both binary and multiclass classification problems, and it provides access to a variety of models and methods. Synthetic data can be generated within the interface to fill missing values, balance class labels, or generate entirely new datasets. It also provides support for feature evaluation and generates explainability scores to indicate which features influence the output the most. We present CLASSify, an open-source tool for simplifying the user experience of solving classification problems without the need for knowledge of mach
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#38754;&#20020;&#30340;&#28431;&#27934;&#21644;&#23545;&#31038;&#20250;&#30340;&#19981;&#33391;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#23545;&#25163;&#37325;&#26032;&#23450;&#20041;&#20026;&#30431;&#21451;&#65292;&#26469;&#23454;&#29616;&#26377;&#30410;&#20110;&#31038;&#20250;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.03614</link><description>&lt;p&gt;
&#29992;&#20110;&#31038;&#20250;&#20844;&#30410;&#30340;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65306;&#23558;&#23545;&#25163;&#37325;&#26032;&#23450;&#20041;&#20026;&#30431;&#21451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally. (arXiv:2310.03614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03614
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#38754;&#20020;&#30340;&#28431;&#27934;&#21644;&#23545;&#31038;&#20250;&#30340;&#19981;&#33391;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#23545;&#25163;&#37325;&#26032;&#23450;&#20041;&#20026;&#30431;&#21451;&#65292;&#26469;&#23454;&#29616;&#26377;&#30410;&#20110;&#31038;&#20250;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#36827;&#23637;&#30340;&#20027;&#35201;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;DNN&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#26679;&#26412;&#26469;&#36843;&#20351;&#22522;&#20110;DNN&#30340;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;AdvML&#65289;&#24341;&#36215;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#27169;&#24577;&#19979;&#30740;&#31350;&#20102;&#36825;&#20123;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;DNN&#23481;&#26131;&#21253;&#21547;&#20869;&#23884;&#20559;&#35265;&#65292;&#24182;&#19988;&#32463;&#24120;&#20135;&#29983;&#38590;&#20197;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21453;&#31038;&#20250;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#31561;&#26032;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#22686;&#21152;&#20102;&#22823;&#35268;&#27169;&#20135;&#29983;&#21453;&#31038;&#20250;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#20026;&#31038;&#20250;&#20844;&#30410;&#32780;&#36827;&#34892;&#30340;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;AdvML4G&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#23427;&#21033;&#29992;AdvML&#28431;&#27934;&#26469;&#21457;&#26126;&#26377;&#30410;&#20110;&#31038;&#20250;&#30340;&#24212;&#29992;&#12290;&#30417;&#31649;&#26426;&#26500;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#21512;&#20316;&#65292;&#25512;&#21160;&#26377;&#30410;&#20110;&#31038;&#20250;&#30340;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#65288;FedSGDA+&#21644;FedSGDA-M&#65289;&#65292;&#24182;&#22312;&#26368;&#24120;&#35265;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#38477;&#20302;&#20102;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;FedSGDA+&#31639;&#27861;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;O(&#949;^{-6})&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984;PL&#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#35777;&#26126;&#20102;FedSGDA-M&#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03613</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#65288;FedSGDA+&#21644;FedSGDA-M&#65289;&#65292;&#24182;&#22312;&#26368;&#24120;&#35265;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#38477;&#20302;&#20102;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;FedSGDA+&#31639;&#27861;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;O(&#949;^{-6})&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984;PL&#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#35777;&#26126;&#20102;FedSGDA-M&#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#22330;&#26223;&#65292;&#21253;&#25324;&#23545;&#25239;&#35757;&#32451;&#12289;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#20197;&#21450; AUROC &#26368;&#22823;&#21270;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#25361;&#25112;&#65292;&#22312;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;&#65288;&#21363;&#21333;&#26426;&#65289;&#29615;&#22659;&#19979;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#22312; FL &#19979;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#31639;&#27861;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#32852;&#37030;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; FL &#31639;&#27861;&#65288;FedSGDA+ &#21644; FedSGDA-M&#65289;&#24182;&#38477;&#20302;&#20102;&#26368;&#24120;&#35265;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#29616;&#26377;&#22797;&#26434;&#24230;&#32467;&#26524;&#12290;&#23545;&#20110;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FedSGDA+ &#24182;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040; $O(\varepsilon^{-6})$&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984; PL &#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; FedSGDA-M &#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp
&lt;/p&gt;</description></item><item><title>GENER&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;GENER&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03611</link><description>&lt;p&gt;
GENER:&#19968;&#31181;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data. (arXiv:2310.03611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03611
&lt;/p&gt;
&lt;p&gt;
GENER&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;GENER&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24050;&#30693;&#22522;&#22240;&#34920;&#36798;&#21644;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#21457;&#29616;&#26032;&#30340;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#21508;&#31181;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#22522;&#22240;&#34920;&#36798;&#27169;&#24335;&#26469;&#39044;&#27979;&#26032;&#30340;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#21033;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENER&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#26469;&#35782;&#21035;&#22522;&#22240;-&#22522;&#22240;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#35757;&#32451;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32467;&#21512;&#20102;BioGRID&amp;DREAM5&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.834&#30340;&#24179;&#22343;AUROC&#20998;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&amp;DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29486;&#32508;&#36848;&#27604;&#36739;&#20102;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03606</link><description>&lt;p&gt;
&#23558;&#29992;&#20110;&#30740;&#31350;&#35770;&#25991;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#19982;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#30340;&#27604;&#36739;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review. (arXiv:2310.03606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29486;&#32508;&#36848;&#27604;&#36739;&#20102;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29486;&#32508;&#36848;&#26088;&#22312;&#27604;&#36739;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#23545;2020&#24180;1&#26376;&#33267;2023&#24180;7&#26376;&#21457;&#34920;&#30340;&#33521;&#25991;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25628;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#38750;&#27954;COVID-19&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#35813;&#36807;&#31243;&#20351;&#29992;&#20102;&#21253;&#25324;PubMed&#12289;&#35895;&#27468;&#23398;&#26415;&#12289;Scopus&#21644;&#31185;&#23398;&#24341;&#25991;&#32034;&#24341;&#31561;&#22810;&#31181;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#35770;&#25991;&#32463;&#36807;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#21462;&#20102;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#24615;&#33021;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#23398;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#30149;&#27602;&#20256;&#25773;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#65292;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#32771;&#34385;&#36825;&#20123;&#35265;&#35299;&#65292;&#20197;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#24182;&#25506;&#32034;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa. The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa. A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process. The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models. The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus. The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different appr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03597</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#20351;&#29992;&#26410;&#30693;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25277;&#26679;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#27966;&#29983;&#30340;&#31639;&#27861;&#20026;&#31639;&#27861;&#24320;&#21457;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#36890;&#36807;&#23457;&#26597;&#36825;&#31181;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#36825;&#31181;&#25277;&#26679;&#26041;&#27861;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#25277;&#26679;&#30340;&#20219;&#20309;&#23454;&#20363;&#21270;&#37117;&#38656;&#35201;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#21644;&#19968;&#20010;&#24230;&#37327;&#26469;&#30830;&#23450;&#27969;&#21160;&#65292;&#20197;&#21450;&#27969;&#21160;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#25512;&#23548;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#20855;&#26377;&#21807;&#19968;&#30340;&#29305;&#24449;&#65288;&#22312;&#25152;&#26377;f-&#25955;&#24230;&#20013;&#65289;&#65292;&#21363;&#30001;&#20854;&#24471;&#21040;&#30340;&#26799;&#24230;&#27969;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#20174;&#19981;&#21464;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#24230;&#37327;&#30340;&#36873;&#25321;&#12290;Fisher-Rao&#24230;&#37327;&#34987;&#31216;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
&lt;/p&gt;</description></item><item><title>TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03589</link><description>&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG]) - &#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03589
&lt;/p&gt;
&lt;p&gt;
TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TimeGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#24050;&#24314;&#31435;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;TimeGPT&#30340;&#38646;-shot&#25512;&#29702;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#30340;&#20854;&#20182;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20026;&#20154;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#35299;&#37322;&#21644;&#33258;&#21160;&#24494;&#20998;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#30340;&#31243;&#24207;&#65292;&#24182;&#25104;&#21151;&#35745;&#31639;&#20986;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.03585</link><description>&lt;p&gt;
&#36328;&#26465;&#20214;&#20998;&#25903;&#30340;&#33258;&#21160;&#24494;&#20998;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Smoothing Methods for Automatic Differentiation Across Conditional Branches. (arXiv:2310.03585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#35299;&#37322;&#21644;&#33258;&#21160;&#24494;&#20998;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#30340;&#31243;&#24207;&#65292;&#24182;&#25104;&#21151;&#35745;&#31639;&#20986;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#24341;&#20837;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#31243;&#24207;&#23545;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#21709;&#24212;&#26354;&#38754;&#20855;&#26377;&#19968;&#23450;&#24179;&#28369;&#24615;&#30340;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#24179;&#28369;&#35299;&#37322;&#65288;SI&#65289;&#26159;&#19968;&#31181;&#25277;&#35937;&#35299;&#37322;&#24418;&#24335;&#65292;&#23427;&#20197;&#39640;&#26031;&#26680;&#36817;&#20284;&#31243;&#24207;&#36755;&#20986;&#30340;&#21367;&#31215;&#65292;&#20174;&#32780;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#24179;&#28369;&#20854;&#36755;&#20986;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SI&#19982;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#12290;&#19982;&#22312;&#24120;&#35268;&#31243;&#24207;&#25191;&#34892;&#20013;&#36827;&#34892;&#30340;&#33258;&#21160;&#24494;&#20998;&#19981;&#21516;&#65292;&#36825;&#20123;&#26799;&#24230;&#36824;&#25429;&#25417;&#20102;&#26367;&#20195;&#25511;&#21046;&#27969;&#36335;&#24452;&#30340;&#24433;&#21709;&#12290;SI&#19982;AD&#30340;&#32452;&#21512;&#20351;&#24471;&#25903;&#25345;&#22522;&#20110;&#26799;&#24230;&#30340;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20363;&#22914;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#23545;&#20223;&#30495;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#25110;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;SI&#20013;&#20026;&#21487;&#34892;&#24615;&#32780;&#36827;&#34892;&#30340;&#36817;&#20284;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#20174;&#21463;&#25439;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#65292;&#23454;&#29616;&#20102;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2310.03581</link><description>&lt;p&gt;
&#20855;&#26377;&#24377;&#24615;&#27493;&#24577;&#30340;&#26412;&#22320;&#23548;&#33322;&#65306;&#23398;&#20064;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#31471;&#21040;&#31471;&#31359;&#36234;
&lt;/p&gt;
&lt;p&gt;
Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End. (arXiv:2310.03581v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#20174;&#21463;&#25439;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#65292;&#23454;&#29616;&#20102;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24517;&#39035;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#23548;&#33322;&#65292;&#21363;&#20351;&#22312;&#21463;&#25439;&#30340;&#22806;&#24863;&#30693;&#25110;&#24863;&#30693;&#22833;&#25928;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#30475;&#19981;&#35265;&#30340;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#26469;&#24341;&#23548;&#25105;&#20204;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#12290;&#19982;&#20381;&#36182;&#21551;&#21457;&#24335;&#21644;&#24322;&#24120;&#26816;&#27979;&#26469;&#26356;&#26032;&#23548;&#33322;&#20449;&#24687;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#25105;&#20204;&#30340;&#23548;&#33322;&#31574;&#30053;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#25439;&#22351;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#23545;&#24863;&#30693;&#22833;&#25928;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#24863;&#30693;&#37117;&#32435;&#20837;&#25105;&#20204;&#30340;&#31574;&#30053;&#36755;&#20837;&#20013;&#65292;&#20174;&#32780;&#20351;&#31574;&#30053;&#33021;&#22815;&#24863;&#30693;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;&#30896;&#25758;&#21644;&#22353;&#27934;&#65292;&#24182;&#24341;&#21457;&#30456;&#24212;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20302;&#24378;&#24230;&#23545;&#25239;&#25915;&#20987;&#21644;&#23545;&#25239;&#34917;&#19969;&#23545;&#20855;&#26377;&#25512;&#24191;&#33021;&#21147;&#30340;NeRFs&#36827;&#34892;&#25915;&#20987;&#65292;&#21518;&#32773;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#39044;&#23450;&#20041;&#36755;&#20986;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03578</link><description>&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#25512;&#24191;&#33021;&#21147;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Targeted Adversarial Attacks on Generalizable Neural Radiance Fields. (arXiv:2310.03578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20302;&#24378;&#24230;&#23545;&#25239;&#25915;&#20987;&#21644;&#23545;&#25239;&#34917;&#19969;&#23545;&#20855;&#26377;&#25512;&#24191;&#33021;&#21147;&#30340;NeRFs&#36827;&#34892;&#25915;&#20987;&#65292;&#21518;&#32773;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#39044;&#23450;&#20041;&#36755;&#20986;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#22330;&#26223;&#34920;&#31034;&#21644;&#28210;&#26579;&#30340;&#24378;&#22823;&#24037;&#20855;-&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#36817;&#26399;&#20986;&#29616;&#12290;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;2D&#35266;&#27979;&#23398;&#20064;&#21512;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#23454;&#29616;&#36924;&#30495;&#19988;&#20132;&#20114;&#24335;&#30340;&#22330;&#26223;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;NeRFs&#22312;&#22686;&#24378;&#29616;&#23454;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#34394;&#25311;&#29615;&#22659;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#21487;&#33021;&#20250;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for 3D scene representation and rendering. These data-driven models can learn to synthesize high-quality images from sparse 2D observations, enabling realistic and interactive scene reconstructions. However, the growing usage of NeRFs in critical applications such as augmented reality, robotics, and virtual environments could be threatened by adversarial attacks.  In this paper we present how generalizable NeRFs can be attacked by both low-intensity adversarial attacks and adversarial patches, where the later could be robust enough to be used in real world applications. We also demonstrate targeted attacks, where a specific, predefined output scene is generated by these attack with success.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#35757;&#32451;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#30340;&#32039;&#20945;&#29305;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#25104;&#27969;&#30340;&#36817;&#20284;&#65292;&#35813;&#36817;&#20284;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20854;&#34928;&#20943;&#36895;&#24230;&#20026;$\Theta_n(\frac{1}{n})$&#65292;&#36825;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03575</link><description>&lt;p&gt;
&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of learning a flow-based generative model from limited sample complexity. (arXiv:2310.03575v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03575
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#35757;&#32451;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#30340;&#32039;&#20945;&#29305;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#25104;&#27969;&#30340;&#36817;&#20284;&#65292;&#35813;&#36817;&#20284;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20854;&#34928;&#20943;&#36895;&#24230;&#20026;$\Theta_n(\frac{1}{n})$&#65292;&#36825;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#19968;&#20010;&#30001;&#20004;&#23618;&#33258;&#32534;&#30721;&#22120;&#21442;&#25968;&#21270;&#30340;&#27969;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#38381;&#24335;&#29305;&#24449;&#21270;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#65292;&#24403;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;$ n $&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#27973;&#23618;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#26102;&#12290;&#22312;&#27492;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#29983;&#25104;&#27969;&#30340;&#23574;&#38160;&#25551;&#36848;&#65292;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#30340;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#36317;&#31163;&#20250;&#34928;&#20943;&#20026;$\Theta_n(\frac{1}{n})$&#12290;&#26368;&#21518;&#65292;&#36825;&#20010;&#36895;&#29575;&#34987;&#35777;&#26126;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number $n$ of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact Bayes-optimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20302;&#20445;&#30495;&#21644;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27531;&#24046;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#39640;&#20445;&#30495;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03572</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#27531;&#24046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Multi-Fidelity Neural Network Computing. (arXiv:2310.03572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20302;&#20445;&#30495;&#21644;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27531;&#24046;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#39640;&#20445;&#30495;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#24265;&#20215;&#30340;&#20302;&#20445;&#30495;&#21644;&#19968;&#20010;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#20026;&#19968;&#20010;&#27531;&#24046;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33021;&#38750;&#32447;&#24615;&#30340;1&#65289;&#27169;&#22411;&#20849;&#20139;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#20302;&#20445;&#30495;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20197;&#21450;2&#65289;&#20004;&#20010;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#21327;&#21516;&#24037;&#20316;&#12290;&#31532;&#19968;&#20010;&#32593;&#32476;&#22312;&#23569;&#37327;&#30340;&#39640;&#20445;&#30495;&#21644;&#20302;&#20445;&#30495;&#25968;&#25454;&#19978;&#23398;&#20064;&#27531;&#24046;&#20989;&#25968;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#36825;&#20010;&#32593;&#32476;&#34987;&#29992;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#21512;&#25104;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#31532;&#20108;&#20010;&#32593;&#32476;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31532;&#20108;&#20010;&#32593;&#32476;&#20316;&#20026;&#25105;&#20204;&#23545;&#39640;&#20445;&#30495;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#25968;&#20540;&#20363;&#23376;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of th
&lt;/p&gt;</description></item><item><title>BID-NeRF&#31639;&#27861;&#25913;&#36827;&#20102;&#22270;&#20687;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#65306;&#24341;&#20837;&#22522;&#20110;&#28145;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#22810;&#22270;&#20687;&#25439;&#22833;&#20989;&#25968;&#65292;&#30465;&#30053;&#20998;&#23618;&#37319;&#26679;&#36807;&#31243;&#65292;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#38388;&#38548;&#23454;&#29616;&#26356;&#39640;&#30340;&#21021;&#22987;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#30340;&#25910;&#25947;&#12290;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#25910;&#25947;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.03563</link><description>&lt;p&gt;
BID-NeRF&#65306;&#20855;&#26377;&#21453;&#21521;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;RGB-D&#22270;&#20687;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields. (arXiv:2310.03563v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03563
&lt;/p&gt;
&lt;p&gt;
BID-NeRF&#31639;&#27861;&#25913;&#36827;&#20102;&#22270;&#20687;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#65306;&#24341;&#20837;&#22522;&#20110;&#28145;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#22810;&#22270;&#20687;&#25439;&#22833;&#20989;&#25968;&#65292;&#30465;&#30053;&#20998;&#23618;&#37319;&#26679;&#36807;&#31243;&#65292;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#38388;&#38548;&#23454;&#29616;&#26356;&#39640;&#30340;&#21021;&#22987;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#30340;&#25910;&#25947;&#12290;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#25910;&#25947;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#25913;&#36827;&#21453;&#21521;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;iNeRF&#65289;&#31639;&#27861;&#65292;&#23558;&#22270;&#20687;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#23450;&#20041;&#20026;&#22522;&#20110;NeRF&#30340;&#36845;&#20195;&#32447;&#24615;&#20248;&#21270;&#12290;NeRF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31354;&#38388;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#21512;&#25104;&#30495;&#23454;&#22330;&#26223;&#25110;&#29289;&#20307;&#30340;&#36924;&#30495;&#26032;&#35270;&#22270;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#25105;&#20204;&#22312;&#23450;&#20301;&#20248;&#21270;&#30446;&#26631;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#28145;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#22270;&#20687;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#20351;&#29992;&#24050;&#30693;&#30456;&#23545;&#23039;&#24577;&#30340;&#22270;&#20687;&#24207;&#21015;&#65292;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#22312;&#20307;&#31215;&#28210;&#26579;&#36807;&#31243;&#20013;&#30465;&#30053;&#20102;&#20998;&#23618;&#37319;&#26679;&#65292;&#24847;&#21619;&#30528;&#21482;&#20351;&#29992;&#31895;&#31961;&#27169;&#22411;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#38388;&#38548;&#21487;&#20197;&#23454;&#29616;&#29978;&#33267;&#26356;&#39640;&#30340;&#21021;&#22987;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#30340;&#25910;&#25947;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20462;&#25913;&#65292;&#25910;&#25947;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#19988;&#25910;&#25947;&#22522;&#20934;&#26174;&#33879;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which defines the image pose estimation problem as a NeRF based iterative linear optimization. NeRFs are novel neural space representation models that can synthesize photorealistic novel views of real-world scenes or objects. Our contributions are as follows: we extend the localization optimization objective with a depth-based loss function, we introduce a multi-image based loss function where a sequence of images with known relative poses are used without increasing the computational complexity, we omit hierarchical sampling during volumetric rendering, meaning only the coarse model is used for pose estimation, and we how that by extending the sampling interval convergence can be achieved even or higher initial pose estimate errors. With the proposed modifications the convergence speed is significantly improved, and the basin of convergence is substantially extended.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#21644;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#23494;&#24230;&#19981;&#22343;&#21248;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.03556</link><description>&lt;p&gt;
&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective. (arXiv:2310.03556v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#21644;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#23494;&#24230;&#19981;&#22343;&#21248;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#27010;&#29575;&#24314;&#27169;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36825;&#38656;&#35201;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#24403;&#21382;&#21490;&#25968;&#25454;&#19981;&#36275;&#26102;&#65292;&#24076;&#26395;&#23558;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#24182;&#29983;&#25104;&#26356;&#22810;&#25968;&#25454;&#12290;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#27169;&#22411;&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#24120;&#29992;&#36873;&#25321;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#36866;&#24212;&#23494;&#24230;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;KDE&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#26680;&#20989;&#25968;&#20855;&#26377;&#29420;&#31435;&#30340;&#24102;&#23485;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#65288;LOO-MLL&#65289;&#20934;&#21017;&#65292;&#20197;&#38450;&#27490;&#24120;&#35268;&#30340;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#20135;&#29983;&#22855;&#24322;&#35299;&#65292;&#24182;&#35777;&#26126;LOO-MLL&#21487;&#20197;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#27492;&#20445;&#35777;&#30340;&#40065;&#26834;&#24615;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20026;&#26680;&#20989;&#25968;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#20102;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic modelling of power systems operation and planning processes depends on data-driven methods, which require sufficiently large datasets. When historical data lacks this, it is desired to model the underlying data generation mechanism as a probability distribution to assess the data quality and generate more data, if needed. Kernel density estimation (KDE) based models are popular choices for this task, but they fail to adapt to data regions with varying densities. In this paper, an adaptive KDE model is employed to circumvent this, where each kernel in the model has an individual bandwidth. The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to prevent the singular solutions that the regular MLL criterion gives rise to, and it is proven that LOO-MLL prevents these. Relying on this guaranteed robustness, the model is extended by assigning learnable weights to the kernels. In addition, a modified expectation-maximization algorithm is employed to accelerat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.03546</link><description>&lt;p&gt;
&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#22312;&#19981;&#21305;&#37197;&#27979;&#37327;&#21644;&#20808;&#39564;&#27169;&#22411;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models. (arXiv:2310.03546v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25554;&#25300;&#24335;&#26410;&#35843;&#25972;&#26391;&#20043;&#19975;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#20351;&#29992;&#22270;&#20687;&#21435;&#22122;&#22120;&#25351;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#19982;&#19981;&#21305;&#37197;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#21644;&#21435;&#22122;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#23578;&#26410;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21518;&#39564;-L2&#25311;&#24230;&#37327;&#24182;&#21033;&#29992;&#23427;&#26469;&#37327;&#21270;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#30340;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#22914;&#20174;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#22270;&#20687;&#21435;&#27169;&#31946;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#21487;&#20197;&#31934;&#30830;&#22320;&#25551;&#36848;&#20854;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characte
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#35745;&#31639;&#39044;&#27979;&#27169;&#22411;&#22833;&#25928;&#27010;&#29575;&#30340;&#39118;&#38505;&#35780;&#20272;&#20219;&#21153;&#65292;&#20351;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#21306;&#38388;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#23432;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.03545</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#39118;&#38505;&#35780;&#20272;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution-free risk assessment of regression-based machine learning algorithms. (arXiv:2310.03545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03545
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#35745;&#31639;&#39044;&#27979;&#27169;&#22411;&#22833;&#25928;&#27010;&#29575;&#30340;&#39118;&#38505;&#35780;&#20272;&#20219;&#21153;&#65292;&#20351;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#21306;&#38388;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#23432;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#24182;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#21644;&#24037;&#31243;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#33719;&#21462;&#39044;&#27979;&#27169;&#22411;&#30340;&#22833;&#25928;&#27010;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#39118;&#38505;&#35780;&#20272;&#20219;&#21153;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#21608;&#22260;&#23450;&#20041;&#30340;&#21306;&#38388;&#20869;&#30495;&#23454;&#26631;&#31614;&#27010;&#29575;&#30340;&#39118;&#38505;&#35780;&#20272;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#39118;&#38505;&#35780;&#20272;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#39044;&#27979;&#21306;&#38388;&#20445;&#35777;&#20197;&#32473;&#23450;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#12290;&#21033;&#29992;&#36825;&#31181;&#21306;&#38388;&#35206;&#30422;&#24615;&#36136;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36817;&#20284;&#22833;&#25928;&#27010;&#29575;&#26159;&#20445;&#23432;&#30340;&#65292;&#21363;&#23427;&#19981;&#27604;ML&#31639;&#27861;&#30340;&#30495;&#23454;&#22833;&#25928;&#27010;&#29575;&#26356;&#20302;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have grown in sophistication over the years and are increasingly deployed for real-life applications. However, when using machine learning techniques in practical settings, particularly in high-risk applications such as medicine and engineering, obtaining the failure probability of the predictive model is critical. We refer to this problem as the risk-assessment task. We focus on regression algorithms and the risk-assessment task of computing the probability of the true label lying inside an interval defined around the model's prediction. We solve the risk-assessment problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a given probability. Using this coverage property, we prove that our approximated failure probability is conservative in the sense that it is not lower than the true failure probability of the ML algorithm. We conduct extensive experiments to empirically study t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#26469;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#32676;&#35770;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#24182;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.03530</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#65292;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#24341;&#23548;&#20102;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#26469;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#32676;&#35770;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#24182;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#32771;&#34385;&#20026;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#65292;&#20294;&#26159;&#20855;&#20307;&#30340;&#32534;&#30721;&#35268;&#21017;&#36824;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#36890;&#36807;&#20851;&#27880;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#65292;&#20174;&#25968;&#25454;&#22495;&#19978;&#30340;&#32676;&#20316;&#29992;&#20013;&#25214;&#21040;&#21442;&#25968;&#22495;&#19978;&#30340;&#21452;&#37325;&#32676;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;Schur&#24341;&#29702;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#26222;&#36941;&#24615;&#23450;&#29702;&#30340;&#26032;&#30340;&#32676;&#35770;&#35777;&#26126;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#26222;&#36941;&#24615;&#23450;&#29702;&#26159;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#35777;&#26126;&#30340;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;&#25105;&#20204;&#21033;&#29992;&#32676;&#35770;&#35770;&#35777;&#35777;&#26126;&#20102;&#36825;&#20123;DNN&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03529</link><description>&lt;p&gt;
&#28145;&#24230;&#33034;&#27874;&#21464;&#25442;&#65306;&#20351;&#29992;Koopman&#31639;&#23376;&#35777;&#26126;&#20102;&#24418;&#24335;&#28145;&#24230;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks. (arXiv:2310.03529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03529
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;&#25105;&#20204;&#21033;&#29992;&#32676;&#35770;&#35770;&#35777;&#35777;&#26126;&#20102;&#36825;&#20123;DNN&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;Koopman&#31639;&#23376;&#26159;&#32676;&#20316;&#29992;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;&#22522;&#20110;&#32676;&#35770;&#35770;&#35777;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;Schur&#24341;&#29702;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;DNN&#26222;&#36866;&#24615;&#30340;&#31616;&#21333;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify hidden layers inside a DNN with group actions on the data space, and formulate the DNN as a dual voice transform with respect to Koopman operator, a linear representation of the group action. Based on the group theoretic arguments, particularly by using Schur's lemma, we show a simple proof of the universality of those DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#27979;&#35797;&#26469;&#35782;&#21035;&#27963;&#21160;&#21464;&#37327;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#39640;&#32500;&#20248;&#21270;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2310.03515</link><description>&lt;p&gt;
&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#19982;&#32452;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization with Group Testing. (arXiv:2310.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#27979;&#35797;&#26469;&#35782;&#21035;&#27963;&#21160;&#21464;&#37327;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#39640;&#32500;&#20248;&#21270;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#31665;&#20989;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#39640;&#32500;&#38382;&#39064;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#30446;&#26631;&#30340;&#26367;&#20195;&#27169;&#22411;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#65292;&#24456;&#38590;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#27979;&#35797;&#26041;&#27861;&#26469;&#35782;&#21035;&#27963;&#21160;&#21464;&#37327;&#65292;&#20197;&#20415;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#32452;&#27979;&#35797;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GTBO&#65289;&#65292;&#39318;&#20808;&#36816;&#34892;&#19968;&#20010;&#27979;&#35797;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#31995;&#32479;&#22320;&#36873;&#25321;&#19968;&#32452;&#21464;&#37327;&#65292;&#24182;&#27979;&#35797;&#23427;&#20204;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24191;&#20026;&#20154;&#30693;&#30340;&#32452;&#27979;&#35797;&#29702;&#35770;&#25193;&#23637;&#21040;&#36830;&#32493;&#33539;&#22260;&#20989;&#25968;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;GTBO&#36890;&#36807;&#23545;&#27963;&#21160;&#32500;&#24230;&#32473;&#20104;&#26356;&#22810;&#37325;&#35270;&#26469;&#25351;&#23548;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#36724;&#23545;&#40784;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;GTBO&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#39640;&#32500;&#20248;&#21270;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is an effective method for optimizing expensive-to-evaluate black-box functions. High-dimensional problems are particularly challenging as the surrogate model of the objective suffers from the curse of dimensionality, which makes accurate modeling difficult. We propose a group testing approach to identify active variables to facilitate efficient optimization in these domains. The proposed algorithm, Group Testing Bayesian Optimization (GTBO), first runs a testing phase where groups of variables are systematically selected and tested on whether they influence the objective. To that end, we extend the well-established theory of group testing to functions of continuous ranges. In the second phase, GTBO guides optimization by placing more importance on the active dimensions. By exploiting the axis-aligned subspace assumption, GTBO is competitive against state-of-the-art methods on several synthetic and real-world high-dimensional optimization tasks. Furthermore, GTBO 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#22885;&#22612;&#21733;&#38203;&#28860;&#30340;&#20934;&#30830;&#31995;&#32479;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21028;&#26029;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65292;&#21487;&#20197;&#30417;&#27979;OEP&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.03512</link><description>&lt;p&gt;
&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#30340;&#22885;&#22612;&#21733;&#38203;&#28860;
&lt;/p&gt;
&lt;p&gt;
Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models. (arXiv:2310.03512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#22885;&#22612;&#21733;&#38203;&#28860;&#30340;&#20934;&#30830;&#31995;&#32479;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21028;&#26029;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65292;&#21487;&#20197;&#30417;&#27979;OEP&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22885;&#22612;&#21733;&#38203;&#28860;&#35745;&#21010;(OEP)&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24180;&#38271;&#32773;&#33030;&#24369;&#12289;&#32908;&#23569;&#30151;&#21644;&#24179;&#34913;&#30340;&#24247;&#22797;&#35745;&#21010;&#12290;&#20934;&#30830;&#22320;&#30417;&#27979;&#24739;&#32773;&#21442;&#19982;OEP&#30340;&#24773;&#20917;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#33258;&#25105;&#25253;&#21578;(&#26085;&#35760;)&#36890;&#24120;&#19981;&#21487;&#38752;&#12290;&#38543;&#30528;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#65292;&#21033;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;(HAR)&#30340;&#31995;&#32479;&#24050;&#32463;&#25913;&#21464;&#20102;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;OEP&#30340;&#20351;&#29992;&#20173;&#28982;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19981;&#26174;&#30524;&#19988;&#20934;&#30830;&#30340;&#31995;&#32479;&#65292;&#20197;&#30417;&#27979;&#24180;&#38271;&#32773;&#21442;&#19982;OEP&#30340;&#24773;&#20917;&#12290;&#25968;&#25454;&#26159;&#20174;&#20329;&#25140;&#21333;&#20010;&#33136;&#37096;IMU&#30340;&#24180;&#38271;&#32773;&#36523;&#19978;&#25910;&#38598;&#30340;&#12290;&#25910;&#38598;&#20102;&#20004;&#32452;&#25968;&#25454;&#65292;&#19968;&#32452;&#26159;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#19968;&#32452;&#26159;&#22312;&#24739;&#32773;&#23478;&#20013;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#31995;&#32479;&#65292;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1)&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;10&#20998;&#38047;&#30340;&#28369;&#21160;&#31383;&#21475;&#35782;&#21035;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;(ADLs)&#65307;2)&#22522;&#20110;&#31532;&#19968;&#38454;&#27573;&#65292;&#20351;&#29992;6&#31186;&#30340;&#28369;&#21160;&#31383;&#21475;&#37325;&#26032;&#35782;&#21035;OEP&#19982;ADLs&#12290;
&lt;/p&gt;
&lt;p&gt;
Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#35745;&#31639;&#38899;&#20048;&#36755;&#20837;&#24207;&#21015;&#30340;&#36817;&#20284;&#27010;&#29575;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#38899;&#20048;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2310.03500</link><description>&lt;p&gt;
&#38899;&#20048;&#26399;&#26395;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models of Music Expectation. (arXiv:2310.03500v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#35745;&#31639;&#38899;&#20048;&#36755;&#20837;&#24207;&#21015;&#30340;&#36817;&#20284;&#27010;&#29575;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#38899;&#20048;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#26354;&#30340;&#24778;&#21916;&#21644;&#26399;&#26395;&#26159;&#24773;&#24863;&#21453;&#24212;&#30340;&#19968;&#31181;&#37325;&#35201;&#29702;&#35770;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#36825;&#19968;&#24605;&#24819;&#36716;&#21270;&#20026;&#38899;&#20048;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20808;&#21069;&#30340;&#38899;&#20048;&#32463;&#39564;&#35745;&#31639;&#20986;&#38899;&#20048;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#27169;&#22411;&#20165;&#38480;&#20110;&#36890;&#36807;&#25163;&#24037;&#29305;&#24449;&#35745;&#31639;&#20934;&#30830;&#27010;&#29575;&#65292;&#25110;&#32773;&#20165;&#38480;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#34920;&#36798;&#38899;&#20048;&#20013;&#22797;&#26434;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27169;&#22411;&#65292;&#26469;&#35745;&#31639;&#38899;&#20048;&#36755;&#20837;&#24207;&#21015;&#30340;&#36817;&#20284;&#27010;&#29575;&#12290;&#36825;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#26399;&#26395;&#36890;&#36807;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#38899;&#20048;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent theory of affective response to music revolves around the concepts of surprisal and expectation. In prior work, this idea has been operationalized in the form of probabilistic models of music which allow for precise computation of song (or note-by-note) probabilities, conditioned on a 'training set' of prior musical or cultural experiences. To date, however, these models have been limited to compute exact probabilities through hand-crafted features or restricted to linear models which are likely not sufficient to represent the complex conditional distributions present in music. In this work, we propose to use modern deep probabilistic generative models in the form of a Diffusion Model to compute an approximate likelihood of a musical input sequence. Unlike prior work, such a generative model parameterized by deep neural networks is able to learn complex non-linear features directly from a training set itself. In doing so, we expect to find that such models are able to more 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.03494</link><description>&lt;p&gt;
&#22914;&#20309;&#27700;&#24179;&#37319;&#26679;&#36807;&#31243;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#27490;&#24191;&#27867;&#37319;&#29992;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#20195;&#29702;&#30340;&#20851;&#38190;&#23616;&#38480;&#26159;&#23427;&#20204;&#26377;&#38480;&#30340;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#25110;&#23618;&#32423;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#65288;ZSG&#65289;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#22833;&#25928;&#27169;&#24335;&#65306;&#36807;&#25311;&#21512;&#21644;&#36807;&#24230;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#21457;&#29616;MI&#19982;&#23454;&#20363;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#19982;&#22343;&#21248;&#37319;&#26679;&#30456;&#27604;&#65292;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#36739;&#20302;&#30340;MI&#65292;&#36825;&#20026;&#36825;&#31867;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;U&#65289;
&lt;/p&gt;
&lt;p&gt;
A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
&lt;/p&gt;</description></item><item><title>TPDR&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20135;&#21697;&#21644;&#31867;&#25551;&#36848;&#21305;&#37197;&#19982;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.03491</link><description>&lt;p&gt;
TPDR&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#27493;&#39588;Transformer&#30340;&#20135;&#21697;&#21644;&#31867;&#25551;&#36848;&#21305;&#37197;&#19982;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method. (arXiv:2310.03491v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03491
&lt;/p&gt;
&lt;p&gt;
TPDR&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20135;&#21697;&#21644;&#31867;&#25551;&#36848;&#21305;&#37197;&#19982;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#19968;&#31867;&#20844;&#21496;&#36127;&#36131;&#20026;&#20854;&#20182;&#20844;&#21496;&#20013;&#20171;&#37319;&#36141;&#22823;&#25209;&#37327;&#30340;&#21508;&#31181;&#20135;&#21697;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#26159;&#36827;&#34892;&#20135;&#21697;&#25551;&#36848;&#30340;&#26631;&#20934;&#21270;&#65292;&#21363;&#23558;&#23458;&#25143;&#25551;&#36848;&#30340;&#21830;&#21697;&#19982;&#30446;&#24405;&#20013;&#25551;&#36848;&#30340;&#20135;&#21697;&#36827;&#34892;&#21305;&#37197;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#22797;&#26434;&#65292;&#22240;&#20026;&#23458;&#25143;&#30340;&#20135;&#21697;&#25551;&#36848;&#21487;&#33021;&#23384;&#22312;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;1&#65289;&#28508;&#22312;&#30340;&#22122;&#22768;&#65307;&#65288;2&#65289;&#30701;&#23567;&#19988;&#19981;&#20855;&#22791;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#32570;&#23569;&#26377;&#20851;&#22411;&#21495;&#21644;&#23610;&#23544;&#30340;&#20449;&#24687;&#65289;&#65307;&#65288;3&#65289;&#36328;&#35821;&#35328;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#30340;&#23458;&#25143;&#20135;&#21697;&#35268;&#26684;&#65288;&#26597;&#35810;&#65289;&#65292;&#36820;&#22238;&#26368;&#21512;&#36866;&#30340;&#26631;&#20934;&#21270;&#25551;&#36848;&#65288;&#21709;&#24212;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TPDR&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#27493;&#39588;Transformer&#30340;&#20135;&#21697;&#21644;&#31867;&#25551;&#36848;&#26816;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25506;&#32034;IS&#21644;SD&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;TPDR&#20351;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#30340;transformers&#20849;&#20139;&#23884;&#20837;&#21521;&#37327;&#31354;&#38388;&#65306;
&lt;/p&gt;
&lt;p&gt;
There is a niche of companies responsible for intermediating the purchase of large batches of varied products for other companies, for which the main challenge is to perform product description standardization, i.e., matching an item described by a client with a product described in a catalog. The problem is complex since the client's product description may be: (1) potentially noisy; (2) short and uninformative (e.g., missing information about model and size); and (3) cross-language. In this paper, we formalize this problem as a ranking task: given an initial client product specification (query), return the most appropriate standardized descriptions (response). In this paper, we propose TPDR, a two-step Transformer-based Product and Class Description Retrieval method that is able to explore the semantic correspondence between IS and SD, by exploiting attention mechanisms and contrastive learning. First, TPDR employs the transformers as two encoders sharing the embedding vector space: 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BTDNet&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;3D&#20998;&#26512;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#20999;&#29255;&#32423;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03485</link><description>&lt;p&gt;
BTDNet:&#19968;&#31181;&#29992;&#20110;&#33041;&#32959;&#30244;&#25918;&#23556;&#22522;&#22240;&#32452;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification. (arXiv:2310.03485v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BTDNet&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;3D&#20998;&#26512;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#20999;&#29255;&#32423;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#20581;&#24247;&#25361;&#25112;&#65292;&#20854;&#20013;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#24418;&#24335;&#20043;&#19968;&#12290;&#20934;&#30830;&#30830;&#23450;O6-&#30002;&#22522;&#40479;&#22028;&#21604;-DNA&#30002;&#22522;&#36716;&#31227;&#37238;(MGMT)&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#23545;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;BTDNet&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#65292;&#21253;&#25324;FLAIR&#12289;T1w&#12289;T1wCE&#21644;T2 3D&#20307;&#31215;&#65292;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#65288;&#21363;&#27599;&#20010;&#20307;&#31215;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#30340;&#20999;&#29255;&#65289;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#65288;&#21363;&#25972;&#20010;3D&#20307;&#31215;&#34987;&#27880;&#37322;&#65292;&#32780;&#19981;&#26159;&#23427;&#21253;&#21547;&#30340;&#29420;&#31435;&#20999;&#29255;&#65289;&#12290;BTDNet&#30001;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;i&#65289;&#25968;&#25454;&#22686;&#24378;&#65288;&#25191;&#34892;&#20960;&#20309;&#21464;&#25442;&#65292;&#25968;&#25454;&#23545;&#30340;&#20984;&#32452;&#21512;&#21644;&#27979;&#35797;&#26102;&#25968;&#25454;&#22686;&#24378;&#65289;&#65307;ii&#65289;3D&#20998;&#26512;&#65288;&#25191;&#34892;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65289;&#65307;iii&#65289;&#29305;&#24449;&#25552;&#21462;&#65288;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20999;&#29255;&#32423;&#29305;&#24449;&#65289;&#65307;iv&#65289;&#20999;&#29255;&#32423;&#20998;&#31867;&#65288;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20999;&#29255;&#32423;&#20998;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors pose significant health challenges worldwide, with glioblastoma being one of the most aggressive forms. Accurate determination of the O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is crucial for personalized treatment strategies. However, traditional methods are labor-intensive and time-consuming. This paper proposes a novel multi-modal approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w, T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet addresses two main challenges: the variable volume lengths (i.e., each volume consists of a different number of slices) and the volume-level annotations (i.e., the whole 3D volume is annotated and not the independent slices that it consists of). BTDNet consists of four components: i) the data augmentation one (that performs geometric transformations, convex combinations of data pairs and test-time data augmentation); ii) the 3D analysis one (that performs glo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#21010;&#20998;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#27492;&#32467;&#26500;&#36824;&#31616;&#21270;&#20102;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#23545;&#20110;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03482</link><description>&lt;p&gt;
&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Geometric Structure of Fully-Connected ReLU-Layers. (arXiv:2310.03482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#21010;&#20998;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#27492;&#32467;&#26500;&#36824;&#31616;&#21270;&#20102;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#23545;&#20110;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;d&#32500;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#21644;&#35299;&#37322;&#12290;ReLU&#23618;&#30340;&#21442;&#25968;&#20250;&#24341;&#23548;&#36755;&#20837;&#22495;&#30340;&#33258;&#28982;&#21010;&#20998;&#65292;&#20351;&#24471;&#22312;&#21010;&#20998;&#30340;&#27599;&#20010;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#12290;&#36825;&#23548;&#33268;&#20102;&#23558;ReLU&#23618;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#19982;&#22312;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#30340;&#25551;&#36848;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#32467;&#26500;&#20415;&#20110;&#31616;&#21270;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#36825;&#22312;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#26102;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#35777;&#26126;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#65292;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such 
&lt;/p&gt;</description></item><item><title>Cadenza&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65292;&#26088;&#22312;&#36890;&#36807;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#26469;&#25552;&#21319;&#21161;&#21548;&#22120;&#38899;&#36136;&#65292;&#22788;&#29702;&#36807;&#31243;&#32771;&#34385;&#38899;&#20048;&#12289;&#22686;&#30410;&#21644;&#21548;&#21147;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.03480</link><description>&lt;p&gt;
Cadenza ICASSP 2024&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Cadenza ICASSP 2024 Grand Challenge. (arXiv:2310.03480v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03480
&lt;/p&gt;
&lt;p&gt;
Cadenza&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65292;&#26088;&#22312;&#36890;&#36807;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#26469;&#25552;&#21319;&#21161;&#21548;&#22120;&#38899;&#36136;&#65292;&#22788;&#29702;&#36807;&#31243;&#32771;&#34385;&#38899;&#20048;&#12289;&#22686;&#30410;&#21644;&#21548;&#21147;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cadenza&#39033;&#30446;&#26088;&#22312;&#25552;&#39640;&#21548;&#21147;&#21463;&#25439;&#20154;&#32676;&#30340;&#38899;&#20048;&#38899;&#36136;&#12290;&#20316;&#20026;&#35813;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65306;&#38754;&#21521;&#21161;&#21548;&#22120;&#30340;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#12290;&#35813;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#23558;&#38899;&#20048;&#20998;&#35299;&#25104;&#20154;&#22768;&#12289;&#36125;&#26031;&#12289;&#40723;&#21644;&#20854;&#20182;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#12290;&#28982;&#21518;&#21487;&#20197;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#26234;&#33021;&#22320;&#36827;&#34892;&#28151;&#38899;&#65292;&#20197;&#25552;&#39640;&#38899;&#39057;&#36136;&#37327;&#12290;&#21478;&#22806;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#12290;&#22788;&#29702;&#36807;&#31243;&#38656;&#35201;&#32771;&#34385;&#38899;&#20048;&#26412;&#36523;&#12289;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#22686;&#30410;&#20197;&#21450;&#21548;&#20247;&#30340;&#21548;&#21147;&#25439;&#22833;&#12290;&#25552;&#20132;&#30340;&#20316;&#21697;&#23558;&#20351;&#29992;Hearing Aid Audio Quality Index&#65288;HAAQI&#65289;&#36825;&#19968;&#20837;&#20405;&#24335;&#23458;&#35266;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;&#30340;&#25552;&#20986;&#25968;&#37327;&#36805;&#36895;&#22686;&#38271;&#65292;&#22312;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#26102;&#23384;&#22312;&#24402;&#22240;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#26469;&#35780;&#20272;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#24378;&#35843;&#38500;&#20102;&#22522;&#20110;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#20043;&#22806;&#30340;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#37117;&#36973;&#21463;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#26356;&#21512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03466</link><description>&lt;p&gt;
&#35780;&#20272;&#23616;&#37096;&#35299;&#37322;&#20013;&#30340;&#24402;&#22240;&#38382;&#39064;&#21450;&#20854;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Blame Problem in Evaluating Local Explanations, and How to Tackle it. (arXiv:2310.03466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03466
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;&#30340;&#25552;&#20986;&#25968;&#37327;&#36805;&#36895;&#22686;&#38271;&#65292;&#22312;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#26102;&#23384;&#22312;&#24402;&#22240;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#26469;&#35780;&#20272;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#24378;&#35843;&#38500;&#20102;&#22522;&#20110;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#20043;&#22806;&#30340;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#37117;&#36973;&#21463;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#26356;&#21512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;&#30340;&#25552;&#20986;&#25968;&#37327;&#36805;&#36895;&#22686;&#38271;&#12290;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#30001;&#20110;&#32570;&#20047;&#26368;&#20248;&#35780;&#20272;&#25351;&#26631;&#65292;&#24320;&#21457;&#26032;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#38376;&#27099;&#36739;&#20302;&#12290;&#27809;&#26377;&#20005;&#26684;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24456;&#38590;&#26377;&#30830;&#20991;&#30340;&#35777;&#25454;&#35777;&#26126;&#26032;&#30340;&#35299;&#37322;&#25216;&#26415;&#33021;&#21542;&#26126;&#26174;&#20248;&#20110;&#20854;&#21069;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#26469;&#35780;&#20272;&#23616;&#37096;&#35299;&#37322;&#65306;&#40065;&#26834;&#24615;&#12289;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12289;&#27169;&#22411;&#38543;&#26426;&#21270;&#20197;&#21450;&#20154;&#31867;&#21442;&#19982;&#35780;&#20272;&#12290;&#20351;&#29992;&#36825;&#31181;&#25552;&#20986;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#24378;&#35843;&#38500;&#20102;&#22522;&#20110;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#20043;&#22806;&#30340;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#37117;&#36973;&#21463;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24402;&#22240;&#38382;&#39064;&#8221;&#30340;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#26159;&#19968;&#31181;&#26356;&#21512;&#29702;&#12289;&#26356;&#33021;&#35780;&#20272;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of local model-agnostic explanation techniques proposed has grown rapidly recently. One main reason is that the bar for developing new explainability techniques is low due to the lack of optimal evaluation measures. Without rigorous measures, it is hard to have concrete evidence of whether the new explanation techniques can significantly outperform their predecessors. Our study proposes a new taxonomy for evaluating local explanations: robustness, evaluation using ground truth from synthetic datasets and interpretable models, model randomization, and human-grounded evaluation. Using this proposed taxonomy, we highlight that all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the "blame problem." In our study, we argue that this category of evaluation measure is a more reasonable method for evaluating local model-agnostic explanations. However, we show that even this category of evaluation measu
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#30740;&#31350;&#21457;&#29616;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#38598;&#20013;&#21270;&#30340;&#26041;&#27861;&#24635;&#26159;&#27604;&#20998;&#25955;&#21270;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#21516;&#26102;&#65292;&#37096;&#20998;&#21442;&#19982;&#22312;&#38598;&#20013;&#21270;&#26041;&#27861;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#20998;&#25955;&#21270;&#26041;&#27861;&#20013;&#65292;&#25299;&#25169;&#32467;&#26500;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#21313;&#20998;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.03461</link><description>&lt;p&gt;
&#21738;&#31181;&#27169;&#24335;&#26356;&#36866;&#21512;&#32852;&#21512;&#23398;&#20064;&#65311;&#20013;&#22830;&#21270;&#36824;&#26159;&#20998;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which mode is better for federated learning? Centralized or Decentralized. (arXiv:2310.03461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03461
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#30740;&#31350;&#21457;&#29616;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#38598;&#20013;&#21270;&#30340;&#26041;&#27861;&#24635;&#26159;&#27604;&#20998;&#25955;&#21270;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#21516;&#26102;&#65292;&#37096;&#20998;&#21442;&#19982;&#22312;&#38598;&#20013;&#21270;&#26041;&#27861;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#20998;&#25955;&#21270;&#26041;&#27861;&#20013;&#65292;&#25299;&#25169;&#32467;&#26500;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#21313;&#20998;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#38598;&#20013;&#21270;&#21644;&#20998;&#25955;&#21270;&#26041;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25552;&#20379;&#36275;&#22815;&#30340;&#35777;&#25454;&#26469;&#34920;&#26126;&#21738;&#31181;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#34429;&#28982;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20998;&#25955;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#36890;&#20449;&#23454;&#29616;&#19982;&#38598;&#20013;&#21270;&#26041;&#27861;&#30456;&#27604;&#36739;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#23427;&#30340;&#27979;&#35797;&#24615;&#33021;&#22987;&#32456;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#20840;&#38754;&#25506;&#32034;&#23427;&#20204;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#36807;&#24230;&#39118;&#38505;&#65292;&#21253;&#25324;&#20248;&#21270;&#21644;&#27867;&#21270;&#30340;&#32852;&#21512;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#19978;&#65292;1&#65289;&#38598;&#20013;&#21270;&#30340;FL&#65288;CFL&#65289;&#24635;&#26159;&#27604;&#20998;&#25955;&#21270;&#30340;FL&#65288;DFL&#65289;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65307;2&#65289;&#20174;CFL&#30340;&#36807;&#24230;&#39118;&#38505;&#21644;&#27979;&#35797;&#35823;&#24046;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#37096;&#20998;&#21442;&#19982;&#27604;&#20840;&#21442;&#19982;&#26356;&#22909;&#65307;3&#65289;&#22312;DFL&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#38543;&#30528;&#35757;&#32451;&#35268;&#27169;&#22686;&#21152;&#32780;&#24615;&#33021;&#23849;&#28291;&#65292;&#25299;&#25169;&#32467;&#26500;&#26377;&#24517;&#35201;&#28385;&#36275;&#19968;&#23450;&#30340;&#35201;&#27714;&#12290;&#22522;&#20110;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Both centralized and decentralized approaches have shown excellent performance and great application value in federated learning (FL). However, current studies do not provide sufficient evidence to show which one performs better. Although from the optimization perspective, decentralized methods can approach the comparable convergence of centralized methods with less communication, its test performance has always been inefficient in empirical studies. To comprehensively explore their behaviors in FL, we study their excess risks, including the joint analysis of both optimization and generalization. We prove that on smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) from perspectives of the excess risk and test error in CFL, adopting partial participation is superior to full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. Based on some
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21512;&#24182;&#65292;&#22312;&#26102;&#22495;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.03456</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#26102;&#22495;&#21160;&#20316;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization. (arXiv:2310.03456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21512;&#24182;&#65292;&#22312;&#26102;&#22495;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#22495;&#21160;&#20316;&#23450;&#20301;&#65288;TAL&#65289;&#26088;&#22312;&#22312;&#26410;&#21098;&#36753;&#35270;&#39057;&#20013;&#35782;&#21035;&#21160;&#20316;&#30340;&#24320;&#22987;&#12289;&#32467;&#26463;&#21644;&#31867;&#21035;&#26631;&#31614;&#12290;&#34429;&#28982;&#26368;&#36817;&#20351;&#29992;Transformer&#32593;&#32476;&#21644;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#22312;TAL&#20219;&#21153;&#20013;&#22686;&#24378;&#20102;&#35270;&#35273;&#29305;&#24449;&#35782;&#21035;&#65292;&#20294;&#22312;&#23558;&#38899;&#39057;&#29305;&#24449;&#25972;&#21512;&#21040;&#27492;&#31867;&#26694;&#26550;&#20013;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#36739;&#23569;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#20998;&#36776;&#29575;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#34701;&#21512;&#65288;MRAV-FF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#21512;&#24182;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#20998;&#23618;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#20998;&#36776;&#22320;&#26435;&#34913;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#38899;&#39057;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#22238;&#24402;&#36793;&#30028;&#30340;&#31934;&#24230;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;MRAV-FF&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#23427;&#19982;&#29616;&#26377;&#30340;FPN TAL&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#22312;&#38899;&#39057;&#25968;&#25454;&#21487;&#29992;&#26102;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Action Localization (TAL) aims to identify actions' start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal resolutions. Central to our approach is a hierarchical gated cross-attention mechanism, which discerningly weighs the importance of audio information at diverse temporal scales. Such a technique not only refines the precision of regression boundaries but also bolsters classification confidence. Importantly, MRAV-FF is versatile, making it compatible with existing FPN TAL architectures and offering a significant enhancement in performance when audio data is available.
&lt;/p&gt;</description></item><item><title>FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03447</link><description>&lt;p&gt;
FLAIM: &#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03447
&lt;/p&gt;
&lt;p&gt;
FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21516;&#26102;&#23454;&#29616;&#21327;&#21516;&#25968;&#25454;&#20849;&#20139;&#23545;&#32452;&#32455;&#33267;&#20851;&#37325;&#35201;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20135;&#29983;&#19982;&#31169;&#26377;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#30456;&#20284;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20551;&#35774;&#25968;&#25454;&#26159;&#38598;&#20013;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#32852;&#37030;&#26041;&#24335;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#32852;&#37030;&#21512;&#25104;&#34920;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;AIM&#36825;&#20010;&#20808;&#36827;&#30340;&#20013;&#24515;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistAIM&#21644;FLAIM&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#21457;AIM&#26159;&#31616;&#21333;&#30340;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#20351;&#20854;&#22312;&#32852;&#37030;&#22330;&#26223;&#20013;&#19981;&#22826;&#36866;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#32852;&#37030;AIM&#21487;&#33021;&#23548;&#33268;&#22312;&#24322;&#26500;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25928;&#29992;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;FLAIM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#25512;&#26029;&#22312;GARCH&#23478;&#26063;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#19968;&#31181;&#21487;&#38752;&#21644;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03435</link><description>&lt;p&gt;
GARCH&#23478;&#26063;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference for GARCH-family Models. (arXiv:2310.03435v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03435
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#22312;GARCH&#23478;&#26063;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#19968;&#31181;&#21487;&#38752;&#21644;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20272;&#35745;GARCH&#23478;&#26063;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#26377;&#38480;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#21464;&#20998;&#25512;&#26029;&#22312;GARCH&#31867;&#27169;&#22411;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#21542;&#26159;&#19968;&#31181;&#21487;&#38752;&#21644;&#21487;&#34892;&#30340;&#26367;&#20195;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28041;&#21450;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#25104;&#20998;&#32929;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#37319;&#29992;&#22810;&#31181;&#21464;&#20998;&#25512;&#26029;&#20248;&#21270;&#31639;&#27861;&#21644;&#27874;&#21160;&#24615;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#12289;&#38750;&#24120;&#33391;&#22909;&#26657;&#20934;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bayesian estimation of GARCH-family models has been typically addressed through Monte Carlo sampling. Variational Inference is gaining popularity and attention as a robust approach for Bayesian inference in complex machine learning models; however, its adoption in econometrics and finance is limited. This paper discusses the extent to which Variational Inference constitutes a reliable and feasible alternative to Monte Carlo sampling for Bayesian inference in GARCH-like models. Through a large-scale experiment involving the constituents of the S&amp;P 500 index, several Variational Inference optimizers, a variety of volatility models, and a case study, we show that Variational Inference is an attractive, remarkably well-calibrated, and competitive method for Bayesian learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20462;&#21098;&#26694;&#26550;&#30340;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65292;&#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#65292;&#20026;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#30340;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.03424</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Neural Language Model Pruning for Automatic Speech Recognition. (arXiv:2310.03424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20462;&#21098;&#26694;&#26550;&#30340;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65292;&#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#65292;&#20026;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#30340;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20462;&#21098;&#26694;&#26550;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20851;&#20110;&#22823;&#35268;&#27169;&#35782;&#21035;&#31995;&#32479;&#30340;&#36825;&#31181;&#28145;&#20837;&#20998;&#26512;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#25253;&#36947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#30340;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#22823;&#23567;&#30340;&#27169;&#22411;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;a) &#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#20960;&#20010;&#22330;&#26223;&#19979;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65307;b) &#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36739;&#23567;&#30340;&#22823;&#23567;&#65307;c) &#20302;&#31209;&#36924;&#36817;&#22312;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#65292;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study model pruning methods applied to Transformer-based neural network language models for automatic speech recognition. We explore three aspects of the pruning frame work, namely criterion, method and scheduler, analyzing their contribution in terms of accuracy and inference speed. To the best of our knowledge, such in-depth analyses on large-scale recognition systems has not been reported in the literature. In addition, we propose a variant of low-rank approximation suitable for incrementally compressing models, and delivering multiple models with varied target sizes. Among other results, we show that a) data-driven pruning outperforms magnitude-driven in several scenarios; b) incremental pruning achieves higher accuracy compared to one-shot pruning, especially when targeting smaller sizes; and c) low-rank approximation presents the best trade-off between size reduction and inference speed-up for moderate compression.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#30340;&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#38382;&#39064;&#30340;&#24418;&#24335;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#30340;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.03419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#30340;&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#38382;&#39064;&#30340;&#24418;&#24335;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#30340;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#23398;&#20064;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#22870;&#21169;&#20998;&#24067;&#20013;&#39034;&#24207;&#29983;&#25104;&#22797;&#21512;&#23545;&#35937;&#30340;&#38543;&#26426;&#31574;&#30053;&#30340;&#25674;&#38144;&#37319;&#26679;&#22120;&#12290;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#22870;&#21169;&#23545;&#35937;&#65292;&#22312;&#31185;&#23398;&#21457;&#29616;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#36890;&#24120;&#26159;&#26681;&#25454;&#32473;&#23450;&#30340;&#22806;&#22312;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20851;&#20110;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#21147;&#37327;&#20197;&#21450;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;GFlowNets&#20197;&#23454;&#29616;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#25928;&#33258;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#21463;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNets&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#30417;&#30563;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#23427;&#23398;&#20064;&#25506;&#32034;&#20505;&#36873;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;OC-GFN&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#36890;&#20449;&#35774;&#35745;&#65292;&#21457;&#29616;&#22312;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20013;&#21387;&#32553;&#20043;&#21069;&#30340;&#31232;&#30095;&#21270;&#24182;&#19981;&#24517;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.03410</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#24863;&#30693;&#30340;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#65306;&#31232;&#30095;&#21270;&#26159;&#21542;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?. (arXiv:2310.03410v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#36890;&#20449;&#35774;&#35745;&#65292;&#21457;&#29616;&#22312;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20013;&#21387;&#32553;&#20043;&#21069;&#30340;&#31232;&#30095;&#21270;&#24182;&#19981;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#65288;OtA FL&#65289;&#26159;&#25351;&#22810;&#20010;&#20195;&#29702;&#36890;&#36807; OtA &#35745;&#31639;&#23558;&#27169;&#22411;&#26356;&#26032;&#21457;&#36865;&#21040;&#20844;&#20849;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;&#19968;&#31181; FL &#31995;&#32479;&#12290;OtA &#35745;&#31639;&#30340;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#21363;&#32447;&#24615;&#22788;&#29702;&#21644;&#20449;&#21495;&#32423;&#21472;&#21152;&#65292;&#20419;&#20351;&#20351;&#29992;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26041;&#27861;&#36827;&#34892;&#32447;&#24615;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#36890;&#36807;&#20449;&#36947;&#20256;&#36755;&#30340;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#12290;&#20197;&#21069;&#20851;&#20110;&#22312; OtA FL &#20013;&#24212;&#29992; CS &#26041;&#27861;&#30340;&#30740;&#31350;&#20027;&#35201;&#20551;&#35774;&#21407;&#22987;&#27169;&#22411;&#26356;&#26032;&#21521;&#37327;&#26159;&#31232;&#30095;&#30340;&#65292;&#25110;&#32773;&#22312;&#21387;&#32553;&#20043;&#21069;&#36827;&#34892;&#20102;&#31232;&#30095;&#21270;&#12290;&#20294;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#32447;&#24615;&#21387;&#32553;&#21644;&#22522;&#20110; CS &#30340;&#37325;&#26500;&#19982;&#30452;&#25509;&#21457;&#36865;&#31232;&#30095;&#21270;&#26356;&#26032;&#21521;&#37327;&#20013;&#30340;&#38750;&#38646;&#20803;&#32032;&#65292;&#22312;&#30456;&#21516;&#30340;&#24635;&#21151;&#29575;&#32422;&#26463;&#19979;&#65292;&#21738;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#24102;&#26377;&#25110;&#19981;&#24102;&#26377;&#31232;&#30095;&#21270;&#30340;&#36890;&#20449;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#32553;&#20043;&#21069;&#30340;&#31232;&#30095;&#21270;&#24182;&#19981;&#24517;&#35201;&#12290;&#26367;&#20195;&#30340;&#20570;&#27861;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air (OtA) Federated Learning (FL) refers to an FL system where multiple agents apply OtA computation for transmitting model updates to a common edge server. Two important features of OtA computation, namely linear processing and signal-level superposition, motivate the use of linear compression with compressed sensing (CS) methods to reduce the number of data samples transmitted over the channel. The previous works on applying CS methods in OtA FL have primarily assumed that the original model update vectors are sparse, or they have been sparsified before compression. However, it is unclear whether linear compression with CS-based reconstruction is more effective than directly sending the non-zero elements in the sparsified update vectors, under the same total power constraint. In this study, we examine and compare several communication designs with or without sparsification. Our findings demonstrate that sparsification before compression is not necessary. Alternatively, spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#26426;&#22120;&#20154;&#36229;&#22768;&#25506;&#22836;&#36827;&#34892;&#26631;&#20934;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#25506;&#22836;&#19982;&#25195;&#25551;&#34920;&#38754;&#30340;&#25509;&#35302;&#28857;&#30340;&#27491;&#20132;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#36229;&#22768;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.03406</link><description>&lt;p&gt;
RUSOpt:&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#26426;&#22120;&#20154;&#36229;&#22768;&#25506;&#22836;&#30340;&#24179;&#38754;&#21644;&#22402;&#30452;&#25195;&#25551;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
RUSOpt: Robotic UltraSound Probe Normalization with Bayesian Optimization for In-plane and Out-plane Scanning. (arXiv:2310.03406v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#26426;&#22120;&#20154;&#36229;&#22768;&#25506;&#22836;&#36827;&#34892;&#26631;&#20934;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#25506;&#22836;&#19982;&#25195;&#25551;&#34920;&#38754;&#30340;&#25509;&#35302;&#28857;&#30340;&#27491;&#20132;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#36229;&#22768;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#36229;&#22768;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#19981;&#21516;&#24739;&#32773;&#19978;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#26426;&#22120;&#21270;&#25506;&#22836;&#30340;&#36866;&#24403;&#23450;&#20301;&#23545;&#36229;&#22768;&#22270;&#20687;&#30340;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#33258;&#21160;&#35843;&#25972;&#36229;&#22768;&#25506;&#22836;&#19982;&#25195;&#25551;&#34920;&#38754;&#30340;&#25509;&#35302;&#28857;&#27491;&#20132;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22768;&#25506;&#22836;&#30340;&#22768;&#23398;&#32806;&#21512;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25628;&#32034;&#22312;&#25195;&#25551;&#34920;&#38754;&#19978;&#39640;&#25928;&#25628;&#32034;&#26631;&#20934;&#21270;&#30340;&#25506;&#22836;&#23450;&#20301;&#12290;&#25105;&#20204;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21033;&#29992;&#25509;&#35302;&#21147;&#27979;&#37327;&#21644;&#22522;&#26412;&#21147;&#23398;&#21407;&#29702;&#26469;&#30830;&#23450;&#27491;&#20132;&#26041;&#21521;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#26041;&#26696;&#26469;&#22788;&#29702;&#22024;&#26434;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#23615;&#33152;&#33009;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one of the significant challenges faced by autonomous robotic ultrasound systems is acquiring high-quality images across different patients. The proper orientation of the robotized probe plays a crucial role in governing the quality of ultrasound images. To address this challenge, we propose a sample-efficient method to automatically adjust the orientation of the ultrasound probe normal to the point of contact on the scanning surface, thereby improving the acoustic coupling of the probe and resulting image quality. Our method utilizes Bayesian Optimization (BO) based search on the scanning surface to efficiently search for the normalized probe orientation. We formulate a novel objective function for BO that leverages the contact force measurements and underlying mechanics to identify the normal. We further incorporate a regularization scheme in BO to handle the noisy objective function. The performance of the proposed strategy has been assessed through experiments on urinary bladde
&lt;/p&gt;</description></item><item><title>EAG-RS&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#38454;&#21151;&#33021;&#20851;&#32852;&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#30340;&#35786;&#26029;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03404</link><description>&lt;p&gt;
EAG-RS:&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;ASD&#35786;&#26029;&#30340;ROI&#36873;&#25321;&#26694;&#26550;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning. (arXiv:2310.03404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03404
&lt;/p&gt;
&lt;p&gt;
EAG-RS&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#38454;&#21151;&#33021;&#20851;&#32852;&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#30340;&#35786;&#26029;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rs-fMRI&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35786;&#26029;&#33041;&#37096;&#30142;&#30149;&#65292;&#29305;&#21035;&#26159;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;rs-fMRI&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#65288;FC&#65289;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30528;&#19968;&#20123;&#37325;&#22823;&#23616;&#38480;&#65292;&#21253;&#25324;&#20351;&#29992;&#32447;&#24615;&#20302;&#38454;FC&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#26102;&#32570;&#20047;&#20805;&#20998;&#30340;&#20449;&#24687;&#65292;&#19981;&#32771;&#34385;ASD&#24739;&#32773;&#30340;&#20010;&#20307;&#29305;&#24449;&#65288;&#21363;&#19981;&#21516;&#30151;&#29366;&#25110;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#38454;&#27573;&#65289;&#65292;&#20197;&#21450;&#20915;&#31574;&#36807;&#31243;&#30340;&#38750;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#36873;&#25321;&#65288;EAG-RS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#35782;&#21035;&#33041;&#21306;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#39640;&#38454;&#21151;&#33021;&#20851;&#32852;&#65292;&#24182;&#36873;&#25321;&#33021;&#22815;&#37492;&#21035;&#30142;&#30149;&#30340;&#21306;&#22495;&#36827;&#34892;&#33041;&#37096;&#30142;&#30149;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models based on resting-state functional magnetic resonance imaging (rs-fMRI) have been widely used to diagnose brain diseases, particularly autism spectrum disorder (ASD). Existing studies have leveraged the functional connectivity (FC) of rs-fMRI, achieving notable classification performance. However, they have significant limitations, including the lack of adequate information while using linear low-order FC as inputs to the model, not considering individual characteristics (i.e., different symptoms or varying stages of severity) among patients with ASD, and the non-explainability of the decision process. To cover these limitations, we propose a novel explainability-guided region of interest (ROI) selection (EAG-RS) framework that identifies non-linear high-order functional associations among brain regions by leveraging an explainable artificial intelligence technique and selects class-discriminative regions for brain disease identification. The proposed framework incl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#20869;&#23481;&#23457;&#26597;&#30340;&#31169;&#19979;&#37096;&#32626;&#65292;&#35752;&#35770;&#20102;&#24341;&#20837;&#21407;&#22240;&#30340;&#24494;&#35843;&#36807;&#31243;&#21644;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#30340;&#21306;&#21035;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03400</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20869;&#23481;&#23457;&#26597;&#65306;&#25968;&#25454;&#24037;&#31243;&#21644;&#30417;&#30563;&#24494;&#35843;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#20869;&#23481;&#23457;&#26597;&#30340;&#31169;&#19979;&#37096;&#32626;&#65292;&#35752;&#35770;&#20102;&#24341;&#20837;&#21407;&#22240;&#30340;&#24494;&#35843;&#36807;&#31243;&#21644;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#30340;&#21306;&#21035;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#21313;&#20159;&#20154;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#27807;&#36890;&#24182;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24182;&#38750;&#25152;&#26377;&#36825;&#20123;&#34920;&#36798;&#37117;&#21451;&#22909;&#25110;&#21512;&#35268;&#65292;&#36825;&#20351;&#24471;&#20869;&#23481;&#23457;&#26597;&#25104;&#20026;&#19968;&#39033;&#19981;&#21487;&#25110;&#32570;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21457;&#23637;&#65292;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#24050;&#25104;&#20026;&#22788;&#29702;&#21508;&#20010;&#39046;&#22495;&#20219;&#21153;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20869;&#23481;&#23457;&#26597;&#39046;&#22495;&#65292;&#20173;&#32570;&#20047;&#35814;&#32454;&#30340;&#24037;&#20316;&#31995;&#32479;&#22320;&#20171;&#32461;&#23454;&#26045;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20415;&#21487;&#20197;&#31169;&#19979;&#37096;&#32626;&#29992;&#20110;&#20869;&#23481;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26159;&#21542;&#24212;&#35813;&#24341;&#20837;&#21407;&#22240;&#65292;&#20197;&#21450;&#23558;&#20854;&#35270;&#20026;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#26159;&#21542;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#31169;&#19979;&#37096;&#32626;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#22312;&#22238;&#31572;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#21516;&#22788;&#29702;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers 
&lt;/p&gt;</description></item><item><title>GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03399</link><description>&lt;p&gt;
GRAPES: &#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03399
&lt;/p&gt;
&lt;p&gt;
GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#32858;&#21512;&#21608;&#22260;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#30340;&#21152;&#28145;&#65292;&#30001;&#20110;&#37051;&#22495;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#24863;&#21463;&#37326;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#28040;&#32791;&#12290;&#22270;&#37319;&#26679;&#36890;&#36807;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;GNNs&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GNNs&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#12290;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#19987;&#27880;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32467;&#26500;&#25110;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GRAPES&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#35782;&#21035;&#29992;&#20110;&#35757;&#32451;GNN&#20998;&#31867;&#22120;&#30340;&#19968;&#32452;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12290;GRAPES&#20351;&#29992;GFlowNet&#26469;&#23398;&#20064;&#32473;&#23450;&#20998;&#31867;&#30446;&#26631;&#30340;&#33410;&#28857;&#37319;&#26679;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22270;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;GRAPES&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;GRAPES&#21363;&#20351;&#22312;&#37319;&#26679;&#27604;&#20363;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Gromov-Wasserstein&#21487;&#23454;&#29616;&#22312;&#32858;&#31867;&#21644;&#38477;&#32500;&#20043;&#38388;&#25554;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#21322;&#26494;&#24347;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#35745;&#31639;&#36755;&#20837;&#21644;&#23884;&#20837;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20943;&#23569;&#26679;&#26412;&#21644;&#29305;&#24449;&#25968;&#37327;&#30340;&#38477;&#32500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23884;&#20837;&#30340;&#32500;&#24230;&#19981;&#21463;&#32422;&#26463;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#30828;&#32858;&#31867;&#12290;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#32858;&#31867;&#34701;&#21512;&#20026;&#20013;&#38388;&#38454;&#27573;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#22312;&#24635;&#32467;&#30495;&#23454;&#25968;&#25454;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.03398</link><description>&lt;p&gt;
&#22312;Gromov-Wasserstein&#20013;&#25554;&#20540;&#32858;&#31867;&#21644;&#38477;&#32500;&#20043;&#38388;
&lt;/p&gt;
&lt;p&gt;
Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein. (arXiv:2310.03398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Gromov-Wasserstein&#21487;&#23454;&#29616;&#22312;&#32858;&#31867;&#21644;&#38477;&#32500;&#20043;&#38388;&#25554;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#21322;&#26494;&#24347;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#35745;&#31639;&#36755;&#20837;&#21644;&#23884;&#20837;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20943;&#23569;&#26679;&#26412;&#21644;&#29305;&#24449;&#25968;&#37327;&#30340;&#38477;&#32500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23884;&#20837;&#30340;&#32500;&#24230;&#19981;&#21463;&#32422;&#26463;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#30828;&#32858;&#31867;&#12290;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#32858;&#31867;&#34701;&#21512;&#20026;&#20013;&#38388;&#38454;&#27573;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#22312;&#24635;&#32467;&#30495;&#23454;&#25968;&#25454;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#26377;&#38477;&#32500;&#30446;&#26631;&#30340;&#36890;&#29992;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#26679;&#26412;&#21644;&#29305;&#24449;&#25968;&#37327;&#12290;&#36890;&#36807;&#21322;&#26494;&#24347;&#30340;Gromov-Wasserstein&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#35745;&#31639;&#36755;&#20837;&#21644;&#23884;&#20837;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#24403;&#23884;&#20837;&#26679;&#26412;&#25968;&#37327;&#19982;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;&#30456;&#21305;&#37197;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24674;&#22797;&#20102;&#32463;&#20856;&#30340;&#27969;&#34892;&#38477;&#32500;&#27169;&#22411;&#12290;&#24403;&#23884;&#20837;&#30340;&#32500;&#24230;&#19981;&#21463;&#32422;&#26463;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#25552;&#20379;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#30828;&#32858;&#31867;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;&#38477;&#32500;&#21644;&#32858;&#31867;&#34701;&#21512;&#20026;&#20013;&#38388;&#38454;&#27573;&#20197;&#24635;&#32467;&#30495;&#23454;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21487;&#35270;&#21270;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a versatile adaptation of existing dimensionality reduction (DR) objectives, enabling the simultaneous reduction of both sample and feature sizes. Correspondances between input and embedding samples are computed through a semi-relaxed Gromov-Wasserstein optimal transport (OT) problem. When the embedding sample size matches that of the input, our model recovers classical popular DR models. When the embedding's dimensionality is unconstrained, we show that the OT plan delivers a competitive hard clustering. We emphasize the importance of intermediate stages that blend DR and clustering for summarizing real data and apply our method to visualize datasets of images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31616;&#21270;&#27493;&#24577;&#20998;&#26512;&#20013;&#30340;&#26102;&#31354;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27493;&#24577;&#22522;&#20110;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#26159;&#20351;&#29992;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#35843;&#25972;&#27599;&#20010;&#34892;&#36208;&#23454;&#20363;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CASIA-B&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03396</link><description>&lt;p&gt;
&#23398;&#20064;&#31616;&#21270;&#27493;&#24577;&#20998;&#26512;&#20013;&#30340;&#26102;&#31354;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning to Simplify Spatial-Temporal Graphs in Gait Analysis. (arXiv:2310.03396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31616;&#21270;&#27493;&#24577;&#20998;&#26512;&#20013;&#30340;&#26102;&#31354;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27493;&#24577;&#22522;&#20110;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#26159;&#20351;&#29992;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#35843;&#25972;&#27599;&#20010;&#34892;&#36208;&#23454;&#20363;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CASIA-B&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#24577;&#20998;&#26512;&#21033;&#29992;&#29420;&#29305;&#30340;&#34892;&#36208;&#27169;&#24335;&#36827;&#34892;&#20154;&#29289;&#35782;&#21035;&#21644;&#22810;&#39046;&#22495;&#35780;&#20272;&#12290;&#22312;&#27493;&#24577;&#20998;&#26512;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#26041;&#27861;&#22240;&#20854;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32780;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#20154;&#20307;&#35299;&#21078;&#23398;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#26102;&#31354;&#22270;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#27493;&#24577;&#22522;&#20110;&#24615;&#21035;&#20272;&#35745;&#30340;&#26102;&#31354;&#22270;&#34920;&#31034;&#65292;&#25552;&#39640;&#35299;&#37322;&#24615;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19978;&#28216;&#27169;&#22411;&#21644;&#19979;&#28216;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#27599;&#20010;&#34892;&#36208;&#23454;&#20363;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22270;&#30340;&#22266;&#23450;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#30452;&#36890;Gumbel-Softmax&#25216;&#24039;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;CASIA-B&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#21040;&#30340;&#22270;&#24418;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gait analysis leverages unique walking patterns for person identification and assessment across multiple domains. Among the methods used for gait analysis, skeleton-based approaches have shown promise due to their robust and interpretable features. However, these methods often rely on hand-crafted spatial-temporal graphs that are based on human anatomy disregarding the particularities of the dataset and task. This paper proposes a novel method to simplify the spatial-temporal graph representation for gait-based gender estimation, improving interpretability without losing performance. Our approach employs two models, an upstream and a downstream model, that can adjust the adjacency matrix for each walking instance, thereby removing the fixed nature of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model is trainable end-to-end. We demonstrate the effectiveness of our approach on the CASIA-B dataset for gait-based gender estimation. The resulting graphs are interp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#32500;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#20272;&#35745;&#36817;&#20284;&#35299;&#26631;&#20934;&#24046;&#30340;UQ&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03393</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for deep learning-based schemes for solving high-dimensional backward stochastic differential equations. (arXiv:2310.03393v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#32500;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#20272;&#35745;&#36817;&#20284;&#35299;&#26631;&#20934;&#24046;&#30340;UQ&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#26041;&#26696;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#35768;&#22810;&#31185;&#23398;&#20852;&#36259;&#12290;&#34429;&#28982;&#23427;&#20204;&#33021;&#22815;&#36817;&#20284;&#35745;&#31639;&#38750;&#24120;&#39640;&#32500;&#30340;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#20294;&#20854;&#21487;&#38752;&#24615;&#23578;&#26410;&#34987;&#30740;&#31350;&#21644;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#28335;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26041;&#26696;&#20013;&#28041;&#21450;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#30740;&#31350;&#20102;&#19981;&#21516;&#26469;&#28304;&#30340;&#24433;&#21709;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#23545;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#22810;&#27425;&#36816;&#34892;&#31639;&#27861;&#24471;&#21040;&#30340;&#36817;&#20284;&#35299;&#30340;&#26631;&#20934;&#24046;&#65288;STD&#65289;&#36827;&#34892;&#35745;&#31639;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#35745;&#31639;&#25104;&#26412;&#30456;&#24403;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;UQ&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20165;&#20351;&#29992;&#19968;&#27425;&#31639;&#27861;&#36816;&#34892;&#26469;&#39640;&#25928;&#22320;&#20272;&#35745;&#36817;&#20284;&#35299;&#30340;&#26631;&#20934;&#24046;&#12290;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#20272;&#35745;&#36817;&#20284;&#35299;&#30340;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based numerical schemes for solving high-dimensional backward stochastic differential equations (BSDEs) have recently raised plenty of scientific interest. While they enable numerical methods to approximate very high-dimensional BSDEs, their reliability has not been studied and is thus not understood. In this work, we study uncertainty quantification (UQ) for a class of deep learning-based BSDE schemes. More precisely, we review the sources of uncertainty involved in the schemes and numerically study the impact of different sources. Usually, the standard deviation (STD) of the approximate solutions obtained from multiple runs of the algorithm with different datasets is calculated to address the uncertainty. This approach is computationally quite expensive, especially for high-dimensional problems. Hence, we develop a UQ model that efficiently estimates the STD of the approximate solution using only a single run of the algorithm. The model also estimates the mean of the ap
&lt;/p&gt;</description></item><item><title>OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.03388</link><description>&lt;p&gt;
OpenPatch:&#19968;&#20010;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;3D&#25340;&#36148;
&lt;/p&gt;
&lt;p&gt;
OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03388
&lt;/p&gt;
&lt;p&gt;
OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#23454;&#39564;&#23460;&#29615;&#22659;&#36801;&#31227;&#21040;&#24320;&#25918;&#19990;&#30028;&#65292;&#38656;&#35201;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#26410;&#30693;&#26465;&#20214;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#37096;&#32626;&#36807;&#31243;&#20013;&#20986;&#29616;&#26032;&#30340;&#31867;&#21035;&#21487;&#33021;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#26816;&#27979;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#33021;&#21147;&#24212;&#35813;&#22312;&#38656;&#35201;&#26102;&#20351;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#20013;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#35757;&#32451;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#32477;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22788;&#29702;2D&#22270;&#20687;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22266;&#26377;&#30340;3D&#29305;&#24615;&#65292;&#24182;&#32463;&#24120;&#28151;&#28102;&#39046;&#22495;&#21644;&#35821;&#20041;&#30340;&#26032;&#39062;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21518;&#32773;&#65292;&#32771;&#34385;&#30001;3D&#28857;&#20113;&#25429;&#25417;&#30340;&#29289;&#20307;&#20960;&#20309;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;OpenPatch&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#31616;&#21333;&#22320;&#25552;&#21462;&#20174;&#20854;&#20013;&#38388;&#29305;&#24449;&#20013;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#19968;&#32452;&#34917;&#19969;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#24674;&#22797;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#24182;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.03378</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Machine learning the interaction network in coupled dynamical systems. (arXiv:2310.03378v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#24674;&#22797;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#24182;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#30740;&#31350;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#20173;&#28982;&#20855;&#26377;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#19968;&#32452;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#20013;&#65292;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#21253;&#21547;&#20102;&#21508;&#20010;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20449;&#24687;&#12290;&#20174;&#20195;&#29702;&#30340;&#21160;&#24577;&#20013;&#25512;&#26029;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32467;&#26524;&#65306;&#24674;&#22797;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#21644;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;&#36825;&#20123;&#20449;&#24687;&#37117;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#26412;&#30740;&#31350;&#23558;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#27169;&#22411;&#24212;&#29992;&#20110;&#20004;&#20010;&#21160;&#21147;&#31995;&#32479;&#65306;&#36890;&#36807;&#32993;&#20811;&#23450;&#24459;&#30456;&#20114;&#20316;&#29992;&#30340;&#32806;&#21512;&#31890;&#23376;&#21644;&#32806;&#21512;&#30456;&#20301;&#65288;Kuramoto&#65289;&#25391;&#33633;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of interacting dynamical systems continues to attract research interest in various fields of science and engineering. In a collection of interacting particles, the interaction network contains information about how various components interact with one another. Inferring the information about the interaction network from the dynamics of agents is a problem of long-standing interest. In this work, we employ a self-supervised neural network model to achieve two outcomes: to recover the interaction network and to predict the dynamics of individual agents. Both these information are inferred solely from the observed trajectory data. This work presents an application of the Neural Relational Inference model to two dynamical systems: coupled particles mediated by Hooke's law interaction and coupled phase (Kuramoto) oscillators.
&lt;/p&gt;</description></item><item><title>Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03365</link><description>&lt;p&gt;
Swin-Tempo: &#20351;&#29992;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03365
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#20855;&#26377;&#26497;&#39640;&#30340;&#33268;&#27515;&#29575;&#65292;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#38450;&#27835;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#32780;&#35328;&#65292;&#35782;&#21035;&#32954;&#32467;&#33410;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#20381;&#36182;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#24050;&#32463;&#20986;&#29616;&#65292;&#24110;&#21161;&#21307;&#29983;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#35782;&#21035;&#32954;&#32467;&#33410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#24448;&#24448;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#35823;&#25253;&#21644;&#28431;&#25253;&#29575;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#20248;&#21183;&#12290;&#21463;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;3D CT&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#35270;&#39057;&#65292;&#23558;&#27599;&#20010;&#20999;&#29255;&#35270;&#20026;&#24103;&#65292;&#23558;&#32954;&#32467;&#33410;&#35270;&#20026;&#30446;&#26631;&#65292;&#23454;&#29616;&#19968;&#20010;&#26102;&#24207;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03358</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#36827;&#34892;&#40065;&#26834;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#35777;&#26126;&#26159;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27450;&#39575;&#30340;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;AT&#24573;&#35270;&#20102;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#23548;&#33268;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#40065;&#26834;&#34920;&#24449;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#25490;&#20182;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#36828;&#31163;&#20854;&#20182;&#31867;&#21035;&#30340;&#29305;&#24449;&#65307;&#65288;2&#65289;&#23545;&#40784;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#21644;&#30456;&#24212;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#24444;&#27492;&#25509;&#36817;&#12290;&#36825;&#20123;&#29305;&#28857;&#28608;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;AT&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#27010;&#29575;&#30340;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#25512;&#24320;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#65292;&#20316;&#20026;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#20316;&#20026;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21457;&#29616;&#33258;&#25105;&#23545;&#24328;&#26041;&#27861;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#32780;PSRO&#33021;&#22815;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03354</link><description>&lt;p&gt;
&#34394;&#26500;&#20132;&#21449;&#29609;: &#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#23398;&#20064;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#20316;&#20026;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21457;&#29616;&#33258;&#25105;&#23545;&#24328;&#26041;&#27861;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#32780;PSRO&#33021;&#22815;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#24328;&#65288;SP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#65292;&#22312;&#36825;&#31181;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#35270;&#20026;&#29615;&#22659;&#30340;&#19968;&#37096;&#20998;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;SP&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#20165;&#38480;&#20110;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#28982;&#32780;&#65292;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#65292;&#38656;&#35201;&#22242;&#38431;&#20013;&#30340;&#26234;&#33021;&#20307;&#30456;&#20114;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#21453;&#20363;&#26469;&#35777;&#26126;SP&#26041;&#27861;&#26080;&#27861;&#20197;&#39640;&#27010;&#29575;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;NE&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23398;&#20064;&#30456;&#23545;&#20110;&#20808;&#21069;&#31574;&#30053;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;PSRO&#21487;&#20197;&#30452;&#25509;&#25193;&#23637;&#20026;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#22330;&#26223;&#65292;&#21516;&#26102;&#23398;&#20064;&#22242;&#38431;&#26368;&#20339;&#21709;&#24212;&#32780;&#25152;&#26377;&#25910;&#25947;&#24615;&#36136;&#22343;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#30452;&#21040;&#25910;&#25947;&#65292;&#36825;&#20351;&#24471;&#23427;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#25968;&#25454;&#20960;&#20309;&#23646;&#24615;&#32771;&#34385;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#30340;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.03353</link><description>&lt;p&gt;
&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression. (arXiv:2310.03353v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#25968;&#25454;&#20960;&#20309;&#23646;&#24615;&#32771;&#34385;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#30340;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#27585;&#28781;&#24615;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20854;&#22312;&#28176;&#36827;&#21644;&#19981;&#21487;&#36870;&#30340;&#30196;&#21574;&#21069;&#20986;&#29616;&#65307;&#22240;&#27492;&#65292;&#23545;&#20854;&#38543;&#26102;&#38388;&#30340;&#36827;&#23637;&#36827;&#34892;&#39044;&#27979;&#23545;&#20110;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#37319;&#29992;&#32467;&#26500;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26469;&#27169;&#25311;AD&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;i&#65289;&#26102;&#38388;&#21464;&#24322;&#24615;&#65292;&#65288;ii&#65289;&#19981;&#23436;&#25972;&#35266;&#27979;&#65292;&#21644;&#65288;iii&#65289;&#26102;&#38388;&#20960;&#20309;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#25968;&#25454;&#21464;&#24322;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#32771;&#34385;&#20854;&#22266;&#26377;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#24314;&#27169;&#26041;&#27861;&#65288;ODE-RGRU&#65289;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#36890;&#36807;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;ODE&#20132;&#32455;&#22312;&#19968;&#36215;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#25104;&#23601;&#65292;&#20294;ODE-RGRU&#22312;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#26102;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#23548;&#33268;&#29305;&#24449;&#21453;&#36716;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a devastating neurodegenerative condition that precedes progressive and irreversible dementia; thus, predicting its progression over time is vital for clinical diagnosis and treatment. Numerous studies have implemented structural magnetic resonance imaging (MRI) to model AD progression, focusing on three integral aspects: (i) temporal variability, (ii) incomplete observations, and (iii) temporal geometric characteristics. However, deep learning-based approaches regarding data variability and sparsity have yet to consider inherent geometrical properties sufficiently. The ordinary differential equation-based geometric modeling method (ODE-RGRU) has recently emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. Despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#21644;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#26469;&#29983;&#25104;&#31283;&#20581;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#38899;&#39057;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#31354;&#20013;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#36827;&#34892;&#20154;&#31867;&#30740;&#31350;&#35780;&#20272;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03349</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#31283;&#20581;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#38899;&#39057;&#23545;&#25239;&#26679;&#26412;&#30340;&#32508;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples. (arXiv:2310.03349v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#21644;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#26469;&#29983;&#25104;&#31283;&#20581;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#38899;&#39057;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#31354;&#20013;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#36827;&#34892;&#20154;&#31867;&#30740;&#31350;&#35780;&#20272;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23545;&#25239;&#26679;&#26412;&#26159;&#32463;&#36807;&#22788;&#29702;&#30340;&#38899;&#39057;&#25991;&#20214;&#65292;&#26088;&#22312;&#27450;&#39575;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#21516;&#26102;&#23545;&#20154;&#31867;&#21548;&#20247;&#21548;&#36215;&#26469;&#20173;&#28982;&#26080;&#23475;&#12290;&#22823;&#22810;&#25968;&#29983;&#25104;&#36825;&#31181;&#26679;&#26412;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#27493;&#31639;&#27861;&#65306;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#38899;&#39057;&#25991;&#20214;&#65292;&#28982;&#21518;&#22312;&#24863;&#30693;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#31639;&#27861;&#65292;&#22312;&#29983;&#25104;&#27493;&#39588;&#20013;&#20351;&#29992;&#20102;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#21644;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#12290;RIR&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#21019;&#24314;&#30340;&#65292;&#20197;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#26679;&#26412;&#23545;&#32463;&#21382;&#36807;&#31354;&#20013;&#25915;&#20987;&#30340;&#36716;&#25442;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21644;&#22312;&#36924;&#30495;&#30340;&#31354;&#20013;&#22330;&#26223;&#20013;&#35780;&#20272;&#31283;&#20581;&#24615;&#65292;&#22312;&#20154;&#31867;&#30740;&#31350;&#20013;&#35780;&#20272;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio adversarial examples are audio files that have been manipulated to fool an automatic speech recognition (ASR) system, while still sounding benign to a human listener. Most methods to generate such samples are based on a two-step algorithm: first, a viable adversarial audio file is produced, then, this is fine-tuned with respect to perceptibility and robustness. In this work, we present an integrated algorithm that uses psychoacoustic models and room impulse responses (RIR) in the generation step. The RIRs are dynamically created by a neural network during the generation process to simulate a physical environment to harden our examples against transformations experienced in over-the-air attacks. We compare the different approaches in three experiments: in a simulated environment and in a realistic over-the-air scenario to evaluate the robustness, and in a human study to evaluate the perceptibility. Our algorithms considering psychoacoustics only or in addition to the robustness sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03342</link><description>&lt;p&gt;
LESSON: &#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework. (arXiv:2310.03342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#39033;&#25209;&#21028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#32479;&#19968;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#36866;&#24212;&#24615;&#22320;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#32473;&#23450;&#20219;&#21153;&#19979;&#30340;&#30456;&#20851;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#36827;&#34892;&#21508;&#31181;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25506;&#32034;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a unified framework for exploration in reinforcement learning (RL) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the MiniGrid and Atari environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#24503;&#22269;-&#21346;&#26862;&#22561;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#23454;&#29616;&#20102;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#36229;&#32479;&#35745;&#23398;&#26041;&#27861;&#23545;&#20215;&#26684;&#30340;&#32479;&#35745;&#24615;&#36136;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.03339</link><description>&lt;p&gt;
&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#21450;&#20854;&#27874;&#21160;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs. (arXiv:2310.03339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#24503;&#22269;-&#21346;&#26862;&#22561;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#23454;&#29616;&#20102;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#36229;&#32479;&#35745;&#23398;&#26041;&#27861;&#23545;&#20215;&#26684;&#30340;&#32479;&#35745;&#24615;&#36136;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30005;&#21147;&#20215;&#26684;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#31649;&#29702;&#21644;&#26234;&#33021;&#24212;&#29992;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#27431;&#27954;&#30340;&#30005;&#21147;&#20215;&#26684;&#22823;&#24133;&#19978;&#28072;&#24182;&#21464;&#24471;&#39640;&#24230;&#27874;&#21160;&#65292;&#32473;&#24050;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#24503;&#22269;-&#21346;&#26862;&#22561;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#27169;&#22411;&#12290;LSTM&#30340;&#24490;&#29615;&#32467;&#26500;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#36235;&#21183;&#65292;&#32780;&#23545;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#36827;&#34892;&#32852;&#21512;&#39044;&#27979;&#21017;&#23454;&#29616;&#20102;&#27010;&#29575;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#21551;&#21457;&#26041;&#27861;&#8212;&#8212;&#36229;&#32479;&#35745;&#23398;&#65292;&#26469;&#35299;&#37322;&#20215;&#26684;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LSTM&#27169;&#22411;&#23545;&#30005;&#21147;&#20215;&#26684;&#21644;&#27874;&#21160;&#24615;&#30340;&#20934;&#30830;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate forecasts of electricity prices are crucial for the management of electric power systems and the development of smart applications. European electricity prices have risen substantially and became highly volatile after the Russian invasion of Ukraine, challenging established forecasting methods. Here, we present a Long Short-Term Memory (LSTM) model for the German-Luxembourg day-ahead electricity prices addressing these challenges. The recurrent structure of the LSTM allows the model to adapt to trends, while the joint prediction of both mean and standard deviation enables a probabilistic prediction. Using a physics-inspired approach - superstatistics - to derive an explanation for the statistics of prices, we show that the LSTM model faithfully reproduces both prices and their volatility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24037;&#20316;&#37325;&#28857;&#30740;&#31350;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#37319;&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.03334</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#30340;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System. (arXiv:2310.03334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24037;&#20316;&#37325;&#28857;&#30740;&#31350;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#37319;&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26159;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#21644;&#32593;&#32476;&#25915;&#20987;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19968;&#31181;&#19981;&#24184;&#30340;&#24773;&#20917;&#65292;&#21363;NIDS&#26412;&#36523;&#21463;&#21040;&#25915;&#20987;&#24182;&#26131;&#21463;&#25915;&#20987;&#65292;&#25105;&#20204;&#21487;&#20197;&#35828;&#65292;&#22914;&#20309;&#20445;&#21355;&#25421;&#21355;&#32773;&#65311;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;AML&#65289;&#20013;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#26088;&#22312;&#36890;&#36807;&#29305;&#24847;&#21046;&#20316;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20135;&#29983;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#23545;&#25239;&#24615;&#26679;&#26412;&#24050;&#25104;&#20026;ML&#21644;DL&#31995;&#32479;&#30340;&#26368;&#22823;&#28431;&#27934;&#65292;&#24182;&#19988;&#26159;&#22312;&#23454;&#26102;&#21644;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#65288;&#20363;&#22914;NIDS&#65289;&#20013;&#37319;&#29992;&#23427;&#20204;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;AML&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#31574;&#30053;&#30340;&#28145;&#20837;&#30740;&#31350;&#24050;&#25104;&#20026;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#28085;&#30422;&#19982;NI&#30456;&#20851;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Intrusion Detection System (NIDS) is a key component in securing the computer network from various cyber security threats and network attacks. However, consider an unfortunate situation where the NIDS is itself attacked and vulnerable more specifically, we can say, How to defend the defender?. In Adversarial Machine Learning (AML), the malicious actors aim to fool the Machine Learning (ML) and Deep Learning (DL) models to produce incorrect predictions with intentionally crafted adversarial examples. These adversarial perturbed examples have become the biggest vulnerability of ML and DL based systems and are major obstacles to their adoption in real-time and mission-critical applications such as NIDS. AML is an emerging research domain, and it has become a necessity for the in-depth study of adversarial attacks and their defence strategies to safeguard the computer network from various cyber security threads. In this research work, we aim to cover important aspects related to NI
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;RICL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#32447;&#24615;&#26368;&#20248;&#26435;&#37325;&#36817;&#20284;&#31639;&#27861;LARICL&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26080;&#20559;&#39564;&#35777;&#38598;&#21644;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#30340;&#26368;&#20339;&#26435;&#37325;&#65292;&#23454;&#29616;&#26080;&#20559;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.03331</link><description>&lt;p&gt;
&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#26080;&#20559;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fine-tune Language Models to Approximate Unbiased In-context Learning. (arXiv:2310.03331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03331
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;RICL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#32447;&#24615;&#26368;&#20248;&#26435;&#37325;&#36817;&#20284;&#31639;&#27861;LARICL&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26080;&#20559;&#39564;&#35777;&#38598;&#21644;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#30340;&#26368;&#20339;&#26435;&#37325;&#65292;&#23454;&#29616;&#26080;&#20559;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;&#22810;&#20010;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20316;&#20026;&#31034;&#20363;&#30340;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#65292;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26045;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#20559;&#24046;&#25110;&#19981;&#24179;&#34913;&#30340;&#36755;&#20837;&#25552;&#31034;&#20250;&#26174;&#33879;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RICL&#65288;&#37325;&#21152;&#26435;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#30340;&#37325;&#21152;&#26435;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#26080;&#20559;&#39564;&#35777;&#38598;&#26469;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#30340;&#26368;&#20339;&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#26080;&#20559;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#37325;&#21152;&#26435;&#31639;&#27861;&#65292;&#19968;&#31181;&#31216;&#20026;LARICL&#65288;&#37325;&#21152;&#26435;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32447;&#24615;&#36817;&#20284;&#65289;&#30340;&#32447;&#24615;&#26368;&#20248;&#26435;&#37325;&#36817;&#20284;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#35757;&#32451;&#25104;&#26412;&#19978;&#35201;&#27714;&#26368;&#23567;&#65292;&#21516;&#26102;&#25552;&#20379;&#39640;&#25928;&#20934;&#30830;&#30340;&#37325;&#21152;&#26435;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. However, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue, we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2310.03325</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#21644;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35268;&#21010;&#27169;&#25311;&#20102;&#20154;&#31867;&#22312;&#25628;&#32034;&#21021;&#22987;&#35270;&#35273;&#29366;&#24577;&#21644;&#26368;&#32456;&#35270;&#35273;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#20013;&#65292;&#35270;&#35273;&#35268;&#21010;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#35270;&#35273;&#35268;&#21010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#20195;&#30340;&#27010;&#24565;&#23398;&#20064;&#22120;&#65288;SCL&#65289;&#65292;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#20998;&#35299;&#30340;&#27010;&#24565;&#34920;&#31034;&#65307;ii&#65289;&#36890;&#36807;&#33258;&#23398;&#31526;&#21495;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#30340;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#65307;iii&#65289;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#35821;&#20041;&#30456;&#20284;&#30340;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#36827;&#34892;&#20851;&#32852;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#27169;&#22411;&#65288;ViCT&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#29366;&#24577;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#22240;&#26524;&#36716;&#25442;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#65292;&#20197;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
&lt;/p&gt;</description></item><item><title>CLIP&#27169;&#22411;&#34920;&#29616;&#26368;&#24046;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#25972;&#20307;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29305;&#23450;&#31867;&#21035;&#37325;&#35201;&#24615;&#36739;&#39640;&#30340;&#39118;&#38505;&#25935;&#24863;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#34913;&#37327;&#26368;&#24046;&#31867;&#21035;&#30340;&#25512;&#29702;&#28151;&#28102;&#30340;&#31867;&#21035;&#21305;&#37197;&#36793;&#30028;&#65288;CMM&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.03324</link><description>&lt;p&gt;
&#30740;&#31350;CLIP&#27169;&#22411;&#30340;&#38480;&#21046;&#65306;&#34920;&#29616;&#26368;&#24046;&#30340;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03324
&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#34920;&#29616;&#26368;&#24046;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#25972;&#20307;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29305;&#23450;&#31867;&#21035;&#37325;&#35201;&#24615;&#36739;&#39640;&#30340;&#39118;&#38505;&#25935;&#24863;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#34913;&#37327;&#26368;&#24046;&#31867;&#21035;&#30340;&#25512;&#29702;&#28151;&#28102;&#30340;&#31867;&#21035;&#21305;&#37197;&#36793;&#30028;&#65288;CMM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#19982;&#35270;&#35273;&#27010;&#24565;&#25972;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#36827;&#34892;&#38646;&#26679;&#26412;&#35782;&#21035;&#12290;&#36890;&#24120;&#26399;&#26395;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25991;&#26412;&#25552;&#31034;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#22312;&#26368;&#24046;&#31867;&#21035;&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;&#25972;&#20307;&#34920;&#29616;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet&#19978;&#65292;&#20849;&#26377;10&#20010;&#31867;&#21035;&#30340;&#31867;&#21035;&#20934;&#30830;&#29575;&#20165;&#20026;0&#65285;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#24050;&#36798;&#21040;64.1&#65285;&#12290;&#36825;&#31181;&#29616;&#35937;&#25581;&#31034;&#20102;&#20351;&#29992;CLIP&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#65292;&#29305;&#23450;&#31867;&#21035;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#20013;&#20004;&#20010;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#34913;&#37327;&#25512;&#29702;&#28151;&#28102;&#30340;&#31867;&#21035;&#21305;&#37197;&#36793;&#30028;&#65288;CMM&#65289;&#12290;CMM&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26368;&#24046;&#34920;&#29616;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) provides a foundation model by integrating natural language into visual concepts, enabling zero-shot recognition on downstream tasks. It is usually expected that satisfactory overall accuracy can be achieved across numerous domains through well-designed textual prompts. However, we found that their performance in the worst categories is significantly inferior to the overall performance. For example, on ImageNet, there are a total of 10 categories with class-wise accuracy as low as 0\%, even though the overall performance has achieved 64.1\%. This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance. To address this issue, we investigate the alignment between the two modalities in the CLIP model and propose the Class-wise Matching Margin (\cmm) to measure the inference confusion. \cmm\ can effectively identify the worst-per
&lt;/p&gt;</description></item><item><title>BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03320</link><description>&lt;p&gt;
BioBridge: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03320
&lt;/p&gt;
&lt;p&gt;
BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;FMs&#20027;&#35201;&#20173;&#22788;&#20110;&#21333;&#27169;&#24577;&#29366;&#24577;&#65292;&#21363;&#29420;&#31435;&#35757;&#32451;&#24182;&#29992;&#20110;&#22788;&#29702;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#23567;&#20998;&#23376;&#32467;&#26500;&#25110;&#20020;&#24202;&#25968;&#25454;&#31561;&#21333;&#19968;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29983;&#29289;&#21307;&#23398;FMs&#30340;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;BioBridge&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#26469;&#23398;&#20064;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#24213;&#23618;&#21333;&#27169;&#24577;FMs&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#26725;&#25509;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;FMs&#20197;&#24314;&#31435;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21487;&#20197;&#20987;&#36133;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65288;&#24179;&#22343;&#25552;&#39640;&#32422;76.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;BioBridge&#34920;&#29616;&#20986;&#39046;&#22495;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#30340;&#27169;&#24577;&#25110;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#20197;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.03314</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#38480;&#27010;&#29575;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#20197;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#26159;&#23454;&#29616;&#39640;&#25928;&#23433;&#20840;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#20381;&#36182;&#20110;&#23558;&#20154;&#31867;&#20851;&#33410;&#28857;&#34920;&#31034;&#20026;&#26576;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35201;&#20040;&#20351;&#29992;&#31163;&#32447;&#22238;&#24402;&#27169;&#22411;&#26469;&#25311;&#21512;&#36229;&#21442;&#25968;&#65292;&#20197;&#25429;&#25417;&#21253;&#21547;&#20154;&#20307;&#21160;&#20316;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#21021;&#22987;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21033;&#29992;&#30740;&#31350;&#20805;&#20998;&#30340;&#20154;&#20307;&#36816;&#21160;&#23398;&#27169;&#22411;&#20197;&#21450;&#36523;&#20307;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#26377;&#21161;&#20110;&#25552;&#39640;&#36825;&#20123;&#39044;&#27979;&#26694;&#26550;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#26126;&#30830;&#36991;&#20813;&#19981;&#21512;&#29702;&#30340;&#20154;&#20307;&#20851;&#33410;&#37197;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#32467;&#21512;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#27169;&#22411;&#20013;&#65292;&#20197;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#12290;&#27492;&#20844;&#24335;&#19982;&#22312;&#32447;&#19978;&#19979;&#25991;&#24863;&#30693;&#32422;&#26463;&#27169;&#22411;&#32467;&#21512;&#65292;&#20197;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#36816;&#21160;&#12290;&#22312;&#20154;&#20307;&#33218;&#36816;&#21160;&#23398;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#36793;&#32536;&#24179;&#28369;&#65288;RES&#65289;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#20219;&#20309;GCL&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#20102;GCL&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03312</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Certifiably Robust Graph Contrastive Learning. (arXiv:2310.03312v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#36793;&#32536;&#24179;&#28369;&#65288;RES&#65289;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#20219;&#20309;GCL&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#20102;GCL&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#34920;&#26126;GCL&#23545;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#32463;&#39564;&#24615;&#26041;&#27861;&#26469;&#22686;&#24378;GCL&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;GCL&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;GCL&#20013;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#21644;&#35777;&#26126;GCL&#40065;&#26834;&#24615;&#30340;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#38543;&#26426;&#36793;&#32536;&#24179;&#28369;&#65288;RES&#65289;&#65292;&#20197;&#30830;&#20445;&#20219;&#20309;GCL&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#34987;&#20445;&#30041;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;GCL&#30340;&#26377;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#26377;&#25928;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#20219;&#20309;GCL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any G
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.03311</link><description>&lt;p&gt;
&#28145;&#24230;&#21464;&#20998;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;--&#19968;&#31181;&#21464;&#20998;&#25439;&#22833;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#20197;&#20854;&#39640;&#31934;&#24230;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#24456;&#22810;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30456;&#20114;&#26435;&#34913;&#12290;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22120;&#22270;&#65292;&#23427;&#25351;&#23450;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#26102;&#35201;&#20445;&#30041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#35299;&#30721;&#22120;&#22270;&#65292;&#23427;&#20026;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#25512;&#23548;&#20102;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(DVIB)&#12289;beta&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(beta-VAE)&#21644;&#28145;&#24230;&#21464;&#20998;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(DVCCA)&#12290;&#35813;&#26694;&#26550;&#33258;&#28982;&#22320;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.03302</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23454;&#39564;&#28041;&#21450;&#21019;&#24314;&#20551;&#35774;&#12289;&#35774;&#35745;&#23454;&#39564;&#12289;&#36816;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;AI&#30740;&#31350;&#20195;&#29702;&#26469;&#25191;&#34892;&#36825;&#20123;&#38271;&#26399;&#30446;&#26631;&#30340;&#20219;&#21153;&#21602;&#65311;&#20026;&#20102;&#26397;&#30528;&#22312;&#27492;&#31867;&#24320;&#25918;&#24615;&#20915;&#31574;&#20219;&#21153;&#19978;&#26500;&#24314;&#21644;&#35780;&#20272;&#30740;&#31350;&#20195;&#29702;&#30340;&#30446;&#26631;&#36808;&#20986;&#19968;&#27493;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#12290;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#35835;&#20889;&#25991;&#20214;&#12289;&#25191;&#34892;&#20195;&#30721;&#21644;&#26816;&#26597;&#36755;&#20986;&#31561;&#21160;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#21160;&#20316;&#65292;&#20195;&#29702;&#21487;&#20197;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;&#28982;&#21518;&#65292;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#29702;&#22312;&#19982;&#24615;&#33021;&#21644;&#25928;&#29575;&#30456;&#20851;&#30340;&#21508;&#31181;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;LLM-
&lt;/p&gt;
&lt;p&gt;
Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;&#65288;LED-GFN&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#22312;&#37319;&#26679;&#23545;&#35937;&#26102;&#30340;&#37096;&#20998;&#25512;&#26029;&#12290;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#28508;&#22312;&#20989;&#25968;&#23545;&#27969;&#20989;&#25968;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#25805;&#20316;&#24207;&#21015;&#20013;&#33021;&#37327;&#27874;&#21160;&#36739;&#22823;&#26102;&#30340;&#35757;&#32451;&#20449;&#21495;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03301</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#37327;&#20998;&#35299;&#29992;&#20110;GFlowNets&#37096;&#20998;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Energy Decompositions for Partial Inference of GFlowNets. (arXiv:2310.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;&#65288;LED-GFN&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#22312;&#37319;&#26679;&#23545;&#35937;&#26102;&#30340;&#37096;&#20998;&#25512;&#26029;&#12290;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#28508;&#22312;&#20989;&#25968;&#23545;&#27969;&#20989;&#25968;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#25805;&#20316;&#24207;&#21015;&#20013;&#33021;&#37327;&#27874;&#21160;&#36739;&#22823;&#26102;&#30340;&#35757;&#32451;&#20449;&#21495;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#20174;Boltzmann&#33021;&#37327;&#20998;&#24067;&#20013;&#37319;&#26679;&#23545;&#35937;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#25913;&#36827;GFlowNet&#30340;&#37096;&#20998;&#25512;&#26029;&#26041;&#27861;&#65306;&#20351;&#29992;&#20013;&#38388;&#29366;&#24577;&#25110;&#36716;&#25442;&#30340;&#35780;&#20272;&#26469;&#35757;&#32451;&#27969;&#20989;&#25968;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#21457;&#23637;&#30340;&#21069;&#30651;&#24615;GFlowNet&#22522;&#20110;&#35780;&#20272;&#20013;&#38388;&#29366;&#24577;&#30340;&#33021;&#37327;&#23545;&#27969;&#20989;&#25968;&#36827;&#34892;&#20102;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20013;&#38388;&#33021;&#37327;&#30340;&#35780;&#20272;&#21487;&#33021;&#65288;i&#65289;&#22826;&#26114;&#36149;&#25110;&#26080;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#65288;ii&#65289;&#29978;&#33267;&#22312;&#25805;&#20316;&#24207;&#21015;&#20013;&#33021;&#37327;&#27874;&#21160;&#36739;&#22823;&#26102;&#25552;&#20379;&#35823;&#23548;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;GFlowNets&#30340;&#33021;&#37327;&#20998;&#35299;&#23398;&#20064;&#65288;LED-GFN&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#65288;i&#65289;&#23558;&#23545;&#35937;&#30340;&#33021;&#37327;&#20998;&#35299;&#20026;&#22312;&#29366;&#24577;&#36716;&#25442;&#19978;&#23450;&#20041;&#30340;&#21487;&#23398;&#20064;&#28508;&#22312;&#20989;&#25968;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#36825;&#20123;&#28508;&#22312;&#20989;&#25968;&#23545;&#27969;&#20989;&#25968;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03298</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#28508;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;&#26041;&#27861;&#22312;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#21644;&#35774;&#35745;&#20248;&#21270;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#20302;&#20445;&#30495;&#24230;&#65288;LF&#65289;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20551;&#23450;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#26159;&#21160;&#24577;&#20998;&#37197;&#36164;&#28304;&#22312;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20381;&#36182;&#20110;&#20445;&#30495;&#24230;&#32423;&#21035;&#30340;&#23618;&#27425;&#20551;&#35774;&#65292;&#25110;&#32773;&#26080;&#27861;&#25429;&#25417;&#22810;&#20010;&#20445;&#30495;&#24230;&#32423;&#21035;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#21033;&#29992;&#20854;&#26469;&#37327;&#21270;&#26410;&#26469;&#26679;&#26412;&#30340;&#20215;&#20540;&#21644;&#23548;&#33322;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#23884;&#20837;&#21644;&#30456;&#20851;&#30340;&#20808;&#39564;-&#21518;&#39564;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#22635;&#20805;&#37319;&#26679;&#36845;&#20195;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#25105;&#20204;&#30830;&#23450;&#20855;&#26377;&#26368;&#22823;&#28508;&#21147;&#24433;&#21709;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
&lt;/p&gt;</description></item><item><title>LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2310.03294</link><description>&lt;p&gt;
LightSeq&#65306;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24207;&#21015;&#32423;&#24182;&#34892;ism
&lt;/p&gt;
&lt;p&gt;
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03294
&lt;/p&gt;
&lt;p&gt;
LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#35299;&#24320;&#22522;&#26412;&#19978;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#26174;&#33879;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#24182;&#34892;&#31995;&#32479;&#65288;&#20363;&#22914;Megatron-LM&#65289;&#23545;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#21306;&#21644;&#35745;&#31639;&#65292;&#24182;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#22823;&#37327;&#36890;&#20449;&#37327;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#20043;&#22806;&#25193;&#23637;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LightSeq&#65292;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;LLM&#30340;&#35757;&#32451;&#12290;LightSeq&#20855;&#26377;&#35768;&#22810;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;LightSeq&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#36866;&#29992;&#20110;Multi-Head&#65292;Multi-Query&#21644;Grouped-Query attention&#31561;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;LightSeq&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#27969;&#34892;&#30340;LLM&#19978;&#19981;&#20165;&#38656;&#27714;&#23569;&#33267;4.7&#20493;&#30340;&#36890;&#20449;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#36890;&#20449;&#19982;&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;LightSeq&#36824;&#20855;&#26377;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;che
&lt;/p&gt;
&lt;p&gt;
Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;PoseAction&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24322;&#27493;&#20132;&#20114;&#32858;&#21512;&#32593;&#32476;&#26469;&#23454;&#29616;&#30149;&#25151;&#24739;&#32773;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#24739;&#32773;&#26816;&#27979;&#21644;&#24120;&#35265;&#21160;&#20316;&#30340;&#39044;&#27979;&#65292;&#21487;&#20197;&#25552;&#39640;&#30149;&#25151;&#30340;&#25252;&#29702;&#25928;&#29575;&#21644;&#38477;&#20302;&#25252;&#29702;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03288</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#30149;&#25151;&#24739;&#32773;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches. (arXiv:2310.03288v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;PoseAction&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24322;&#27493;&#20132;&#20114;&#32858;&#21512;&#32593;&#32476;&#26469;&#23454;&#29616;&#30149;&#25151;&#24739;&#32773;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#24739;&#32773;&#26816;&#27979;&#21644;&#24120;&#35265;&#21160;&#20316;&#30340;&#39044;&#27979;&#65292;&#21487;&#20197;&#25552;&#39640;&#30149;&#25151;&#30340;&#25252;&#29702;&#25928;&#29575;&#21644;&#38477;&#20302;&#25252;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30149;&#25151;&#20013;&#23454;&#26102;&#26234;&#33021;&#22320;&#26816;&#27979;&#21644;&#39044;&#27979;&#24739;&#32773;&#30340;&#34892;&#20026;&#65292;&#23588;&#20854;&#26159;&#20182;&#20204;&#30340;&#36816;&#21160;&#25110;&#21160;&#20316;&#65292;&#23545;&#20110;&#38477;&#20302;&#20303;&#38498;&#25252;&#29702;&#25104;&#26412;&#21644;&#25552;&#39640;&#21307;&#25252;&#20154;&#21592;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22812;&#38388;&#25110;&#39640;&#23792;&#26399;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#24739;&#32773;&#24182;&#35782;&#21035;&#20854;&#21160;&#20316;&#12290;&#25105;&#20204;&#21033;&#29992;OpenPose&#20316;&#20026;&#20934;&#30830;&#30340;&#24739;&#32773;&#26816;&#27979;&#22120;&#65292;&#26469;&#35782;&#21035;&#35270;&#39057;&#27969;&#20013;&#20154;&#20307;&#30340;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;AlphAction&#30340;&#24322;&#27493;&#20132;&#20114;&#32858;&#21512;&#65288;AIA&#65289;&#32593;&#32476;&#26469;&#39044;&#27979;&#24050;&#26816;&#27979;&#21040;&#30340;&#24739;&#32773;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;PoseAction&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26469;&#33258;NTU&#21307;&#23398;&#35270;&#39057;&#29255;&#27573;&#26469;&#39044;&#27979;&#30149;&#25151;&#21306;&#22495;&#30340;12&#31181;&#24120;&#35265;&#21160;&#20316;&#65292;&#20363;&#22914;&#36434;&#36314;&#12289;&#33016;&#30171;&#21644;&#25684;&#20498;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time intelligent detection and prediction of subjects' behavior particularly their movements or actions is critical in the ward. This approach offers the advantage of reducing in-hospital care costs and improving the efficiency of healthcare workers, which is especially true for scenarios at night or during peak admission periods. Therefore, in this work, we propose using computer vision (CV) and deep learning (DL) methods for detecting subjects and recognizing their actions. We utilize OpenPose as an accurate subject detector for recognizing the positions of human subjects in the video stream. Additionally, we employ AlphAction's Asynchronous Interaction Aggregation (AIA) network to predict the actions of detected subjects. This integrated model, referred to as PoseAction, is proposed. At the same time, the proposed model is further trained to predict 12 common actions in ward areas, such as staggering, chest pain, and falling down, using medical-related video clips from the NTU 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20108;&#36827;&#21046;&#32423;&#21464;&#24322;&#30340;&#24378;&#20581;Windows&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#26816;&#27979;&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#39044;&#22788;&#29702;&#21644;&#22270;&#20687;&#25552;&#21462;&#30340;&#26041;&#26696;&#20197;&#24212;&#23545;&#23545;&#25239;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20256;&#32479;&#27169;&#22411;&#23545;&#25239;&#23041;&#32961;&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.03285</link><description>&lt;p&gt;
&#29123;&#28903;&#23545;&#25239;&#24615;&#26725;&#26753;&#65306;&#38024;&#23545;&#20108;&#36827;&#21046;&#32423;&#21464;&#24322;&#30340;&#24378;&#20581;Windows&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations. (arXiv:2310.03285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20108;&#36827;&#21046;&#32423;&#21464;&#24322;&#30340;&#24378;&#20581;Windows&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#26816;&#27979;&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#39044;&#22788;&#29702;&#21644;&#22270;&#20687;&#25552;&#21462;&#30340;&#26041;&#26696;&#20197;&#24212;&#23545;&#23545;&#25239;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20256;&#32479;&#27169;&#22411;&#23545;&#25239;&#23041;&#32961;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24378;&#20581;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29616;&#26377;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;&#12290;&#25105;&#20204;&#23545;&#23454;&#38469;&#30340;&#20108;&#36827;&#21046;&#32423;&#40657;&#30418;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#36827;&#34892;&#20102;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26816;&#27979;&#24341;&#25806;&#20013;&#26131;&#21463;&#25915;&#20987;&#30340;&#26131;&#22833;&#24615;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#21487;&#21033;&#29992;&#24615;&#12290;&#36890;&#36807;&#31361;&#20986;&#36719;&#20214;&#20013;&#30340;&#26131;&#22833;&#24615;&#20449;&#24687;&#28192;&#36947;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#36719;&#20214;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#28040;&#38500;&#25915;&#20987;&#38754;&#65292;&#21363;&#22635;&#20805;&#21435;&#38500;&#12289;&#36719;&#20214;&#21093;&#31163;&#21644;&#20132;&#21449;&#20449;&#24687;&#37325;&#32622;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24212;&#23545;&#26032;&#20852;&#30340;&#33410;&#27573;&#27880;&#20837;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#33410;&#27573;&#20381;&#36182;&#20449;&#24687;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#36719;&#20214;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#21033;&#29992;&#36719;&#20214;&#20013;&#21508;&#20010;&#33410;&#27573;&#20869;&#30340;&#27719;&#38598;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24378;&#20581;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#32531;&#35299;&#23545;&#25239;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#23545;&#25239;&#23545;&#25239;&#24615;&#23041;&#32961;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward robust malware detection, we explore the attack surface of existing malware detection systems. We conduct root-cause analyses of the practical binary-level black-box adversarial malware examples. Additionally, we uncover the sensitivity of volatile features within the detection engines and exhibit their exploitability. Highlighting volatile information channels within the software, we introduce three software pre-processing steps to eliminate the attack surface, namely, padding removal, software stripping, and inter-section information resetting. Further, to counter the emerging section injection attacks, we propose a graph-based section-dependent information extraction scheme for software representation. The proposed scheme leverages aggregated information within various sections in the software to enable robust malware detection and mitigate adversarial settings. Our experimental results show that traditional malware detection models are ineffective against adversarial threats
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03281</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#30721;mRNA&#30340;5' UTR&#35821;&#35328;&#27169;&#22411;&#21644;&#21151;&#33021;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5' UTR&#26159;mRNA&#20998;&#23376;&#24320;&#31471;&#30340;&#35843;&#25511;&#21306;&#22495;&#65292;&#22312;&#35843;&#25511;&#32763;&#35793;&#36807;&#31243;&#21644;&#24433;&#21709;&#34507;&#30333;&#34920;&#36798;&#27700;&#24179;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35299;&#30721;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#24207;&#21015;&#21151;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;5' UTR&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;UTR-LM&#12290;UTR-LM&#22312;&#22810;&#20010;&#29289;&#31181;&#30340;&#20869;&#28304;&#24615;5' UTR&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#21253;&#25324;&#20108;&#32423;&#32467;&#26500;&#21644;&#26368;&#23567;&#33258;&#30001;&#33021;&#22312;&#20869;&#30340;&#26377;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;UTR-LM&#36827;&#34892;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#27169;&#22411;&#22312;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#26368;&#22810;42%&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#32763;&#35793;&#25928;&#29575;&#21644;mRNA&#34920;&#36798;&#27700;&#24179;&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;&#26368;&#22810;60%&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26410;&#27880;&#37322;&#30340;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#65292;&#24182;&#23558;AUPR&#19982;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20174;0.37&#25552;&#39640;&#33267;0.52&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#20256;&#36755;&#27169;&#24335;&#30340;&#21019;&#26032;&#23548;&#39057;&#20998;&#37197;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#23548;&#39057;&#27745;&#26579;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03278</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#23548;&#39057;&#27745;&#26579;&#38382;&#39064;&#24182;&#23454;&#29616;&#29289;&#32852;&#32593;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Pilot Contamination and Enabling IoT Scalability in Massive MIMO Systems. (arXiv:2310.03278v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#20256;&#36755;&#27169;&#24335;&#30340;&#21019;&#26032;&#23548;&#39057;&#20998;&#37197;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#23548;&#39057;&#27745;&#26579;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;MIMO&#39044;&#35745;&#22312;5G&#32593;&#32476;&#30340;&#21457;&#23637;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#23548;&#39057;&#27745;&#26579;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30446;&#21069;&#22312;&#30456;&#37051;&#23567;&#21306;&#20013;&#37325;&#29992;&#27491;&#20132;&#23548;&#39057;&#24207;&#21015;&#30340;&#20570;&#27861;&#23548;&#33268;&#38590;&#20197;&#21306;&#20998;&#26469;&#33258;&#19981;&#21516;&#23567;&#21306;&#30340;&#23548;&#39057;&#24207;&#21015;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22686;&#21152;&#27491;&#20132;&#23548;&#39057;&#24207;&#21015;&#30340;&#25968;&#37327;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#23558;&#26356;&#22810;&#30340;&#26102;&#39057;&#36164;&#28304;&#29992;&#20110;&#23548;&#39057;&#20256;&#36755;&#32780;&#38750;&#25968;&#25454;&#20256;&#36755;&#12290;&#36825;&#20063;&#38459;&#30861;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20010;&#23567;&#21306;&#20869;&#23481;&#32435;&#22823;&#37327;&#29289;&#32852;&#32593;&#35774;&#22791;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#22522;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#25968;&#25454;&#20256;&#36755;&#27169;&#24335;&#35774;&#35745;&#20102;&#21019;&#26032;&#30340;&#23548;&#39057;&#20998;&#37197;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#23558;&#27491;&#20132;&#23548;&#39057;&#24207;&#21015;&#20998;&#37197;&#32473;&#35774;&#22791;&#31751;&#32780;&#38750;&#20010;&#21035;&#35774;&#22791;&#65292;&#20801;&#35768;&#22810;&#20010;&#35774;&#22791;&#21608;&#26399;&#24615;&#22320;&#20351;&#29992;&#30456;&#21516;&#30340;&#23548;&#39057;&#20256;&#36755;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive MIMO is expected to play an important role in the development of 5G networks. This paper addresses the issue of pilot contamination and scalability in massive MIMO systems. The current practice of reusing orthogonal pilot sequences in adjacent cells leads to difficulty in differentiating incoming inter- and intra-cell pilot sequences. One possible solution is to increase the number of orthogonal pilot sequences, which results in dedicating more space of coherence block to pilot transmission than data transmission. This, in turn, also hinders the scalability of massive MIMO systems, particularly in accommodating a large number of IoT devices within a cell. To overcome these challenges, this paper devises an innovative pilot allocation scheme based on the data transfer patterns of IoT devices. The scheme assigns orthogonal pilot sequences to clusters of devices instead of individual devices, allowing multiple devices to utilize the same pilot for periodically transmitting data. M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#23376;&#22270;&#19978;&#20197;&#29255;&#27573;&#32423;&#21035;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24120;&#35265;&#29255;&#27573;&#36827;&#34892;&#23545;&#27604;&#21644;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#39044;&#35757;&#32451;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03274</link><description>&lt;p&gt;
&#20998;&#23376;&#22270;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fragment-based Pretraining and Finetuning on Molecular Graphs. (arXiv:2310.03274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#23376;&#22270;&#19978;&#20197;&#29255;&#27573;&#32423;&#21035;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24120;&#35265;&#29255;&#27573;&#36827;&#34892;&#23545;&#27604;&#21644;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#39044;&#35757;&#32451;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#22270;&#30340;&#23646;&#24615;&#39044;&#27979;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#26080;&#26631;&#35760;&#30340;&#20998;&#23376;&#25968;&#25454;&#21464;&#24471;&#20016;&#23500;&#65292;&#36825;&#20419;&#36827;&#20102;&#21270;&#23398;&#39046;&#22495;GNN&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#29255;&#27573;&#32423;&#21035;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;GNN&#65292;&#36825;&#26159;&#20811;&#26381;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#39044;&#35757;&#32451;&#38480;&#21046;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#25240;&#20013;&#21150;&#27861;&#12290;&#20511;&#37492;&#26368;&#36817;&#22312;&#21407;&#29702;&#23376;&#22270;&#25366;&#25496;&#19978;&#30340;&#24037;&#20316;&#25216;&#26415;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#24120;&#35265;&#29255;&#27573;&#35789;&#27719;&#65292;&#28085;&#30422;&#20102;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20174;&#25552;&#21462;&#30340;&#35789;&#27719;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#22522;&#20110;&#29255;&#27573;&#30340;&#23545;&#27604;&#21644;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#32852;&#21512;&#39044;&#35757;&#32451;&#20102;&#22522;&#20110;&#20998;&#23376;&#22270;&#21644;&#22522;&#20110;&#29255;&#27573;&#22270;&#30340;&#20004;&#20010;&#19981;&#21516;GNN&#65292;&#34920;&#31034;&#20998;&#23376;&#20869;&#30340;&#39640;&#38454;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#24378;&#21046;&#29255;&#27573;&#23884;&#20837;&#21644;&#32858;&#21512;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, which serves as a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principle subgraph mining, we obtain a compact vocabulary of prevalent fragments that span a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one based on molecular graphs and one based on fragment graphs, which represents high-order connectivity within molecules. By enforcing the consistency between the fragment embedding and the aggregated embedding of the cor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20856;&#22411;&#26041;&#27861;MONet&#19978;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#26088;&#22312;&#38416;&#26126;&#22810;&#30446;&#26631;&#34920;&#31034;&#23398;&#20064;&#20013;&#29289;&#20307;&#20998;&#21106;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#36866;&#24403;&#29289;&#20307;&#20998;&#21106;&#30340;&#26041;&#27861;&#24182;&#23545;VAE&#27491;&#21017;&#21270;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.03273</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#34920;&#31034;&#23398;&#20064;&#20013;&#29289;&#20307;&#20998;&#21106;&#26426;&#21046;&#30340;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning. (arXiv:2310.03273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20856;&#22411;&#26041;&#27861;MONet&#19978;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#26088;&#22312;&#38416;&#26126;&#22810;&#30446;&#26631;&#34920;&#31034;&#23398;&#20064;&#20013;&#29289;&#20307;&#20998;&#21106;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#36866;&#24403;&#29289;&#20307;&#20998;&#21106;&#30340;&#26041;&#27861;&#24182;&#23545;VAE&#27491;&#21017;&#21270;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#29289;&#20307;&#30340;&#32452;&#21512;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#35270;&#35273;&#36755;&#20837;&#12290;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#21106;&#25104;&#21333;&#20010;&#29289;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#29289;&#20307;&#32534;&#30721;&#21040;&#27599;&#20010;&#28508;&#22312;&#21521;&#37327;&#20013;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;&#20309;&#23454;&#29616;&#36866;&#24403;&#30340;&#29289;&#20307;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#23545;&#28508;&#22312;&#21521;&#37327;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#22240;&#27492;&#65292;&#23578;&#19981;&#28165;&#26970;VAE&#27491;&#21017;&#21270;&#26159;&#21542;&#26377;&#21161;&#20110;&#36866;&#24403;&#30340;&#29289;&#20307;&#20998;&#21106;&#12290;&#20026;&#20102;&#38416;&#26126;&#22810;&#30446;&#26631;&#34920;&#31034;&#23398;&#20064;&#20013;&#29289;&#20307;&#20998;&#21106;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#23545;MONet&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#26041;&#27861;&#12290;MONet&#20351;&#29992;&#30001;&#27880;&#24847;&#21147;&#25513;&#30721;&#21644;&#23545;&#24212;&#20110;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#28508;&#22312;&#21521;&#37327;&#32452;&#25104;&#30340;&#23545;&#26469;&#34920;&#31034;&#22810;&#20010;&#29289;&#20307;&#12290;&#27599;&#20010;&#28508;&#22312;&#21521;&#37327;&#37117;&#26159;&#20174;&#36755;&#20837;&#22270;&#20687;&#21644;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#32534;&#30721;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-object representation learning aims to represent complex real-world visual input using the composition of multiple objects. Representation learning methods have often used unsupervised learning to segment an input image into individual objects and encode these objects into each latent vector. However, it is not clear how previous methods have achieved the appropriate segmentation of individual objects. Additionally, most of the previous methods regularize the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not clear whether VAE regularization contributes to appropriate object segmentation. To elucidate the mechanism of object segmentation in multi-object representation learning, we conducted an ablation study on MONet, which is a typical method. MONet represents multiple objects using pairs that consist of an attention mask and the latent vector corresponding to the attention mask. Each latent vector is encoded from the input image and attention mask. Then,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniPredict&#65292;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#22791;&#29702;&#35299;&#22810;&#26679;&#21270;&#34920;&#26684;&#36755;&#20837;&#21644;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniPredict&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.03266</link><description>&lt;p&gt;
UniPredict&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#36890;&#29992;&#30340;&#34920;&#26684;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniPredict&#65292;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#22791;&#29702;&#35299;&#22810;&#26679;&#21270;&#34920;&#26684;&#36755;&#20837;&#21644;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniPredict&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21028;&#21035;&#24314;&#27169;&#65292;&#24182;&#22312;&#20551;&#35774;&#22266;&#23450;&#30446;&#26631;&#21015;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36816;&#31639;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#30340;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;UniPredict&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;LLM&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#30340;&#34920;&#26684;&#36755;&#20837;&#24182;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;169&#20010;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20010;&#22810;&#21151;&#33021;&#30340;UniPredict&#27169;&#22411;&#22312;&#19982;&#26368;&#20339;&#30340;&#26641;&#25552;&#21319;&#27169;&#22411;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;5.4%&#21040;13.4%&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets with the capability of comprehending diverse tabular inputs and predicting for target variables following the input instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#30005;&#21147;&#26381;&#21153;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#32463;&#24120;&#36973;&#21463;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#25581;&#31034;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#23384;&#22312;&#24182;&#24378;&#35843;&#20102;&#25913;&#21892;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03258</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30340;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#65292;&#26816;&#27979;&#30005;&#21147;&#26381;&#21153;&#20844;&#24179;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets. (arXiv:2310.03258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03258
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#30005;&#21147;&#26381;&#21153;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#32463;&#24120;&#36973;&#21463;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#25581;&#31034;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#23384;&#22312;&#24182;&#24378;&#35843;&#20102;&#25913;&#21892;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20844;&#27491;&#26159;&#36328;&#23398;&#31185;&#33021;&#28304;&#30740;&#31350;&#30340;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28151;&#28102;&#21464;&#37327;&#12289;&#27835;&#30103;&#25928;&#24212;&#30340;&#22797;&#26434;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#35782;&#21035;&#33021;&#28304;&#37096;&#38376;&#20013;&#30340;&#31995;&#32479;&#20559;&#35265;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22260;&#32469;&#33021;&#28304;&#20844;&#27491;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23376;&#32452;&#20998;&#26512;&#26469;&#31649;&#29702;&#21508;&#31181;&#22240;&#32032;&#65292;&#24182;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#26469;&#32531;&#35299;&#27599;&#20010;&#23376;&#32452;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#20540;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23458;&#25143;&#32423;&#20572;&#30005;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20154;&#21475;&#22240;&#32032;&#65288;&#22914;&#25910;&#20837;&#21644;&#24180;&#40836;&#65289;&#23545;&#20572;&#30005;&#25345;&#32493;&#26102;&#38388;&#30340;&#21453;&#20107;&#23454;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#24635;&#26159;&#32463;&#21382;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#26080;&#35770;&#22825;&#27668;&#26465;&#20214;&#22914;&#20309;&#12290;&#36825;&#34920;&#26126;&#30005;&#21147;&#31995;&#32479;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#27880;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy justice is a growing area of interest in interdisciplinary energy research. However, identifying systematic biases in the energy sector remains challenging due to confounding variables, intricate heterogeneity in treatment effects, and limited data availability. To address these challenges, we introduce a novel approach for counterfactual causal analysis centered on energy justice. We use subgroup analysis to manage diverse factors and leverage the idea of transfer learning to mitigate data scarcity in each subgroup. In our numerical analysis, we apply our method to a large-scale customer-level power outage data set and investigate the counterfactual effect of demographic factors, such as income and age of the population, on power outage durations. Our results indicate that low-income and elderly-populated areas consistently experience longer power outages, regardless of weather conditions. This points to existing biases in the power system and highlights the need for focused im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#28508;&#22312;&#21521;&#37327;&#12289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#20998;&#23376;&#36827;&#34892;&#35757;&#32451;&#21518;&#36827;&#34892;&#27169;&#22411;&#20998;&#24067;&#30340;&#36880;&#28176;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2310.03253</link><description>&lt;p&gt;
&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Molecule Design by Latent Prompt Transformer. (arXiv:2310.03253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#28508;&#22312;&#21521;&#37327;&#12289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#20998;&#23376;&#36827;&#34892;&#35757;&#32451;&#21518;&#36827;&#34892;&#27169;&#22411;&#20998;&#24067;&#30340;&#36880;&#28176;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#25214;&#21040;&#20855;&#26377;&#30446;&#26631;&#21270;&#23398;&#25110;&#29983;&#29289;&#24615;&#36136;&#26368;&#20248;&#20540;&#30340;&#20998;&#23376;&#65292;&#35813;&#20540;&#21487;&#20197;&#30001;&#29616;&#26377;&#36719;&#20214;&#35745;&#31639;&#24471;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#28508;&#22312;&#21521;&#37327;&#65292;&#20854;&#20808;&#39564;&#20998;&#24067;&#30001;&#39640;&#26031;&#30333;&#22122;&#22768;&#21521;&#37327;&#30340;Unet&#21464;&#25442;&#24314;&#27169;&#12290;&#65288;2&#65289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#65288;1&#65289;&#20013;&#32473;&#23450;&#28508;&#22312;&#21521;&#37327;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20197;&#65288;1&#65289;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#30340;&#22240;&#26524;Transformer&#27169;&#22411;&#12290;&#65288;3&#65289;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#26681;&#25454;&#65288;1&#65289;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#36827;&#34892;&#38750;&#32447;&#24615;&#22238;&#24402;&#39044;&#27979;&#20998;&#23376;&#30340;&#30446;&#26631;&#24615;&#36136;&#20540;&#12290;&#25105;&#20204;&#31216;&#35813;&#25552;&#20986;&#30340;&#27169;&#22411;&#20026;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#12290;&#22312;&#23545;&#29616;&#26377;&#20998;&#23376;&#21450;&#20854;&#24615;&#36136;&#20540;&#36827;&#34892;&#21021;&#27493;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36880;&#28176;&#36716;&#31227;&#27169;&#22411;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a latent prompt Transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. Our proposed model consists of three components. (1) A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector. (2) A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). We adopt the causal Transformer model that takes the latent vector in (1) as prompt. (3) A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). We call the proposed model the latent prompt Transformer model. After initial training of the model on existing molecules and their property values, we then gradually shift the model distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#20381;&#36182;&#25968;&#25454;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#19978;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03243</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sparse Deep Learning for Time Series Data: Theory and Applications. (arXiv:2310.03243v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#20381;&#36182;&#25968;&#25454;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#19978;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#25552;&#21319;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#21464;&#37327;&#36873;&#25321;&#21644;&#22823;&#35268;&#27169;&#32593;&#32476;&#21387;&#32553;&#31561;&#39046;&#22495;&#24615;&#33021;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35266;&#27979;&#30456;&#20114;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#38382;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39034;&#24207;&#25968;&#25454;&#31561;&#35266;&#27979;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#19978;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#30740;&#31350;&#20855;&#26377;&#20381;&#36182;&#25968;&#25454;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39044;&#27979;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#28176;&#36817;&#22320;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#27491;&#30830;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#32988;&#36807;&#20102;&#35832;&#22914;&#20381;&#29031;&#24615;&#39044;&#27979;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty qua
&lt;/p&gt;</description></item><item><title>&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03240</link><description>&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#65306;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03240
&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#30740;&#31350;&#39046;&#22495;&#26159;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#26174;&#24335;&#20851;&#31995;&#29305;&#24449;&#34920;&#31034;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#23545;&#35937;&#65292;&#19968;&#20010;&#8220;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#8221;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#25152;&#26377;&#25104;&#23545;&#20851;&#31995;&#30340;&#20851;&#31995;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#8220;&#20851;&#31995;&#21367;&#31215;&#8221;&#23618;&#23558;&#20851;&#31995;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#23545;&#35937;&#24207;&#21015;&#65292;&#27599;&#20010;&#23545;&#35937;&#25551;&#36848;&#21069;&#19968;&#23618;&#26576;&#32676;&#23545;&#35937;&#20869;&#30340;&#20851;&#31995;&#12290;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#22270;&#20803;&#28388;&#27874;&#22120;&#20195;&#34920;&#35201;&#19982;&#20851;&#31995;&#24352;&#37327;&#22312;&#27599;&#20010;&#20998;&#32452;&#20013;&#36827;&#34892;&#27604;&#36739;&#30340;&#20851;&#31995;&#27169;&#26495;&#12290;&#36890;&#36807;&#37325;&#22797;&#36825;&#20010;&#36807;&#31243;&#65292;&#24471;&#21040;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#30340;&#21160;&#26426;&#21644;&#32454;&#33410;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35777;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31354;&#38388;&#21453;&#28436;&#20013;&#23454;&#29616;&#20102;&#23545;&#22320;&#36136;&#30899;&#20648;&#23384;&#21382;&#21490;&#21305;&#37197;&#30340;&#20934;&#30830;&#25512;&#26029;&#65292;&#21487;&#20197;&#22312;&#24037;&#19994;&#35268;&#27169;&#30899;&#20648;&#23384;&#25805;&#20316;&#20013;&#25552;&#39640;&#22320;&#19979;&#27700;&#31649;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03228</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#21453;&#28436;&#21644;&#26102;&#31354;&#25968;&#25454;&#21442;&#25968;&#21270;&#30340;&#22320;&#36136;&#30899;&#20648;&#23384;&#21382;&#21490;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization. (arXiv:2310.03228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31354;&#38388;&#21453;&#28436;&#20013;&#23454;&#29616;&#20102;&#23545;&#22320;&#36136;&#30899;&#20648;&#23384;&#21382;&#21490;&#21305;&#37197;&#30340;&#20934;&#30830;&#25512;&#26029;&#65292;&#21487;&#20197;&#22312;&#24037;&#19994;&#35268;&#27169;&#30899;&#20648;&#23384;&#25805;&#20316;&#20013;&#25552;&#39640;&#22320;&#19979;&#27700;&#31649;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#27979;&#25968;&#25454;&#30340;&#21382;&#21490;&#21305;&#37197;&#21487;&#20197;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#24037;&#19994;&#35268;&#27169;&#30899;&#20648;&#23384;&#25805;&#20316;&#20013;&#30340;&#22320;&#19979;&#27700;&#31649;&#29702;&#12290;&#22312;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#21516;&#21270;&#20013;&#65292;&#36890;&#36807;&#20462;&#25913;&#22320;&#36136;&#27169;&#22411;&#21442;&#25968;&#26469;&#20351;&#24471;&#27969;&#21160;&#27169;&#25311;&#32467;&#26524;&#19982;&#35266;&#27979;&#20540;&#21563;&#21512;&#12290;&#32780;&#22312;&#25968;&#25454;&#31354;&#38388;&#21453;&#28436;&#65288;DSI&#65289;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#32422;1000&#20010;&#20808;&#39564;&#27169;&#25311;&#32467;&#26524;&#65292;&#25968;&#25454;&#21442;&#25968;&#21270;&#21644;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#30340;&#21518;&#39564;&#25277;&#26679;&#65292;&#21487;&#20197;&#30452;&#25509;&#25512;&#26029;&#20986;&#21382;&#21490;&#21305;&#37197;&#21518;&#30340;&#24863;&#20852;&#36259;&#37327;&#65292;&#20363;&#22914;&#20197;&#35266;&#23519;&#20026;&#26465;&#20214;&#30340;&#21518;&#39564;&#21387;&#21147;&#21644;&#39281;&#21644;&#24230;&#22330;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#22312;&#25968;&#25454;&#31354;&#38388;&#21453;&#28436;&#20013;&#23454;&#26045;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#19968;&#31995;&#21015;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#26102;&#31354;&#21387;&#21147;&#21644;CO2&#39281;&#21644;&#24230;&#22330;&#12290;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#29992;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#36827;&#34892;&#32500;&#24230;&#38477;&#20302;&#65292;&#20351;&#29992;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;convLSTM&#65289;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#36830;&#32493;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
History matching based on monitoring data will enable uncertainty reduction, and thus improved aquifer management, in industrial-scale carbon storage operations. In traditional model-based data assimilation, geomodel parameters are modified to force agreement between flow simulation results and observations. In data-space inversion (DSI), history-matched quantities of interest, e.g., posterior pressure and saturation fields conditioned to observations, are inferred directly, without constructing posterior geomodels. This is accomplished efficiently using a set of O(1000) prior simulation results, data parameterization, and posterior sampling within a Bayesian setting. In this study, we develop and implement (in DSI) a deep-learning-based parameterization to represent spatio-temporal pressure and CO2 saturation fields at a set of time steps. The new parameterization uses an adversarial autoencoder (AAE) for dimension reduction and a convolutional long short-term memory (convLSTM) networ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;GSE&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;MASE&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MASE&#23558;&#26080;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#20043;&#21069;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03225</link><description>&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#26222;&#36866;&#30340;&#24418;&#24335;&#21270;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;GSE&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;MASE&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MASE&#23558;&#26080;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#20043;&#21069;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#23433;&#20840;&#25506;&#32034;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#23433;&#20840;&#25506;&#32034;&#65288;GSE&#65289;&#38382;&#39064;&#65292;&#20316;&#20026;&#24120;&#35265;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#24418;&#24335;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GSE&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23433;&#20840;&#25506;&#32034;&#30340;&#20803;&#31639;&#27861;MASE&#65292;&#23427;&#23558;&#26080;&#32422;&#26463;&#30340;RL&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#24403;&#21069;&#22238;&#21512;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#23433;&#20840;&#36829;&#35268;&#20043;&#21069;&#36866;&#24403;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#22312;&#26410;&#26469;&#22238;&#21512;&#20013;&#21457;&#29983;&#12290;MASE&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#21487;&#20197;&#22312;&#20445;&#35777;&#39640;&#27010;&#29575;&#19979;&#19981;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#65292;&#20248;&#21270;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#30340;MASE&#21464;&#20307;&#65306;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21478;&#19968;&#31181;&#21017;&#32467;&#21512;&#20102;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03223</link><description>&lt;p&gt;
TacoGFN: &#38024;&#23545;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet
&lt;/p&gt;
&lt;p&gt;
TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#26159;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#20013;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#30340;&#20998;&#23376;&#20013;&#24456;&#38590;&#23454;&#29616;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#32467;&#21512;&#25913;&#21892;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23558;&#21475;&#34955;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#23450;&#20041;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;TacoGFN&#65292;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#26041;&#27861;&#26469;&#21152;&#24555;&#23545;&#25509;&#24471;&#20998;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;TacoGFN&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20960;&#36718;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25913;&#21892;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#25506;&#32034;&#26356;&#22810;&#30340;&#20998;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
&lt;/p&gt;</description></item><item><title>Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.03221</link><description>&lt;p&gt;
Know2BIO: &#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs. (arXiv:2310.03221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03221
&lt;/p&gt;
&lt;p&gt;
Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#21644;&#38598;&#25104;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20174;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#32452;&#35013;KG&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#23454;&#20307;&#23545;&#40784;&#65292;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36319;&#19978;&#31185;&#23398;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#35889;&#30340;&#20195;&#34920;&#33021;&#21147;&#36890;&#24120;&#21463;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Know2BIO&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#36890;&#29992;&#24322;&#26500;KG&#22522;&#20934;&#12290;Know2BIO&#25972;&#21512;&#20102;&#26469;&#33258;30&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25429;&#25417;&#20102;11&#20010;&#29983;&#29289;&#21307;&#23398;&#31867;&#21035;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#23427;&#30446;&#21069;&#21253;&#21547;&#32422;219,000&#20010;&#33410;&#28857;&#21644;&#32422;6,200,000&#20010;&#36793;&#12290;Know2BIO&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#31034;&#33258;&#21160;&#26356;&#26032;&#20197;&#21453;&#26144;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;Know2BIO&#36824;&#38468;&#24102;&#22810;&#27169;&#24577;&#25968;&#25454;&#65306;&#21253;&#25324;&#25991;&#26412;&#25551;&#36848;&#12289;&#34507;&#30333;&#36136;&#21644;&#21270;&#21512;&#29289;&#24207;&#21015;&#31561;&#33410;&#28857;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#25913;&#36827;&#30340;&#38271;&#26399; MCMC&#37319;&#26679;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03218</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#25913;&#36827;&#30340; MCMC &#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#25913;&#36827;&#30340;&#38271;&#26399; MCMC&#37319;&#26679;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#20063;&#31216;&#20026;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#22312;&#20844;&#24335;&#21270;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#24378;&#24314;&#27169;&#33021;&#21147;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#36215;&#20102;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38750;&#25910;&#25947;&#30340;&#30701;&#26399; MCMC &#36827;&#34892;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#26469;&#23398;&#20064;&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#65307;&#23454;&#36341;&#20013;&#36864;&#21270;&#30340; MCMC &#37319;&#26679;&#36136;&#37327;&#36890;&#24120;&#23548;&#33268;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#22810;&#27169;&#24577;&#21644;/&#25110;&#39640;&#32500;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#37319;&#26679;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25674;&#38144;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#26399; MCMC &#37319;&#26679;&#65292;&#24182;&#22522;&#20110;&#27492;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;EBM&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#25454;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;MCMC&#25674;&#38144;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#38271;&#26399;MCMC&#37319;&#26679;&#22120;&#12290;&#22312;&#20960;&#20010;&#22270;&#20687;&#24314;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#30340;&#24418;&#24335;&#35201;&#32032;&#21644;&#23454;&#38469;&#35201;&#32032;&#65292;&#36890;&#36807;&#25581;&#31034;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#21644;&#36807;&#31243;&#65292;&#24418;&#24335;&#24314;&#31435;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#32467;&#21512;&#23454;&#38469;&#32771;&#34385;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#23433;&#20840;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03217</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#30340;&#24418;&#24335;&#21644;&#23454;&#38469;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
Formal and Practical Elements for the Certification of Machine Learning Systems. (arXiv:2310.03217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#30340;&#24418;&#24335;&#35201;&#32032;&#21644;&#23454;&#38469;&#35201;&#32032;&#65292;&#36890;&#36807;&#25581;&#31034;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#21644;&#36807;&#31243;&#65292;&#24418;&#24335;&#24314;&#31435;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#32467;&#21512;&#23454;&#38469;&#32771;&#34385;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#23433;&#20840;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#19982;&#33258;&#20027;&#39134;&#34892;&#30456;&#20851;&#30340;&#24863;&#30693;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24448;&#24448;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;&#33322;&#31354;&#33322;&#22825;&#36719;&#20214;&#19981;&#21516;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#19981;&#26159;&#25163;&#24037;&#32534;&#30721;&#25110;&#20174;&#29289;&#29702;&#23398;&#27966;&#29983;&#30340;&#65292;&#32780;&#26159;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#33258;&#21160;&#35843;&#25972;&#65292;&#20854;&#20540;&#36890;&#24120;&#19981;&#31526;&#21512;&#29289;&#29702;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#35201;&#27714;&#19981;&#33021;&#30452;&#25509;&#36861;&#28335;&#21040;&#20195;&#30721;&#34892;&#65292;&#38459;&#30861;&#20102;&#24403;&#21069;&#33258;&#19979;&#32780;&#19978;&#30340;&#33322;&#31354;&#33322;&#22825;&#35748;&#35777;&#33539;&#24335;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65306;1&#65289;&#25581;&#31034;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#21644;&#36807;&#31243;&#65292;2&#65289;&#27491;&#24335;&#24314;&#31435;&#36825;&#20123;&#36807;&#31243;&#25152;&#25552;&#20379;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;3&#65289;&#34917;&#20805;&#36825;&#20123;&#24418;&#24335;&#35201;&#32032;&#19982;&#23454;&#38469;&#32771;&#34385;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#23436;&#25972;&#30340;&#23433;&#20840;&#20851;&#38190;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35748;&#35777;&#35770;&#35777;&#12290;&#22522;&#20110;&#21487;&#25193;&#23637;&#30340;&#32479;&#35745;&#39564;&#35777;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, machine learning has demonstrated impressive results, often surpassing human capabilities in sensing tasks relevant to autonomous flight. Unlike traditional aerospace software, the parameters of machine learning models are not hand-coded nor derived from physics but learned from data. They are automatically adjusted during a training phase, and their values do not usually correspond to physical requirements. As a result, requirements cannot be directly traced to lines of code, hindering the current bottom-up aerospace certification paradigm. This paper attempts to address this gap by 1) demystifying the inner workings and processes to build machine learning models, 2) formally establishing theoretical guarantees given by those processes, and 3) complementing these formal elements with practical considerations to develop a complete certification argument for safety-critical machine learning systems. Based on a scalable statistical verifier, our proposed framework i
&lt;/p&gt;</description></item><item><title>PDR-CapsNet&#26159;&#19968;&#31181;&#26356;&#28145;&#12289;&#26356;&#33410;&#33021;&#30340;&#33014;&#22218;&#32593;&#32476;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#31574;&#30053;&#20943;&#36731;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2310.03212</link><description>&lt;p&gt;
PDR-CapsNet:&#21160;&#24577;&#36335;&#30001;&#33014;&#22218;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#33410;&#33021;&#24182;&#34892;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in Capsule Networks. (arXiv:2310.03212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03212
&lt;/p&gt;
&lt;p&gt;
PDR-CapsNet&#26159;&#19968;&#31181;&#26356;&#28145;&#12289;&#26356;&#33410;&#33021;&#30340;&#33014;&#22218;&#32593;&#32476;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#31574;&#30053;&#20943;&#36731;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26368;&#22823;&#27744;&#21270;&#23618;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#26059;&#36716;&#21644;&#35270;&#28857;&#21464;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#33014;&#22218;&#32593;&#32476;(CapsNets)&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#31216;&#20026;&#21160;&#24577;&#36335;&#30001;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;CapsNets&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#38656;&#35201;&#27604;CNNs&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24182;&#34892;&#21160;&#24577;&#36335;&#30001;&#33014;&#22218;&#32593;&#32476;(PDR-CapsNet)&#65292;&#36825;&#26159;&#19968;&#31181;&#27604;CapsNet&#26356;&#28145;&#12289;&#26356;&#33410;&#33021;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12289;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#26356;&#20302;&#30340;&#36807;&#25311;&#21512;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#21270;&#31574;&#30053;&#65292;PDR-CapsNet&#20943;&#36731;&#20102;CapsNet&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22686;&#21152;&#20102;&#21534;&#21520;&#37327;&#65292;&#39640;&#25928;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;83.55%&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;87.26%&#30340;&#21442;&#25968;&#65292;32.27%&#21644;47.40%
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) have produced state-of-the-art results for image classification tasks. However, they are limited in their ability to handle rotational and viewpoint variations due to information loss in max-pooling layers. Capsule Networks (CapsNets) employ a computationally-expensive iterative process referred to as dynamic routing to address these issues. CapsNets, however, often fall short on complex datasets and require more computational resources than CNNs. To overcome these challenges, we introduce the Parallel Dynamic Routing CapsNet (PDR-CapsNet), a deeper and more energy-efficient alternative to CapsNet that offers superior performance, less energy consumption, and lower overfitting rates. By leveraging a parallelization strategy, PDR-CapsNet mitigates the computational complexity of CapsNet and increases throughput, efficiently using hardware resources. As a result, we achieve 83.55\% accuracy while requiring 87.26\% fewer parameters, 32.27\% and 47.40\%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20998;&#24067;&#24335;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#20855;&#26377;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#31995;&#32479;&#32593;&#32476;&#12290;&#23545;&#20110;&#24050;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#25200;&#21160;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#20445;&#35777;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{T}\log T)$&#12290;&#23545;&#20110;&#26410;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#25506;&#32034;-&#25215;&#35834;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#20195;&#29702;&#22312;&#25506;&#32034;&#38454;&#27573;&#20849;&#21516;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#20351;&#29992;&#25552;&#20986;&#30340;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03206</link><description>&lt;p&gt;
LTI&#31995;&#32479;&#20013;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#25511;&#21046;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances. (arXiv:2310.03206v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20998;&#24067;&#24335;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#20855;&#26377;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#31995;&#32479;&#32593;&#32476;&#12290;&#23545;&#20110;&#24050;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#25200;&#21160;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#20445;&#35777;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{T}\log T)$&#12290;&#23545;&#20110;&#26410;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#25506;&#32034;-&#25215;&#35834;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#20195;&#29702;&#22312;&#25506;&#32034;&#38454;&#27573;&#20849;&#21516;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#20351;&#29992;&#25552;&#20986;&#30340;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20998;&#24067;&#24335;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#20855;&#26377;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#31995;&#32479;&#32593;&#32476;&#65288;&#20854;&#21160;&#24577;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#65289;&#12290;&#23384;&#22312;&#19968;&#20010;&#20840;&#23616;&#32593;&#32476;&#25104;&#26412;&#65292;&#30001;&#19968;&#20010;&#26102;&#21464;&#30340;&#20984;&#20989;&#25968;&#26469;&#25551;&#36848;&#65292;&#20854;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#28436;&#21270;&#65292;&#24182;&#19988;&#23616;&#37096;&#20195;&#29702;&#21482;&#33021;&#39034;&#24207;&#21644;&#37096;&#20998;&#35266;&#27979;&#21040;&#23427;&#12290;&#27599;&#20010;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#32452;&#25511;&#21046;&#24207;&#21015;&#65292;&#20197;&#20415;&#33021;&#22815;&#19982;&#20107;&#21518;&#20855;&#26377;&#20840;&#23616;&#25104;&#26412;&#35775;&#38382;&#26435;&#38480;&#30340;&#26368;&#20339;&#38598;&#20013;&#24335;&#25511;&#21046;&#31574;&#30053;&#31454;&#20105;&#12290;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#23545;&#20110;&#24050;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#25200;&#21160;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#20445;&#35777;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{T}\log T)$&#65292;&#20854;&#20013;$T$&#20026;&#26102;&#38388;&#31383;&#21475;&#12290;&#23545;&#20110;&#26410;&#30693;&#21160;&#24577;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#25506;&#32034;-&#25215;&#35834;&#26041;&#27861;&#65292;&#22312;&#25506;&#32034;&#38454;&#27573;&#25152;&#26377;&#20195;&#29702;&#20849;&#21516;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#65292;&#22312;&#23398;&#20064;&#38454;&#27573;&#25105;&#20204;&#25552;&#20986;&#30340;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the distributed online control problem over a network of linear time-invariant (LTI) systems (with possibly unknown dynamics) in the presence of adversarial perturbations. There exists a global network cost that is characterized by a time-varying convex function, which evolves in an adversarial manner and is sequentially and partially observed by local agents. The goal of each agent is to generate a control sequence that can compete with the best centralized control policy in hindsight, which has access to the global cost. This problem is formulated as a regret minimization. For the case of known dynamics, we propose a fully distributed disturbance feedback controller that guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is the time horizon. For the unknown dynamics case, we design a distributed explore-then-commit approach, where in the exploration phase all agents jointly learn the system dynamics, and in the learning phase our proposed control algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26426;&#22120;&#35843;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#24635;&#32467;&#20986;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#65292;&#21457;&#29616;DRL&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03195</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26426;&#22120;&#35843;&#24230;&#65306;&#26041;&#27861;&#35770;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26426;&#22120;&#35843;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#24635;&#32467;&#20986;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#65292;&#21457;&#29616;DRL&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#35843;&#24230;&#26088;&#22312;&#36890;&#36807;&#28385;&#36275;&#21046;&#36896;&#35268;&#21017;&#21644;&#20316;&#19994;&#35268;&#33539;&#30340;&#21069;&#25552;&#19979;&#65292;&#20248;&#21270;&#20316;&#19994;&#20998;&#37197;&#32473;&#26426;&#22120;&#12290;&#36825;&#31181;&#20248;&#21270;&#21487;&#20197;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12289;&#25552;&#39640;&#23458;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#24230;&#21644;&#22686;&#24378;&#29983;&#20135;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854; NP-hard &#30340;&#24615;&#36136;&#65292;&#26426;&#22120;&#35843;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20316;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#33258;1995&#24180;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#23427;&#23558;&#36825;&#20123;&#26041;&#27861;&#26681;&#25454;&#35745;&#31639;&#32452;&#20214;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#24471;&#20986;&#32467;&#35770;&#65292;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#20248;&#20110;&#31934;&#30830;&#27714;&#35299;&#22120;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine scheduling aims to optimize job assignments to machines while adhering to manufacturing rules and job specifications. This optimization leads to reduced operational costs, improved customer demand fulfillment, and enhanced production efficiency. However, machine scheduling remains a challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement Learning (DRL), a key component of artificial general intelligence, has shown promise in various domains like gaming and robotics. Researchers have explored applying DRL to machine scheduling problems since 1995. This paper offers a comprehensive review and comparison of DRL-based approaches, highlighting their methodology, applications, advantages, and limitations. It categorizes these approaches based on computational components: conventional neural networks, encoder-decoder architectures, graph neural networks, and metaheuristic algorithms. Our review concludes that DRL-based methods outperform exact solvers, heuristi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#22312;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23398;&#20064;&#19981;&#24819;&#20851;&#30340;&#30456;&#20851;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03182</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#22312;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23398;&#20064;&#19981;&#24819;&#20851;&#30340;&#30456;&#20851;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#21307;&#30103;&#20445;&#20581;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#26377;&#26395;&#20943;&#36731;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#20419;&#36827;&#23545;&#24739;&#32773;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#26102;&#65292;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#31070;&#32463;&#27169;&#22411;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#19981;&#30456;&#20851;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#25152;&#26399;&#26395;&#30340;&#29305;&#24449;&#65292;&#36825;&#22312;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#26102;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#36275;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#19981;&#21516;&#24180;&#40836;&#30340;&#24739;&#32773;&#65289;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#22312;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#26102;&#65292;&#20102;&#35299;&#27169;&#22411;&#20026;&#20309;&#20570;&#20986;&#20915;&#31574;&#23545;&#20110;&#21487;&#20449;&#24230;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2310.03178</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#28389;&#29992;&#38480;&#21046;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20419;&#36827;&#20102;&#22810;&#26041;&#20043;&#38388;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#24182;&#22240;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#23398;&#20064;&#25928;&#29575;&#25552;&#21319;&#26041;&#38754;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;FL&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;FL&#38754;&#20020;&#30528;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#35282;&#24230;&#65292;&#20197;&#21450;&#38598;&#20013;&#24335;&#21644;&#20998;&#25955;&#24335;FL&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;FL&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as dir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#21477;&#27861;&#29305;&#24449;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#26469;&#25512;&#33616;&#27979;&#35797;&#29992;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#35745;&#31639;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#29983;&#25104;&#21644;&#32500;&#25252;&#27979;&#35797;&#21333;&#20803;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03174</link><description>&lt;p&gt;
&#29992;&#20195;&#30721;&#21477;&#27861;&#29305;&#24449;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27979;&#35797;&#29992;&#20363;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Test Case Recommendations with Distributed Representation of Code Syntactic Features. (arXiv:2310.03174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#21477;&#27861;&#29305;&#24449;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#26469;&#25512;&#33616;&#27979;&#35797;&#29992;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#35745;&#31639;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#29983;&#25104;&#21644;&#32500;&#25252;&#27979;&#35797;&#21333;&#20803;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36719;&#20214;&#28304;&#20195;&#30721;&#12289;&#35774;&#35745;&#21644;&#38656;&#27714;&#30340;&#25345;&#32493;&#21464;&#21270;&#65292;&#21333;&#20803;&#27979;&#35797;&#29992;&#20363;&#30340;&#39057;&#32321;&#20462;&#25913;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#30001;&#20110;&#25163;&#21160;&#32500;&#25252;&#36719;&#20214;&#27979;&#35797;&#22871;&#20214;&#26159;&#32321;&#29712;&#12289;&#36153;&#26102;&#21644;&#26114;&#36149;&#30340;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#32500;&#25252;&#27979;&#35797;&#21333;&#20803;&#30340;&#36807;&#31243;&#33258;&#21160;&#21270;&#23558;&#26174;&#33879;&#24433;&#21709;&#36719;&#20214;&#27979;&#35797;&#27969;&#31243;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#28304;&#20195;&#30721;&#26041;&#27861;&#21644;&#27979;&#35797;&#29992;&#20363;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#23646;&#24615;&#26469;&#25512;&#33616;&#23545;&#24320;&#21457;&#20154;&#21592;&#26368;&#30456;&#20851;&#21644;&#26377;&#29992;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26041;&#27861;&#32423;&#28304;&#20195;&#30721;&#21644;&#21333;&#20803;&#27979;&#35797;&#36716;&#25442;&#20026;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#21521;&#37327;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20195;&#30721;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#26816;&#32034;&#32473;&#23450;&#26041;&#27861;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#26041;&#27861;&#23884;&#20837;&#19982;&#20808;&#21069;&#23884;&#20837;&#30340;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequent modifications of unit test cases are inevitable due to software's continuous underlying changes in source code, design, and requirements. Since manually maintaining software test suites is tedious, timely, and costly, automating the process of generation and maintenance of test units will significantly impact the effectiveness and efficiency of software testing processes.  To this end, we propose an automated approach which exploits both structural and semantic properties of source code methods and test cases to recommend the most relevant and useful unit tests to the developers. The proposed approach initially trains a neural network to transform method-level source code, as well as unit tests, into distributed representations (embedded vectors) while preserving the importance of the structure in the code. Retrieving the semantic and structural properties of a given method, the approach computes cosine similarity between the method's embedding and the previously-embedded trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20462;&#25913;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#19981;&#21464;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25703;&#27585;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2310.03166</link><description>&lt;p&gt;
&#30772;&#22351;&#21040;&#24213;&#65306;&#23545;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors. (arXiv:2310.03166v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20462;&#25913;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#19981;&#21464;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25703;&#27585;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#65288;ML-PWD&#65289;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#32593;&#39029;HTML&#20195;&#30721;&#30340;&#23545;&#25239;&#24615;&#31713;&#25913;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#25915;&#20987;&#30001;&#20110;&#32570;&#20047;&#20248;&#21270;&#25152;&#37319;&#29992;&#30340;&#31713;&#25913;&#30340;&#20351;&#29992;&#20197;&#21450;&#20165;&#20851;&#27880;HTML&#20195;&#30721;&#30340;&#29305;&#23450;&#20803;&#32032;&#32780;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#35774;&#35745;&#19968;&#32452;&#26032;&#39062;&#30340;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#20123;&#31713;&#25913;&#20801;&#35768;&#20462;&#25913;&#36755;&#20837;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#32780;&#26080;&#38656; compromiser&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#65292;&#21363;&#31713;&#25913;&#22312;&#35774;&#35745;&#19978;&#26159;&#21151;&#33021;&#21644;&#28210;&#26579;&#20445;&#25345;&#19981;&#21464;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#38656;&#35201;&#24212;&#29992;&#21738;&#20123;&#31713;&#25913;&#20197;&#32469;&#36807;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#21363;&#21487;&#25703;&#27585;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;ML-PWD&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36739;&#24369;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning phishing webpage detectors (ML-PWD) have been shown to suffer from adversarial manipulations of the HTML code of the input webpage. Nevertheless, the attacks recently proposed have demonstrated limited effectiveness due to their lack of optimizing the usage of the adopted manipulations, and they focus solely on specific elements of the HTML code. In this work, we overcome these limitations by first designing a novel set of fine-grained manipulations which allow to modify the HTML code of the input phishing webpage without compromising its maliciousness and visual appearance, i.e., the manipulations are functionality- and rendering-preserving by design. We then select which manipulations should be applied to bypass the target detector by a query-efficient black-box optimization algorithm. Our experiments show that our attacks are able to raze to the ground the performance of current state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker attacks develo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03165</link><description>&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#23618;&#21098;&#26525;&#31616;&#21270;DNN&#26550;&#26500;&#21644;&#25439;&#22833;&#26354;&#38754;&#12290;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26368;&#36817;&#34987;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#26816;&#26597;DNN&#30340;&#26435;&#37325;&#23618;&#35889;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30830;&#23450;&#35201;&#20174;DNN&#30340;&#26435;&#37325;&#23618;&#20013;&#21435;&#38500;&#30340;&#22855;&#24322;&#20540;&#30340;&#25968;&#37327;&#65292;&#36825;&#26377;&#21161;&#20110;&#31616;&#21270;DNN&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#22521;&#35757;&#31616;&#21333;&#30340;DNN&#27169;&#22411;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;DNN&#30340;&#20219;&#20309;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#23618;&#65292;&#20943;&#23569;&#20102;&#23618;&#30340;&#21442;&#25968;&#24182;&#31616;&#21270;&#20102;DNN&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#27979;&#35797;&#38598;&#30340;&#20934;&#30830;&#24615;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;DNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNAR&#30340;&#31639;&#27861;&#25554;&#20214;&#65292;&#23427;&#36890;&#36807;&#24402;&#19968;&#21270;&#36864;&#28779;&#27491;&#21017;&#21270;&#26469;&#35843;&#33410;&#27599;&#27425;&#26356;&#26032;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26435;&#37325;&#34928;&#20943;&#23545;&#20840;&#23616;&#30446;&#26631;&#30340;&#20248;&#21270;&#30446;&#26631;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03163</link><description>&lt;p&gt;
FedNAR: &#24102;&#26377;&#24402;&#19968;&#21270;&#36864;&#28779;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FedNAR: Federated Optimization with Normalized Annealing Regularization. (arXiv:2310.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNAR&#30340;&#31639;&#27861;&#25554;&#20214;&#65292;&#23427;&#36890;&#36807;&#24402;&#19968;&#21270;&#36864;&#28779;&#27491;&#21017;&#21270;&#26469;&#35843;&#33410;&#27599;&#27425;&#26356;&#26032;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26435;&#37325;&#34928;&#20943;&#23545;&#20840;&#23616;&#30446;&#26631;&#30340;&#20248;&#21270;&#30446;&#26631;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#65292;&#26435;&#37325;&#34928;&#20943;&#26159;&#19968;&#31181;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20063;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#38450;&#27490;&#26412;&#22320;&#23458;&#25143;&#31471;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#32034;&#20102;&#26435;&#37325;&#34928;&#20943;&#30340;&#36873;&#25321;&#65292;&#24182;&#30830;&#23450;&#26435;&#37325;&#34928;&#20943;&#20540;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#34429;&#28982;&#38450;&#27490;&#36807;&#25311;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26435;&#37325;&#34928;&#20943;&#21487;&#33021;&#24341;&#20837;&#23545;&#20840;&#23616;&#30446;&#26631;&#30340;&#19981;&#21516;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#36825;&#19968;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#12289;&#22810;&#21151;&#33021;&#30340;&#31639;&#27861;&#25554;&#20214;&#65292;&#21363;&#8220;&#24102;&#26377;&#24402;&#19968;&#21270;&#36864;&#28779;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#20248;&#21270;&#8221;&#65288;FedNAR&#65289;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26799;&#24230;&#21644;&#26435;&#37325;&#34928;&#20943;&#36827;&#34892;&#32852;&#21512;&#35009;&#21098;&#26469;&#35843;&#33410;&#27599;&#27425;&#26356;&#26032;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#22270;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#20195;&#29702;&#25152;&#20351;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;</title><link>http://arxiv.org/abs/2310.03161</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#23545;&#20110;&#35782;&#21035;&#26102;&#38388;&#24310;&#38271;&#22686;&#24378;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#22270;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#20195;&#29702;&#25152;&#20351;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20851;&#27880;&#27169;&#22411;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;OpenAI Gym Atari-2600&#28216;&#25103;&#22871;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#25552;&#21462;&#21644;&#21472;&#21152;&#27880;&#24847;&#21147;&#22270;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#20195;&#29702;&#20351;&#29992;&#30340;&#20449;&#24687;&#20197;&#36873;&#25321;&#21160;&#20316;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;gym-Atari&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#33021;&#25552;&#20379;&#20851;&#20110;&#20195;&#29702;&#22914;&#20309;&#24863;&#30693;&#20854;&#29615;&#22659;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#20351;&#29992;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#39044;&#27979;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#21644;&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#30740;&#31350;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#39044;&#27979;&#21306;&#38388;&#20840;&#38754;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03158</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#19981;&#30830;&#23450;&#29305;&#24449;&#26354;&#32447;&#30340;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Assessment of Prediction Intervals Using Uncertainty Characteristics Curves. (arXiv:2310.03158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#39044;&#27979;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#21644;&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#30740;&#31350;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#39044;&#27979;&#21306;&#38388;&#20840;&#38754;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24050;&#32463;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;AI&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#36890;&#24120;&#20351;&#29992;&#26657;&#20934;&#21040;&#19987;&#26377;&#25805;&#20316;&#28857;&#30340;&#39044;&#27979;&#21306;&#38388;&#26469;&#37327;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#19981;&#21516;&#30740;&#31350;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#30456;&#23545;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;(1)&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#27010;&#24565;&#21644;(2)&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#28857;&#19981;&#21487;&#30693;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#21306;&#38388;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19981;&#30830;&#23450;&#29305;&#24449;&#26354;&#32447;&#65292;&#24182;&#22312;&#36873;&#23450;&#22330;&#26223;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#39044;&#27979;&#21306;&#38388;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24403;&#21069;&#38656;&#27714;&#65292;&#22240;&#27492;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24037;&#20855;&#31665;&#30340;&#23453;&#36149;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.
&lt;/p&gt;</description></item><item><title>FedHyper&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#32463;&#39564;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03156</link><description>&lt;p&gt;
FedHyper:&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#19982;&#36229;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03156
&lt;/p&gt;
&lt;p&gt;
FedHyper&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#32463;&#39564;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#19968;&#31995;&#21015;&#22797;&#26434;&#25361;&#25112;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#20247;&#22810;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#65292;&#23398;&#20064;&#29575;&#30340;&#36866;&#24212;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26377;&#26395;&#26174;&#33879;&#25552;&#39640;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedHyper&#65292;&#19968;&#31181;&#19987;&#20026;FL&#35774;&#35745;&#30340;&#22522;&#20110;&#36229;&#26799;&#24230;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#31639;&#27861;&#12290;FedHyper&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#21487;&#20197;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#35843;&#25972;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#23398;&#20064;&#29575;&#12290;&#27492;&#22806;&#65292;FedHyper&#19981;&#20165;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181;&#21021;&#22987;&#23398;&#20064;&#29575;&#37197;&#32622;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#31283;&#20581;&#24615;&#65292;&#36824;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#32321;&#29712;&#30340;&#32463;&#39564;&#24615;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;FedHyper&#25910;&#25947;&#36895;&#24230;&#30340;&#20840;&#38754;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30446;&#26631;&#26159;&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#65292;&#36890;&#36807;&#30740;&#31350;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#19977;&#20010;&#23618;&#27425;&#65292;&#25581;&#31034;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#65292;&#24182;&#21019;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03152</link><description>&lt;p&gt;
&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards out-of-distribution generalizable predictions of chemical kinetics properties. (arXiv:2310.03152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30446;&#26631;&#26159;&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#65292;&#36890;&#36807;&#30740;&#31350;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#19977;&#20010;&#23618;&#27425;&#65292;&#25581;&#31034;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#65292;&#24182;&#21019;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20272;&#35745;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#25214;&#21040;&#20102;&#24212;&#29992;&#12290;&#38543;&#30528;&#36890;&#36807;&#8220;AI4drug discovery&#8221;&#37492;&#23450;&#30340;&#33647;&#29289;&#20998;&#23376;&#30340;&#32047;&#31215;&#65292;&#19979;&#19968;&#20010;&#24613;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#35774;&#35745;&#39640;&#36890;&#37327;&#21270;&#23398;&#21512;&#25104;&#36807;&#31243;&#65292;&#20197;&#20272;&#35745;&#26410;&#35265;&#21453;&#24212;&#20013;&#26410;&#30693;&#20998;&#23376;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#20855;&#22791;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65288;&#32467;&#26500;&#12289;&#26465;&#20214;&#21644;&#26426;&#21046;&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;OOD&#26465;&#20214;&#19979;&#30340;&#21453;&#24212;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#24418;OOD&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) techniques have found applications in estimating chemical kinetics properties. With the accumulated drug molecules identified through "AI4drug discovery", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#23545;FLAN-T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23545;&#20854;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.03150</link><description>&lt;p&gt;
&#22312;&#38750;&#24120;&#36793;&#32536;&#19978;&#23545;LLMs&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;&#65306;&#22909;&#12289;&#22351;&#21644;&#19985;(arXiv:2310.03150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#23545;FLAN-T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23545;&#20854;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22522;&#30784;&#27169;&#22411;&#22240;&#20026;&#25552;&#20379;&#20102;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#19982;&#25968;&#25454;&#20132;&#20114;&#21644;&#24555;&#36895;&#26816;&#32034;&#20449;&#24687;&#30340;&#26032;&#26426;&#20250;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;LLMs&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#30001;&#20110;&#27861;&#24459;&#25110;&#25216;&#26415;&#38480;&#21046;&#21487;&#33021;&#38590;&#20197;&#35775;&#38382;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#31169;&#26377;&#35745;&#31639;&#36164;&#28304;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25193;&#22823;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25968;&#25454;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#37319;&#29992;&#30828;&#20214;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;FLAN-T5&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#24494;&#35843;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;80M&#21040;3B&#65292;&#24182;&#24212;&#29992;&#20110;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24494;&#35266;&#27700;&#24179;&#30340;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#27169;&#22411;&#30340;FLOP&#21033;&#29992;&#29575;&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20013;&#24515;GPU&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#32593;&#32476;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20855;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.  This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#21644;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#24230;&#20559;&#24046;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30456;&#23545;&#22686;&#30410;&#39640;&#36798;65.27%&#12290;</title><link>http://arxiv.org/abs/2310.03148</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#20943;&#23569;&#22810;&#39046;&#22495;&#35270;&#39057;&#25512;&#33616;&#20013;&#30340;&#27969;&#34892;&#24230;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations. (arXiv:2310.03148v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#21644;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#24230;&#20559;&#24046;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30456;&#23545;&#22686;&#30410;&#39640;&#36798;65.27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#33258;&#28982;&#20135;&#29983;&#30340;&#21508;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#20840;&#29699;&#27969;&#34892;&#29289;&#21697;&#30340;&#26174;&#33879;&#39033;&#30446;&#20559;&#35265;&#12290;&#23616;&#37096;&#27969;&#34892;&#39033;&#30446;&#21487;&#33021;&#20250;&#34987;&#20840;&#29699;&#27969;&#34892;&#39033;&#30446;&#25152;&#25513;&#30422;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30340;&#35266;&#30475;&#27169;&#24335;/&#32479;&#35745;&#25968;&#25454;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#21095;&#21464;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#38656;&#35201;&#23398;&#20064;&#29305;&#23450;&#30340;&#29992;&#25143;&#23884;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#25216;&#26415;&#65292;&#20197;&#21450;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#27969;&#34892;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19978;&#37319;&#26679;&#26469;&#20016;&#23500;&#21547;&#26377;&#27963;&#36291;&#29992;&#25143;&#34920;&#31034;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#20511;&#21161;MTL&#26469;&#23398;&#20064;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#29992;&#25143;&#23884;&#20837;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#19981;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;PR-AUC&#25351;&#26631;&#19978;&#26174;&#31034;&#20986;&#20102;&#39640;&#36798;65.27%&#30340;&#30456;&#23545;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users' viewership patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings. In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed techniques.~Noticeably, we show improved relative gain of up to $65.27\%$ in PR-AUC metric
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#25991;&#21442;&#19982;&#24230;&#39044;&#27979;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;Twitter&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#27969;&#31243;&#65292;&#25506;&#35752;&#20102;&#20165;&#20973;&#19978;&#19979;&#25991;&#26159;&#21542;&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;&#25512;&#25991;&#30340;&#21442;&#19982;&#24230;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03147</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#25991;&#21442;&#19982;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Context-Based Tweet Engagement Prediction. (arXiv:2310.03147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#25991;&#21442;&#19982;&#24230;&#39044;&#27979;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;Twitter&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#27969;&#31243;&#65292;&#25506;&#35752;&#20102;&#20165;&#20973;&#19978;&#19979;&#25991;&#26159;&#21542;&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;&#25512;&#25991;&#30340;&#21442;&#19982;&#24230;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Twitter&#30446;&#21069;&#26159;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20043;&#19968;&#12290;&#20854;&#29992;&#25143;&#21487;&#20197;&#20998;&#20139;&#12289;&#38405;&#35835;&#21644;&#21442;&#19982;&#30701;&#25512;&#25991;&#12290;&#22312;2020&#24180;ACM&#25512;&#33616;&#31995;&#32479;&#20250;&#35758;&#19978;&#65292;Twitter&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#23567;&#32422;&#20026;70GB&#30340;&#25968;&#25454;&#38598;&#65292;&#20379;&#24180;&#24230;RecSys&#25361;&#25112;&#36187;&#20351;&#29992;&#12290;2020&#24180;&#30340;RecSys&#25361;&#25112;&#36187;&#36992;&#35831;&#21442;&#19982;&#22242;&#38431;&#21019;&#24314;&#27169;&#22411;&#65292;&#39044;&#27979;&#32473;&#23450;&#29992;&#25143;-&#25512;&#25991;&#32452;&#21512;&#30340;&#21442;&#19982;&#24230;&#21487;&#33021;&#24615;&#12290;&#25552;&#20132;&#30340;&#27169;&#22411;&#39044;&#27979;&#28857;&#36190;&#12289;&#22238;&#22797;&#12289;&#36716;&#21457;&#21644;&#24341;&#29992;&#30340;&#21442;&#19982;&#24230;&#65292;&#24182;&#22522;&#20110;&#20004;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65306;&#31934;&#30830;&#29575;-&#21484;&#22238;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65288;PRAUC&#65289;&#21644;&#30456;&#23545;&#20132;&#21449;&#29109;&#65288;RCE&#65289;&#12290;&#22312;&#36825;&#31687;&#23398;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;RecSys 2020&#25361;&#25112;&#36187;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#27969;&#31243;&#65292;&#30740;&#31350;&#20165;&#20973;&#19978;&#19979;&#25991;&#33021;&#21542;&#39044;&#27979;&#25512;&#25991;&#21442;&#19982;&#24230;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;TU Wien&#30340;Little Big Data Cluster&#19978;&#37319;&#29992;Spark&#24341;&#25806;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#25105;&#20204;&#25163;&#21160;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twitter is currently one of the biggest social media platforms. Its users may share, read, and engage with short posts called tweets. For the ACM Recommender Systems Conference 2020, Twitter published a dataset around 70 GB in size for the annual RecSys Challenge. In 2020, the RecSys Challenge invited participating teams to create models that would predict engagement likelihoods for given user-tweet combinations. The submitted models predicting like, reply, retweet, and quote engagements were evaluated based on two metrics: area under the precision-recall curve (PRAUC) and relative cross-entropy (RCE).  In this diploma thesis, we used the RecSys 2020 Challenge dataset and evaluation procedure to investigate how well context alone may be used to predict tweet engagement likelihood. In doing so, we employed the Spark engine on TU Wien's Little Big Data Cluster to create scalable data preprocessing, feature engineering, feature selection, and machine learning pipelines. We manually create
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03146</link><description>&lt;p&gt;
&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#22312;&#31751;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#25968;&#25454;&#19978;&#25913;&#21892;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data. (arXiv:2310.03146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#22312;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#25200;&#12290;&#39318;&#20808;&#65292;&#23427;&#20551;&#35774;&#35757;&#32451;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#23558;&#26679;&#26412;&#25353;&#20849;&#20139;&#30340;&#27979;&#37327;&#20540;&#36827;&#34892;&#20998;&#32452;&#65288;&#20363;&#22914;&#65292;&#30740;&#31350;&#21442;&#19982;&#32773;&#25110;&#32454;&#32990;&#65289;&#65292;&#36829;&#21453;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#26174;&#31034;&#20986;&#24615;&#33021;&#19979;&#38477;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#21644;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#30528;&#31751;&#28151;&#28102;&#24341;&#36215;&#30340;&#31532;&#19968;&#22411;&#21644;&#31532;&#20108;&#22411;&#38169;&#35823;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#36890;&#24120;&#34987;&#35757;&#32451;&#20197;&#23454;&#29616;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#34987;&#20302;&#20272;&#30340;&#32676;&#20307;&#65292;&#22312;&#36151;&#27454;&#25209;&#20934;&#25110;&#30830;&#23450;&#20581;&#24247;&#20445;&#38505;&#36153;&#29575;&#31561;&#20851;&#38190;&#39046;&#22495;&#24341;&#20837;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#12290;MEDL&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#20869;&#23481;&#20998;&#21035;&#37327;&#21270;&#31751;&#19981;&#21464;&#30340;&#22266;&#23450;&#25928;&#24212;&#21644;&#31751;&#29305;&#23450;&#30340;&#38543;&#26426;&#25928;&#24212;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#19968;&#20010;&#31751;&#23545;&#25163;&#65292;&#40723;&#21169;&#31751;&#38388;&#24046;&#24322;&#30340;&#26368;&#23567;&#21270;&#65307;
&lt;/p&gt;
&lt;p&gt;
Traditional deep learning (DL) suffers from two core problems. Firstly, it assumes training samples are independent and identically distributed. However, numerous real-world datasets group samples by shared measurements (e.g., study participants or cells), violating this assumption. In these scenarios, DL can show compromised performance, limited generalization, and interpretability issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly, models are typically trained for overall accuracy, often neglecting underrepresented groups and introducing biases in crucial areas like loan approvals or determining health insurance rates, such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourage
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25682;&#24323;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.03123</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25682;&#24323;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#35843;&#25972;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20869;&#23384;&#38480;&#21046;&#65306;&#24320;&#28304;PTMs&#22823;&#23567;&#30340;&#25345;&#32493;&#22686;&#38271;&#20351;&#24471;&#21363;&#20351;&#23545;&#20854;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#24494;&#35843;&#20063;&#23545;&#35768;&#22810;&#20174;&#19994;&#32773;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#65288;2&#65289;&#27169;&#22411;&#38544;&#31169;&#24615;&#65306;&#29616;&#26377;&#30340;PTMs&#36890;&#24120;&#20316;&#20026;&#20844;&#20849;API&#26381;&#21153;&#65292;&#20854;&#21442;&#25968;&#26080;&#27861;&#26377;&#25928;&#25110;&#23450;&#21046;&#22320;&#36827;&#34892;&#24494;&#35843;&#12290;&#65288;3&#65289;&#25968;&#25454;&#38544;&#31169;&#24615;&#65306;&#23545;PTMs&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#23616;&#37096;&#21270;&#30340;&#24182;&#19988;&#19981;&#20849;&#20139;&#32473;&#20844;&#20247;&#12290;&#20026;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#40657;&#30418;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins
&lt;/p&gt;</description></item><item><title>OpenMM 8&#26159;&#19968;&#20010;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#21151;&#33021;&#12289;&#20248;&#21270;&#35745;&#31639;&#36895;&#24230;&#21644;&#25552;&#20379;&#39640;&#32423;&#25509;&#21475;&#65292;&#20351;&#24471;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#25104;&#20026;&#19968;&#31181;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03121</link><description>&lt;p&gt;
OpenMM 8&#65306;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials. (arXiv:2310.03121v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03121
&lt;/p&gt;
&lt;p&gt;
OpenMM 8&#26159;&#19968;&#20010;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#21151;&#33021;&#12289;&#20248;&#21270;&#35745;&#31639;&#36895;&#24230;&#21644;&#25552;&#20379;&#39640;&#32423;&#25509;&#21475;&#65292;&#20351;&#24471;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#25104;&#20026;&#19968;&#31181;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#35282;&#33394;&#12290;OpenMM&#20998;&#23376;&#21160;&#21147;&#23398;&#24037;&#20855;&#21253;&#30340;&#26368;&#26032;&#29256;&#26412;&#24341;&#20837;&#20102;&#26032;&#21151;&#33021;&#65292;&#20197;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#12290;&#20219;&#24847;&#30340;PyTorch&#27169;&#22411;&#21487;&#28155;&#21152;&#21040;&#27169;&#25311;&#20013;&#65292;&#24182;&#21487;&#29992;&#20110;&#35745;&#31639;&#21147;&#21644;&#33021;&#37327;&#12290;&#26356;&#39640;&#32423;&#30340;&#25509;&#21475;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#20351;&#29992;&#36890;&#29992;&#39044;&#35757;&#32451;&#21183;&#20989;&#25968;&#23545;&#24863;&#20852;&#36259;&#30340;&#20998;&#23376;&#36827;&#34892;&#24314;&#27169;&#12290;&#20248;&#21270;&#30340;CUDA&#26680;&#20989;&#25968;&#21644;&#33258;&#23450;&#20041;&#30340;PyTorch&#25805;&#20316;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;&#27700;&#20013;&#27169;&#25311;&#20102;CDK8&#21644;&#32511;&#33394;&#33639;&#20809;&#34507;&#30333;(GFP)&#33394;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#29305;&#24615;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#21464;&#24471;&#23454;&#38469;&#65292;&#24182;&#21482;&#26377;&#36866;&#24230;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning plays an important and growing role in molecular simulation. The newest version of the OpenMM molecular dynamics toolkit introduces new features to support the use of machine learning potentials. Arbitrary PyTorch models can be added to a simulation and used to compute forces and energy. A higher-level interface allows users to easily model their molecules of interest with general purpose, pretrained potential functions. A collection of optimized CUDA kernels and custom PyTorch operations greatly improves the speed of simulations. We demonstrate these features on simulations of cyclin-dependent kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water. Taken together, these features make it practical to use machine learning to improve the accuracy of simulations at only a modest increase in cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#21487;&#31227;&#26893;&#30340;&#30005;&#30913;&#20391;&#20449;&#36947;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#35774;&#22791;&#21464;&#21270;&#24615;&#12289;&#29615;&#22659;&#22240;&#32032;&#21644;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;EM-SCA&#32467;&#26524;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03119</link><description>&lt;p&gt;
&#36328;&#29289;&#32852;&#32593;&#35774;&#22791;&#21487;&#31227;&#26893;&#30340;&#30005;&#30913;&#20391;&#20449;&#36947;&#20998;&#26512;&#65306;&#25361;&#25112;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset. (arXiv:2310.03119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03119
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#21487;&#31227;&#26893;&#30340;&#30005;&#30913;&#20391;&#20449;&#36947;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#35774;&#22791;&#21464;&#21270;&#24615;&#12289;&#29615;&#22659;&#22240;&#32032;&#21644;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;EM-SCA&#32467;&#26524;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;Internet of Things&#65289;&#26159;&#25351;&#20114;&#32852;&#30340;&#29289;&#29702;&#35774;&#22791;&#12289;&#36710;&#36742;&#12289;&#23478;&#29992;&#30005;&#22120;&#31561;&#23884;&#20837;&#20256;&#24863;&#22120;&#12289;&#36719;&#20214;&#21644;&#36830;&#25509;&#24615;&#30340;&#32593;&#32476;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25910;&#38598;&#21644;&#20132;&#25442;&#25968;&#25454;&#12290;&#29289;&#32852;&#32593;&#21462;&#35777;&#26159;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#21644;&#20998;&#26512;&#25968;&#23383;&#35777;&#25454;&#65292;&#20197;&#35843;&#26597;&#21487;&#33021;&#21457;&#29983;&#22312;&#36825;&#20123;&#36830;&#25509;&#35774;&#22791;&#19978;&#30340;&#32593;&#32476;&#29359;&#32618;&#12289;&#23433;&#20840;&#28431;&#27934;&#21644;&#20854;&#20182;&#24694;&#24847;&#27963;&#21160;&#12290;&#29305;&#21035;&#26159;&#65292;&#30005;&#30913;&#20391;&#20449;&#36947;&#20998;&#26512;(EM-SCA)&#24050;&#25104;&#20026;&#29289;&#32852;&#32593;&#21462;&#35777;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#19981;&#24178;&#39044;&#36825;&#20123;&#35774;&#22791;&#25110;&#31363;&#21548;&#23427;&#20204;&#30340;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#25581;&#31034;&#26377;&#20851;&#29289;&#32852;&#32593;&#35774;&#22791;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#23494;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#30340;&#21464;&#21270;&#24615;&#12289;&#29615;&#22659;&#22240;&#32032;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#20250;&#38480;&#21046;EM-SCA&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36328;&#29289;&#32852;&#32593;&#35774;&#22791;&#21487;&#31227;&#26893;&#24615;&#30340;EM-SCA&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#38480;&#21046;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#65292;&#20063;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT (Internet of Things) refers to the network of interconnected physical devices, vehicles, home appliances, and other items embedded with sensors, software, and connectivity, enabling them to collect and exchange data. IoT Forensics is collecting and analyzing digital evidence from IoT devices to investigate cybercrimes, security breaches, and other malicious activities that may have taken place on these connected devices. In particular, EM-SCA has become an essential tool for IoT forensics due to its ability to reveal confidential information about the internal workings of IoT devices without interfering these devices or wiretapping their networks. However, the accuracy and reliability of EM-SCA results can be limited by device variability, environmental factors, and data collection and processing methods. Besides, there is very few research on these limitations that affects significantly the accuracy of EM-SCA approaches for the crossed-IoT device portability as well as limited res
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36890;&#36807;&#20915;&#31574;&#35268;&#21017;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21487;&#21152;&#24615;&#20027;&#25928;&#24212;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#36817;&#20284;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.03112</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Leveraging Model-based Trees as Interpretable Surrogate Models for Model Distillation. (arXiv:2310.03112v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36890;&#36807;&#20915;&#31574;&#35268;&#21017;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21487;&#21152;&#24615;&#20027;&#25928;&#24212;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#36817;&#20284;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26367;&#20195;&#27169;&#22411;&#22312;&#36890;&#36807;&#27169;&#22411;&#33976;&#39311;&#22238;&#39038;&#24615;&#22320;&#35299;&#37322;&#22797;&#26434;&#32780;&#24378;&#22823;&#30340;&#40657;&#30418;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20316;&#20026;&#26367;&#20195;&#27169;&#22411;&#65292;&#36890;&#36807;&#20915;&#31574;&#35268;&#21017;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#12290;&#22312;&#27599;&#20010;&#21306;&#22495;&#20869;&#65292;&#20351;&#29992;&#22522;&#20110;&#21487;&#21152;&#24615;&#20027;&#25928;&#24212;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#36817;&#20284;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#31639;&#27861;&#65288;SLIM&#65292;GUIDE&#65292;MOB&#21644;CTree&#65289;&#22312;&#29983;&#25104;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20445;&#30495;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#31283;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#25429;&#25417;&#20132;&#20114;&#25928;&#24212;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#25143;&#29305;&#23450;&#30340;&#25512;&#33616;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate models play a crucial role in retrospectively interpreting complex and powerful black box machine learning models via model distillation. This paper focuses on using model-based trees as surrogate models which partition the feature space into interpretable regions via decision rules. Within each region, interpretable models based on additive main effects are used to approximate the behavior of the black box model, striking for an optimal balance between interpretability and performance. Four model-based tree algorithms, namely SLIM, GUIDE, MOB, and CTree, are compared regarding their ability to generate such surrogate models. We investigate fidelity, interpretability, stability, and the algorithms' capability to capture interaction effects through appropriate splits. Based on our comprehensive analyses, we finally provide an overview of user-specific recommendations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#21462;&#19981;&#21516;&#23454;&#39564;&#27169;&#24577;&#30340;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03111</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data. (arXiv:2310.03111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#21462;&#19981;&#21516;&#23454;&#39564;&#27169;&#24577;&#30340;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#34429;&#28982;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;LVMs&#65289;&#22312;&#25551;&#36848;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#29992;&#20110;&#21333;&#19968;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#35782;&#21035;&#19981;&#21516;&#23454;&#39564;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;LVM&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#20102;&#19981;&#21516;&#12289;&#21516;&#26102;&#35760;&#24405;&#30340;&#23454;&#39564;&#27169;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#29992;&#20110;&#31070;&#32463;&#23574;&#23792;&#25968;&#25454;&#30340;LVM&#65292;&#24182;&#20855;&#26377;&#26102;&#38388;&#24179;&#28369;&#28508;&#31354;&#38388;&#65292;&#19982;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;GP-VAEs&#21516;&#26679;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#26469;&#25551;&#36848;&#28508;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#21040;&#35266;&#27979;&#20540;&#20855;&#26377;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28508;&#21464;&#37327;&#20998;&#21306;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning lat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#24120;&#32452;&#32455;&#26679;&#26412;&#26500;&#24314;&#8220;&#27491;&#24120;&#32452;&#32455;&#22270;&#35889;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36825;&#31181;&#22270;&#35889;&#26469;&#25552;&#39640;WSI&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#22312;107&#20010;&#27491;&#24120;&#30382;&#32932;WSI&#19978;&#24314;&#31435;&#27491;&#24120;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;553&#20010;&#34920;&#30382;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;cSCC&#65289;&#30340;WSI&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03106</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#27491;&#24120;&#32452;&#32455;&#30340;&#22270;&#35889;&#65292;&#36890;&#36807;&#24322;&#24120;&#26816;&#27979;&#23545;WSI&#34917;&#19969;&#36827;&#34892;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection. (arXiv:2310.03106v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#24120;&#32452;&#32455;&#26679;&#26412;&#26500;&#24314;&#8220;&#27491;&#24120;&#32452;&#32455;&#22270;&#35889;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36825;&#31181;&#22270;&#35889;&#26469;&#25552;&#39640;WSI&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#22312;107&#20010;&#27491;&#24120;&#30382;&#32932;WSI&#19978;&#24314;&#31435;&#27491;&#24120;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;553&#20010;&#34920;&#30382;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;cSCC&#65289;&#30340;WSI&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#65292;&#20462;&#21098;&#21315;&#20806;&#20687;&#32032;&#30340;&#20840;&#23610;&#23544;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#24050;&#25552;&#20986;&#19968;&#20123;&#26041;&#27861;&#26469;&#36873;&#25321;&#19968;&#37096;&#20998;&#34917;&#19969;&#20316;&#20026;WSI&#34920;&#31034;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#37117;&#26088;&#22312;&#23545;&#27599;&#20010;WSI&#20013;&#30340;&#30149;&#29702;&#25439;&#20260;&#36827;&#34892;&#20998;&#31867;&#25110;&#26816;&#27979;&#65292;&#20294;&#24120;&#35268;&#32452;&#32455;&#26679;&#26412;&#20013;&#27491;&#24120;&#32452;&#32455;&#30340;&#22256;&#25200;&#20316;&#29992;&#21644;&#20887;&#20313;&#24615;&#36136;&#36890;&#24120;&#34987;&#24573;&#35270;&#22312;WSI&#34920;&#31034;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20351;&#29992;&#20174;&#27491;&#24120;&#32452;&#32455;&#27963;&#26816;&#20013;&#33719;&#24471;&#30340;WSI&#26679;&#26412;&#20165;&#20165;&#29992;&#20110;&#26500;&#24314;&#8220;&#27491;&#24120;&#32452;&#32455;&#22270;&#35889;&#8221;&#30340;&#27010;&#24565;&#12290;&#36825;&#26679;&#30340;&#22270;&#35889;&#21487;&#20197;&#29992;&#20110;&#28040;&#38500;&#27491;&#24120;&#32452;&#32455;&#26679;&#26412;&#30340;&#27491;&#24120;&#29255;&#27573;&#65292;&#20174;&#32780;&#22686;&#21152;&#34917;&#19969;&#38598;&#21512;&#30340;&#20195;&#34920;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;107&#20010;&#27491;&#24120;&#30382;&#32932;WSI&#24314;&#31435;&#20102;&#19968;&#20010;&#27491;&#24120;&#22270;&#35889;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#25913;&#36827;&#24050;&#24314;&#31435;&#30340;&#25351;&#26631;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#22914;Yottixel&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;553&#20010;&#34920;&#30382;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;cSCC&#65289;&#30340;WSI&#26469;&#23637;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Patching gigapixel whole slide images (WSIs) is an important task in computational pathology. Some methods have been proposed to select a subset of patches as WSI representation for downstream tasks. While most of the computational pathology tasks are designed to classify or detect the presence of pathological lesions in each WSI, the confounding role and redundant nature of normal histology in tissue samples are generally overlooked in WSI representations. In this paper, we propose and validate the concept of an "atlas of normal tissue" solely using samples of WSIs obtained from normal tissue biopsies. Such atlases can be employed to eliminate normal fragments of tissue samples and hence increase the representativeness collection of patches. We tested our proposed method by establishing a normal atlas using 107 normal skin WSIs and demonstrated how established indexes and search engines like Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma (cSCC) to show
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21487;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;DP-SGD&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30456;&#20284;&#24615;&#25439;&#22833;&#20989;&#25968;&#30340;$L_2$&#25935;&#24863;&#24230;&#22686;&#38271;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03104</link><description>&lt;p&gt;
&#38750;&#21487;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;DP-SGD&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-SGD for non-decomposable objective functions. (arXiv:2310.03104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21487;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;DP-SGD&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30456;&#20284;&#24615;&#25439;&#22833;&#20989;&#25968;&#30340;$L_2$&#25935;&#24863;&#24230;&#22686;&#38271;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#24320;&#21457;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35265;&#27493;&#39588;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#23569;&#26631;&#31614;&#65292;&#38656;&#35201;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#23545;&#27604;&#25439;&#22833;&#65292;&#26469;&#20248;&#21270;&#30456;&#20284;&#36755;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#26368;&#22823;&#21270;&#19981;&#21516;&#36755;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#38543;&#30528;&#38544;&#31169;&#38382;&#39064;&#30340;&#22686;&#22810;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#29983;&#25104;&#36755;&#20837;&#30340;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;$L_2$&#25935;&#24863;&#24230;&#20250;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;DP-SGD&#65289;&#29305;&#21035;&#19981;&#21033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#21464;&#20307;&#65292;&#29992;&#20110;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#24120;&#29992;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#22788;&#29702;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#20351;&#24471;&#26799;&#24230;&#30340;&#25935;&#24863;&#24230;&#23545;&#20110;&#25209;&#37327;&#22823;&#23567;&#26159;$O(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity can grow with increasing batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions -- in particular the commonly used contrastive loss -- that manipulates gradients of the objective function in a novel way to obtain a senstivity of the summed gradient that is $O(1)$ for batch size
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03103</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#20043;&#38388;&#26222;&#36941;&#23384;&#22312;&#39046;&#22495;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#38590;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#29616;&#23454;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21452;&#25552;&#31034;&#35843;&#20248;&#65288;Fed-DPT&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Fed-DPT&#37319;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#35843;&#20248;&#26469;&#20419;&#36827;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#22823;&#37327;&#30340;Fed-DPT&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;11&#65285;&#65292;&#32467;&#26524;&#30340;&#26631;&#20934;&#24046;&#20943;&#23567;&#20102;75&#65285;&#65292;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#20102;30&#65285;&#12290;</title><link>http://arxiv.org/abs/2310.03088</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Accelerating Power System State Estimation. (arXiv:2310.03088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;11&#65285;&#65292;&#32467;&#26524;&#30340;&#26631;&#20934;&#24046;&#20943;&#23567;&#20102;75&#65285;&#65292;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#20102;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#20272;&#35745;&#26159;&#30005;&#21147;&#31995;&#32479;&#25511;&#21046;&#20013;&#24515;&#30340;&#22522;&#30707;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#31995;&#32479;&#22312;&#36830;&#32493;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#36816;&#34892;&#29366;&#24577;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#21152;&#36895;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#30417;&#27979;&#30005;&#21147;&#31995;&#32479;&#30340;&#36816;&#34892;&#24773;&#20917;&#12290;&#20256;&#32479;&#30340;&#29366;&#24577;&#20272;&#35745;&#25216;&#26415;&#24448;&#24448;&#20381;&#36182;&#20110;&#36845;&#20195;&#31639;&#27861;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#30005;&#21147;&#31995;&#32479;&#26469;&#35828;&#21487;&#33021;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;PINNs&#30340;&#20869;&#22312;&#29289;&#29702;&#30693;&#35782;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#29366;&#24577;&#20272;&#35745;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#21319;&#20102;11&#65285;&#65292;&#32467;&#26524;&#30340;&#26631;&#20934;&#24046;&#20943;&#23567;&#20102;75&#65285;&#65292;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#20102;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
State estimation is the cornerstone of the power system control center since it provides the operating condition of the system in consecutive time intervals. This work investigates the application of physics-informed neural networks (PINNs) for accelerating power systems state estimation in monitoring the operation of power systems. Traditional state estimation techniques often rely on iterative algorithms that can be computationally intensive, particularly for large-scale power systems. In this paper, a novel approach that leverages the inherent physical knowledge of power systems through the integration of PINNs is proposed. By incorporating physical laws as prior knowledge, the proposed method significantly reduces the computational complexity associated with state estimation while maintaining high accuracy. The proposed method achieves up to 11% increase in accuracy, 75% reduction in standard deviation of results, and 30% faster convergence, as demonstrated by comprehensive experim
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#26082;&#33021;&#22815;&#25913;&#21892;DNA&#24207;&#21015;&#20998;&#26512;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#65292;&#20063;&#20026;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03086</link><description>&lt;p&gt;
&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03086
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#26082;&#33021;&#22815;&#25913;&#21892;DNA&#24207;&#21015;&#20998;&#26512;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#65292;&#20063;&#20026;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#38761;&#26032;&#20102;&#29983;&#29289;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#22312;&#25105;&#20204;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#21382;&#21490;&#12289;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#65306;DNA&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#65292;&#20197;&#21450;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#35201;&#20805;&#20998;&#21457;&#25381;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#28508;&#21147;&#65292;&#20851;&#38190;&#26159;&#35299;&#20915;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#23545;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#26512;DNA&#24207;&#21015;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#22312;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#20174;&#32780;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has become a powerful tool in computational biology, revolutionising the analysis and interpretation of biological data over time. In our article review, we delve into various aspects of deep learning in computational biology. Specifically, we examine its history, advantages, and challenges. Our focus is on two primary applications: DNA sequence classification and prediction, as well as protein structure prediction from sequence data. Additionally, we provide insights into the outlook for this field. To fully harness the potential of deep learning in computational biology, it is crucial to address the challenges that come with it. These challenges include the requirement for large, labelled datasets and the interpretability of deep learning models. The use of deep learning in the analysis of DNA sequences has brought about a significant transformation in the detection of genomic variants and the analysis of gene expression. This has greatly contributed to the advancement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#21387;&#32553;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#24178;&#20928;&#20449;&#21495;&#25110;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#23558;&#27491;&#21017;&#21270;&#22120;&#19982;&#25968;&#25454;&#20998;&#24067;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#24674;&#22797;&#22797;&#26434;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.03085</link><description>&lt;p&gt;
&#26080;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#21387;&#32553;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising. (arXiv:2310.03085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#21387;&#32553;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#24178;&#20928;&#20449;&#21495;&#25110;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#23558;&#27491;&#21017;&#21270;&#22120;&#19982;&#25968;&#25454;&#20998;&#24067;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#24674;&#22797;&#22797;&#26434;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#21033;&#29992;&#26469;&#33258;&#24178;&#20928;&#20449;&#21495;&#25110;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#21435;&#22122;&#30340;&#38382;&#39064;&#12290;&#22914;&#26524;&#33021;&#22815;&#33719;&#24471;&#36866;&#24212;&#25968;&#25454;&#24615;&#36136;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#37319;&#29992;&#21464;&#20998;&#26041;&#27861;&#36827;&#34892;&#21435;&#22122;&#38750;&#24120;&#39640;&#25928;&#12290;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#19982;&#25968;&#25454;&#30340;&#20998;&#24067;&#31995;&#32479;&#22320;&#20851;&#32852;&#36215;&#26469;&#12290;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#21487;&#20197;&#20174;&#19968;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#24674;&#22797;&#22797;&#26434;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#20943;&#23569;&#20219;&#21153;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#23558;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;&#35843;&#25972;&#20026;&#29992;DNN&#21442;&#25968;&#21270;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#21464;&#20307;&#65292;&#29992;&#20110;&#20174;&#19968;&#20010;&#34987;&#22823;&#24133;&#21387;&#32553;&#30340;&#25968;&#25454;&#24211;&#20013;&#24674;&#22797;&#28145;&#24230;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#36825;&#20123;&#31639;&#27861;&#20248;&#20110;&#26368;&#21021;&#25552;&#20986;&#30340;&#21482;&#38024;&#23545;&#20302;&#32500;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#36845;&#20195;&#37117;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#21463;&#30410;&#20110;&#32463;&#20856;&#30340;SGD&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of denoising with the help of prior information taken from a database of clean signals or images. Denoising with variational methods is very efficient if a regularizer well adapted to the nature of the data is available. Thanks to the maximum a posteriori Bayesian framework, such regularizer can be systematically linked with the distribution of the data. With deep neural networks (DNN), complex distributions can be recovered from a large training database.To reduce the computational burden of this task, we adapt the compressive learning framework to the learning of regularizers parametrized by DNN. We propose two variants of stochastic gradient descent (SGD) for the recovery of deep regularization parameters from a heavily compressed database. These algorithms outperform the initially proposed method that was limited to low-dimensional signals, each iteration using information from the whole database. They also benefit from classical SGD convergence guarantees. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03084</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#38544;&#21547;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23450;&#20301;&#36825;&#20123;&#34920;&#31034;&#24182;&#23558;&#20854;&#35299;&#31163;&#20986;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21253;&#21547;&#20102;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65306;&#36127;&#36131;&#32534;&#30721;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#29305;&#23450;&#30693;&#35782;&#30340;&#29305;&#23450;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#26469;&#21457;&#29616;&#36825;&#20123;&#23376;&#32593;&#32476;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#31934;&#30830;&#22320;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;GPT2&#21464;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39640;&#24230;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;98%+&#65289;&#65292;&#23427;&#20204;&#20165;&#36127;&#36131;&#29305;&#23450;&#30340;&#20851;&#31995;&#30693;&#35782;&#38598;&#21512;&#12290;&#24403;&#21024;&#38500;&#36825;&#20123;&#23376;&#32593;&#32476;&#26102;&#65292;&#21097;&#20313;&#30340;&#32593;&#32476;&#20173;&#20445;&#25345;&#20102;&#22823;&#37096;&#20998;&#20854;&#21021;&#22987;&#23481;&#37327;&#65288;&#23545;&#35821;&#35328;&#21644;&#20854;&#20182;&#35760;&#24518;&#20851;&#31995;&#30340;&#24314;&#27169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#31283;&#20581;&#24615;&#21644;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#65292;&#21516;&#26102;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03055</link><description>&lt;p&gt;
&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#21450;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#31283;&#20581;&#24615;&#21644;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#65292;&#21516;&#26102;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#24314;&#31435;&#22312;&#21407;&#22987;&#30340;LAB&#31639;&#27861;&#65288;Reddy&#31561;&#20154;&#65292;2023&#24180;&#65289;&#22522;&#30784;&#19978;&#65292;&#35813;&#31639;&#27861;&#27169;&#25311;&#20102;&#32676;&#20307;&#20869;&#37096;&#30340;&#31454;&#20105;&#21644;&#23398;&#20064;&#34892;&#20026;&#65292;&#24314;&#31435;&#20102;&#23618;&#27425;&#35282;&#33394;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#36718;&#30424;&#36172;&#26041;&#27861;&#21644;&#32553;&#20943;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#32676;&#32452;&#38388;&#31454;&#20105;&#65292;&#24182;&#36880;&#27493;&#32553;&#23567;&#26679;&#26412;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;CEC 2005&#21644;CEC 2017&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#36827;&#34892;&#39564;&#35777;&#12290;&#20351;&#29992;&#21452;&#36793;&#21644;&#25104;&#23545;&#31526;&#21495;&#31209;Wilcoxon&#27979;&#35797;&#20197;&#21450;Friedman&#31209;&#27979;&#35797;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#36827;&#21644;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#20197;&#21450;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#65288;C-SSR&#65289;&#26041;&#27861;&#65292;&#20351;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;C-SSR&#26041;&#27861;&#20351;&#31639;&#27861;&#33021;&#22815;&#35782;&#21035;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21487;&#34892;&#21306;&#22495;&#20043;&#38388;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A modified LAB algorithm is introduced in this paper. It builds upon the original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm that models competitive and learning behaviours within a group, establishing hierarchical roles. The proposed algorithm incorporates the roulette wheel approach and a reduction factor introducing inter-group competition and iteratively narrowing down the sample space. The algorithm is validated by solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions are validated using standard statistical tests such as two-sided and pairwise signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited improved and superior robustness as well as search space exploration capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR) method is proposed, making the algorithm capable to solve constrained problems. The C-SSR method enables the algorithm to identify clusters of feasible regions, satisfying 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03054</link><description>&lt;p&gt;
&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#26799;&#24230;&#27969;&#30340;&#21518;&#39564;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel. (arXiv:2310.03054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#20010;MMD&#65292;&#20063;&#34987;&#31216;&#20026;&#33021;&#37327;&#36317;&#31163;&#65292;&#20855;&#26377;&#20687;&#36890;&#36807;&#20999;&#29255;&#21644;&#25490;&#24207;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#30340;&#20960;&#20010;&#26377;&#30410;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#36817;&#20284;&#30495;&#23454;&#24773;&#20917;&#21644;&#35266;&#23519;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#20026;&#21518;&#39564;&#20998;&#24067;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31890;&#23376;&#27969;&#30830;&#23454;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#36890;&#36807;&#25968;&#23383;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#21253;&#25324;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#35832;&#22914;&#36229;&#20998;&#36776;&#29575;&#12289;&#20462;&#22797;&#21644;&#20302;&#21058;&#37327;&#21644;&#26377;&#38480;&#35282;&#24230;&#35774;&#32622;&#19979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#31561;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#20809;&#32420;&#31070;&#32463;&#26550;&#26500;&#30340;&#40065;&#26834;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03049</link><description>&lt;p&gt;
QuATON: &#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
QuATON: Quantization Aware Training of Optical Neurons. (arXiv:2310.03049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#20809;&#32420;&#31070;&#32463;&#26550;&#26500;&#30340;&#40065;&#26834;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#31070;&#32463;&#26550;&#26500;&#65288;ONA&#65289;&#20351;&#29992;&#20855;&#26377;&#20248;&#21270;&#29289;&#29702;&#21442;&#25968;&#30340;&#32534;&#30721;&#20803;&#20214;&#36827;&#34892;&#26234;&#33021;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#21046;&#36896;&#20855;&#26377;&#35774;&#35745;&#24615;&#33021;&#30340;ONA&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21046;&#36896;&#25216;&#26415;&#30340;&#38480;&#21046;&#36890;&#24120;&#38480;&#21046;&#20102;&#35757;&#32451;&#21442;&#25968;&#30340;&#21487;&#23454;&#29616;&#31934;&#24230;&#12290;&#29289;&#29702;&#32422;&#26463;&#20063;&#21487;&#33021;&#38480;&#21046;&#29289;&#29702;&#21442;&#25968;&#21487;&#20197;&#23481;&#32435;&#30340;&#20540;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;ONA&#24212;&#22312;&#21487;&#23454;&#29616;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#23558;&#35757;&#32451;&#30446;&#26631;&#36716;&#21270;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#20854;&#26356;&#38590;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20174;&#27169;&#25311;&#21040;&#23454;&#29616;&#30340;&#20851;&#38190;&#38382;&#39064;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19968;&#31181;ONA&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;ONA&#34987;&#31216;&#20026;&#34893;&#23556;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical neural architectures (ONAs) use coding elements with optimized physical parameters to perform intelligent measurements. However, fabricating ONAs while maintaining design performances is challenging. Limitations in fabrication techniques often limit the realizable precision of the trained parameters. Physical constraints may also limit the range of values the physical parameters can hold. Thus, ONAs should be trained within the implementable constraints. However, such physics-based constraints reduce the training objective to a constrained optimization problem, making it harder to optimize with existing gradient-based methods. To alleviate these critical issues that degrade performance from simulation to realization we propose a physics-informed quantization-aware training framework. Our approach accounts for the physical constraints during the training process, leading to robust designs. We evaluate our approach on an ONA proposed in the literature, named a diffractive deep ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23558;&#20998;&#23376;&#29289;&#31181;&#12289;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#24182;&#23637;&#31034;&#20102;DiffMix&#30456;&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#25913;&#36827;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03047</link><description>&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#21270;&#23398;&#29289;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#28151;&#21512;&#29289;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures. (arXiv:2310.03047v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23558;&#20998;&#23376;&#29289;&#31181;&#12289;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#24182;&#23637;&#31034;&#20102;DiffMix&#30456;&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#25913;&#36827;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#28151;&#21512;&#29289;&#28385;&#36275;&#22810;&#30446;&#26631;&#24615;&#33021;&#24230;&#37327;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#21644;&#30005;&#21270;&#23398;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#23558;&#20998;&#23376;&#29289;&#31181;&#65292;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#26469;&#25193;&#23637;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#36880;&#20998;&#37327;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20174;&#20108;&#20803;&#28151;&#21512;&#29289;&#30340;&#28909;&#21147;&#23398;&#24320;&#22987;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#36827;&#19968;&#27493;&#22312;&#22810;&#32452;&#20998;&#30005;&#35299;&#36136;&#28151;&#21512;&#29289;&#19978;&#23545;&#20854;&#36755;&#36816;&#24615;&#36136;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DiffMix&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical mixtures, satisfying multi-objective performance metrics and constraints, enable their use in chemical processes and electrochemical devices. In this work, we develop a differentiable chemical-physics framework for modeling chemical mixtures, DiffMix, where geometric deep learning (GDL) is leveraged to map from molecular species, compositions and environment conditions, to physical coefficients in the mixture physics laws. In particular, we extend mixture thermodynamic and transport laws by creating learnable physical coefficients, where we use graph neural networks as the molecule encoder and enforce component-wise permutation-invariance. We start our model evaluations with thermodynamics of binary mixtures, and further benchmarked multicomponent electrolyte mixtures on their transport properties, in order to test the model generalizability. We show improved prediction accuracy and model robustness of DiffMix than its purely data-driven variants. Furthermore, we demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#21477;&#32423;&#21453;&#39304;&#20449;&#24687;&#26469;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36866;&#24212;&#26368;&#26032;&#30340;BERT-based&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#21477;&#23376;&#36873;&#25321;&#21644;&#39033;&#30446;&#25490;&#24207;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03043</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#19982;&#21477;&#32423;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback. (arXiv:2310.03043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#21477;&#32423;&#21453;&#39304;&#20449;&#24687;&#26469;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36866;&#24212;&#26368;&#26032;&#30340;BERT-based&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#21477;&#23376;&#36873;&#25321;&#21644;&#39033;&#30446;&#25490;&#24207;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#30340;&#20132;&#20114;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#36825;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#36991;&#20813;&#26080;&#20851;&#20449;&#24687;&#24182;&#25429;&#25417;&#29992;&#25143;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#29616;&#26377;&#30340;&#26368;&#26032;&#31995;&#32479;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#25972;&#21512;&#36825;&#20123;&#20132;&#20114;&#65292;&#20294;&#26159;&#24573;&#30053;&#20102;&#21477;&#32423;&#21453;&#39304;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21453;&#39304;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;RL&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#21644;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#65288;DQ&#65289;&#26041;&#27861;DQrank&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;DQrank&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26032;&#25216;&#26415;BERT-based&#27169;&#22411;&#26469;&#36873;&#25321;&#20851;&#38190;&#21477;&#23376;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#30340;&#21442;&#19982;&#24230;&#23545;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#21046;&#26469;&#26356;&#22909;&#22320;&#25506;&#32034;&#26368;&#20248;&#34892;&#21160;&#12290;DQrank&#36824;&#21033;&#29992;DQ&#20013;&#30340;&#32463;&#39564;&#37325;&#29616;&#26426;&#21046;&#26469;&#23384;&#20648;&#21453;&#39304;&#21477;&#23376;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive search can provide a better experience by incorporating interaction feedback from the users. This can significantly improve search accuracy as it helps avoid irrelevant information and captures the users' search intents. Existing state-of-the-art (SOTA) systems use reinforcement learning (RL) models to incorporate the interactions but focus on item-level feedback, ignoring the fine-grained information found in sentence-level feedback. Yet such feedback requires extensive RL action space exploration and large amounts of annotated data. This work addresses these challenges by proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based models, the SOTA in natural language processing, to select crucial sentences based on users' engagement and rank the items to obtain more satisfactory responses. We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to ob
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#26469;&#35780;&#20272;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#27169;&#22411;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#24230;&#21644;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03033</link><description>&lt;p&gt;
&#25552;&#21319;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#39640;&#20934;&#30830;&#24615;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#26469;&#35780;&#20272;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#27169;&#22411;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#24230;&#21644;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#26631;&#24535;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23545;&#36947;&#36335;&#23433;&#20840;&#21644;&#20132;&#36890;&#31649;&#29702;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#36974;&#25377;&#31561;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20998;&#31867;&#22120;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#39640;&#20934;&#30830;&#24615;BNN&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#32039;&#20945;&#30340;&#22823;&#23567;&#65292;&#20197;&#36866;&#24212;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#36164;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#23618;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36825;&#20123;&#23618;&#21253;&#25324;&#20108;&#36827;&#21046;&#21367;&#31215;&#12289;&#26368;&#22823;&#27744;&#21270;&#12289;&#25209;&#24402;&#19968;&#21270;&#21644;&#20840;&#36830;&#25509;&#23618;&#12290;&#39564;&#35777;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#30001;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#65288;905k-1.7M&#65289;&#12289;&#36755;&#20837;&#32500;&#24230;&#65288;2.7k-12k&#65289;&#21644;&#21306;&#22495;&#25968;&#37327;&#65288;43&#65289;&#20197;&#21450;&#31070;&#32463;&#20803;&#30340;&#20107;&#23454;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signs play a critical role in road safety and traffic management for autonomous driving systems. Accurate traffic sign classification is essential but challenging due to real-world complexities like adversarial examples and occlusions. To address these issues, binary neural networks offer promise in constructing classifiers suitable for resource-constrained devices.  In our previous work, we proposed high-accuracy BNN models for traffic sign recognition, focusing on compact size for limited computation and energy resources. To evaluate their local robustness, this paper introduces a set of benchmark problems featuring layers that challenge state-of-the-art verification tools. These layers include binarized convolutions, max pooling, batch normalization, fully connected. The difficulty of the verification problem is given by the high number of network parameters (905k - 1.7 M), of the input dimension (2.7k-12k), and of the number of regions (43) as well by the fact that the neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03032</link><description>&lt;p&gt;
&#22270;&#22686;&#24378;&#20248;&#21270;&#22120;&#29992;&#20110;&#32467;&#26500;&#24863;&#30693;&#25512;&#33616;&#23884;&#20837;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#34394;&#25311;&#34920;&#31034;&#65292;&#24182;&#19988;&#26159;&#21518;&#32493;&#20915;&#31574;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#26356;&#26032;&#26426;&#21046;&#65292;&#31216;&#20026;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#65292;&#20197;&#40723;&#21169;&#30456;&#20851;&#33410;&#28857;&#22312;&#27599;&#19968;&#27493;&#20013;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#19982;&#36890;&#24120;&#20316;&#20026;&#20013;&#38388;&#37096;&#20998;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;SEvo&#33021;&#22815;&#30452;&#25509;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#21487;&#24573;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;SEvo&#30340;&#25910;&#25947;&#24615;&#36136;&#21450;&#20854;&#21487;&#33021;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20197;&#35777;&#26126;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;SEvo&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30697;&#20272;&#35745;&#26657;&#27491;&#30340;SEvo&#22686;&#24378;AdamW&#20013;&#65292;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#25928;&#26524;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26377;&#25928;&#25512;&#33616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03030</link><description>&lt;p&gt;
GPT-MolBERTa&#65306;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;GPT&#20998;&#23376;&#29305;&#24449;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03030
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Transformer&#26550;&#26500;&#30340;&#20986;&#29616;&#21450;&#20854;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26032;&#39046;&#22495;&#24050;&#32463;&#24320;&#21551;&#12290;&#23613;&#31649;SMILES&#26159;&#26368;&#24120;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20581;&#22766;&#24615;&#12289;&#20016;&#23500;&#20449;&#24687;&#21644;&#35268;&#33539;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25104;&#20026;&#21487;&#25512;&#24191;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-MolBERTa&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#39044;&#27979;&#20854;&#24615;&#36136;&#12290;&#20351;&#29992;ChatGPT&#25910;&#38598;&#20102;326000&#20010;&#20998;&#23376;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25551;&#36848;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;LLM&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32454;&#35843;&#38454;&#27573;&#20351;&#29992;&#20102;BERT&#21644;RoBERTa&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-MolBERTa&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
&lt;/p&gt;</description></item><item><title>SAF&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28201;&#24230;&#36229;&#21442;&#25968;&#23545;&#21407;&#23376;&#21152;&#26435;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#25581;&#31034;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;</title><link>http://arxiv.org/abs/2310.03028</link><description>&lt;p&gt;
SAF: &#26234;&#33021;&#32858;&#21512;&#26694;&#26550;&#65292;&#25581;&#31034;&#21407;&#23376;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#39044;&#27979;&#29575;
&lt;/p&gt;
&lt;p&gt;
SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery. (arXiv:2310.03028v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03028
&lt;/p&gt;
&lt;p&gt;
SAF&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28201;&#24230;&#36229;&#21442;&#25968;&#23545;&#21407;&#23376;&#21152;&#26435;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#25581;&#31034;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#34920;&#31034;&#23398;&#20064;&#65292;&#26377;&#28508;&#21147;&#36890;&#36807;&#22312;&#30789;&#22522;&#20013;&#31579;&#36873;&#22823;&#37327;&#21270;&#23398;&#31354;&#38388;&#26469;&#20419;&#36827;&#33647;&#29289;&#21457;&#29616;&#12290;&#34920;&#31034;&#20998;&#23376;&#30340;&#19968;&#31181;&#25104;&#21151;&#26041;&#27861;&#26159;&#23558;&#20854;&#35270;&#20026;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#21407;&#23376;&#26469;&#34920;&#31034;&#21270;&#21512;&#29289;&#65292;&#36825;&#38656;&#35201;&#23545;&#21407;&#23376;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#12290;&#24120;&#35265;&#30340;&#32858;&#21512;&#25805;&#20316;&#65292;&#22914;&#21462;&#24179;&#22343;&#65292;&#20250;&#23548;&#33268;&#22312;&#21407;&#23376;&#32423;&#21035;&#19978;&#20002;&#22833;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#21407;&#23376;&#37117;&#20351;&#29992;&#31867;&#20284;&#20110;&#28201;&#24230;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#38750;&#32447;&#24615;&#21152;&#26435;&#65292;&#37319;&#29992;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#36825;&#31181;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#40644;&#37329;&#26631;&#20934;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25239;&#29983;&#32032;&#27963;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#21464;&#28201;&#24230;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#22320;&#25581;&#31034;&#23545;&#27963;&#24615;&#39044;&#27979;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, and representation learning in particular, has the potential to facilitate drug discovery by screening a large chemical space in silico. A successful approach for representing molecules is to treat them as a graph and utilize graph neural networks. One of the key limitations of such methods is the necessity to represent compounds with different numbers of atoms, which requires aggregating the atom's information. Common aggregation operators, such as averaging, result in loss of information at the atom level. In this work, we propose a novel aggregating approach where each atom is weighted non-linearly using the Boltzmann distribution with a hyperparameter analogous to temperature. We show that using this weighted aggregation improves the ability of the gold standard message-passing neural network to predict antibiotic activity. Moreover, by changing the temperature hyperparameter, our approach can reveal the atoms that are important for activity prediction in a smooth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYNFUSION&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#34701;&#21512;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03027</link><description>&lt;p&gt;
&#22270;&#21644;Transformer&#29305;&#24449;&#30340;&#21327;&#21516;&#34701;&#21512;&#21487;&#22686;&#24378;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction. (arXiv:2310.03027v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYNFUSION&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#34701;&#21512;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26159;&#35745;&#31639;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Transformer&#30340;&#36827;&#23637;&#34920;&#26126;&#23427;&#20204;&#26159;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#30340;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;Transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#26126;&#30830;&#32771;&#34385;&#24213;&#23618;&#20998;&#23376;&#32467;&#26500;&#26041;&#38754;&#19981;&#36275;&#65292;&#32780;&#20165;&#20381;&#38752;GNN&#29305;&#24449;&#34920;&#31034;&#19981;&#36275;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#38544;&#34255;&#30340;&#20998;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#30456;&#20284;&#20998;&#23376;&#20043;&#38388;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SYNFUSION&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21327;&#21516;&#32452;&#21512;&#20102;&#26469;&#33258;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#20840;&#23616;&#20998;&#23376;&#32467;&#26500;&#21644;&#21508;&#20010;&#21407;&#23376;&#30340;&#29305;&#24615;&#12290;&#22312;MoleculeNet&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SYNFUSION&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;5&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;6&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is a critical task in computational drug discovery. While recent advances in Graph Neural Networks (GNNs) and Transformers have shown to be effective and promising, they face the following limitations: Transformer self-attention does not explicitly consider the underlying molecule structure while GNN feature representation alone is not sufficient to capture granular and hidden interactions and characteristics that distinguish similar molecules. To address these limitations, we propose SYNFUSION, a novel approach that synergistically combines pre-trained features from GNNs and Transformers. This approach provides a comprehensive molecular representation, capturing both the global molecule structure and the individual atom characteristics. Experimental results on MoleculeNet benchmarks demonstrate superior performance, surpassing previous models in 5 out of 7 classification datasets and 4 out of 6 regression datasets. The performance of SYN-FUSION has been
&lt;/p&gt;</description></item><item><title>IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02995</link><description>&lt;p&gt;
IBCL&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#29992;&#20110;&#20219;&#21153;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning. (arXiv:2310.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02995
&lt;/p&gt;
&lt;p&gt;
IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36830;&#32493;&#23398;&#20064;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20808;&#21069;&#30340;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#23384;&#22312;&#22810;&#20010;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#35299;&#20915;&#20102;&#19981;&#21516;&#30340;&#20219;&#21153;&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#35752;&#35770;&#20102;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#19982;&#20559;&#22909;&#25968;&#37327;&#25104;&#27604;&#20363;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#29978;&#33267;&#26159;&#26080;&#38480;&#22810;&#20010;&#20559;&#22909;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#36127;&#25285;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Imprecise Bayesian Continual Learning (IBCL)&#12290;&#22312;&#26032;&#20219;&#21153;&#20986;&#29616;&#26102;&#65292;IBCL(1)&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#26356;&#26032;&#30693;&#35782;&#24211;&#65292;(2)&#33719;&#24471;&#20102;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#20219;&#21153;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#20174;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#32957;&#30340;&#21028;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02964</link><description>&lt;p&gt;
&#21512;&#27169;&#24335;&#21270;&#32957;&#30340;&#39034;&#24207;&#21644;&#22270;&#24418;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Co-modeling the Sequential and Graphical Route for Peptide. (arXiv:2310.02964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#20174;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#32957;&#30340;&#21028;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32957;&#26159;&#30001;&#22810;&#20010;&#27688;&#22522;&#37240;&#30340;&#33073;&#27700;&#32553;&#21512;&#24418;&#25104;&#30340;&#12290;&#32957;&#30340;&#20027;&#35201;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#20026;&#27688;&#22522;&#37240;&#24207;&#21015;&#25110;&#30001;&#21407;&#23376;&#21644;&#21270;&#23398;&#38190;&#32452;&#25104;&#30340;&#20998;&#23376;&#22270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#39034;&#24207;&#21644;&#22270;&#24418;&#32957;&#24418;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#36335;&#24452;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21516;&#19968;&#31181;&#32957;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#26041;&#24335;&#19981;&#21516;&#12290;&#23558;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#35270;&#20026;&#20174;&#19981;&#21516;&#35282;&#24230;&#36827;&#34892;&#25512;&#29702;&#30340;&#20004;&#20010;&#19987;&#23478;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#34701;&#21512;&#19987;&#23478;&#30693;&#35782;&#65292;&#20016;&#23500;&#23398;&#21040;&#30340;&#34920;&#31034;&#20197;&#25552;&#39640;&#21028;&#21035;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;RepCon&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#22686;&#24378;&#20174;&#35299;&#32806;&#30340;&#39034;&#24207;&#21644;&#22270;&#24418;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peptides are formed by the dehydration condensation of multiple amino acids. The primary structure of a peptide can be represented either as an amino acid sequence or as a molecular graph consisting of atoms and chemical bonds. Previous studies have indicated that deep learning routes specific to sequential and graphical peptide forms exhibit comparable performance on downstream tasks. Despite the fact that these models learn representations of the same modality of peptides, we find that they explain their predictions differently. Considering sequential and graphical models as two experts making inferences from different perspectives, we work on fusing expert knowledge to enrich the learned representations for improving the discriminative performance. To achieve this, we propose a peptide co-modeling method, RepCon, which employs a contrastive learning-based framework to enhance the mutual information of representations from decoupled sequential and graphical end-to-end models. It cons
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02676</link><description>&lt;p&gt;
PostRainBench: &#19968;&#31181;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#21644;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02676
&lt;/p&gt;
&lt;p&gt;
PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#31185;&#23398;&#21644;&#31038;&#20250;&#37325;&#35201;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#27169;&#25311;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26377;&#38480;&#65292;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;&#22256;&#38590;&#12290;&#23558;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20043;&#21069;&#36827;&#34892;&#36807;&#21518;&#22788;&#29702;&#30340;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#20301;&#32622;&#30340;&#38477;&#27700;&#25968;&#25454;&#22833;&#34913;&#21644;&#22810;&#20010;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20934;&#30830;&#39044;&#27979;&#22823;&#38632;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PostRainBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#21464;&#37327;NWP&#21518;&#22788;&#29702;&#22522;&#20934;&#65292;&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;NWP&#21518;&#22788;&#29702;&#38477;&#27700;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28192;&#36947;&#27880;&#24847;&#21147;&#27169;&#22411;CAMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#23454;&#29616;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.02671</link><description>&lt;p&gt;
&#36229;&#36234;&#31283;&#23450;&#24615;&#65306;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods. (arXiv:2310.02671v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#23454;&#29616;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26159;&#19968;&#31181;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#36825;&#20123;&#38382;&#39064;&#19982;&#26368;&#20248;&#20572;&#27490;&#25110;&#29305;&#23450;&#20379;&#24212;&#38142;&#38382;&#39064;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#30456;&#20851;&#12290;&#19982;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;MDP&#19981;&#21516;&#65292;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#26159;&#31283;&#23450;&#30340;&#65292;&#31574;&#30053;&#24517;&#39035;&#22312;&#27599;&#20010;&#26102;&#26399;&#21333;&#29420;&#36827;&#34892;&#23398;&#20064;&#12290;&#23454;&#38469;&#19978;&#65292;&#24448;&#24448;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#65292;&#24573;&#35270;&#20102;&#21160;&#24577;&#35268;&#21010;&#25152;&#26263;&#31034;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#21644;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#65292;&#20854;&#20013;&#21442;&#25968;&#22312;&#26102;&#38388;&#19978;&#20197;&#21453;&#21521;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#23545;&#20110;&#34920;&#26684;Softmax&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#23545;&#21516;&#26102;&#21644;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#22312;&#31934;&#30830;&#26799;&#24230;&#21644;&#37319;&#26679;&#26799;&#24230;&#35774;&#32622;&#19979;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#19988;&#27809;&#26377;&#24341;&#20837;&#27491;&#21017;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite-time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamic policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02156</link><description>&lt;p&gt;
&#27010;&#29575;&#37325;&#36830;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02156
&lt;/p&gt;
&lt;p&gt;
PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20316;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#36755;&#20837;&#30340;&#24378;&#22823;&#24037;&#20855;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#22270;&#32467;&#26500;&#19978;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#32570;&#22833;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#23616;&#37096;&#32858;&#21512;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#36807;&#24230;&#21387;&#32553;&#21644;&#22312;&#25429;&#25417;&#30456;&#20851;&#22270;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25512;&#26029;&#19982;&#32473;&#23450;&#39044;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#22270;&#32467;&#26500;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#31934;&#30830;&#21644;&#21487;&#24494;&#20998;&#30340;k-&#23376;&#38598;&#37319;&#26679;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#27010;&#29575;&#37325;&#36830;&#30340;MPNN (PR-MPNN)&#65292;&#23427;&#20204;&#23398;&#20064;&#22312;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#30340;&#21516;&#26102;&#28155;&#21152;&#30456;&#20851;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39318;&#27425;&#25506;&#32034;&#20102;PR-MPNN&#22914;&#20309;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#20999;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
&lt;/p&gt;</description></item><item><title>DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;</title><link>http://arxiv.org/abs/2310.02027</link><description>&lt;p&gt;
DeepHGCN&#65306;&#26397;&#30528;&#26356;&#28145;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02027
&lt;/p&gt;
&lt;p&gt;
DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HGCN&#65289;&#22312;&#25552;&#21462;&#20998;&#23618;&#22270;&#20449;&#24687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#21452;&#26354;&#25805;&#20316;&#21644;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;HGCN&#21463;&#38480;&#20110;&#27973;&#23618;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;GCNs&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20294;&#26159;&#24320;&#21457;&#21452;&#26354;&#27835;&#30103;&#26041;&#27861;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25805;&#20316;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#20197;&#36866;&#24212;&#21452;&#26354;&#24615;&#36136;&#12290;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepHGCN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22823;&#22823;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#25928;&#26524;&#30340;&#28145;&#23618;&#22810;&#23618;HGCN&#26550;&#26500;&#12290;DeepHGCN&#20855;&#26377;&#20004;&#20010;&#28145;&#23618;HGCN&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#32447;&#24615;&#26144;&#23556;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26377;&#25928;&#30340;&#21452;&#26354;&#27531;&#24046;&#36830;&#25509;&#21644;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20419;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.01690</link><description>&lt;p&gt;
&#20351;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#65292;&#39123;&#39118;&#21464;&#24471;&#26356;&#21152;&#24378;&#28872;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#27861;&#27604;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#33719;&#21462;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#25104;&#20687;&#12289;&#36965;&#24863;&#21644;&#22823;&#27668;&#25968;&#25454;&#65292;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#36827;&#34892;&#39123;&#39118;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20845;&#20010;&#20027;&#35201;&#30406;&#22320;&#30340;51&#20010;&#39123;&#39118;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32423;&#32852;&#27169;&#22411;&#30340;&#26368;&#32456;&#39044;&#27979;&#22312;36&#23567;&#26102;&#20869;&#26174;&#31034;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25152;&#26377;&#19977;&#39033;&#20219;&#21153;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#21644;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#30340;&#20540;&#37117;&#36229;&#36807;&#20102;0.5&#21644;20dB&#12290;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#25193;&#25955;&#27169;&#22411;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#39640;&#24615;&#33021;&#38656;&#27714;&#65288;&#22914;&#39123;&#39118;&#39044;&#27979;&#65289;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#35745;&#31639;&#32463;&#27982;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#36947;&#36335;&#21943;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24694;&#21155;&#22825;&#27668;&#26816;&#27979;&#32593;&#32476;&#36807;&#28388;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20013;&#30340;&#21943;&#38654;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#38647;&#36798;&#30446;&#26631;&#36807;&#28388;&#34394;&#20551;&#26816;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#29615;&#22659;&#24863;&#30693;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00944</link><description>&lt;p&gt;
&#22312;&#22810;&#38632;&#26465;&#20214;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Robust 3D Object Detection In Rainy Conditions. (arXiv:2310.00944v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#36947;&#36335;&#21943;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24694;&#21155;&#22825;&#27668;&#26816;&#27979;&#32593;&#32476;&#36807;&#28388;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20013;&#30340;&#21943;&#38654;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#38647;&#36798;&#30446;&#26631;&#36807;&#28388;&#34394;&#20551;&#26816;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#29615;&#22659;&#24863;&#30693;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#34987;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#20934;&#30830;&#24863;&#30693;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#21040;&#38634;&#12289;&#38654;&#21644;&#38632;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#26085;&#24120;&#29616;&#35937;&#20250;&#24341;&#20837;&#19981;&#24517;&#35201;&#30340;&#22122;&#38899;&#21040;&#27979;&#37327;&#20013;&#65292;&#20005;&#37325;&#38477;&#20302;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#25552;&#39640;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#23545;&#36947;&#36335;&#21943;&#38654;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#20808;&#36827;&#30340;&#24694;&#21155;&#22825;&#27668;&#26816;&#27979;&#32593;&#32476;&#26469;&#36807;&#28388;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20013;&#30340;&#21943;&#38654;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#23545;&#22330;&#26223;&#20013;&#30340;&#24694;&#21155;&#22825;&#27668;&#24433;&#21709;&#36739;&#23567;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#24863;&#30693;&#29615;&#22659;&#12290;&#38500;&#20102;&#24694;&#21155;&#22825;&#27668;&#36807;&#28388;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#38647;&#36798;&#30446;&#26631;&#36827;&#19968;&#27493;&#36807;&#28388;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#23545;&#36947;&#36335;&#21943;&#38654;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR sensors are used in autonomous driving applications to accurately perceive the environment. However, they are affected by adverse weather conditions such as snow, fog, and rain. These everyday phenomena introduce unwanted noise into the measurements, severely degrading the performance of LiDAR-based perception systems. In this work, we propose a framework for improving the robustness of LiDAR-based 3D object detectors against road spray. Our approach uses a state-of-the-art adverse weather detection network to filter out spray from the LiDAR point cloud, which is then used as input for the object detector. In this way, the detected objects are less affected by the adverse weather in the scene, resulting in a more accurate perception of the environment. In addition to adverse weather filtering, we explore the use of radar targets to further filter false positive detections. Tests on real-world data show that our approach improves the robustness to road spray of several popular 3D 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECG-SL&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21010;&#20998;&#24515;&#36339;&#29255;&#27573;&#24182;&#25552;&#21462;&#32467;&#26500;&#29305;&#24449;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;ECG&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00818</link><description>&lt;p&gt;
ECG-SL: &#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;ECG-Segment&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal. (arXiv:2310.00818v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECG-SL&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21010;&#20998;&#24515;&#36339;&#29255;&#27573;&#24182;&#25552;&#21462;&#32467;&#26500;&#29305;&#24449;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;ECG&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#26159;&#30417;&#27979;&#20154;&#31867;&#24515;&#33039;&#27963;&#21160;&#30340;&#37325;&#35201;&#20449;&#21495;&#12290;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;ECG&#24515;&#36339;&#27874;&#24418;&#30340;&#21608;&#26399;&#24615;&#21644;&#24418;&#24577;&#23646;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ECG-Segment&#30340;&#23398;&#20064;(ECG-SL)&#26694;&#26550;&#65292;&#20197;&#26126;&#30830;&#22320;&#24314;&#27169;ECG&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ECG&#20449;&#21495;&#39318;&#20808;&#34987;&#21010;&#20998;&#20026;&#24515;&#36339;&#29255;&#27573;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#29255;&#27573;&#20013;&#25552;&#21462;&#32467;&#26500;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26500;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#27169;&#22411;&#26469;&#23398;&#20064;&#19981;&#21516;&#20020;&#24202;&#20219;&#21153;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;ECG&#20449;&#21495;&#20294;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is an essential signal in monitoring human heart activities. Researchers have achieved promising results in leveraging ECGs in clinical applications with deep learning models. However, the mainstream deep learning approaches usually neglect the periodic and formative attribute of the ECG heartbeat waveform. In this work, we propose a novel ECG-Segment based Learning (ECG-SL) framework to explicitly model the periodic nature of ECG signals. More specifically, ECG signals are first split into heartbeat segments, and then structural features are extracted from each of the segments. Based on the structural features, a temporal model is designed to learn the temporal information for various clinical tasks. Further, due to the fact that massive ECG signals are available but the labeled data are very limited, we also explore self-supervised learning strategy to pre-train the models, resulting significant improvement for downstream tasks. The proposed method outperforms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Resource-aware Federated Foundation Models (RaFFM)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#30340;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#26469;&#35299;&#20915;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#22522;&#20110;Transformer&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#32593;&#32476;&#36793;&#32536;&#26681;&#25454;&#24322;&#26500;&#36164;&#28304;&#32422;&#26463;&#36827;&#34892;&#21160;&#24577;&#32553;&#25918;&#12290;</title><link>http://arxiv.org/abs/2310.00247</link><description>&lt;p&gt;
&#26550;&#25509;&#22522;&#30784;&#27169;&#22411;&#19982;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning. (arXiv:2310.00247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Resource-aware Federated Foundation Models (RaFFM)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#30340;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#26469;&#35299;&#20915;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#22522;&#20110;Transformer&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#32593;&#32476;&#36793;&#32536;&#26681;&#25454;&#24322;&#26500;&#36164;&#28304;&#32422;&#26463;&#36827;&#34892;&#21160;&#24577;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#19978;&#20248;&#21270;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;FL&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#22823;&#23567;&#21644;&#23545;&#36164;&#28304;&#30340;&#38656;&#27714;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#36793;&#32536;FL&#31995;&#32479;&#30340;&#36164;&#28304;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;Resource-aware Federated Foundation Models&#65288;RaFFM&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;RaFFM&#24341;&#20837;&#20102;&#38024;&#23545;FL&#22330;&#26223;&#30340;&#19987;&#38376;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#65292;&#22914;&#26174;&#33879;&#21442;&#25968;&#20248;&#20808;&#32423;&#21644;&#39640;&#24615;&#33021;&#23376;&#32593;&#32476;&#25552;&#21462;&#12290;&#36825;&#20123;&#31639;&#27861;&#20351;&#24471;&#22522;&#20110;Transformer&#30340;&#32473;&#23450;FMs&#21487;&#20197;&#22312;&#32593;&#32476;&#36793;&#32536;&#26681;&#25454;&#24322;&#26500;&#36164;&#28304;&#32422;&#26463;&#36827;&#34892;&#21160;&#24577;&#32553;&#25918;&#65292;&#26080;&#35770;&#26159;&#22312;FL&#30340;&#20248;&#21270;&#36824;&#26159;&#37096;&#32626;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) offers privacy-preserving decentralized machine learning, optimizing models at edge clients without sharing private data. Simultaneously, foundation models (FMs) have gained traction in the artificial intelligence (AI) community due to their exceptional performance across various tasks. However, integrating FMs into FL presents challenges, primarily due to their substantial size and intensive resource requirements. This is especially true when considering the resource heterogeneity in edge FL systems. We present an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to address these challenges. RaFFM introduces specialized model compression algorithms tailored for FL scenarios, such as salient parameter prioritization and high-performance subnetwork extraction. These algorithms enable dynamic scaling of given transformer-based FMs to fit heterogeneous resource constraints at the network edge during both FL's optimization and deployment stag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00035</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;LoRA&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#34920;&#29616;&#20026;&#36807;&#20110;&#33258;&#20449;&#12289;&#26657;&#20934;&#19981;&#20339;&#20197;&#21450;&#23545;&#27979;&#35797;&#25968;&#25454;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#20123;&#20302;&#31209;&#36866;&#37197;&#22120;&#34920;&#31034;&#30340;&#21442;&#25968;&#25968;&#37327;&#38750;&#24120;&#23567;&#65292;&#27604;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17357</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#31227;&#21160;&#26041;&#26696;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#21463;&#38480;&#35774;&#22791;&#29615;&#22659;&#20013;&#65292;&#36138;&#23146;&#30340;&#36880;&#23618;&#25110;&#36880;&#27169;&#22359;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32469;&#36807;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#27492;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20572;&#28382;&#38382;&#39064;&#65292;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#26356;&#28145;&#23618;&#22312;&#19968;&#23450;&#28145;&#24230;&#21518;&#20572;&#27490;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19982;&#20998;&#24067;&#31354;&#38388;&#20013;&#26799;&#24230;&#27969;&#30340;&#26368;&#23567;&#21270;&#31227;&#21160;&#26041;&#27861;&#30456;&#21551;&#21457;&#30340;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;TRGL&#65288;Transport Regularized Greedy Learning&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20250;&#23548;&#33268;&#27169;&#22359;&#21270;&#36138;&#23146;&#26041;&#27861;&#26159;&#35268;&#21017;&#30340;&#65292;&#24182;&#36880;&#27493;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20043;&#21518;&#65292;&#21508;&#31181;&#26550;&#26500;&#65288;&#22914;ResNets&#65292;Transformers&#21644;VGG&#65289;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20854;&#20248;&#20110;&#20854;&#20182;&#27169;&#22359;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#29978;&#33267;&#32463;&#24120;&#20248;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#39640;&#36798;60%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.17348</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25191;&#34892;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;ANNs&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#25913;&#21464;&#36755;&#20837;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;ANNs&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23545;&#25239;&#35757;&#32451;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#28155;&#21152;&#20102;&#26679;&#26412;&#29992;&#20110;&#23545;&#25239;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#22686;&#21152;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26159;&#38750;&#24120;&#35745;&#31639;&#28040;&#32791;&#39640;&#30340;&#12290;&#19982;ANNs&#19981;&#21516;&#65292;&#20154;&#31867;&#19981;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;BP&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;BP&#21644;&#8220;Error to Pertu"&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.15325</link><description>&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31185;&#23398;&#21457;&#29616;&#21644;&#24037;&#31243;&#35774;&#35745;&#21463;&#38480;&#20110;&#29289;&#29702;&#23454;&#39564;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#30452;&#35273;&#36873;&#25321;&#30340;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#20540;&#27169;&#25311;&#26159;&#29289;&#29702;&#23454;&#39564;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#39046;&#22495;&#26469;&#35828;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;AI&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#20363;&#22914;&#26102;&#31354;&#36807;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#20301;&#32622;&#36827;&#34892;&#22806;&#25512;&#21644;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36827;&#34892;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#12290;&#31070;&#32463;&#36816;&#31639;&#31526;&#21487;&#20197;&#22686;&#24378;&#29978;&#33267;&#26367;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#22120;&#65292;&#20363;&#22914;&#35745;&#31639;&#21147;&#23398;&#27969;&#20307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#21270;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#20248;&#21270;&#20102;&#20256;&#32479;CFD&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.15294</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiple Physics-Informed Neural Network for Biomedical Tube Flows. (arXiv:2309.15294v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#21270;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#20248;&#21270;&#20102;&#20256;&#32479;CFD&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#20960;&#20309;&#30340;&#27969;&#20307;&#21147;&#23398;&#35745;&#31639;&#22312;&#29983;&#29289;&#21307;&#23398;&#34880;&#31649;&#21644;&#27668;&#36947;&#27969;&#20307;&#21147;&#23398;&#35780;&#20272;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24050;&#32463;&#25104;&#20026;&#20256;&#32479;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#19981;&#38169;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;PINN&#23545;&#20110;&#27599;&#31181;&#29305;&#23450;&#27969;&#21160;&#22330;&#26223;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22240;&#27492;&#26080;&#27861;&#35777;&#26126;&#20854;&#20027;&#27969;&#20351;&#29992;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21508;&#31181;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#32463;&#36807;&#21442;&#25968;&#21270;&#21644;&#39044;&#35757;&#32451;&#22312;PINN&#19978;&#65292;&#20174;&#32780;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#29702;&#24819;&#21270;&#30340;2D&#29421;&#31364;&#31649;&#36947;&#27969;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26469;&#30830;&#23450;&#32593;&#32476;&#26550;&#26500;&#12289;&#31649;&#36947;&#29305;&#23450;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluid dynamics computations for tube-like geometries are important for biomedical evaluation of vascular and airway fluid dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged as a good alternative to traditional computational fluid dynamics (CFD) methods. The vanilla PINN, however, requires much longer training time than the traditional CFD methods for each specific flow scenario and thus does not justify its mainstream use. Here, we explore the use of the multi-case PINN approach for calculating biomedical tube flows, where varied geometry cases are parameterized and pre-trained on the PINN, such that results for unseen geometries can be obtained in real time. Our objective is to identify network architecture, tube-specific, and regularization strategies that can optimize this, via experiments on a series of idealized 2D stenotic tube flows.
&lt;/p&gt;</description></item><item><title>LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.14331</link><description>&lt;p&gt;
LinGCN: &#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21516;&#24577;&#21152;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14331
&lt;/p&gt;
&lt;p&gt;
LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24050;&#32463;&#22312;&#20010;&#20154;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#37096;&#32626;GCN&#24341;&#21457;&#20102;&#23545;&#23458;&#25143;&#25968;&#25454;&#21487;&#33021;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#65292;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#21487;&#20197;&#30830;&#20445;&#25935;&#24863;&#23458;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#24341;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LinGCN&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#24182;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LinGCN&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#23637;&#24320;&#65306;&#65288;1&#65289;&#21487;&#24494;&#30340;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#65292;&#25645;&#37197;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#65292;&#36890;&#36807;&#19982;&#27169;&#22411;&#26435;&#37325;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#20197;&#28385;&#36275;&#20248;&#21270;&#30446;&#26631;&#12290;&#36825;&#31181;&#31574;&#30053;&#20419;&#36827;&#20102;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.14073</link><description>&lt;p&gt;
&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65306;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#31639;&#36825;&#20010;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20083;&#25151;MR&#22270;&#20687;&#30331;&#35760;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#30331;&#35760;&#32593;&#32476;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#20248;&#30340;&#30331;&#35760;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#24494;&#20998;&#21516;&#32986;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.13777</link><description>&lt;p&gt;
&#29992;&#20110;&#20083;&#25151;MRI&#30340;&#19981;&#21516;&#20998;&#36776;&#29575;&#28145;&#24230;&#23398;&#20064;&#21464;&#24418;&#30331;&#35760;
&lt;/p&gt;
&lt;p&gt;
Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI. (arXiv:2309.13777v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20083;&#25151;MR&#22270;&#20687;&#30331;&#35760;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#30331;&#35760;&#32593;&#32476;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#20248;&#30340;&#30331;&#35760;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#24494;&#20998;&#21516;&#32986;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20083;&#25151;&#25163;&#26415;&#35268;&#21010;&#20013;&#65292;&#20934;&#30830;&#22320;&#30331;&#35760;&#19981;&#21516;&#20307;&#20301;&#19979;&#30340;MR&#22270;&#20687;&#26377;&#28508;&#21147;&#25913;&#21892;&#20083;&#33146;&#30284;&#27835;&#30103;&#36807;&#31243;&#20013;&#32959;&#30244;&#30340;&#26412;&#22320;&#21270;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;&#30331;&#35760;&#26041;&#27861;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;&#21307;&#23398;&#24433;&#20687;&#30331;&#35760;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20083;&#25151;MR&#22270;&#20687;&#20013;&#32570;&#20047;&#20016;&#23500;&#30340;&#32441;&#29702;&#20449;&#24687;&#20197;&#21450;&#21464;&#24418;&#38656;&#35201;&#20855;&#26377;&#24494;&#20998;&#21516;&#32986;&#30340;&#22256;&#38590;&#65292;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#22312;&#20083;&#25151;&#22270;&#20687;&#30331;&#35760;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20083;&#25151;MR&#22270;&#20687;&#30331;&#35760;&#30340;&#20855;&#26377;&#24494;&#20998;&#21516;&#32986;&#32422;&#26463;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#30789;&#28608;&#21457;&#21644;&#20307;&#20869;&#23454;&#39564;&#30340;&#26089;&#26399;&#23454;&#39564;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#19968;&#20010;&#30331;&#35760;&#32593;&#32476;&#65292;&#38500;&#20102;&#25552;&#20379;&#24494;&#20998;&#21516;&#32986;&#20445;&#35777;&#22806;&#65292;&#36824;&#33021;&#20135;&#29983;&#26356;&#20248;&#30340;&#20083;&#25151;&#22270;&#20687;&#30331;&#35760;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In breast surgical planning, accurate registration of MR images across patient positions has the potential to improve the localisation of tumours during breast cancer treatment. While learning-based registration methods have recently become the state-of-the-art approach for most medical image registration tasks, these methods have yet to make inroads into breast image registration due to certain difficulties-the lack of rich texture information in breast MR images and the need for the deformations to be diffeomophic. In this work, we propose learning strategies for breast MR image registration that are amenable to diffeomorphic constraints, together with early experimental results from in-silico and in-vivo experiments. One key contribution of this work is a registration network which produces superior registration outcomes for breast images in addition to providing diffeomorphic guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12488</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#31283;&#23450;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#25805;&#20316;&#31526;&#33539;&#25968;&#20250;&#22686;&#38271;&#65292;&#30452;&#21040;&#25509;&#36817;$2/\eta$&#65292;&#20043;&#21518;&#20250;&#22312;&#35813;&#20540;&#21608;&#22260;&#27874;&#21160;&#12290;&#26681;&#25454;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#65292;$2/\eta$&#34987;&#31216;&#20026;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#30830;&#23450;&#20102;&#19968;&#20010;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#65292;SAM&#26159;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;GD&#21464;&#31181;&#12290;&#19982;GD&#19981;&#21516;&#65292;SAM&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;&#36890;&#36807;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#30830;&#23450;&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
&lt;/p&gt;</description></item><item><title>PIE&#26159;&#19968;&#20010;&#26032;&#30340;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#22270;&#20687;&#29305;&#24449;&#26469;&#20934;&#30830;&#27169;&#25311;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#65292;&#24182;&#22312;&#39564;&#35777;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11745</link><description>&lt;p&gt;
PIE: &#36890;&#36807;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11745
&lt;/p&gt;
&lt;p&gt;
PIE&#26159;&#19968;&#20010;&#26032;&#30340;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#22270;&#20687;&#29305;&#24449;&#26469;&#20934;&#30830;&#27169;&#25311;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#65292;&#24182;&#22312;&#39564;&#35777;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20020;&#24202;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#36830;&#32493;&#30340;&#21307;&#23398;&#25104;&#20687;&#30417;&#27979;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#65288;PIE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36924;&#30495;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#38024;&#23545;&#27599;&#20010;&#24739;&#32773;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#20854;&#35270;&#20026;&#20855;&#26377;&#25351;&#25968;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;PIE&#30456;&#23545;&#20110;&#31283;&#23450;&#25193;&#25955;&#28459;&#28216;&#21644;&#22522;&#20110;&#39118;&#26684;&#30340;&#29305;&#24449;&#29983;&#25104;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Mani
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.09476</link><description>&lt;p&gt;
&#26426;&#26800;&#21270;&#29983;&#25104;&#22120;2.0: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#28216;&#25103;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#28216;&#25103;&#35774;&#35745;&#65288;AGD&#65289;&#26159;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#28216;&#25103;&#30740;&#31350;&#30340;&#19968;&#20010;&#38271;&#26399;&#35838;&#39064;&#12290; AGD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20154;&#31867;&#29609;&#23478;&#28216;&#25103;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#26159;&#23458;&#35266;&#20989;&#25968;&#25110;AI&#20195;&#29702;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#37096;&#20998;&#36825;&#20123;&#36817;&#20284;&#22120;&#26159;&#38745;&#24577;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#19981;&#33021;&#21453;&#26144;&#20154;&#31867;&#29609;&#23478;&#22312;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#20013;&#12290;&#25105;&#20204;&#22312;Unity&#20013;&#37325;&#26032;&#21019;&#24314;&#20102;&#32463;&#20856;&#30340;AGD&#29615;&#22659;Mechanic Maker&#20316;&#20026;&#19968;&#20010;&#20840;&#26032;&#30340;&#24320;&#28304;&#29983;&#25104;&#35268;&#21017;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RL&#19982;A*&#20195;&#29702;&#22522;&#32447;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35268;&#21017;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#21644;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25216;&#26415;&#65288;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#65289;&#26469;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#32463;&#36807;&#32508;&#21512;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.09472</link><description>&lt;p&gt;
&#36890;&#36807;&#32423;&#21035;&#20462;&#22797;&#26469;&#37325;&#26500;&#29616;&#26377;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Existing Levels through Level Inpainting. (arXiv:2309.09472v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#21644;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25216;&#26415;&#65288;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#65289;&#26469;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#32463;&#36807;&#32508;&#21512;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#21644;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30340;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCGML&#65289;&#26469;&#29983;&#25104;&#21508;&#31181;&#28216;&#25103;&#20013;&#30340;&#32423;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#32423;&#21035;&#20462;&#22797;&#30340;&#23376;&#38382;&#39064;&#19978;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#21463;&#22270;&#20687;&#20462;&#22797;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#36825;&#19968;&#39046;&#22495;&#20013;&#35843;&#25972;&#20102;&#20004;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#25105;&#20204;&#30340;&#29305;&#23450;&#29992;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65306;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#12290;&#36890;&#36807;&#19968;&#20010;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#32423;&#21035;&#20462;&#22797;&#20219;&#21153;&#25552;&#20379;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#28436;&#31034;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) and Procedural Content Generation via Machine Learning (PCGML) have been used in prior work for generating levels in various games. This paper introduces Content Augmentation and focuses on the subproblem of level inpainting, which involves reconstructing and extending video game levels. Drawing inspiration from image inpainting, we adapt two techniques from this domain to address our specific use case. We present two approaches for level inpainting: an Autoencoder and a U-net. Through a comprehensive case study, we demonstrate their superior performance compared to a baseline method and discuss their relative merits. Furthermore, we provide a practical demonstration of both approaches for the level inpainting task and offer insights into potential directions for future research.
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07056</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#22270;&#20687;&#27169;&#25311;&#65306;&#35299;&#26512;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#23454;&#39564;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#20854;&#36923;&#36753;&#32972;&#21518;&#30340;&#19981;&#36879;&#26126;&#24615;&#32473;&#35299;&#37322;&#20854;&#21457;&#29616;&#30340;&#25361;&#25112;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;inception&#8221;&#25110;&#8220;&#28145;&#24230;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#34987;&#21457;&#26126;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25925;&#20107;&#20174;&#23545;&#37327;&#23376;&#31995;&#32479;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24320;&#22987;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#8220;&#21453;&#36716;&#8221;&#31070;&#32463;&#32593;&#32476;--&#23454;&#38469;&#19978;&#26159;&#35810;&#38382;&#23427;&#22914;&#20309;&#24819;&#35937;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#37327;&#23376;&#31995;&#32479;&#65292;&#20197;&#21450;&#22914;&#20309;&#36830;&#32493;&#20462;&#25913;&#37327;&#23376;&#31995;&#32479;&#20197;&#25913;&#21464;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#21021;&#22987;&#23646;&#24615;&#20998;&#24067;&#65292;&#25105;&#20204;&#21487;&#20197;&#27010;&#24565;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36739;&#27973;&#23618;&#65292;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31616;&#21333;&#30340;&#23646;&#24615;&#65292;&#32780;&#22312;&#36739;&#28145;&#23618;&#27425;&#19978;...&#65288;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05395</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Practical Homomorphic Aggregation for Byzantine ML. (arXiv:2309.05395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21487;&#29992;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27491;&#22312;&#20998;&#24067;&#24335;&#25299;&#25169;&#20013;&#37096;&#32626;&#65292;&#19981;&#21516;&#30340;&#33410;&#28857;&#36890;&#36807;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20132;&#25442;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#26799;&#24230;&#65289;&#26469;&#20849;&#21516;&#35757;&#32451;&#20854;&#20010;&#20307;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#20004;&#31181;&#23041;&#32961;&#12290;&#39318;&#20808;&#65292;&#25308;&#21344;&#24237;&#24335;&#33410;&#28857;&#21487;&#20197;&#36890;&#36807;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#38169;&#35823;&#30340;&#26799;&#24230;&#65289;&#21333;&#29420;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#32531;&#35299;&#27492;&#31867;&#34892;&#20026;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#38750;&#32447;&#24615;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#20405;&#29359;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#26368;&#36817;&#30340;&#25915;&#20987;&#24050;&#32463;&#34920;&#26126;&#65292;&#20132;&#25442;&#65288;&#26410;&#21152;&#23494;&#30340;&#65289;&#26799;&#24230;&#20351;&#24471;&#19968;&#20010;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#33021;&#22815;&#24674;&#22797;&#20986;&#25152;&#26377;&#33410;&#28857;&#30340;&#25968;&#25454;&#12290;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#19968;&#31181;&#37329;&#26631;&#20934;&#23433;&#20840;&#21407;&#35821;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#38750;&#25308;&#21344;&#24237;&#22330;&#26223;&#20013;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. Howev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#22823;&#23494;&#24230;&#27604;&#29575;&#21644;&#38388;&#25509;&#30417;&#30563;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01807</link><description>&lt;p&gt;
&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#29992;&#20110;&#31163;&#32447;&#29615;&#22659;&#19979;&#30340;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#22823;&#23494;&#24230;&#27604;&#29575;&#21644;&#38388;&#25509;&#30417;&#30563;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;RL&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21363;&#20351;&#26159;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31283;&#20581;&#31574;&#30053;&#65292;&#20063;&#38656;&#35201;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#20351;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#65288;MIS&#65289;&#26694;&#26550;&#26469;&#35780;&#20272;&#20219;&#20309;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;MIS&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22823;&#30340;&#23494;&#24230;&#27604;&#29575;&#20559;&#31163;&#21512;&#29702;&#33539;&#22260;&#65292;&#65288;2&#65289;&#38388;&#25509;&#30417;&#30563;&#65292;&#38656;&#35201;&#38388;&#25509;&#25512;&#26029;&#27604;&#29575;&#65292;&#20174;&#32780;&#21152;&#21095;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#22120;&#20013;&#30446;&#26631;&#31574;&#30053;&#30340;&#21344;&#20301;&#21464;&#37327;&#65292;&#24182;&#23558;&#23494;&#24230;&#27604;&#29575;&#23398;&#20064;&#20026;&#20004;&#20010;&#21487;&#20998;&#21035;&#23398;&#20064;&#30340;&#39033;&#30340;&#20056;&#31215;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;US-SFNet&#65292;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#21464;&#30340;&#35786;&#26029;&#12290;&#36890;&#36807;&#20351;&#29992;Conv-FFT&#22359;&#26469;&#24314;&#27169;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16738</link><description>&lt;p&gt;
US-SFNet:&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;US-SFNet&#65292;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#21464;&#30340;&#35786;&#26029;&#12290;&#36890;&#36807;&#20351;&#29992;Conv-FFT&#22359;&#26469;&#24314;&#27169;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#26159;&#35786;&#26029;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#30340;&#35786;&#26029;&#20027;&#35201;&#20381;&#36182;&#20110;&#21307;&#21153;&#20154;&#21592;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#20986;&#29616;&#35823;&#35786;&#12290;&#23613;&#31649;&#36805;&#36895;&#21457;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25913;&#36827;&#21508;&#31181;&#36229;&#22768;&#22270;&#20687;&#30340;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#39048;&#37096;&#28107;&#24052;&#32467;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#35786;&#26029;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;3392&#24352;&#27491;&#24120;&#28107;&#24052;&#32467;&#12289;&#33391;&#24615;&#28107;&#24052;&#32467;&#30149;&#28790;&#12289;&#24694;&#24615;&#21407;&#21457;&#28107;&#24052;&#32467;&#30149;&#28790;&#21644;&#24694;&#24615;&#36716;&#31227;&#28107;&#24052;&#32467;&#30149;&#28790;&#30340;&#22270;&#20687;&#12290;&#37492;&#20110;&#36229;&#22768;&#22270;&#20687;&#26159;&#30001;&#22768;&#27874;&#22312;&#19981;&#21516;&#30340;&#36523;&#20307;&#32452;&#32455;&#20013;&#21453;&#23556;&#21644;&#25955;&#23556;&#20135;&#29983;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Conv-FFT&#22359;&#12290;&#23427;&#23558;&#21367;&#31215;&#25805;&#20316;&#19982;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30456;&#32467;&#21512;&#65292;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph node lesions. However, the diagnoses of these images largely hinge on the expertise of medical practitioners, rendering the process susceptible to misdiagnoses. Although rapidly developing deep learning has substantially improved the diagnoses of diverse ultrasound images, there remains a conspicuous research gap concerning cervical lymph nodes. The objective of our work is to accurately diagnose cervical lymph node lesions by leveraging a deep learning model. To this end, we first collected 3392 images containing normal lymph nodes, benign lymph node lesions, malignant primary lymph node lesions, and malignant metastatic lymph node lesions. Given that ultrasound images are generated by the reflection and scattering of sound waves across varied bodily tissues, we proposed the Conv-FFT Block. It integrates convolutional operations with the fast Fourier transform to more astutely model the images. Building upon thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.16150</link><description>&lt;p&gt;
&#20351;&#29992;&#36974;&#34109;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#24577;&#24490;&#29615;&#36827;&#34892;MRI&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;&#26088;&#22312;&#26816;&#27979;&#19982;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#30340;&#20219;&#20309;&#27169;&#24335;&#19981;&#21516;&#30340;&#27169;&#24335;&#65292;&#36890;&#24120;&#31216;&#20026;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#32780;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#32852;&#30340;&#25163;&#21160;&#20998;&#21106;&#12290;&#30001;&#20110;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#25928;&#65292;&#26816;&#27979;&#24322;&#24120;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#65288;MMCCD&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;MRI&#20013;&#20998;&#21106;&#21508;&#31181;&#27169;&#24335;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#22522;&#26412;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#20316;&#20026;&#21551;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#26426;&#21046;&#12290;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#23398;&#20064;&#32452;&#32455;&#29305;&#24322;&#30340;&#27169;&#24577;&#26144;&#23556;&#65292;&#36825;&#26159;&#32452;&#32455;&#29983;&#29702;&#23398;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#26144;&#23556;&#26080;&#27861;&#23558;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#26410;&#36935;&#21040;&#30340;&#32452;&#32455;&#25110;&#22270;&#20687;&#27169;&#24335;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#65292;&#20351;&#24471;&#24322;&#24120;&#33021;&#22815;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
&lt;/p&gt;</description></item><item><title>BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12439</link><description>&lt;p&gt;
BaDExpert: &#25552;&#21462;&#21518;&#38376;&#21151;&#33021;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12439
&lt;/p&gt;
&lt;p&gt;
BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19978;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#24694;&#24847;&#34892;&#20026;&#65288;&#21518;&#38376;&#65289;&#31192;&#23494;&#22320;&#26893;&#20837;DNN&#20013;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#23646;&#20110;&#21518;&#26399;&#24320;&#21457;&#30340;&#38450;&#24481;&#33539;&#30068;&#65292;&#29420;&#31435;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#24456;&#31616;&#21333; - &#22312;&#19968;&#23567;&#32452;&#26377;&#24847;&#20041;&#30340;&#38169;&#35823;&#26631;&#35760;&#30340;&#24178;&#20928;&#26679;&#26412;&#19978;&#24494;&#35843;&#21518;&#38376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#20854;&#24536;&#35760;&#27491;&#24120;&#21151;&#33021;&#20294;&#20173;&#20445;&#30041;&#21518;&#38376;&#21151;&#33021;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#30340;&#27169;&#22411;&#65288;&#31216;&#20026;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#25552;&#21462;&#30340;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#36807;&#28388;&#25481;&#21518;&#38376;&#36755;&#20837;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25511;&#21046;&#31934;&#24230;&#12289;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#21644;&#32553;&#30701;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#25311;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11546</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#34892;&#30340;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards practical reinforcement learning for tokamak magnetic control. (arXiv:2307.11546v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25511;&#21046;&#31934;&#24230;&#12289;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#21644;&#32553;&#30701;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#25311;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23454;&#26102;&#25511;&#21046;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#31561;&#31163;&#23376;&#20307;&#30913;&#25511;&#21046;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#39304;&#25511;&#21046;&#26041;&#27861;&#30456;&#27604;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#32570;&#28857;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;RL&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#28857;&#65306;&#23454;&#29616;&#26356;&#39640;&#30340;&#23545;&#31561;&#31163;&#23376;&#20307;&#23646;&#24615;&#30340;&#25511;&#21046;&#31934;&#24230;&#65292;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#65292;&#24182;&#20943;&#23569;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;\cite{degrave2022magnetic}&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23545;&#20195;&#29702;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#31639;&#27861;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#25311;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#24418;&#29366;&#31934;&#24230;&#25552;&#39640;&#20102;65&#65285;&#65292;&#23454;&#29616;&#20102;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#22312;&#38271;&#26399;&#20559;&#24046;&#19978;&#30340;&#22823;&#24133;&#20943;&#23569;&#65292;&#24182;&#19988;&#23558;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;3&#20493;&#25110;&#26356;&#22810;&#12290;&#25105;&#20204;&#20351;&#29992;&#21319;&#32423;&#21518;&#30340;RL&#25511;&#21046;&#22120;&#22312;TCV&#25176;&#21345;&#39532;&#20811;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36798;&#21040;&#30340;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65\% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#30340;&#26679;&#26412;&#25286;&#20998;&#26041;&#27861;&#30340;&#22885;&#31192;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.07726</link><description>&lt;p&gt;
&#36808;&#21521;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#65306;&#26679;&#26412;&#25286;&#20998;&#22312;&#36229;&#21442;&#25968;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection. (arXiv:2307.07726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#30340;&#26679;&#26412;&#25286;&#20998;&#26041;&#27861;&#30340;&#22885;&#31192;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#23454;&#36341;&#25104;&#21151;&#26102;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#22914;&#36924;&#36817;&#33021;&#21147;&#12289;&#32479;&#35745;&#24615;&#36136;&#21644;&#27867;&#21270;&#24615;&#33021;&#31561;&#30340;&#30740;&#31350;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#19968;&#31181;&#24120;&#35265;&#23454;&#36341;&#32972;&#21518;&#30340;&#22885;&#31192;&#65306;&#26679;&#26412;&#25286;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#28176;&#36827;&#22320;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#32593;&#32476;&#32467;&#26500;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When artificial neural networks have demonstrated exceptional practical success in a variety of domains, investigations into their theoretical characteristics, such as their approximation power, statistical properties, and generalization performance, have made significant strides. In this paper, we construct a novel theory for understanding the effectiveness of neural networks by discovering the mystery underlying a common practice during neural network model construction: sample splitting. Our theory demonstrates that, the optimal hyperparameters derived from sample splitting can enable a neural network model that asymptotically minimizes the prediction risk. We conduct extensive experiments across different application scenarios and network architectures, and the results manifest our theory's effectiveness.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05435</link><description>&lt;p&gt;
One-Versus-Others Attention: &#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36229;&#36234;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#25968;&#36890;&#24120;&#23569;&#20110;&#22235;&#20010;&#65288;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#39046;&#22495;&#65292;&#25968;&#25454;&#36755;&#20837;&#21487;&#33021;&#21253;&#25324;X&#23556;&#32447;&#12289;PET&#25195;&#25551;&#12289;MRI&#12289;&#36951;&#20256;&#31579;&#26597;&#12289;&#20020;&#24202;&#31508;&#35760;&#31561;&#65292;&#36825;&#23601;&#38656;&#35201;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#20004;&#20004;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20294;&#23545;&#20110;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#23545;&#20110;$n$&#20010;&#27169;&#24577;&#65292;&#35745;&#31639;&#27880;&#24847;&#21147;&#23558;&#23548;&#33268;$n \choose 2$&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#20013;&#31435;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#65292;&#35813;&#26426;&#21046;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04333</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26377;&#21487;&#33021;&#36890;&#36807;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36825;&#20123;&#25915;&#20987;&#24433;&#21709;&#30340;&#31639;&#27861;&#23545;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23545;&#25239;&#38450;&#24481;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38450;&#24481;&#20381;&#36182;&#20110;&#39034;&#24207;&#27169;&#25311;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#24182;&#19988;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22312;&#30001;&#22522;&#20110;&#20998;&#25968;&#20808;&#39564;&#25351;&#23548;&#30340;&#26041;&#21521;&#19978;&#23545;&#21407;&#22987;&#24178;&#20928;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#26469;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
&lt;/p&gt;</description></item><item><title>DISCO-10M&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#36164;&#28304;&#21644;CLAP&#23884;&#20837;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13512</link><description>&lt;p&gt;
DISCO-10M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DISCO-10M: A Large-Scale Music Dataset. (arXiv:2306.13512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13512
&lt;/p&gt;
&lt;p&gt;
DISCO-10M&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#36164;&#28304;&#21644;CLAP&#23884;&#20837;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25968;&#25454;&#38598;&#22312;&#25512;&#21160;&#38899;&#20048;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#38899;&#20048;&#25968;&#25454;&#38598;&#23384;&#22312;&#35268;&#27169;&#26377;&#38480;&#12289;&#21487;&#35775;&#38382;&#24615;&#24046;&#21644;&#32570;&#20047;&#38899;&#39057;&#36164;&#28304;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DISCO-10M&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20026;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36807;&#28388;&#27969;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#21644;&#38899;&#39057;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39044;&#35745;&#31639;&#30340;CLAP&#23884;&#20837;&#21644;DISCO-10M&#65292;&#20415;&#20110;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#23884;&#20837;&#20351;&#24471;&#23545;&#25152;&#25552;&#20379;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#39640;&#25928;&#25506;&#32034;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;DISCO-10M&#65292;&#25512;&#21160;&#38899;&#20048;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#26032;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#27665;&#20027;&#21270;&#21644;&#20419;&#36827;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>PINNacle&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;(PINNs)&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#32422;10&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#21508;&#31181;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.08827</link><description>&lt;p&gt;
PINNacle: &#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs. (arXiv:2306.08827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08827
&lt;/p&gt;
&lt;p&gt;
PINNacle&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;(PINNs)&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#32422;10&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#21508;&#31181;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#19978;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PINNacle&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;PINNacle&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;20&#22810;&#20010;&#19981;&#21516;&#30340;PDEs&#65292;&#21253;&#25324;&#28909;&#20256;&#23548;&#12289;&#27969;&#20307;&#21160;&#21147;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#30005;&#30913;&#23398;&#12290;&#36825;&#20123;PDEs&#21253;&#21547;&#20102;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#22797;&#26434;&#20960;&#20309;&#12289;&#22810;&#23610;&#24230;&#29616;&#35937;&#12289;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#24230;&#12290;PINNacle&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#31665;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;10&#31181;&#26368;&#20808;&#36827;&#30340;PINN&#26041;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20026;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#24615;&#33021;&#30340;&#25163;&#27573;&#65292;PINNacle&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08586</link><description>&lt;p&gt;
FedJETs&#65306;&#20855;&#26377;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#39640;&#25928;&#21450;&#26102;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#21019;&#24314;&#33021;&#22815;&#36866;&#24212;&#27599;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#19978;&#19979;&#25991;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20010;&#24615;&#21270;&#38656;&#35201;&#20351;&#29992;&#23458;&#25143;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#22312;&#26032;&#26469;&#30340;&#23458;&#25143;&#31471;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22312;&#38544;&#31169;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23454;&#29616;&#21450;&#26102;&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedJETs&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;FL&#35774;&#32622;&#20013;&#20351;&#29992;&#8220;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#8221;&#26694;&#26550;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23458;&#25143;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#23376;&#38598;&#19978;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#26368;&#30456;&#20851;&#30340;&#19987;&#23478;&#12290;&#25105;&#20204;&#30340;&#38376;&#25511;&#20989;&#25968;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20849;&#20139;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#20854;&#21363;&#26102;&#30340;&#36335;&#30001;&#20915;&#31574;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;18&#65285;&#65292;&#36798;&#21040;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve just-in-time personalization in these scenarios. We propose FedJETs, a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\% in state of the art F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03364</link><description>&lt;p&gt;
&#22312;&#21333;&#20301;&#29699;&#19978;&#23398;&#20064;&#34920;&#31034;&#65306;&#24212;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21407;&#29702;&#26469;&#23398;&#20064;&#20998;&#24067;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#38024;&#23545;&#23545;&#31216;&#26041;&#21521;&#25968;&#25454;&#24314;&#31435;&#20102; von Mises-Fisher &#20998;&#24067;&#21644;&#35282;&#39640;&#26031;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34987;&#25512;&#21521;&#22266;&#23450;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#12290;&#36825;&#20351;&#24471;&#23427;&#36866;&#21512;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#65292;&#22240;&#27492;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#19981;&#20877;&#21487;&#29992;&#65292;&#24403;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#21482;&#33021;&#30475;&#19968;&#27425;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36127;&#25968;&#25454;&#25110;&#20219;&#21153;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#30340;&#25209;&#22788;&#29702;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#22312;MoDo&#31639;&#27861;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20057</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#65306;&#20248;&#21270;&#12289;&#27867;&#21270;&#21644;&#20914;&#31361;&#36991;&#20813;
&lt;/p&gt;
&lt;p&gt;
Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#22312;MoDo&#31639;&#27861;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#23398;&#20064;&#20934;&#21017;&#25110;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;MOL&#65289;&#38382;&#39064;&#32463;&#24120;&#20986;&#29616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#29992;&#20110;MOL&#65292;&#22914;MGDA&#21450;&#20854;&#21464;&#31181;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#36991;&#20813;&#30446;&#26631;&#20914;&#31361;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#23613;&#31649;&#20854;&#30452;&#35266;&#21560;&#24341;&#20154;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;MGDA&#30340;&#26032;&#38543;&#26426;&#21464;&#20307;-&#22810;&#30446;&#26631;&#26799;&#24230;&#21452;&#37319;&#26679;&#65288;MoDo&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22522;&#20110;&#21160;&#24577;&#21152;&#26435;&#30340;MoDo&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;&#20854;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;MGDA&#32972;&#21518;&#30340;&#20851;&#38190;&#21407;&#29702;-&#27839;&#30528;&#36991;&#20813;&#20914;&#31361;&#30340;&#26041;&#21521;&#36827;&#34892;&#26356;&#26032;-&#21487;&#33021;&#20250;&#38459;&#30861;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#23454;&#29616;${\cal O}(1/\sqrt{n})$&#30340;&#26368;&#20248;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\cal O}(1/\sqrt{n})$ popu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.16102</link><description>&lt;p&gt;
&#25581;&#31034;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#25351;&#30340;&#26159;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#21464;&#24471;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20250;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26159;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#36824;&#23384;&#22312;&#20105;&#35758;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#38750;&#32447;&#24615;&#26102;&#21464;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#38750;&#40784;&#27425;&#30697;&#38453;&#20056;&#31215;&#21644;&#32852;&#21512;&#35889;&#21322;&#24452;&#29702;&#35770;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27969;&#34892;&#35266;&#28857;&#30456;&#21453;&#65292;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#19981;&#33021;&#38450;&#27490;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;&#65292;&#24182;&#19988;&#21576;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#23545;&#31216;GCN&#30340;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#31867;&#21035;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15871</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#32479;&#35745;&#29992;&#20110;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#30340;&#22522;&#20110;&#27169;&#25311;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Statistics for Simulation-based Inference under Model Misspecification. (arXiv:2305.15871v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#26041;&#27861;&#65288;&#22914;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#65292;&#21512;&#25104;&#20284;&#28982;&#24615;&#21644;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;NPE&#65289;&#65289;&#20381;&#36182;&#20110;&#27169;&#25311;&#32479;&#35745;&#37327;&#20197;&#25512;&#26029;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#20250;&#20135;&#29983;&#19981;&#21487;&#20449;&#21644;&#35823;&#23548;&#24615;&#30340;&#25512;&#35770;&#32467;&#26524;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#36328;&#19981;&#21516;&#31867;&#21035;&#30340;SBI&#26041;&#27861;&#30340;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#12290;&#21033;&#29992;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#30830;&#23450;SBI&#20013;&#30340;&#35823;&#24046;&#31243;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#24809;&#32602;&#37027;&#20123;&#22686;&#21152;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#12290;&#20197;NPE&#21644;ABC&#20026;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#24037;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26080;&#32447;&#30005;&#20256;&#25773;&#39046;&#22495;&#30340;&#23454;&#38469;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalises those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.15357</link><description>&lt;p&gt;
&#36890;&#36807;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#22312;&#27599;&#27425;&#37319;&#26679;&#26102;&#24615;&#33021;&#27874;&#21160;&#24456;&#22823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#30340;&#37319;&#26679;&#22120;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#23548;&#33268;&#20854;&#26080;&#25928;&#21644;&#19981;&#31283;&#23450;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#20445;&#35777;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#38543;&#26426;&#24615;&#35270;&#20026;&#19968;&#31181;&#26426;&#36935;&#65306;&#20840;&#38754;&#20998;&#26512;&#21644;&#21033;&#29992;&#23427;&#23548;&#33268;&#20102;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#20351;&#19968;&#31995;&#21015;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#21463;&#30410;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#27714;&#35299;&#25193;&#25955;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;&#25193;&#25955;ODE&#65289;&#21644;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#65288;BC&#65289;&#65292;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#20998;&#26512;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15086</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#23454;&#29616;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20174;&#22122;&#22768;&#29983;&#25104;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#39640;&#26031;&#20808;&#39564;&#20551;&#35774;&#65292;&#23427;&#20204;&#22312;&#38750;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#34203;&#23450;&#35860;&#26725;&#26159;&#19968;&#31181;&#23398;&#20064; SDE &#20197;&#22312;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#34203;&#23450;&#35860;&#26725;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#37197;&#23545;&#36716;&#25442;&#26041;&#38754;&#24182;&#19981;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#65288;UNSB&#65289;&#65292;&#23427;&#23558;&#34203;&#23450;&#35860;&#26725;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#30340; SDE&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; UNSB &#26159;&#21487;&#20280;&#32553;&#30340;&#65292;&#24182;&#19988;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12766</link><description>&lt;p&gt;
&#23558; Emergent In-Context Learning &#35299;&#37322;&#20026;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#32463;&#20856;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#36807;&#31243;&#30456;&#27604;&#65292;&#20026;&#20102;&#23558;LLMs&#29992;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#65292;&#21363;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#25110;&#26356;&#26032;&#29616;&#26377;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#38750;&#24120;&#26377;&#24847;&#24605;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#39044;&#35757;&#32451;LLMs&#22914;&#20309;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#24403;&#38754;&#20020;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;LLMs&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#65292;&#26469;&#30740;&#31350;&#20026;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#26009;&#24211;&#20043;&#21518;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26680;&#22238;&#24402; $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#20462;&#25913;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#35268;&#24459;&#65292;&#21457;&#29616;&#23558;&#25152;&#26377;&#30005;&#32593;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;</title><link>http://arxiv.org/abs/2304.14420</link><description>&lt;p&gt;
&#20351;&#29992;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32593;&#32476;&#32423;&#32852;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Network Cascade Vulnerability using Constrained Bayesian Optimization. (arXiv:2304.14420v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#20462;&#25913;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#35268;&#24459;&#65292;&#21457;&#29616;&#23558;&#25152;&#26377;&#30005;&#32593;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30005;&#32593;&#30340;&#33030;&#24369;&#24615;&#24120;&#24120;&#26159;&#36890;&#36807;&#25932;&#25163;&#33021;&#22815;&#23545;&#32593;&#32476;&#36896;&#25104;&#30340;&#25439;&#23475;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#25915;&#20987;&#30340;&#32423;&#32852;&#24433;&#21709;&#36890;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#32423;&#32852;&#26159;&#22823;&#35268;&#27169;&#20572;&#30005;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20462;&#25913;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#21482;&#35201;&#32593;&#32476;&#24179;&#34913;&#29366;&#24577;&#19981;&#25913;&#21464;&#65292;&#25915;&#20987;&#23601;&#21487;&#20197;&#20445;&#25345;&#19981;&#34987;&#26816;&#27979;&#21040;&#12290;&#36825;&#26500;&#25104;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#40657;&#30418;&#23376;&#20989;&#25968;&#22522;&#30784;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24120;&#35782;&#30456;&#21453;&#65292;&#23558;&#25152;&#26377;&#32593;&#32476;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#25214;&#21040;&#33021;&#22815;&#20135;&#29983;&#19982;&#23454;&#20363;&#30456;&#24403;&#20005;&#37325;&#30340;&#32423;&#32852;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of power grid vulnerability are often assessed by the amount of damage an adversary can exact on the network. However, the cascading impact of such attacks is often overlooked, even though cascades are one of the primary causes of large-scale blackouts. This paper explores modifications of transmission line protection settings as candidates for adversarial attacks, which can remain undetectable as long as the network equilibrium state remains unaltered. This forms the basis of a black-box function in a Bayesian optimization procedure, where the objective is to find protection settings that maximize network degradation due to cascading. Extensive experiments reveal that, against conventional wisdom, maximally misconfiguring the protection settings of all network lines does not cause the most cascading. More surprisingly, even when the degree of misconfiguration is resource constrained, it is still possible to find settings that produce cascades comparable in severity to instanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.11004</link><description>&lt;p&gt;
&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Under Ideal Joint Classifier Assumption. (arXiv:2304.11004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20026;&#26356;&#39640;&#25928;&#23567;&#22411;&#32593;&#32476;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#26469;&#25351;&#23548;&#26356;&#23567;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#25552;&#20379;&#30693;&#35782;&#36716;&#31227;&#30340;&#22522;&#30784;&#26426;&#21046;&#23578;&#19981;&#22815;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#30693;&#35782;&#33976;&#39311;&#65288;IJCKD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#25512;&#23548;&#20986;&#30340;&#25968;&#23398;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#23398;&#29983;&#32593;&#32476;&#35823;&#24046;&#30028;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#20854;&#20316;&#20026;&#25945;&#24072;&#30340;&#20989;&#25968;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a powerful technique to compress large neural networks into smaller, more efficient networks. Softmax regression representation learning is a popular approach that uses a pre-trained teacher network to guide the learning of a smaller student network. While several studies explored the effectiveness of softmax regression representation learning, the underlying mechanism that provides knowledge transfer is not well understood. This paper presents Ideal Joint Classifier Knowledge Distillation (IJCKD), a unified framework that provides a clear and comprehensive understanding of the existing knowledge distillation methods and a theoretical foundation for future research. Using mathematical techniques derived from a theory of domain adaptation, we provide a detailed analysis of the student network's error bound as a function of the teacher. Our framework enables efficient knowledge transfer between teacher and student networks and can be applied to various applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21487;&#38752;&#24615;&#26377;&#25152;&#24046;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.08979</link><description>&lt;p&gt;
ChatGPT&#21487;&#38752;&#24615;&#30340;&#27979;&#37327;&#19982;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (arXiv:2304.08979v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21487;&#38752;&#24615;&#26377;&#25152;&#24046;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#65292;&#29992;&#25143;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#27491;&#22312;&#21457;&#29983;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#20256;&#32479;&#30340;&#25628;&#32034;&#24341;&#25806;&#19981;&#21516;&#65292;ChatGPT&#20174;&#27169;&#22411;&#26412;&#36523;&#26816;&#32034;&#30693;&#35782;&#24182;&#20026;&#29992;&#25143;&#29983;&#25104;&#31572;&#26696;&#12290;ChatGPT&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#31572;&#33021;&#21147;&#21560;&#24341;&#20102;&#36229;&#36807;1&#20159;&#29992;&#25143;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#20851;&#20110;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;5695&#20010;&#38382;&#39064;&#36328;&#36234;&#21313;&#20010;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#39046;&#22495;&#65292;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#30340;&#21487;&#38752;&#24615;&#22240;&#19981;&#21516;&#39046;&#22495;&#32780;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;OpenAI&#35774;&#35745;&#30340;&#31995;&#32479;&#35282;&#33394;&#21487;&#20197;&#24433;&#21709;ChatGPT&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;ChatGPT&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#26159;&#21333;&#20010;&#23383;&#31526;&#30340;&#26356;&#25913;&#20063;&#20250;&#23545;&#20854;&#21487;&#38752;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;ChatGPT&#21487;&#38752;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#25968;&#21644;&#20960;&#20309;&#27169;&#22411;&#26469;&#30740;&#31350;&#32593;&#32476;&#31354;&#38388;&#36890;&#20449;&#12290;&#36890;&#36807;&#23450;&#20041;&#26102;&#38388;&#21464;&#21270;&#22270;&#65288;TVG&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#30697;&#38453;&#21644;&#21322;&#29615;&#24615;&#36136;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;TVG&#30340;&#36890;&#20449;&#23481;&#37327;&#30340;&#26032;&#32479;&#35745;&#37327;&#24182;&#20998;&#26512;STARLINK&#21355;&#26143;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36827;&#19968;&#27493;&#20998;&#26512;STARLINK&#30340;&#24378;&#36830;&#25509;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#33021;&#22815;&#27169;&#25311;&#22320;&#29699;&#21644;&#28779;&#26143;&#32593;&#32476;&#22330;&#26223;&#30340;&#21322;&#29615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01150</link><description>&lt;p&gt;
&#20195;&#25968;&#21644;&#20960;&#20309;&#27169;&#22411;&#22312;&#31354;&#38388;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Algebraic and Geometric Models for Space Networking. (arXiv:2304.01150v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#25968;&#21644;&#20960;&#20309;&#27169;&#22411;&#26469;&#30740;&#31350;&#32593;&#32476;&#31354;&#38388;&#36890;&#20449;&#12290;&#36890;&#36807;&#23450;&#20041;&#26102;&#38388;&#21464;&#21270;&#22270;&#65288;TVG&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#30697;&#38453;&#21644;&#21322;&#29615;&#24615;&#36136;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;TVG&#30340;&#36890;&#20449;&#23481;&#37327;&#30340;&#26032;&#32479;&#35745;&#37327;&#24182;&#20998;&#26512;STARLINK&#21355;&#26143;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36827;&#19968;&#27493;&#20998;&#26512;STARLINK&#30340;&#24378;&#36830;&#25509;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#33021;&#22815;&#27169;&#25311;&#22320;&#29699;&#21644;&#28779;&#26143;&#32593;&#32476;&#22330;&#26223;&#30340;&#21322;&#29615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#31354;&#38388;&#36890;&#20449;&#30340;&#26032;&#30340;&#20195;&#25968;&#21644;&#20960;&#20309;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#26102;&#38388;&#21464;&#21270;&#22270;&#65288;TVG&#65289;&#30340;&#26032;&#23450;&#20041;&#65292;&#35813;&#23450;&#20041;&#37319;&#29992;&#23454;&#32447;&#38598;&#21512;P&#65288;R&#65289;&#20013;&#30340;&#30697;&#38453;&#20540;&#36827;&#34892;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;P&#65288;R&#65289;&#30340;&#21322;&#29615;&#24615;&#36136;&#65292;&#20351;&#29992;&#30697;&#38453;&#20056;&#27861;&#21644;&#25130;&#26029;&#30340;&#20811;&#33713;&#23612;&#26143;&#21495;&#26469;&#24314;&#27169;TVG&#20013;&#30340;&#22810;&#36339;&#36890;&#20449;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;TVG&#30340;&#36890;&#20449;&#23481;&#37327;&#30340;&#26032;&#22411;&#32479;&#35745;&#37327;&#65292;&#31216;&#20026;&#23551;&#21629;&#26354;&#32447;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#26679;&#26412;&#30340;&#38543;&#26426;&#36873;&#25321;&#30340;STARLINK&#21355;&#26143;&#36827;&#34892;&#26085;&#38271;&#27169;&#25311;&#26469;&#29983;&#25104;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20123;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#21551;&#21457;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#26469;&#36827;&#19968;&#27493;&#20998;&#26512;&#30830;&#23450;&#22823;&#26679;&#26412;&#30340;STARLINK&#22312;&#26102;&#31354;&#19978;&#30340;&#24378;&#36830;&#25509;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#25311;&#22320;&#29699;&#21644;&#28779;&#26143;&#20043;&#38388;&#30340;&#32593;&#32476;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#22815;&#27169;&#25311;&#20256;&#25773;&#24310;&#36831;&#20197;&#21450;&#24310;&#36831;&#23481;&#24525;&#32593;&#32476;&#20013;&#24120;&#35265;&#21327;&#35758;&#30340;&#21508;&#31181;&#21322;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce some new algebraic and geometric perspectives on networked space communications. Our main contribution is a novel definition of a time-varying graph (TVG), defined in terms of a matrix with values in subsets of the real line P(R). We leverage semi-ring properties of P(R) to model multi-hop communication in a TVG using matrix multiplication and a truncated Kleene star. This leads to novel statistics on the communication capacity of TVGs called lifetime curves, which we generate for large samples of randomly chosen STARLINK satellites, whose connectivity is modeled over day-long simulations. Determining when a large subsample of STARLINK is temporally strongly connected is further analyzed using novel metrics introduced here that are inspired by topological data analysis (TDA). To better model networking scenarios between the Earth and Mars, we introduce various semi-rings capable of modeling propagation delay as well as protocols common to Delay Tolerant Netwo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.00195</link><description>&lt;p&gt;
&#25277;&#35937;&#22120;&#65306;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20851;&#31995;&#23398;&#20064;&#36716;&#21270;&#20026;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;iNaturalist 2021&#19982;ImageNet&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#65292;&#20197;&#21450;&#36873;&#25321;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#26102;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16887</link><description>&lt;p&gt;
&#25506;&#31350;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;iNaturalist 2021&#19982;ImageNet&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#65292;&#20197;&#21450;&#36873;&#25321;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#26102;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#22914;&#20309;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#8220;&#32454;&#21040;&#31895;&#8221;&#30340;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#26631;&#31614;&#27604;&#30446;&#26631;&#38382;&#39064;&#26356;&#32454;&#31890;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;iNaturalist 2021&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#23545;&#20110;&#22522;&#32447;&#38169;&#35823;&#29575;&#26377;8.76&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#19979;&#26465;&#20214;&#23545;&#20110;&#25913;&#36827;&#38750;&#24120;&#20851;&#38190;&#65306;1&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#22823;&#19988;&#26377;&#24847;&#20041;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;2&#65289;&#20854;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#30340;&#21151;&#33021;&#24378;&#28872;&#23545;&#40784;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;3&#65289;&#36873;&#25321;&#20102;&#36866;&#24403;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;ImageNet21k&#19978;&#30340;&#21494;&#26631;&#31614;&#39044;&#35757;&#32451;&#20135;&#29983;&#20102;&#27604;&#20854;&#20182;&#21512;&#20316;&#26631;&#31614;&#26356;&#22909;&#30340;ImageNet1k&#36801;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the "fine-to-coarse" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa
&lt;/p&gt;</description></item><item><title>GOAL&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#30340;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14655</link><description>&lt;p&gt;
GOAL:&#20026;&#23454;&#26102;&#36275;&#29699;&#35299;&#35828;&#29983;&#25104;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation. (arXiv:2303.14655v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14655
&lt;/p&gt;
&lt;p&gt;
GOAL&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#30340;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#29983;&#21160;&#12289;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#65288;&#21363;&#20851;&#20110;&#29305;&#23450;&#39046;&#22495;&#22330;&#26223;&#30340;&#35814;&#32454;&#35780;&#35770;&#65292;&#24182;&#20855;&#26377;&#36866;&#24403;&#30340;&#25512;&#29702;&#65289;&#65292;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GOAL&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#29255;&#27573;&#12289;22k&#20010;&#21477;&#23376;&#21644;42k&#20010;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65288;KGVC&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#24615;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#19968;&#23453;&#36149;&#32780;&#23454;&#29992;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/THU-KEG/goal &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent emergence of video captioning models, how to generate vivid, fine-grained video descriptions based on the background knowledge (i.e., long and informative commentary about the domain-specific scenes with appropriate reasoning) is still far from being solved, which however has great applications such as automatic sports narrative. In this paper, we present GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k knowledge triples for proposing a challenging new task setting as Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental adaption of existing methods to show the difficulty and potential directions for solving this valuable and applicable task. Our data and code are available at https://github.com/THU-KEG/goal.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09874</link><description>&lt;p&gt;
&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#38669;&#21202;&#26031;&#24052;&#27931;&#21644;&#24343;&#38647;&#24503;&#38463;&#29305;&#32435;&#22827;&#25552;&#20986;&#20102;&#24863;&#23448;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#36866;&#24212;&#29615;&#22659;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#26089;&#26399;&#35270;&#35273;&#30340;&#36827;&#21270;&#26159;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20256;&#36882;&#20851;&#20110;&#36755;&#20837;&#20449;&#21495;&#30340;&#20449;&#24687;&#12290;&#25353;&#29031;&#39321;&#20892;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#36890;&#36807;&#33258;&#28982;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#27010;&#29575;&#26469;&#25551;&#36848;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20197;&#21069;&#26080;&#27861;&#30452;&#25509;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#30340;&#27010;&#29575;&#12290;&#23613;&#31649;&#36825;&#31181;&#24819;&#27861;&#30340;&#25506;&#32034;&#26159;&#38388;&#25509;&#30340;&#65292;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#23494;&#24230;&#30340;&#36807;&#24230;&#31616;&#21270;&#27169;&#22411;&#25110;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#37325;&#29616;&#21508;&#31181;&#29983;&#29702;&#21644;&#24515;&#29702;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#30830;&#23450;&#30693;&#35273;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#24847;&#35265;&#30456;&#20851;&#24615;&#24456;&#39640;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#30340;&#20195;&#29702;&#65292;&#20197;&#21450;&#19968;&#20010;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#30452;&#25509;&#20272;&#35745;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;Barlow&#21644;Attneave&#29702;&#35770;&#39044;&#27979;&#30340;&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#30693;&#35273;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#26469;&#35828;&#26126;&#36825;&#19968;&#21457;&#29616;&#65292;&#36825;&#26159;&#36890;&#36807;&#35270;&#35273;&#25628;&#32034;&#23454;&#39564;&#27979;&#37327;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;DMSB&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#28385;&#36275;&#26102;&#38388;&#19978;&#20301;&#32622;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#31995;&#32479;&#30340;&#24179;&#28369;&#24230;&#37327;&#20540;&#26679;&#26465;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#22810;&#36793;&#38469;&#36712;&#36857;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20026;&#35299;&#20915;&#20855;&#26377;&#21508;&#31181;&#31867;&#22411;&#30340;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#36712;&#36857;&#37325;&#24314;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.01751</link><description>&lt;p&gt;
&#28145;&#21160;&#37327;&#22810;&#37325;&#36793;&#38469;Schr\"odinger&#26725;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Momentum Multi-Marginal Schr\"odinger Bridge. (arXiv:2303.01751v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;DMSB&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#28385;&#36275;&#26102;&#38388;&#19978;&#20301;&#32622;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#31995;&#32479;&#30340;&#24179;&#28369;&#24230;&#37327;&#20540;&#26679;&#26465;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#22810;&#36793;&#38469;&#36712;&#36857;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20026;&#35299;&#20915;&#20855;&#26377;&#21508;&#31181;&#31867;&#22411;&#30340;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#36712;&#36857;&#37325;&#24314;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31895;&#30053;&#26102;&#38388;&#38388;&#38548;&#19979;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#20174;&#20998;&#24067;&#20013;&#37325;&#24314;&#20154;&#21475;&#21160;&#24577;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22914;&#27969;&#27169;&#22411;&#25110;Schr\"odinger&#26725;&#27169;&#22411;&#34920;&#29616;&#20986;&#35825;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#25512;&#26029;&#20986;&#30340;&#26679;&#26412;&#36712;&#36857;&#26410;&#33021;&#35299;&#37322;&#28508;&#22312;&#30340;&#38543;&#26426;&#24615;&#65292;&#25110;&#32773;&#26159;DMSB&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#28385;&#36275;&#26102;&#38388;&#19978;&#20301;&#32622;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#31995;&#32479;&#30340;&#24179;&#28369;&#24230;&#37327;&#20540;&#26679;&#26465;&#12290;&#36890;&#36807;&#35843;&#25972;&#33879;&#21517;&#30340;Bregman&#36845;&#20195;&#21644;&#23558;&#27604;&#20363;&#25311;&#21512;&#36845;&#20195;&#25193;&#23637;&#21040;&#30456;&#31354;&#38388;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#39640;&#25928;&#22788;&#29702;&#20102;&#39640;&#32500;&#22810;&#36793;&#38469;&#36712;&#36857;&#25512;&#26029;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#21333;&#32454;&#32990;RNA&#24207;&#21015;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;DMSB&#26694;&#26550;&#20026;&#35299;&#20915;&#20855;&#26377;&#21508;&#31181;&#31867;&#22411;&#30340;&#36793;&#38469;&#32422;&#26463;&#30340;&#38543;&#26426;&#36712;&#36857;&#37325;&#24314;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a crucial challenge to reconstruct population dynamics using unlabeled samples from distributions at coarse time intervals. Recent approaches such as flow-based models or Schr\"odinger Bridge (SB) models have demonstrated appealing performance, yet the inferred sample trajectories either fail to account for the underlying stochasticity or are $\underline{D}$eep $\underline{M}$omentum Multi-Marginal $\underline{S}$chr\"odinger $\underline{B}$ridge(DMSB), a novel computational framework that learns the smooth measure-valued spline for stochastic systems that satisfy position marginal constraints across time. By tailoring the celebrated Bregman Iteration and extending the Iteration Proportional Fitting to phase space, we manage to handle high-dimensional multi-marginal trajectory inference tasks efficiently. Our algorithm outperforms baselines significantly, as evidenced by experiments for synthetic datasets and a real-world single-cell RNA sequence dataset. Additionally, the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;GANs&#26102;&#65292;&#36890;&#36807;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#19968;&#20123;&#20462;&#25913;&#21644;&#20248;&#21270;&#65292;&#22914;&#22686;&#21152;&#37492;&#21035;&#22120;&#30340;&#35757;&#32451;&#27493;&#39588;&#21644;&#20351;&#29992;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#31561;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#32467;&#26524;&#21644;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.02936</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31169;&#26377;GANs
&lt;/p&gt;
&lt;p&gt;
Private GANs, Revisited. (arXiv:2302.02936v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02936
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;GANs&#26102;&#65292;&#36890;&#36807;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#19968;&#20123;&#20462;&#25913;&#21644;&#20248;&#21270;&#65292;&#22914;&#22686;&#21152;&#37492;&#21035;&#22120;&#30340;&#35757;&#32451;&#27493;&#39588;&#21644;&#20351;&#29992;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#31561;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#32467;&#26524;&#21644;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;GANs&#30340;&#32463;&#20856;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23545;&#37492;&#21035;&#22120;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#22312;&#25913;&#36827;&#35757;&#32451;&#21518;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#25913;&#21892;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#29616;&#26377;&#23454;&#26045;&#35813;&#26041;&#27861;&#30340;&#35770;&#36848;&#24573;&#35270;&#20102;&#23545;&#37492;&#21035;&#22120;&#26356;&#26032;&#28155;&#21152;&#22122;&#22768;&#22914;&#20309;&#38459;&#30861;&#37492;&#21035;&#22120;&#35757;&#32451;&#65292;&#30772;&#22351;&#20102;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#25104;&#21151;&#35757;&#32451;GAN&#25152;&#24517;&#38656;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#22797;&#26041;&#27861;&#8212;&#8212;&#22312;&#29983;&#25104;&#22120;&#27493;&#39588;&#20043;&#38388;&#36827;&#34892;&#26356;&#22810;&#30340;&#37492;&#21035;&#22120;&#27493;&#39588;&#8212;&#8212;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#24179;&#31561;&#65292;&#24182;&#25913;&#21892;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24674;&#22797;&#24179;&#31561;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20854;&#20182;&#20462;&#25913;&#8212;&#8212;&#21363;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;&#30340;&#37492;&#21035;&#22120;&#26356;&#26032;&#39057;&#29575;&#8212;&#8212;&#20197;&#25913;&#21892;&#37492;&#21035;&#22120;&#35757;&#32451;&#65292;&#24182;&#22312;&#29983;&#25104;&#36136;&#37327;&#19978;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#22270;&#20687;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the canonical approach for training differentially private GANs -- updating the discriminator with differentially private stochastic gradient descent (DPSGD) -- can yield significantly improved results after modifications to training. Specifically, we propose that existing instantiations of this approach neglect to consider how adding noise only to discriminator updates inhibits discriminator training, disrupting the balance between the generator and discriminator necessary for successful GAN training. We show that a simple fix -- taking more discriminator steps between generator steps -- restores parity between the generator and discriminator and improves results.  Additionally, with the goal of restoring parity, we experiment with other modifications -- namely, large batch sizes and adaptive discriminator update frequency -- to improve discriminator training and see further improvements in generation quality. Our results demonstrate that on standard image synthesis bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#65292;&#21487;&#20197;&#22312;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2302.02787</link><description>&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#20004;&#31181;&#30495;&#23454;&#21010;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative models for two-ground-truth partitions in networks. (arXiv:2302.02787v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#65292;&#21487;&#20197;&#22312;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#22522;&#20934;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#21516;&#26102;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of approaches have been proposed to characterise the mesoscale structure of networks - most often as a partition based on patterns variously called communities, blocks, or clusters. Clearly, distinct methods designed to detect different types of patterns may provide a variety of answers to the network's mesoscale structure. Yet, even multiple runs of a given method can sometimes yield diverse and conflicting results, producing entire landscapes of partitions which potentially include multiple (locally optimal) mesoscale explanations of the network. Such ambiguity motivates a closer look at the ability of these methods to find multiple qualitatively different 'ground truth' partitions in a network. Here, we propose the stochastic cross-block model (SCBM), a generative model which allows for two distinct partitions to be built into the mesoscale structure of a single benchmark network. We demonstrate a use case of the benchmark model by appraising the power of stochastic block m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00942</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#22120;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#29992;&#20110;&#23545;&#32534;&#30721;&#28857;&#20113;&#30340;&#22270;&#24418;&#36827;&#34892;&#39640;&#25928;&#22330;&#31215;&#20998;&#12290;&#31532;&#19968;&#31867;&#31639;&#27861;&#20351;&#29992;&#28857;&#20113;&#32593;&#26684;&#22270;&#30340;&#26377;&#30028;&#20111;&#26684;&#65292;&#31532;&#20108;&#31867;&#31639;&#27861;&#21017;&#20351;&#29992;&#28857;&#20113;&#30340;&#27969;&#34892;&#30340;&#949;-&#26368;&#36817;&#37051;&#22270;&#34920;&#31034;&#26041;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#34987;&#30475;&#20316; Fast Multipole Methods(FMMs) &#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#20294;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25991;&#31456;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#28857;&#20043;&#38388;&#27493;&#38271;&#20998;&#24067;&#65288;&#22914;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#65289;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#23398;&#12290;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25991;&#31456;&#33719;&#24471;&#20102;&#32467;&#26500;&#22270;&#35770;&#30340;&#26032;&#32467;&#26524;&#12290;&#25991;&#31456;&#36824;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#34920;&#38754;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;&#29992;&#20110;&#32593;&#26684;&#21160;&#24577;&#24314;&#27169;&#65289;&#65292;&#28857;&#20113;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20197;&#21450;Gromov-Wasserstein&#21464;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#23545;MeSH&#27010;&#24565;&#36827;&#34892;&#33258;&#21160;&#32454;&#21270;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09350</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32034;&#24341;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature. (arXiv:2301.09350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#23545;MeSH&#27010;&#24565;&#36827;&#34892;&#33258;&#21160;&#32454;&#21270;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#35821;&#20041;&#32034;&#24341;&#36890;&#24120;&#22312;MeSH&#25551;&#36848;&#31526;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#23558;&#20960;&#20010;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#32452;&#21512;&#22312;&#19968;&#36215;&#24182;&#35270;&#20026;&#21333;&#20010;&#20027;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;MeSH&#27010;&#24565;&#30340;&#32423;&#21035;&#19978;&#33258;&#21160;&#32454;&#21270;&#20027;&#39064;&#27880;&#37322;&#12290;&#26041;&#27861;&#65306;&#30001;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#22522;&#20110;&#27010;&#24565;&#20986;&#29616;&#22312;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#24369;&#30417;&#30563;&#65292;&#36825;&#20063;&#36890;&#36807;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#35774;&#35745;&#31574;&#30053;&#20197;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#29305;&#27530;&#25361;&#25112;&#12290;&#26032;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#22238;&#39038;&#24615;&#22330;&#26223;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#22522;&#20110;&#24050;&#32463;&#25552;&#21319;&#20026;&#25551;&#36848;&#31526;&#30340;&#27010;&#24565;&#12290;&#32467;&#26524;&#65306;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#27010;&#24565;&#20986;&#29616;&#26159;&#26368;&#24378;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23439;F1&#20998;&#25968;&#32422;&#20026;0.63&#65292;&#36328;&#22810;&#20010;&#26631;&#31614;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36229;&#36807;4&#20010;&#30334;&#20998;&#28857;&#12290;&#32467;&#35770;&#65306;&#32467;&#26524;&#34920;&#26126;&#65292;&#27010;&#24565;&#20986;&#29616;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32034;&#24341;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Semantic indexing of biomedical literature is usually done at the level of MeSH descriptors with several related but distinct biomedical concepts often grouped together and treated as a single topic. This study proposes a new method for the automated refinement of subject annotations at the level of MeSH concepts. Methods: Lacking labelled data, we rely on weak supervision based on concept occurrence in the abstract of an article, which is also enhanced by dictionary-based heuristics. In addition, we investigate deep learning approaches, making design choices to tackle the particular challenges of this task. The new method is evaluated on a large-scale retrospective scenario, based on concepts that have been promoted to descriptors. Results: In our experiments concept occurrence was the strongest heuristic achieving a macro-F1 score of about 0.63 across several labels. The proposed method improved it further by more than 4pp. Conclusion: The results suggest that concept occu
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05603</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05603
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21457;&#23637;&#65292;&#24182;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#25104;&#20026;&#39318;&#36873;&#12290;&#36825;&#19968;&#36827;&#23637;&#20027;&#35201;&#24402;&#21151;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#35745;&#31639;&#36164;&#28304;&#19982;&#20808;&#36827;&#31639;&#27861;&#30340;&#31995;&#32479;&#21327;&#21516;&#65292;&#29992;&#20197;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#19979;&#26080;&#38480;&#22686;&#38271;&#30340;&#25968;&#25454;&#36880;&#28176;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#32553;&#20943;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#32508;&#21512;&#20986;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#26159;&#21542;&#26126;&#30830;&#27169;&#20223;&#30446;&#26631;&#25968;&#25454;&#30340;&#24615;&#33021;&#23558;&#20854;&#20998;&#31867;&#20026;&#20803;&#23398;&#20064;&#21644;&#25968;&#25454;&#21305;&#37197;&#26694;&#26550;&#12290;&#23613;&#31649;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20915;&#31574;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2212.06921</link><description>&lt;p&gt;
&#26631;&#31614;&#25439;&#22833;&#65306;&#36890;&#36807;&#30452;&#25509;&#25439;&#22833;&#26500;&#24314;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Losses over Labels: Weakly Supervised Learning via Direct Loss Construction. (arXiv:2212.06921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20915;&#31574;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#25104;&#26412;&#36807;&#39640;&#65292;&#32534;&#31243;&#24369;&#30417;&#30563;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26032;&#20852;&#33539; Paradigm&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#25968;&#25454;&#23376;&#38598;&#25552;&#20379;&#22122;&#22768;&#26631;&#31614;&#12290;&#36825;&#20123;&#24369;&#26631;&#31614;&#34987;&#32452;&#21512;&#65288;&#36890;&#24120;&#36890;&#36807;&#22270;&#27169;&#22411;&#65289;&#24418;&#25104;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#19979;&#28216;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20256;&#32479;&#24369;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#30340;&#22522;&#26412;&#21069;&#25552;&#65306;&#26082;&#28982;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#25152;&#26377;&#30340;&#8220;&#26631;&#31614;&#8221;&#20449;&#24687;&#65292;&#20026;&#20160;&#20040;&#36824;&#38656;&#35201;&#29983;&#25104;&#20266;&#26631;&#31614;&#21602;&#65311;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#35758;&#30452;&#25509;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24809;&#32602;&#27169;&#22411;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#21551;&#21457;&#24335;&#26041;&#27861;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#34701;&#20837;&#27604;&#26631;&#20934;&#24369;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#20013;&#20351;&#29992;&#30340;&#26356;&#22810;&#20449;&#24687;&#65292;&#20363;&#22914;&#21551;&#21457;&#24335;&#26041;&#27861;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#65292;&#36825;&#26126;&#30830;&#22320;&#25351;&#23548;&#20102;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#37319;&#29992;&#20102;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#31665;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.06074</link><description>&lt;p&gt;
&#24102;&#26377;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Regression with Label Differential Privacy. (arXiv:2212.06074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#37319;&#29992;&#20102;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#31665;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#26631;&#31614;&#20540;&#30340;&#20840;&#23616;&#20808;&#39564;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#21487;&#20197;&#31169;&#23494;&#22320;&#33719;&#21462;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22312;&#32473;&#23450;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#19979;&#26368;&#20248;&#30340;&#26631;&#31614;DP&#38543;&#26426;&#21270;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#26426;&#21046;&#37319;&#29992;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;&#26368;&#20248;&#31665;&#20540;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a "randomized response on bins", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.02648</link><description>&lt;p&gt;
Spuriosity Rankings: &#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20381;&#36182;&#25152;&#24341;&#36215;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#26114;&#36149;&#30340;&#25913;&#21464;&#65292;&#32780;&#26159;&#26356;&#22909;&#22320;&#21033;&#29992;&#24050;&#26377;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#31867;&#20869;&#25490;&#24207;&#65292;&#20197;&#34913;&#37327;&#20854;&#34394;&#20551;&#24615;&#65288;&#21363;&#24120;&#35265;&#34394;&#20551;&#32447;&#32034;&#30340;&#23384;&#22312;&#31243;&#24230;&#65289;&#12290;&#36890;&#36807;&#34394;&#20551;&#24615;&#25490;&#21517;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65288;&#21363;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#65289;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#29978;&#33267;&#21487;&#20197;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#20998;&#31867;&#22836;&#37096;&#65292;&#20197;&#26497;&#23569;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#26469;&#26377;&#25928;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#26356;&#20844;&#27491;&#22788;&#29702;&#65292;&#26080;&#35770;&#34394;&#20551;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;5000&#20010;&#31867;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65288;&#20854;&#20013;630&#20010;&#26159;&#34394;&#20551;&#30340;&#65289;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;325k&#20010;&#36719;&#20998;&#21106;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#27169;&#25311;&#20013;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#24182;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01856</link><description>&lt;p&gt;
&#20154;&#31867;&#29983;&#29289;&#29289;&#29702;&#23398;&#20316;&#20026;&#32593;&#32476;&#26435;&#37325;&#65306;&#29992;&#20110;&#21160;&#24577;&#27169;&#25311;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation. (arXiv:2211.01856v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01856
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#27169;&#25311;&#20013;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#24182;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;&#23545;&#20110;&#30740;&#31350;&#29983;&#29702;&#26426;&#21046;&#21644;&#24320;&#21457;&#20154;&#26426;&#30028;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#24403;&#29983;&#25104;&#22823;&#37327;&#27169;&#25311;&#25110;&#27169;&#25311;&#20855;&#26377;&#36830;&#32493;&#21464;&#21270;&#32467;&#26500;&#21442;&#25968;&#30340;&#21160;&#24577;&#20107;&#20214;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#20540;&#27169;&#22411;&#29366;&#24577;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24314;&#27169;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;&#20316;&#20026;&#36825;&#19968;&#27010;&#24565;&#30340;&#31034;&#33539;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioMime&#65292;&#19968;&#31181;&#28151;&#21512;&#32467;&#26500;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#21464;&#21270;&#26399;&#38388;&#23545;&#29305;&#23450;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#20934;&#30830;&#12289;&#36229;&#24555;&#36895;&#21644;&#20219;&#24847;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#29983;&#29702;&#21644;&#20020;&#24202;&#30740;&#31350;&#20197;&#21450;&#25903;&#25345;&#20449;&#21495;&#20998;&#26512;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulations of biophysical systems are fundamental for studying physiological mechanisms and developing human machine interfaces. Whilst advanced numerical methods, such as finite element models, can excel in this task, they are extremely computationally expensive to use when generating a large number of simulations or simulating dynamic events with continuously changing structural parameters. We propose an architecture that uses a conditional generative model to interpolate between the numerical model states, dramatically lowering the modeling time while maintaining a high generation accuracy. As a demonstration of this concept, we present BioMime, a hybrid-structured generative model that enables an accurate, ultra-fast, and arbitrarily high temporal-resolution simulation of a specific biophysical system during dynamic changes. This methodology has wide applications in physiological and clinical research as well as in supporting data augmentation strategies for signal analysis, repre
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.00635</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;LLM&#31934;&#35843;&#26041;&#27861;&#65306;&#26356;&#23569;&#29305;&#21270;&#12289;&#26356;&#22810;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00635
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25552;&#31034;&#30340;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31934;&#35843;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;&#36890;&#24120;&#20351;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36807;&#20110;&#19987;&#38376;&#21270;&#65292;&#24182;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#22312;&#38656;&#35201;&#22788;&#29702;&#27809;&#26377;&#31934;&#35843;&#25968;&#25454;&#30340;&#20854;&#20182;&#20219;&#21153;&#26102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21333;&#20219;&#21153;&#31934;&#35843;&#30830;&#23454;&#20250;&#38477;&#20302;LLM&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#36951;&#24536;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#26684;&#24335;&#29305;&#21270;&#65292;&#21363;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#31934;&#35843;&#20219;&#21153;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#26684;&#24335;&#29305;&#21270;&#21457;&#29983;&#22312;&#31934;&#35843;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Tuning with MOdel Tuning (ProMoT)&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#24037;&#20855;&#65292;&#23558;&#22270;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#35268;&#27169;&#20026;&#19975;&#20159;&#26465;&#36793;&#21644;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#30340;&#32423;&#21035;&#12290;&#36890;&#36807;&#20174;&#19987;&#26377;&#25968;&#25454;&#38598;&#23398;&#20064;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#20110;&#30740;&#31350;&#21508;&#31181;&#22270;&#26041;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21152;&#24555;&#21407;&#22411;&#24320;&#21457;&#21644;&#26032;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#32467;&#26500;&#21644;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#33021;&#22815;&#22312;&#19981;&#21516;&#35268;&#27169;&#19978;&#25193;&#23637;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.01944</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21512;&#25104;&#22270;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Large Scale Synthetic Graph Dataset Generation. (arXiv:2210.01944v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#24037;&#20855;&#65292;&#23558;&#22270;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#35268;&#27169;&#20026;&#19975;&#20159;&#26465;&#36793;&#21644;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#30340;&#32423;&#21035;&#12290;&#36890;&#36807;&#20174;&#19987;&#26377;&#25968;&#25454;&#38598;&#23398;&#20064;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#20110;&#30740;&#31350;&#21508;&#31181;&#22270;&#26041;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21152;&#24555;&#21407;&#22411;&#24320;&#21457;&#21644;&#26032;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#32467;&#26500;&#21644;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#33021;&#22815;&#22312;&#19981;&#21516;&#35268;&#27169;&#19978;&#25193;&#23637;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20110;&#24320;&#21457;&#21644;&#24212;&#29992;&#28145;&#24230;&#22270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#36234;&#26469;&#36234;&#39640;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#65292;&#22823;&#22810;&#25968;&#30456;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#37117;&#24456;&#23567;&#65292;&#25110;&#32773;&#22312;&#24212;&#29992;&#39046;&#22495;&#19978;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#20855;&#26377;&#19975;&#20159;&#26465;&#36793;&#21644;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#30340;&#35268;&#27169;&#12290;&#35813;&#24037;&#20855;&#20174;&#19987;&#26377;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#37322;&#25918;&#32473;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30740;&#31350;&#21508;&#31181;&#22270;&#26041;&#27861;&#65292;&#25512;&#21160;&#21407;&#22411;&#24320;&#21457;&#21644;&#26032;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#30340;&#31034;&#20363;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26500;&#21644;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#25193;&#23637;&#21040;&#19981;&#21516;&#35268;&#27169;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#20854;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems. Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain. This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes. The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#65288;SSMCTB&#65289;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#38598;&#25104;&#20102;&#37325;&#26500;&#21151;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#20449;&#24687;&#36974;&#34109;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.12148</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#65288;SSMCTB&#65289;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#38598;&#25104;&#20102;&#37325;&#26500;&#21151;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#20449;&#24687;&#36974;&#34109;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#24037;&#19994;&#29983;&#20135;&#32447;&#19978;&#30340;&#20135;&#21697;&#25925;&#38556;&#26816;&#27979;&#12289;&#35270;&#39057;&#30417;&#25511;&#20013;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#26816;&#27979;&#20197;&#21450;&#21307;&#23398;&#25195;&#25551;&#20013;&#30340;&#30149;&#21464;&#26816;&#27979;&#31561;&#12290;&#26080;&#35770;&#26159;&#21738;&#20010;&#39046;&#22495;&#65292;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21333;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#23398;&#20064;&#20165;&#22312;&#27491;&#24120;&#31034;&#20363;&#19978;&#36827;&#34892;&#12290;&#25104;&#21151;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23478;&#26063;&#22522;&#20110;&#23398;&#20064;&#37325;&#26500;&#36974;&#34109;&#30340;&#27491;&#24120;&#36755;&#20837;&#65288;&#20363;&#22914;&#34917;&#19969;&#12289;&#26410;&#26469;&#24103;&#31561;&#65289;&#65292;&#24182;&#23558;&#37325;&#26500;&#35823;&#24046;&#30340;&#22823;&#23567;&#20316;&#20026;&#24322;&#24120;&#31243;&#24230;&#30340;&#25351;&#31034;&#22120;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359; (SSMCTB)&#65292;&#35813;&#22359;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#21253;&#21547;&#20102;&#22522;&#20110;&#37325;&#26500;&#30340;&#21151;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#30417;&#30563;&#22359;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#38754;&#36827;&#34892;&#20449;&#24687;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection has recently gained increasing attention in the field of computer vision, likely due to its broad set of applications ranging from product fault detection on industrial production lines and impending event detection in video surveillance to finding lesions in medical scans. Regardless of the domain, anomaly detection is typically framed as a one-class classification task, where the learning is conducted on normal examples only. An entire family of successful anomaly detection methods is based on learning to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and exerting the magnitude of the reconstruction error as an indicator for the abnormality level. Unlike other reconstruction-based methods, we present a novel self-supervised masked convolutional transformer block (SSMCTB) that comprises the reconstruction-based functionality at a core architectural level. The proposed self-supervised block is extremely flexible, enabling information masking at a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#24863;&#30693;&#35821;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#23545;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2208.12266</link><description>&lt;p&gt;
&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#35821;&#38899;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Decoding speech perception from non-invasive brain recordings. (arXiv:2208.12266v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#24863;&#30693;&#35821;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#23545;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#30005;&#27963;&#21160;&#20013;&#35299;&#30721;&#35821;&#38899;&#19968;&#30452;&#26159;&#21307;&#30103;&#20445;&#20581;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#26399;&#24453;&#24050;&#20037;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#20405;&#20837;&#24615;&#35774;&#22791;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65306;&#22522;&#20110;&#39045;&#20869;&#35760;&#24405;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29616;&#22312;&#21487;&#20197;&#35299;&#30721;&#22522;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#23383;&#27597;&#12289;&#21333;&#35789;&#12289;&#39057;&#35889;&#22270;&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#33258;&#28982;&#35821;&#38899;&#21644;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#20581;&#24247;&#20010;&#20307;&#30340;&#38750;&#20405;&#20837;&#24615;&#35760;&#24405;&#20013;&#35299;&#30721;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#30340;&#24863;&#30693;&#35821;&#38899;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;175&#21517;&#24535;&#24895;&#32773;&#30340;&#33041;&#30913;&#22270;&#25110;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#20182;&#20204;&#22312;&#21548;&#30701;&#31687;&#25925;&#20107;&#21644;&#23396;&#31435;&#30340;&#21477;&#23376;&#26102;&#35760;&#24405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;3&#31186;&#30340;&#33041;&#30913;&#22270;&#20449;&#21495;&#20013;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#30456;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;1000&#20010;&#20197;&#19978;&#30340;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in that regard: deep learning algorithms trained on intracranial recordings now start to decode elementary linguistic features (e.g. letters, words, spectrograms). However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we introduce a model trained with contrastive-learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to short stories and isolated sentences. The results show that our model can identify, from 3 seconds of MEG signals, the corresponding speech segment with up to 41% accuracy out of more than 1,0
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKF&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#21644;&#33976;&#39311;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.11447</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#34701;&#21512;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Handling Data Heterogeneity in Federated Learning via Knowledge Distillation and Fusion. (arXiv:2207.11447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKF&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#21644;&#33976;&#39311;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#25903;&#25345;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#35774;&#22791;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31227;&#38382;&#39064;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#21644;&#27169;&#22411;&#20844;&#24179;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;FedKF&#30340;&#32852;&#37030;&#23398;&#20064;&#20840;&#23616;-&#23616;&#37096;&#30693;&#35782;&#34701;&#21512;&#26041;&#26696;&#12290;FedKF&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#35753;&#26381;&#21153;&#22120;&#36820;&#22238;&#20840;&#23616;&#30693;&#35782;&#65292;&#20197;&#19982;&#27599;&#20010;&#35774;&#22791;&#26412;&#22320;&#30693;&#35782;&#34701;&#21512;&#65292;&#20174;&#32780;&#20351;&#26412;&#22320;&#27169;&#22411;&#33021;&#22815;&#21521;&#20840;&#23616;&#26368;&#20248;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#32531;&#35299;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;FedKF&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25903;&#25345;&#31934;&#30830;&#20840;&#23616;&#30693;&#35782;&#34920;&#31034;&#30340;&#20027;-&#36741;&#27169;&#22411;&#32858;&#21512;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20840;&#23616;&#30693;&#35782;&#65288;&#23884;&#20837;&#22312;&#20840;&#23616;&#27169;&#22411;&#20013;&#65289;&#32780;&#19981;&#38656;&#35201;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) supports distributed training of a global machine learning model across multiple devices with the help of a central server. However, data heterogeneity across different devices leads to the client model drift issue and results in model performance degradation and poor model fairness. To address the issue, we design Federated learning with global-local Knowledge Fusion (FedKF) scheme in this paper. The key idea in FedKF is to let the server return the global knowledge to be fused with the local knowledge in each training round so that the local model can be regularized towards the global optima. Therefore, the client model drift issue can be mitigated. In FedKF, we first propose the active-inactive model aggregation technique that supports a precise global knowledge representation. Then, we propose a data-free knowledge distillation (KD) approach to enable each client model to learn the global knowledge (embedded in the global model) while each client model can s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#28508;&#22312;&#31354;&#38388;EBMs&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.05895</link><description>&lt;p&gt;
&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#29992;&#20110;&#21487;&#35299;&#37322;&#25991;&#26412;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#28508;&#22312;&#31354;&#38388;EBMs&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#20854;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#26377;&#36259;&#30340;&#23581;&#35797;&#65292;&#26088;&#22312;&#23454;&#29616;&#25991;&#26412;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#28508;&#22312;&#31354;&#38388;EBMs&#20063;&#32487;&#25215;&#20102;&#25968;&#25454;&#31354;&#38388;EBMs&#30340;&#19968;&#20123;&#32570;&#38519;&#65307;&#23454;&#36341;&#20013;&#36864;&#21270;&#30340;MCMC&#37319;&#26679;&#36136;&#37327;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#36136;&#37327;&#24046;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22797;&#26434;&#28508;&#22312;&#32467;&#26500;&#30340;&#25968;&#25454;&#19978;&#12290;&#21463;&#21040;&#26368;&#36817;&#21033;&#29992;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#37319;&#26679;&#38382;&#39064;&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#26032;&#22411;&#20849;&#29983;&#20851;&#31995;&#65292;&#31216;&#20026;&#28508;&#22312;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20960;&#20309;&#32858;&#31867;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21450;&#20449;&#24687;&#29942;&#39048;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#23398;&#20064;&#28508;&#31354;&#38388;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#21450;&#22270;&#29305;&#24449;&#24863;&#30693;&#65292;&#24182;&#24212;&#29992;&#20110;&#36807;&#31243;&#30417;&#27979;&#20013;&#12290;</title><link>http://arxiv.org/abs/2205.05250</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#20851;&#32852;&#34920;&#31034;&#27861;&#21450;&#20854;&#22312;&#36807;&#31243;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;: &#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal associations representation and application for process monitoring using graph convolution neural network. (arXiv:2205.05250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#21450;&#22270;&#29305;&#24449;&#24863;&#30693;&#65292;&#24182;&#24212;&#29992;&#20110;&#36807;&#31243;&#30417;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#35874;&#21516;&#34892;&#21644;&#23398;&#32773;&#20204;&#23545;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#27880;&#21644;&#20851;&#24515;&#12290;&#22312;&#19987;&#23478;&#12289;&#32534;&#36753;&#21644;&#23457;&#31295;&#20154;&#30340;&#35780;&#35770;&#21644;&#25351;&#23548;&#19979;&#65292;&#36825;&#39033;&#24037;&#20316;&#24050;&#34987;&#25509;&#21463;&#21457;&#34920;&#22312;&#12298;&#36807;&#31243;&#23433;&#20840;&#19982;&#29615;&#22659;&#20445;&#25252;&#12299;&#26399;&#21002;&#19978;&#12290;&#26412;&#25991;&#30340;&#20027;&#39064;&#26159;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#21464;&#37327;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20851;&#32852;&#65292;&#36825;&#26159;&#25351;&#22312;&#21160;&#24577;&#24037;&#19994;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#21363;&#36825;&#20123;&#21464;&#37327;&#19981;&#20165;&#22312;&#26102;&#38388;&#19978;&#39640;&#24230;&#30456;&#20851;&#65292;&#20063;&#22312;&#31354;&#38388;&#19978;&#30456;&#20114;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#65288;&#26102;&#38388;&#20449;&#24687;&#65289;&#21644;&#22270;&#29305;&#24449;&#24863;&#30693;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#26381;&#20174;&#25913;&#36827;&#30340;&#39640;&#26031;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#32780;&#22270;&#32593;&#32476;&#21487;&#20197;&#30001;&#30417;&#27979;&#21464;&#37327;&#21450;&#20854;&#36793;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thank you very much for the attention and concern of colleagues and scholars in this work. With the comments and guidance of experts, editors, and reviewers, this work has been accepted for publishing in the journal "Process Safety and Environmental Protection". The theme of this paper relies on the Spatial-temporal associations of numerous variables in the same industrial processes, which refers to numerous variables obtained in dynamic industrial processes with Spatial-temporal correlation characteristics, i.e., these variables are not only highly correlated in time but also interrelated in space. To handle this problem, three key issues need to be well addressed: variable characteristics modeling and representation, graph network construction (temporal information), and graph characteristics perception. The first issue is implemented by assuming the data follows one improved Gaussian distribution, while the graph network can be defined by the monitoring variables and their edges whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MRI&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33258;&#30417;&#30563;&#21435;&#22122;&#32593;&#32476;&#21644;&#25554;&#20837;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#27491;&#21017;&#21270;&#21435;&#22122;&#26469;&#20248;&#21270;MRI&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2205.03519</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#28145;&#24230;&#23637;&#24320;&#37325;&#24314;&#26041;&#27861;&#30340;&#27491;&#21017;&#21270;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MRI&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33258;&#30417;&#30563;&#21435;&#22122;&#32593;&#32476;&#21644;&#25554;&#20837;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#27491;&#21017;&#21270;&#21435;&#22122;&#26469;&#20248;&#21270;MRI&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#21463;&#21040;&#36825;&#19968;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#37325;&#24314;&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#29305;&#21035;&#26159;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;MRI&#24212;&#29992;&#26469;&#35828;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DURED-Net&#30340;&#26032;&#22411;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#21435;&#22122;&#32593;&#32476;&#21644;&#25554;&#20837;&#24335;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;MR&#22270;&#20687;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#28155;&#21152;&#21033;&#29992;&#22270;&#20687;&#29289;&#29702;&#23398;&#30340;&#26174;&#24335;&#20808;&#39564;&#26469;&#25552;&#39640;MR&#37325;&#24314;&#20013;Noise2Noise&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#21435;&#22122;&#65288;RED&#65289;&#23454;&#29616;&#20102;&#21435;&#22122;&#32593;&#32476;&#22312;MRI&#37325;&#24314;&#20013;&#30340;&#21457;&#25381;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MRI&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25910;&#25947;&#24615;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.05923</link><description>&lt;p&gt;
&#19968;&#31181;&#20195;&#25968;&#25910;&#25947;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization. (arXiv:2204.05923v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25910;&#25947;&#24615;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#38543;&#26426;&#39033;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#26681;&#25454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20540;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#12290;&#22312;&#27169;&#25311;&#36864;&#28779;&#30340;&#26415;&#35821;&#20013;&#65292;&#28201;&#24230;&#26159;&#19982;&#29366;&#24577;&#30456;&#20851;&#30340;&#12290;&#20511;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#27010;&#29575;&#21644;&#21442;&#25968;&#31354;&#38388;&#20013;&#20855;&#26377;&#20195;&#25968;&#25910;&#25947;&#24615;&#65292;&#36825;&#27604;&#20165;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#22122;&#22768;&#39033;&#25511;&#21046;&#30340;&#32463;&#20856;&#25910;&#25947;&#36895;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25910;&#25947;&#35777;&#26126;&#22522;&#20110;&#31639;&#27861;&#30340;&#23454;&#38469;&#31163;&#25955;&#35774;&#32622;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22914;&#25991;&#29486;&#20013;&#36890;&#24120;&#20570;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#65292;&#20197;&#23637;&#31034;&#31639;&#27861;&#23545;&#20110;&#30456;&#23545;&#22797;&#26434;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;WGANs&#30340;1-Wasserstein&#36317;&#31163;&#36827;&#34892;&#20102;&#26368;&#20248;&#21270;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#22823;&#23567;&#22266;&#23450;&#26102;&#30340;&#26368;&#20248;&#26041;&#26696;&#19982;&#26368;&#23567;&#21270;&#26679;&#26412;&#28857;&#20043;&#38388;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#21644;&#26377;&#23494;&#20999;&#20851;&#32852;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#26679;&#26412;&#22823;&#23567;&#36235;&#21521;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;WGANs&#33021;&#22815;&#20197;&#32473;&#23450;&#30340;&#25910;&#25947;&#29575;&#26080;&#38480;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#65292;&#21069;&#25552;&#26159;&#29983;&#25104;Lipschitz&#20989;&#25968;&#26063;&#22686;&#38271;&#21512;&#36866;&#12290;</title><link>http://arxiv.org/abs/2201.02824</link><description>&lt;p&gt;
WGANs&#30340;&#26368;&#20248;1-Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Optimal 1-Wasserstein Distance for WGANs. (arXiv:2201.02824v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;WGANs&#30340;1-Wasserstein&#36317;&#31163;&#36827;&#34892;&#20102;&#26368;&#20248;&#21270;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#22823;&#23567;&#22266;&#23450;&#26102;&#30340;&#26368;&#20248;&#26041;&#26696;&#19982;&#26368;&#23567;&#21270;&#26679;&#26412;&#28857;&#20043;&#38388;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#21644;&#26377;&#23494;&#20999;&#20851;&#32852;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#26679;&#26412;&#22823;&#23567;&#36235;&#21521;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;WGANs&#33021;&#22815;&#20197;&#32473;&#23450;&#30340;&#25910;&#25947;&#29575;&#26080;&#38480;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#65292;&#21069;&#25552;&#26159;&#29983;&#25104;Lipschitz&#20989;&#25968;&#26063;&#22686;&#38271;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32972;&#21518;&#30340;&#25968;&#23398;&#21147;&#37327;&#24341;&#21457;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29702;&#35770;&#38382;&#39064;&#12290;&#20026;&#20102;&#34920;&#24449;&#29983;&#25104;&#20998;&#24067;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25105;&#20204;&#23545;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#39046;&#22495;&#20013;&#30340;Wasserstein GANs&#65288;WGANs&#65289;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#31354;&#38388;&#20026;&#21333;&#21464;&#37327;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#24471;&#20986;&#20102;&#22312;&#36755;&#20986;&#31354;&#38388;&#32500;&#24230;&#26080;&#20851;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#29305;&#21035;&#34920;&#26126;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#65292;&#26368;&#20248;WGANs&#19982;&#36830;&#25509;&#36335;&#24452;&#26368;&#23567;&#21270;&#26679;&#26412;&#28857;&#20043;&#38388;&#30340;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#21644;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;WGANs&#33021;&#22815;&#20197;&#32473;&#23450;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#29983;&#25104;&#30340;Lipschitz&#20989;&#25968;&#26063;&#36866;&#24403;&#22686;&#38271;&#30340;&#26465;&#20214;&#19979;&#65292;&#26080;&#38480;&#25509;&#36817;&#65288;&#23545;&#20110;1-Wasserstein&#36317;&#31163;&#65289;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#39034;&#20415;&#25512;&#23548;&#20102;&#21322;&#31163;&#25955;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mathematical forces at work behind Generative Adversarial Networks raise challenging theoretical issues. Motivated by the important question of characterizing the geometrical properties of the generated distributions, we provide a thorough analysis of Wasserstein GANs (WGANs) in both the finite sample and asymptotic regimes. We study the specific case where the latent space is univariate and derive results valid regardless of the dimension of the output space. We show in particular that for a fixed sample size, the optimal WGANs are closely linked with connected paths minimizing the sum of the squared Euclidean distances between the sample points. We also highlight the fact that WGANs are able to approach (for the 1-Wasserstein distance) the target distribution as the sample size tends to infinity, at a given convergence rate and provided the family of generative Lipschitz functions grows appropriately. We derive in passing new results on optimal transport theory in the semi-discre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2112.08417</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#22240;&#26524;&#31062;&#20808;&#22270;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterization of causal ancestral graphs for time series with latent confounders. (arXiv:2112.08417v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#28382;&#21518;&#29305;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#29305;&#24449;&#21270;&#20102;&#36825;&#20123;&#22270;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#24403;&#21069;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#21035;&#30340;&#36866;&#24403;&#23376;&#38598;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#36825;&#20123;&#26032;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#25152;&#23398;&#21040;&#30340;&#20869;&#23481;&#30456;&#27604;&#65292;&#36825;&#31181;&#22270;&#24418;&#34920;&#31034;&#21253;&#21547;&#26356;&#22810;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel class of graphical models for representing time lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences -- without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FA-LD&#30340;&#32852;&#37030;&#24179;&#22343;Langevin&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22343;&#20540;&#39044;&#27979;&#12290;&#31639;&#27861;&#32771;&#34385;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#30740;&#31350;&#20102;&#27880;&#20837;&#22122;&#22768;&#12289;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21464;&#21270;&#30340;&#23398;&#20064;&#29575;&#31561;&#22240;&#32032;&#23545;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#20026;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.05120</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#24179;&#22343; Langevin &#21160;&#21147;&#23398;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convergence of Federated Averaging Langevin Dynamics. (arXiv:2112.05120v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05120
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FA-LD&#30340;&#32852;&#37030;&#24179;&#22343;Langevin&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22343;&#20540;&#39044;&#27979;&#12290;&#31639;&#27861;&#32771;&#34385;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#30740;&#31350;&#20102;&#27880;&#20837;&#22122;&#22768;&#12289;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21464;&#21270;&#30340;&#23398;&#20064;&#29575;&#31561;&#22240;&#32032;&#23545;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#20026;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22343;&#20540;&#39044;&#27979;&#30340;&#32852;&#37030;&#24179;&#22343; Langevin &#31639;&#27861;&#65288;FA-LD&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#33324;&#27169;&#22411;&#30340;&#27491;&#24120;&#21518;&#39564;&#20998;&#24067;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#20026; FA-LD &#24320;&#21457;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#38024;&#23545;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#30740;&#31350;&#20102;&#27880;&#20837;&#22122;&#22768;&#12289;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21464;&#21270;&#30340;&#23398;&#20064;&#29575;&#23545;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#26041;&#38754;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#20043;&#22788;&#22312;&#20110;&#65292;Langevin &#31639;&#27861;&#20013;&#27880;&#20837;&#22122;&#22768;&#19981;&#20250;&#25439;&#23475;&#36890;&#20449;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312; FA-LD &#31639;&#27861;&#20013;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#20351;&#29992;&#29420;&#31435;&#21644;&#30456;&#20851;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36890;&#20449;&#12289;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#12290;&#30001;&#20110;&#26412;&#22320;&#35774;&#22791;&#21487;&#33021;&#22312;&#32852;&#37030;&#32593;&#32476;&#20013;&#21464;&#24471;&#19981;&#27963;&#36291;&#65292;
&lt;/p&gt;
&lt;p&gt;
We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions with distributed clients. In particular, we generalize beyond normal posterior distributions and consider a general class of models. We develop theoretical guarantees for FA-LD for strongly log-concave distributions with non-i.i.d data and study how the injected noise and the stochastic-gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. Such an analysis sheds light on the optimal choice of local updates to minimize communication costs. Important to our approach is that the communication efficiency does not deteriorate with the injected noise in the Langevin algorithms. In addition, we examine in our FA-LD algorithm both independent and correlated noise used over different clients. We observe there is a trade-off between the pairs among communication, accuracy, and data privacy. As local devices may become inactive in federated ne
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#28857;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;MHP&#21442;&#25968;&#21644;&#35889;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2111.02062</link><description>&lt;p&gt;
&#36328;&#25968;&#25454;&#31890;&#24230;&#38142;&#25509;&#65306;&#25311;&#21512;&#22810;&#21464;&#37327;Hawkes&#36807;&#31243;&#21040;&#37096;&#20998;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data. (arXiv:2111.02062v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#28857;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;MHP&#21442;&#25968;&#21644;&#35889;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;Hawkes&#36807;&#31243;(MHP)&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#27969;&#65292;&#20854;&#20013;&#20107;&#20214;&#22312;&#33258;&#36523;&#32500;&#24230;&#20869;&#65288;&#36890;&#36807;&#33258;&#28608;&#65289;&#25110;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#65288;&#36890;&#36807;&#20132;&#21449;&#28608;&#21457;&#65289;&#29983;&#25104;&#26032;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#26576;&#20123;&#32500;&#24230;&#20013;&#30340;&#20010;&#21035;&#20107;&#20214;&#26102;&#38388;&#25139;&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#65292;&#21482;&#26377;&#22312;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#20107;&#20214;&#35745;&#25968;&#26159;&#24050;&#30693;&#30340;&#65292;&#34987;&#31216;&#20026;&#37096;&#20998;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#12290;MHP&#19981;&#36866;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#65292;&#22240;&#20026;&#20854;&#20272;&#35745;&#38656;&#35201;&#20107;&#20214;&#26102;&#38388;&#25139;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#36807;&#31243;&#65292;&#19982;MHP&#20849;&#20139;&#21442;&#25968;&#31561;&#25928;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;PMBP&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PMBP&#36807;&#31243;&#21487;&#20197;&#36817;&#20284;MHP&#21442;&#25968;&#24182;&#24674;&#22797;&#35889;&#21322;&#24452;&#65292;&#20351;&#29992;&#21512;&#25104;&#20107;&#20214;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multivariate Hawkes process (MHP) is widely used for analyzing data streams that interact with each other, where events generate new events within their own dimension (via self-excitation) or across different dimensions (via cross-excitation). However, in certain applications, the timestamps of individual events in some dimensions are unobservable, and only event counts within intervals are known, referred to as partially interval-censored data. The MHP is unsuitable for handling such data since its estimation requires event timestamps. In this study, we introduce the Partial Mean Behavior Poisson (PMBP) process, a novel point process which shares parameter equivalence with the MHP and can effectively model both timestamped and interval-censored data. We demonstrate the capabilities of the PMBP process using synthetic and real-world datasets. Firstly, we illustrate that the PMBP process can approximate MHP parameters and recover the spectral radius using synthetic event histories. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#21306;&#22495;&#31454;&#20105;&#65288;DRC&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#21069;&#26223;&#23545;&#35937;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#33021;&#37327;&#20808;&#39564;&#21644;&#29983;&#25104;&#24335;&#22270;&#20687;&#24314;&#27169;&#65292;&#21033;&#29992;&#20687;&#32032;&#37325;&#26032;&#20998;&#37197;&#26469;&#25429;&#25417;&#32972;&#26223;&#21306;&#22495;&#30340;&#35268;&#24459;&#24615;&#65292;&#24182;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#25214;&#21040;&#21069;&#26223;-&#32972;&#26223;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2110.15497</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21069;&#26223;&#25552;&#21462;&#26041;&#27861;&#65306;&#22522;&#20110;&#28145;&#24230;&#21306;&#22495;&#31454;&#20105;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Foreground Extraction via Deep Region Competition. (arXiv:2110.15497v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#21306;&#22495;&#31454;&#20105;&#65288;DRC&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#21069;&#26223;&#23545;&#35937;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#33021;&#37327;&#20808;&#39564;&#21644;&#29983;&#25104;&#24335;&#22270;&#20687;&#24314;&#27169;&#65292;&#21033;&#29992;&#20687;&#32032;&#37325;&#26032;&#20998;&#37197;&#26469;&#25429;&#25417;&#32972;&#26223;&#21306;&#22495;&#30340;&#35268;&#24459;&#24615;&#65292;&#24182;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#25214;&#21040;&#21069;&#26223;-&#32972;&#26223;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#21306;&#22495;&#31454;&#20105;&#65288;DRC&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20197;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#21069;&#26223;&#23545;&#35937;&#12290;&#21069;&#26223;&#25552;&#21462;&#21487;&#20197;&#30475;&#20316;&#26159;&#36890;&#29992;&#22270;&#20687;&#20998;&#21106;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20854;&#37325;&#28857;&#26159;&#35782;&#21035;&#21644;&#35299;&#24320;&#32972;&#26223;&#20013;&#30340;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;&#33021;&#37327;&#30340;&#20808;&#39564;&#19982;&#29983;&#25104;&#24335;&#22270;&#20687;&#24314;&#27169;&#65288;Mixture of Experts&#65292;MoE&#65289;&#30456;&#32467;&#21512;&#37325;&#26032;&#24605;&#32771;&#21069;&#26223;&#25552;&#21462;&#65292;&#36827;&#19968;&#27493;&#24341;&#20837;&#23398;&#20064;&#30340;&#20687;&#32032;&#37325;&#26032;&#20998;&#37197;&#20316;&#20026;&#25429;&#25417;&#32972;&#26223;&#21306;&#22495;&#35268;&#24459;&#24615;&#30340;&#37325;&#35201;&#24402;&#32435;&#20559;&#35265;&#12290;&#36890;&#36807;&#36825;&#31181;&#24314;&#27169;&#65292;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;Expectation-Maximization&#65292;EM&#65289;&#33258;&#28982;&#22320;&#25214;&#21040;&#21069;&#26223;-&#32972;&#26223;&#20998;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#22312;&#20998;&#21306;&#36807;&#31243;&#20013;&#28151;&#21512;&#20998;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#19982;&#36890;&#29992;&#22270;&#20687;&#20998;&#21106;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#8212;&#8212;&#21306;&#22495;&#31454;&#20105;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Deep Region Competition (DRC), an algorithm designed to extract foreground objects from images in a fully unsupervised manner. Foreground extraction can be viewed as a special case of generic image segmentation that focuses on identifying and disentangling objects from the background. In this work, we rethink the foreground extraction by reconciling energy-based prior with generative image modeling in the form of Mixture of Experts (MoE), where we further introduce the learned pixel re-assignment as the essential inductive bias to capture the regularities of background regions. With this modeling, the foreground-background partition can be naturally found through Expectation-Maximization (EM). We show that the proposed method effectively exploits the interaction between the mixture components during the partitioning process, which closely connects to region competition, a seminal approach for generic image segmentation. Experiments demonstrate that DRC exhibits more competit
&lt;/p&gt;</description></item><item><title>Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.14883</link><description>&lt;p&gt;
Colossal-AI: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14883
&lt;/p&gt;
&lt;p&gt;
Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21333;&#20010;GPU&#30340;&#26377;&#38480;&#20869;&#23384;&#36164;&#28304;&#65292;&#36873;&#25321;&#26368;&#20339;&#24182;&#34892;&#31574;&#30053;&#30340;&#26368;&#20339;&#23454;&#36341;&#20173;&#28982;&#32570;&#20047;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#21644;&#24182;&#34892;&#35745;&#31639;&#26041;&#38754;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;Colossal-AI&#31995;&#32479;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#30340;&#39034;&#24207;&#20195;&#30721;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#12290;&#23427;&#25903;&#25345;&#25968;&#25454;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#12289;&#24352;&#37327;&#24182;&#34892;&#21644;&#24207;&#21015;&#24182;&#34892;&#31561;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#31995;&#32479;&#30456;&#27604;&#65292;Colossal-AI&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;2.76&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.  The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#23558;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#24377;&#24615;&#32467;&#21512;&#36215;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2110.03991</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;SGD&#20013;&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Combining Differential Privacy and Byzantine Resilience in Distributed SGD. (arXiv:2110.03991v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#23558;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#24377;&#24615;&#32467;&#21512;&#36215;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#24377;&#24615;&#65288;BR&#65289;&#26159;&#29616;&#20195;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290;&#36825;&#20004;&#20010;&#27010;&#24565;&#22312;&#36807;&#21435;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#26631;&#20934;&#21442;&#25968;&#26381;&#21153;&#22120;&#26550;&#26500;&#20013;&#65292;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#22312;&#23384;&#22312;&#24694;&#24847;&#65288;&#25308;&#21344;&#24237;&#65289;&#24037;&#20316;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450;&#20854;&#20182;&#35802;&#23454;&#24037;&#20316;&#32773;&#25552;&#20379;&#22122;&#22768;&#20449;&#24687;&#20197;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#30340;&#31243;&#24230;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#24377;&#24615;&#23454;&#36341;&#30340;&#25972;&#21512;&#24182;&#19981;&#31616;&#21333;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#20851;&#20110;&#20998;&#24067;&#24335;SGD&#25910;&#25947;&#24615;&#30340;&#32467;&#26524;&#65292;&#22312;&#35802;&#23454;&#24037;&#20316;&#32773;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;&#26102;&#26080;&#25928;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20381;&#36182;&#20110;$(\alpha,f)$-&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#29702;&#35770;.
&lt;/p&gt;
&lt;p&gt;
Privacy and Byzantine resilience (BR) are two crucial requirements of modern-day distributed machine learning. The two concepts have been extensively studied individually but the question of how to combine them effectively remains unanswered. This paper contributes to addressing this question by studying the extent to which the distributed SGD algorithm, in the standard parameter-server architecture, can learn an accurate model despite (a) a fraction of the workers being malicious (Byzantine), and (b) the other fraction, whilst being honest, providing noisy information to the server to ensure differential privacy (DP). We first observe that the integration of standard practices in DP and BR is not straightforward. In fact, we show that many existing results on the convergence of distributed SGD under Byzantine faults, especially those relying on $(\alpha,f)$-Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, we revisit the theory 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.03890</link><description>&lt;p&gt;
&#12298;&#25512;&#26029;&#35299;&#37322;&#30340;&#20844;&#29702;&#32858;&#21512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21518;&#32493;&#27169;&#22411;&#36924;&#36817;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#30340;&#40065;&#26834;&#24615;&#30340;&#36817;&#26399;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#30340;&#25512;&#26029;&#35299;&#37322;&#30340;&#20852;&#36215;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#25512;&#26029;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#20010;&#36275;&#20197;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#23376;&#38598;&#29305;&#24449;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#20005;&#26684;&#21644;&#21487;&#38752;&#30340;&#65292;&#20294;&#25512;&#26029;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#20197;&#26377;&#22810;&#20010;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#35268;&#27169;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20844;&#29702;&#19978;&#23545;&#36825;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35777;&#26126;&#27599;&#20010;&#26041;&#27861;&#37117;&#26159;&#33391;&#23450;&#20041;&#30340;&#19988;&#31526;&#21512;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#30340;&#20809;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#26469;&#22823;&#35268;&#27169;&#30340;&#22825;&#20307;&#26242;&#21464;&#25968;&#25454;&#12290;&#36825;&#31181;&#21464;&#25442;&#22120;&#26550;&#26500;&#25903;&#25345;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#21487;&#28789;&#27963;&#28155;&#21152;&#38468;&#21152;&#29305;&#24449;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.06178</link><description>&lt;p&gt;
&#20851;&#27880;&#22825;&#20307;&#26242;&#21464;&#29616;&#35937;&#65306;&#20171;&#32461;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#30340;&#20809;&#24230;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Paying Attention to Astronomical Transients: Introducing the Time-series Transformer for Photometric Classification. (arXiv:2105.06178v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#30340;&#20809;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#26469;&#22823;&#35268;&#27169;&#30340;&#22825;&#20307;&#26242;&#21464;&#25968;&#25454;&#12290;&#36825;&#31181;&#21464;&#25442;&#22120;&#26550;&#26500;&#25903;&#25345;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#21487;&#28789;&#27963;&#28155;&#21152;&#38468;&#21152;&#29305;&#24449;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#21208;&#27979;&#39033;&#30446;&#65292;&#22914;&#40065;&#23486;&#22825;&#25991;&#21488;&#30340;&#36951;&#20135;&#31354;&#38388;&#19982;&#26102;&#38388;&#21208;&#27979;(LSST)&#65292;&#23558;&#35266;&#27979;&#21040;&#27604;&#20197;&#24448;&#20219;&#20309;&#19968;&#39033;&#21208;&#27979;&#37117;&#22810;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#22825;&#20307;&#26242;&#21464;&#20107;&#20214;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#30340;&#20809;&#24230;&#25968;&#25454;&#20013;&#65292;&#21333;&#38752;&#20154;&#31867;&#26080;&#27861;&#23545;&#25152;&#26377;&#36825;&#20123;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22825;&#20307;&#26242;&#21464;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#26029;&#25552;&#21319;&#30340;&#25104;&#21151;&#12290;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#26368;&#21021;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#19988;&#26368;&#36817;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#26680;&#24515;&#26159;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#25903;&#25345;&#23548;&#20837;&#20219;&#24847;&#25968;&#37327;&#30340;&#38468;&#21152;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23558;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#24212;&#29992;&#20110;&#20809;&#24230;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future surveys such as the Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will observe an order of magnitude more astrophysical transient events than any previous survey before. With this deluge of photometric data, it will be impossible for all such events to be classified by humans alone. Recent efforts have sought to leverage machine learning methods to tackle the challenge of astronomical transient classification, with ever improving success. Transformers are a recently developed deep learning architecture, first proposed for natural language processing, that have shown a great deal of recent success. In this work we develop a new transformer architecture, which uses multi-head self attention at its core, for general multi-variate time-series data. Furthermore, the proposed time-series transformer architecture supports the inclusion of an arbitrary number of additional features, while also offering interpretability. We apply the time-series transformer to t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#36890;&#36807;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#27010;&#29575;&#24615;&#27010;&#24565;&#23450;&#20041;&#21644;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#23384;&#20648;&#19978;&#30028;&#65292;&#24182;&#19982;&#21521;&#37327;&#34920;&#31034;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2104.07454</link><description>&lt;p&gt;
&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#36890;&#36807;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#27010;&#29575;&#24615;&#27010;&#24565;&#23450;&#20041;&#21644;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#23384;&#20648;&#19978;&#30028;&#65292;&#24182;&#19982;&#21521;&#37327;&#34920;&#31034;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#32463;&#20856;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#20013;&#36890;&#36807;&#23384;&#20648;&#32467;&#26500;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#31070;&#32463;&#22270;&#28789;&#26426;&#65288;NTM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;RNN&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#23454;&#29616;&#20102;&#21487;&#32534;&#31243;&#35745;&#31639;&#26426;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#23398;&#20064;&#31616;&#21333;&#30340;&#31639;&#27861;&#20219;&#21153;&#12290;&#30697;&#38453;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30697;&#38453;&#34920;&#31034;&#65292;&#19982;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#34920;&#31034;&#30340;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#21487;&#20197;&#22266;&#26377;&#22320;&#20445;&#30041;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#35748;&#20026;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20855;&#26377;&#26356;&#22909;&#30340;&#23384;&#20648;&#23481;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Fisher&#20449;&#24687;&#23450;&#20041;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#30697;&#38453;RNN&#23384;&#20648;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;&#22312;&#21508;&#31181;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#30340;&#19978;&#30028;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#21521;&#37327;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;s
&lt;/p&gt;
&lt;p&gt;
It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2102.00696</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#30340;&#21367;&#31215;LSTM&#36827;&#34892;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#36890;&#24120;&#38656;&#35201;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20943;&#23569;&#20102;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20869;&#25581;&#31034;&#20986;&#21019;&#26032;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38598;&#25104;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#34701;&#20837;&#21040;&#27169;&#22411;&#26550;&#26500;&#20013;&#12290;&#19982;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;ConvLSTM&#12289;TrajGRU&#21644;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22825;&#27668;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#28041;&#21450;&#39640;&#35268;&#27169;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#20540;&#22825;&#27668;&#25968;&#25454;&#38598;&#65292;&#21363;ERA5&#23567;&#26102;&#32423;&#27668;&#21387;&#27700;&#24179;&#25968;&#25454;&#38598;&#21644;WeatherBench&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Numerical weather forecasting using high-resolution physical models often requires extensive computational resources on supercomputers, which diminishes their wide usage in most real-life applications. As a remedy, applying deep learning methods has revealed innovative solutions within this field. To this end, we introduce a novel deep learning architecture for forecasting high-resolution spatio-temporal weather data. Our approach extends the conventional encoder-decoder structure by integrating Convolutional Long-short Term Memory and Convolutional Neural Networks. In addition, we incorporate attention and context matcher mechanisms into the model architecture. Our Weather Model achieves significant performance improvements compared to baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our experimental evaluation involves high-scale, real-world benchmark numerical weather datasets, namely the ERA5 hourly dataset on pressure levels and WeatherBench. Our results demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27979;&#35797;&#24773;&#20917;&#19979;&#65292;DCL&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25104;&#26412;&#21644;&#26368;&#20248;&#24615;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2011.15122</link><description>&lt;p&gt;
&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#29992;&#20110;&#24211;&#23384;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Controlled Learning for Inventory Control. (arXiv:2011.15122v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27979;&#35797;&#24773;&#20917;&#19979;&#65292;DCL&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25104;&#26412;&#21644;&#26368;&#20248;&#24615;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#20256;&#32479;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21253;&#25324;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#30446;&#30340;&#65292;&#26159;&#21542;&#26159;&#24211;&#23384;&#25511;&#21046;&#24212;&#29992;&#20013;&#26368;&#36866;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65311;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#29305;&#28857;&#37327;&#36523;&#23450;&#21046;&#30340;DRL&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26159;&#21542;&#20248;&#20110;DRL&#21644;&#20256;&#32479;&#22522;&#20934;&#65311;&#26041;&#27861;/&#32467;&#26524;&#65306;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#22411;DRL&#26694;&#26550;&#8212;&#8212;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#24211;&#23384;&#38382;&#39064;&#12290;&#27604;&#36739;&#24615;&#35780;&#20272;&#34920;&#26126;&#65292;DCL&#22312;&#38144;&#21806;&#25439;&#22833;&#24211;&#23384;&#25511;&#21046;&#12289;&#26131;&#33104;&#24211;&#23384;&#31995;&#32479;&#21644;&#20855;&#26377;&#38543;&#26426;&#24341;&#23548;&#26102;&#38388;&#30340;&#24211;&#23384;&#31995;&#32479;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24179;&#22343;&#25104;&#26412;&#65292;&#24182;&#19988;&#32500;&#25345;&#20102;&#26368;&#22810;0.1%&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#23454;&#39564;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problem Definition: Are traditional deep reinforcement learning (DRL) algorithms, developed for a broad range of purposes including game-play and robotics, the most suitable machine learning algorithms for applications in inventory control? To what extent would DRL algorithms tailored to the unique characteristics of inventory control problems provide superior performance compared to DRL and traditional benchmarks? Methodology/results: We propose and study Deep Controlled Learning (DCL), a new DRL framework based on approximate policy iteration specifically designed to tackle inventory problems. Comparative evaluations reveal that DCL outperforms existing state-of-the-art heuristics in lost sales inventory control, perishable inventory systems, and inventory systems with random lead times, achieving lower average costs across all test instances and maintaining an optimality gap of no more than 0.1\%. Notably, the same hyperparameter set is utilized across all experiments, underscoring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#20984;&#24809;&#32602;&#20989;&#25968;MCP&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19982;MCP&#26102;&#26356;&#39640;&#25928;&#19988;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2010.11559</link><description>&lt;p&gt;
&#20351;&#29992;MCP&#26041;&#27861;&#23398;&#20064;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Learning Graph Laplacian with MCP. (arXiv:2010.11559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#20984;&#24809;&#32602;&#20989;&#25968;MCP&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19982;MCP&#26102;&#26356;&#39640;&#25928;&#19988;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28385;&#36275;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#38750;&#20984;&#24809;&#32602;&#20989;&#25968;&#65306;&#26368;&#23567;&#26368;&#22823;&#20985;&#24809;&#32602;&#20989;&#25968;&#65288;MCP&#65289;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;&#20984;&#24046;&#20998;&#31639;&#27861;&#26469;&#35299;&#20915;MCP&#24809;&#32602;&#30340;&#22270;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#20984;&#24046;&#20998;&#31639;&#27861;&#30340;&#27599;&#20010;&#23376;&#38382;&#39064;&#37117;&#20855;&#26377;&#19968;&#20010;&#20248;&#31168;&#30340;&#24615;&#36136;&#65292;&#21363;&#20854;&#23545;&#20598;&#38382;&#39064;&#20013;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#36830;&#32493;&#21487;&#24494;&#30340;&#21322;&#20809;&#28369;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#39640;&#25928;&#30340;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#20984;&#24046;&#20998;&#31639;&#27861;&#30340;&#23376;&#38382;&#39064;&#12290;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#38750;&#20984;&#24809;&#32602;MCP&#22312;&#20419;&#36827;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19982;MCP&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a graph under the Laplacian constraint with a non-convex penalty: minimax concave penalty (MCP). For solving the MCP penalized graphical model, we design an inexact proximal difference-of-convex algorithm (DCA) and prove its convergence to critical points. We note that each subproblem of the proximal DCA enjoys the nice property that the objective function in its dual problem is continuously differentiable with a semismooth gradient. Therefore, we apply an efficient semismooth Newton method to subproblems of the proximal DCA. Numerical experiments on various synthetic and real data sets demonstrate the effectiveness of the non-convex penalty MCP in promoting sparsity. Compared with the existing state-of-the-art method, our method is demonstrated to be more efficient and reliable for learning graph Laplacian with MCP.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/1912.05957</link><description>&lt;p&gt;
&#25991;&#26412;&#20316;&#20026;&#29615;&#22659;:&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.05957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#21487;&#20197;&#26174;&#33879;&#20419;&#36827;&#20449;&#24687;&#30340;&#20934;&#30830;&#34920;&#36798;&#12290;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#21046;&#23450;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#36827;&#34892;&#35782;&#21035;&#65292;&#32780;&#19981;&#35770;&#20854;&#38271;&#24230;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39640;&#25928;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#25991;&#26412;&#26469;&#30830;&#23450;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;Weebit&#21644;&#21073;&#26725;&#32771;&#35797;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#36890;&#36807;&#28145;&#20837;&#20102;&#35299;&#20219;&#21153;&#38656;&#27714;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/1802.00810</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;: &#19968;&#20010;&#31616;&#27905;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Genomics: A Concise Overview. (arXiv:1802.00810v3 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1802.00810
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#36890;&#36807;&#28145;&#20837;&#20102;&#35299;&#20219;&#21153;&#38656;&#27714;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#27979;&#24207;&#31561;&#22522;&#22240;&#32452;&#30740;&#31350;&#30340;&#36827;&#23637;&#24050;&#23558;&#29616;&#20195;&#22522;&#22240;&#32452;&#23398;&#25512;&#21521;&#20102;"&#22823;&#25968;&#25454;"&#23398;&#31185;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#28856;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#22522;&#22240;&#32452;&#23398;&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;&#19982;&#24378;&#22823;&#31639;&#27861;&#30340;&#32039;&#24613;&#38656;&#27714;&#30456;&#24179;&#34892;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#26469;&#35828;&#65292;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#36229;&#20986;&#25105;&#20204;&#30340;&#30693;&#35782;&#25506;&#32034;&#22522;&#22240;&#32452;&#30340;&#35299;&#35835;&#12290;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#35813;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#20174;&#22522;&#22240;&#32452;&#23398;&#35282;&#24230;&#30475;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20415;&#23558;&#27599;&#20010;&#29305;&#23450;&#20219;&#21153;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#24182;&#23545;&#21457;&#23637;&#29616;&#20195;&#22522;&#22240;&#32452;&#23398;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#23454;&#36341;&#32771;&#34385;&#36827;&#34892;&#20102;&#35780;&#36848;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#31616;&#26126;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into "big data" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applicati
&lt;/p&gt;</description></item></channel></rss>