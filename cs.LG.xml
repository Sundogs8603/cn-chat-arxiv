<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17370</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24212;&#29992;&#20026;&#27169;&#25311;&#22823;&#27668;&#30340;&#21487;&#33021;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#21464;&#38761;&#12290;&#22312;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#33719;&#21462;&#20687;&#36825;&#26679;&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;&#27169;&#22411;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#20840;&#29699;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#24314;&#27169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20998;&#23618;&#27169;&#22411;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
&lt;/p&gt;</description></item><item><title>3D-Mol&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17366</link><description>&lt;p&gt;
3D-Mol: &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;3D&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information. (arXiv:2309.17366v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17366
&lt;/p&gt;
&lt;p&gt;
3D-Mol&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20026;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#26089;&#26399;&#31579;&#36873;&#21644;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;3D&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#21333;&#20010;&#20998;&#23376;&#34920;&#31034;&#22810;&#20010;&#23454;&#38469;&#20998;&#23376;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-Mol&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#12290;&#20026;&#20102;&#20934;&#30830;&#34920;&#31034;&#23436;&#25972;&#30340;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#20998;&#35299;&#25104;&#19977;&#20010;&#20960;&#20309;&#22270;&#24418;&#26469;&#25552;&#21462;3D&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;20M&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#30456;&#21516;&#25299;&#25169;&#32467;&#26500;&#30340;&#26500;&#35937;&#35270;&#20026;&#27491;&#26679;&#26412;&#23545;&#65292;&#23558;&#30456;&#21453;&#30340;&#26500;&#35937;&#35270;&#20026;&#36127;&#26679;&#26412;&#23545;&#65292;&#32780;&#26435;&#37325;&#21017;&#30001;&#26500;&#35937;&#20043;&#38388;&#30340;&#24046;&#24322;&#30830;&#23450;&#12290;&#25105;&#20204;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23558;3D-Mol&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17357</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#31227;&#21160;&#26041;&#26696;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#21463;&#38480;&#35774;&#22791;&#29615;&#22659;&#20013;&#65292;&#36138;&#23146;&#30340;&#36880;&#23618;&#25110;&#36880;&#27169;&#22359;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32469;&#36807;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#27492;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20572;&#28382;&#38382;&#39064;&#65292;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#26356;&#28145;&#23618;&#22312;&#19968;&#23450;&#28145;&#24230;&#21518;&#20572;&#27490;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19982;&#20998;&#24067;&#31354;&#38388;&#20013;&#26799;&#24230;&#27969;&#30340;&#26368;&#23567;&#21270;&#31227;&#21160;&#26041;&#27861;&#30456;&#21551;&#21457;&#30340;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;TRGL&#65288;Transport Regularized Greedy Learning&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20250;&#23548;&#33268;&#27169;&#22359;&#21270;&#36138;&#23146;&#26041;&#27861;&#26159;&#35268;&#21017;&#30340;&#65292;&#24182;&#36880;&#27493;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20043;&#21518;&#65292;&#21508;&#31181;&#26550;&#26500;&#65288;&#22914;ResNets&#65292;Transformers&#21644;VGG&#65289;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20854;&#20248;&#20110;&#20854;&#20182;&#27169;&#22359;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#29978;&#33267;&#32463;&#24120;&#20248;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#39640;&#36798;60%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.17348</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25191;&#34892;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;ANNs&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#25913;&#21464;&#36755;&#20837;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;ANNs&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23545;&#25239;&#35757;&#32451;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#28155;&#21152;&#20102;&#26679;&#26412;&#29992;&#20110;&#23545;&#25239;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#22686;&#21152;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26159;&#38750;&#24120;&#35745;&#31639;&#28040;&#32791;&#39640;&#30340;&#12290;&#19982;ANNs&#19981;&#21516;&#65292;&#20154;&#31867;&#19981;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;BP&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;BP&#21644;&#8220;Error to Pertu"&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#28040;&#38500;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#26368;&#22823;&#31243;&#24230;&#20445;&#30041;&#20998;&#31867;&#25928;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#34893;&#29983;&#20986;&#20855;&#26377;&#20154;&#21475;&#24179;&#34913;&#21644;&#30495;&#23454;&#24615;&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2309.17347</link><description>&lt;p&gt;
&#20154;&#21475;&#24179;&#34913;: &#20943;&#36731;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Demographic Parity: Mitigating Biases in Real-World Data. (arXiv:2309.17347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#28040;&#38500;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#26368;&#22823;&#31243;&#24230;&#20445;&#30041;&#20998;&#31867;&#25928;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#34893;&#29983;&#20986;&#20855;&#26377;&#20154;&#21475;&#24179;&#34913;&#21644;&#30495;&#23454;&#24615;&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#20915;&#31574;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#22788;&#29702;&#29983;&#27963;&#20013;&#30340;&#35768;&#22810;&#20915;&#31574;&#65292;&#20854;&#20013;&#21253;&#25324;&#25935;&#24863;&#39046;&#22495;&#65292;&#22914;&#25307;&#32856;&#12289;&#36151;&#27454;&#29978;&#33267;&#21009;&#32602;&#12290;&#20915;&#31574;&#27969;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#21382;&#21490;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21382;&#21490;&#35757;&#32451;&#25968;&#25454;&#32463;&#24120;&#21253;&#21547;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#20854;&#20182;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#20256;&#25773;&#21040;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#24182;&#24433;&#21709;&#35745;&#31639;&#26426;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#31243;&#24230;&#20445;&#30041;&#20998;&#31867;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#28040;&#38500;&#19981;&#24819;&#35201;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#22987;&#32456;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36890;&#36807;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#34893;&#29983;&#20986;&#21807;&#19968;&#32534;&#30721;&#20154;&#21475;&#24179;&#34913;&#21644;&#30495;&#23454;&#24615;&#30340;&#28176;&#36827;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#20174;&#20844;&#20849;&#26222;&#26597;&#35760;&#24405;&#20013;&#25512;&#23548;&#20986;&#36825;&#26679;&#19968;&#20010;&#28176;&#36827;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#26469;&#35757;&#32451;&#25104;&#29087;&#30340;&#20998;&#31867;&#22120;&#12290;&#23545;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-based decision systems are widely used to automate decisions in many aspects of everyday life, which include sensitive areas like hiring, loaning and even criminal sentencing. A decision pipeline heavily relies on large volumes of historical real-world data for training its models. However, historical training data often contains gender, racial or other biases which are propagated to the trained models influencing computer-based decisions. In this work, we propose a robust methodology that guarantees the removal of unwanted biases while maximally preserving classification utility. Our approach can always achieve this in a model-independent way by deriving from real-world data the asymptotic dataset that uniquely encodes demographic parity and realism. As a proof-of-principle, we deduce from public census records such an asymptotic dataset from which synthetic samples can be generated to train well-established classifiers. Benchmarking the generalization capability of these cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#36890;&#29992;&#27169;&#22411;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#36890;&#36807;&#23450;&#20041;&#21644;&#21033;&#29992;&#35821;&#20041;&#27169;&#24335;&#25552;&#21462;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.17342</link><description>&lt;p&gt;
&#12298;&#38754;&#21521;&#36890;&#29992;&#27169;&#22411;&#30340;&#33258;&#30001;&#25968;&#25454;&#36873;&#25321;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Free Data Selection with General-Purpose Models. (arXiv:2309.17342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#36890;&#29992;&#27169;&#22411;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#36890;&#36807;&#23450;&#20041;&#21644;&#21033;&#29992;&#35821;&#20041;&#27169;&#24335;&#25552;&#21462;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#29702;&#24819;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#26368;&#22823;&#21270;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65289;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#32321;&#29712;&#30340;&#27969;&#31243;&#65292;&#21453;&#22797;&#36827;&#34892;&#32791;&#26102;&#30340;&#27169;&#22411;&#35757;&#32451;&#21644;&#25209;&#37327;&#25968;&#25454;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30001;&#25968;&#25454;&#36873;&#25321;&#65288;FreeSel&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#26032;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20174;&#36890;&#29992;&#27169;&#22411;&#30340;&#20013;&#38388;&#29305;&#24449;&#20013;&#25552;&#21462;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#22270;&#20687;&#20013;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36317;&#31163;&#30340;&#37319;&#26679;&#22312;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#27169;&#24335;&#32423;&#21035;&#19978;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;FreeSel&#32469;&#36807;&#20102;&#21407;&#26469;&#30340;&#32791;&#26102;&#35757;&#32451;&#21644;&#25209;&#37327;&#25968;&#25454;&#36873;&#25321;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixQuant&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23618;&#26435;&#37325;&#19978;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17341</link><description>&lt;p&gt;
MixQuant: &#24102;&#26377;&#20301;&#23485;&#20248;&#21270;&#25628;&#32034;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixQuant&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23618;&#26435;&#37325;&#19978;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#21019;&#24314;&#39640;&#25928;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22312;&#27604;f32&#28014;&#28857;&#31934;&#24230;&#26356;&#20302;&#30340;&#20301;&#23485;&#19978;&#25191;&#34892;&#35745;&#31639;&#21644;&#23384;&#20648;&#24352;&#37327;&#26469;&#23454;&#29616;&#12290;&#37327;&#21270;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#21644;&#23454;&#26102;&#31995;&#32479;&#19978;&#37096;&#32626;DNNs&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#33293;&#20837;&#35823;&#24046;&#24341;&#36215;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#35745;&#31639;&#19981;&#20934;&#30830;&#65292;&#36827;&#32780;&#38477;&#20302;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20559;&#32622;&#21644;&#28608;&#27963;&#23545;&#37327;&#21270;&#26356;&#25935;&#24863;&#65292;&#26368;&#22909;&#20445;&#25345;&#20840;&#31934;&#24230;&#25110;&#29992;&#26356;&#39640;&#30340;&#20301;&#23485;&#36827;&#34892;&#37327;&#21270;&#65292;&#25105;&#20204;&#34920;&#26126;&#19968;&#20123;&#26435;&#37325;&#27604;&#20854;&#20182;&#26435;&#37325;&#26356;&#25935;&#24863;&#65292;&#24212;&#22312;&#20854;&#37327;&#21270;&#20301;&#23485;&#19978;&#21453;&#26144;&#20986;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixQuant&#65292;&#19968;&#31181;&#22522;&#20110;&#33293;&#20837;&#35823;&#24046;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#27599;&#20010;&#23618;&#26435;&#37325;&#30340;&#26368;&#20339;&#23450;&#21046;&#37327;&#21270;&#20301;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and
&lt;/p&gt;</description></item><item><title>Outage-Watch&#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#25552;&#26089;&#39044;&#27979;&#20113;&#26381;&#21153;&#20013;&#26029;&#65292;&#36890;&#36807;&#25429;&#33719;&#36136;&#37327;&#25351;&#26631;&#30340;&#24694;&#21270;&#24773;&#20917;&#65292;&#25913;&#21892;&#28789;&#27963;&#24615;&#24182;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17340</link><description>&lt;p&gt;
Outage-Watch: &#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#25552;&#26089;&#39044;&#27979;&#26381;&#21153;&#20013;&#26029;
&lt;/p&gt;
&lt;p&gt;
Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer. (arXiv:2309.17340v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17340
&lt;/p&gt;
&lt;p&gt;
Outage-Watch&#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#25552;&#26089;&#39044;&#27979;&#20113;&#26381;&#21153;&#20013;&#26029;&#65292;&#36890;&#36807;&#25429;&#33719;&#36136;&#37327;&#25351;&#26631;&#30340;&#24694;&#21270;&#24773;&#20917;&#65292;&#25913;&#21892;&#28789;&#27963;&#24615;&#24182;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#26381;&#21153;&#26080;&#22788;&#19981;&#22312;&#65292;&#20851;&#38190;&#20113;&#26381;&#21153;&#30340;&#25925;&#38556;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#20445;&#30041;&#23458;&#25143;&#24182;&#38450;&#27490;&#25910;&#20837;&#25439;&#22833;&#65292;&#25552;&#20379;&#39640;&#21487;&#38752;&#24615;&#20445;&#35777;&#23545;&#20110;&#36825;&#20123;&#26381;&#21153;&#26469;&#35828;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#39044;&#27979;&#26381;&#21153;&#20013;&#26029;&#26159;&#19968;&#31181;&#20943;&#36731;&#20005;&#37325;&#31243;&#24230;&#21644;&#24674;&#22797;&#26102;&#38388;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32597;&#35265;&#24615;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#25925;&#38556;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#25925;&#38556;&#22312;&#21487;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#23450;&#20041;&#27169;&#31946;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Outage-Watch&#23558;&#20851;&#38190;&#30340;&#26381;&#21153;&#20013;&#26029;&#23450;&#20041;&#20026;&#19968;&#32452;&#25351;&#26631;&#25152;&#25429;&#33719;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#24694;&#21270;&#24773;&#20917;&#12290;Outage-Watch&#36890;&#36807;&#20351;&#29992;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#26469;&#39044;&#27979;QoS&#25351;&#26631;&#26159;&#21542;&#20250;&#36229;&#36807;&#38408;&#20540;&#24182;&#24341;&#21457;&#26497;&#31471;&#20107;&#20214;&#65292;&#25552;&#21069;&#26816;&#27979;&#27492;&#31867;&#20013;&#26029;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#27169;&#25311;QoS&#25351;&#26631;&#30340;&#20998;&#24067;&#20197;&#25552;&#39640;&#28789;&#27963;&#24615;&#65292;&#32780;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#28145;&#24230;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#21450;&#38024;&#23545;&#36328;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#35813;&#26550;&#26500;&#22312;&#21333;&#34920;&#21644;&#36328;&#34920;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17339</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36328;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#25193;&#23637;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scaling Experiments in Self-Supervised Cross-Table Representation Learning. (arXiv:2309.17339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#28145;&#24230;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#21450;&#38024;&#23545;&#36328;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#35813;&#26550;&#26500;&#22312;&#21333;&#34920;&#21644;&#36328;&#34920;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20998;&#26512;&#28145;&#24230;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#25193;&#23637;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#19987;&#38376;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#21644;&#36328;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#29305;&#23450;&#30340;&#20998;&#35789;&#22120;&#21644;&#20849;&#20139;&#30340;Transformer&#39592;&#24178;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#21253;&#25324;&#21333;&#34920;&#21644;&#36328;&#34920;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#36974;&#34109;&#21333;&#20803;&#24674;&#22797;&#30446;&#26631;&#36827;&#34892;&#32570;&#22833;&#20540;&#22635;&#20805;&#12290;&#20026;&#20102;&#20102;&#35299;&#25105;&#20204;&#26041;&#27861;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;&#22823;&#32422;$10^4$&#21040;$10^7$&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;76&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;135M&#20010;&#35757;&#32451;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32447;&#24615;&#25506;&#27979;&#26041;&#27861;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#35780;&#20272;&#25105;&#20204;&#26550;&#26500;&#22312;&#21333;&#34920;&#21644;&#36328;&#34920;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17338</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#36890;&#36807;&#21435;&#25481;&#33322;&#28857;&#26469;&#25913;&#36827;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#30340;&#22810;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#26412;&#36136;&#32473;&#20934;&#30830;&#24314;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#24517;&#39035;&#26377;&#25928;&#22320;&#20174;&#36807;&#21435;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22534;&#21472;&#27169;&#22411;&#20013;&#30340;&#21333;&#29420;&#32452;&#20214;&#23398;&#20064;&#26102;&#38388;&#36816;&#21160;&#65292;&#20197;&#25429;&#25417;&#26102;&#38388;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Temporal Waypoint Dropping&#65288;TWD&#65289;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#12290;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#23398;&#20064;&#21487;&#20197;&#36843;&#20351;&#27169;&#22411;&#25913;&#21892;&#20854;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#32852;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#35266;&#27979;&#21040;&#30340;&#36712;&#36857;&#33322;&#28857;&#24207;&#21015;&#26159;&#23436;&#25972;&#30340;&#65292;&#24573;&#30053;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#31649;&#36947;&#24847;&#35782;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#25351;&#21335;&#21644;&#24037;&#20855;&#26469;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17337</link><description>&lt;p&gt;
&#21521;&#23454;&#29616;&#31649;&#36947;&#24847;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#36808;&#36827;&#65306;&#24320;&#21457;&#23454;&#38469;&#25351;&#21335;&#21644;&#24037;&#20855;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#31649;&#36947;&#24847;&#35782;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#25351;&#21335;&#21644;&#24037;&#20855;&#26469;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#20943;&#23569;&#20559;&#35265;&#38382;&#39064;&#24120;&#24120;&#34987;&#31616;&#21270;&#20026;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#36873;&#25321;&#30340;&#20844;&#24179;&#24615;&#24230;&#37327;&#26631;&#20934;&#26469;&#23454;&#29616;&#65292;&#35201;&#20040;&#26159;&#22312;&#20248;&#21270;&#38454;&#27573;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#35201;&#20040;&#26159;&#22312;&#21518;&#22788;&#29702;&#27169;&#22411;&#36755;&#20986;&#26102;&#65292;&#25110;&#32773;&#36890;&#36807;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21628;&#21505;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37319;&#21462;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#25152;&#20570;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#30830;&#23450;&#38024;&#23545;&#38382;&#39064;&#26681;&#26412;&#21407;&#22240;&#32780;&#19981;&#26159;&#20854;&#30151;&#29366;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#23613;&#31649;&#25105;&#20204;&#36190;&#21516;&#36825;&#31181;&#22522;&#20110;&#31649;&#36947;&#30340;&#26041;&#27861;&#26159;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35299;&#20915;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;"&#23454;&#26045;"&#36825;&#31181;&#26041;&#27861;&#12290;&#26681;&#25454;&#25105;&#20204;&#20316;&#20026;&#25945;&#32946;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#27809;&#26377;&#28165;&#26224;&#30340;&#25351;&#21335;&#21644;&#24037;&#20855;&#21253;&#65292;&#21363;&#20351;&#25317;&#26377;&#19987;&#19994;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#30340;&#20010;&#20154;&#20063;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#30340;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#30446;&#26631;&#19988;&#33021;&#23481;&#24525;&#24322;&#24120;&#20540;&#21644;&#23614;&#37325;&#26679;&#26412;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#36845;&#20195;&#25910;&#25947;&#21040;&#38598;&#20013;&#20998;&#24067;&#24182;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17316</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#23454;&#29616;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Stochastic Optimization via Gradient Quantile Clipping. (arXiv:2309.17316v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#30340;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#30446;&#26631;&#19988;&#33021;&#23481;&#24525;&#24322;&#24120;&#20540;&#21644;&#23614;&#37325;&#26679;&#26412;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#36845;&#20195;&#25910;&#25947;&#21040;&#38598;&#20013;&#20998;&#24067;&#24182;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#20998;&#20301;&#25968;&#20316;&#20026;&#21098;&#20999;&#38408;&#20540;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31574;&#30053;&#22312;&#20809;&#28369;&#30446;&#26631;&#65288;&#20984;&#25110;&#38750;&#20984;&#65289;&#19979;&#25552;&#20379;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#23481;&#24525;&#23614;&#37325;&#26679;&#26412;&#65288;&#21253;&#25324;&#26080;&#38480;&#26041;&#24046;&#65289;&#21644;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#31867;&#20284;&#20110; Huber &#27745;&#26579;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#21033;&#29992;&#20102;&#24658;&#23450;&#27493;&#38271;&#30340; SGD &#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#20197;&#29420;&#29305;&#30340;&#26041;&#24335;&#22788;&#29702;&#21098;&#20999;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010;&#38598;&#20013;&#20998;&#24067;&#65292;&#24182;&#23548;&#20986;&#20102;&#26368;&#32456;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#27492;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds. We prove that this new strategy provides a robust and efficient optimization algorithm for smooth objectives (convex or non-convex), that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliers in the data stream akin to Huber contamination. Our mathematical analysis leverages the connection between constant step size SGD and Markov chains and handles the bias introduced by clipping in an original way. For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error. In the non-convex case, we prove that the limit distribution is localized on a neighborhood with low gradient. We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustn
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.17310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#23569;&#37327;&#25968;&#25454;&#28857;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#23450;&#20041;&#20026;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;(LOOD)&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26469;&#24314;&#27169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20351;&#29992;&#24191;&#27867;&#30340;&#32463;&#39564;&#20998;&#26512;&#39564;&#35777;&#20102;LOOD&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#20197;&#21450;&#27844;&#28431;&#31243;&#24230;&#39640;&#30340;&#20301;&#32622;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#25968;&#25454;&#35760;&#24518;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20248;&#21270;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#21644;&#20998;&#31867;&#30340;&#21270;&#23398;&#20803;&#32032;&#19982;&#38190;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#25913;&#36827;&#65292;&#20854;&#22312;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#26041;&#38754;&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36807;&#24050;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17296</link><description>&lt;p&gt;
&#23548;&#33322;&#31561;&#21464;&#25193;&#25955;&#22522;&#29983;&#25104;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#29992;&#20110;&#20174;&#22836;&#29983;&#25104;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation. (arXiv:2309.17296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#21644;&#20998;&#31867;&#30340;&#21270;&#23398;&#20803;&#32032;&#19982;&#38190;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#25913;&#36827;&#65292;&#20854;&#22312;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#26041;&#38754;&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36807;&#24050;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26159;&#26448;&#26009;&#31185;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22823;&#20998;&#23376;&#32467;&#26500;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#37325;&#28857;&#20851;&#27880;&#20197;&#21069;&#30340;&#31354;&#30333;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#20854;&#22312;QM9&#21644;GEOM-Drugs&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#22987;&#32456;&#22823;&#22823;&#36229;&#36807;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;EQGAT-diff&#37319;&#29992;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#65292;&#32780;&#21270;&#23398;&#20803;&#32032;&#21644;&#38190;&#31867;&#22411;&#26159;&#20998;&#31867;&#30340;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#21152;&#26435;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#25910;&#25947;&#21644;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#23545;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep generative diffusion models are a promising avenue for de novo 3D molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.17290</link><description>&lt;p&gt;
&#25628;&#32034;&#20998;&#25955;&#30340;&#35760;&#24518;&#65306;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26159;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20316;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#31616;&#21270;&#29702;&#35770;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#29289;&#20851;&#32852;&#35760;&#24518;&#12290;&#21407;&#22987;&#30340;Hopfield&#32593;&#32476;&#36890;&#36807;&#32534;&#30721;&#20108;&#20803;&#20851;&#32852;&#27169;&#24335;&#26469;&#23384;&#20648;&#35760;&#24518;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#31216;&#20026;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31361;&#35302;&#23398;&#20064;&#26426;&#21046;&#12290;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#23454;&#29616;&#25351;&#25968;&#32423;&#23481;&#37327;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#33021;&#37327;&#20989;&#25968;&#19981;&#33021;&#30452;&#25509;&#21387;&#32553;&#20026;&#20108;&#20803;&#31361;&#35302;&#32806;&#21512;&#65292;&#24182;&#19988;&#20063;&#19981;&#33021;&#30452;&#25509;&#25552;&#20379;&#26032;&#30340;&#31361;&#35302;&#23398;&#20064;&#35268;&#21017;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#30456;&#31561;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#65288;RVD&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#20043;&#21069;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17278</link><description>&lt;p&gt;
&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#22312;&#20581;&#22766;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Recommendation via Real-time Vicinal Defense. (arXiv:2309.17278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#65288;RVD&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#20043;&#21069;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24694;&#24847;&#25968;&#25454;&#25554;&#20837;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#25552;&#20379;&#26377;&#20559;&#35265;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20581;&#22766;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#25110;&#29305;&#23450;&#20110;&#25915;&#20987;&#30340;&#65292;&#32570;&#20047;&#24191;&#27867;&#24615;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65289;&#20391;&#37325;&#20110;&#36867;&#36920;&#25915;&#20987;&#65292;&#22312;&#27602;&#21270;&#25915;&#20987;&#19978;&#26377;&#24456;&#24369;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#8212;&#8212;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#65288;RVD&#65289;&#65292;&#23427;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#22312;&#20026;&#27599;&#20010;&#29992;&#25143;&#36827;&#34892;&#25512;&#33616;&#20043;&#21069;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;RVD&#22312;&#25512;&#26029;&#38454;&#27573;&#24037;&#20316;&#65292;&#20197;&#30830;&#20445;&#23454;&#26102;&#24615;&#26679;&#26412;&#30340;&#20581;&#22766;&#24615;&#65292;&#22240;&#27492;&#26080;&#38656;&#26356;&#25913;&#27169;&#22411;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#26356;&#21152;&#23454;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RVD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have been shown to be vulnerable to poisoning attacks, where malicious data is injected into the dataset to cause the recommender system to provide biased recommendations. To defend against such attacks, various robust learning methods have been proposed. However, most methods are model-specific or attack-specific, making them lack generality, while other methods, such as adversarial training, are oriented towards evasion attacks and thus have a weak defense strength in poisoning attacks.  In this paper, we propose a general method, Real-time Vicinal Defense (RVD), which leverages neighboring training data to fine-tune the model before making a recommendation for each user. RVD works in the inference phase to ensure the robustness of the specific sample in real-time, so there is no need to change the model structure and training process, making it more practical. Extensive experimental results demonstrate that RVD effectively mitigates targeted poisoning attacks acr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#36125;&#21494;&#26031;&#24515;&#26234;&#35770;&#26426;&#21046;&#35774;&#35745;&#20102;&#25945;&#24072;&#20195;&#29702;&#65292;&#20854;&#36890;&#36807;&#26500;&#24314;&#23398;&#20064;&#32773;&#20869;&#37096;&#29366;&#24577;&#27169;&#22411;&#24182;&#21033;&#29992;&#27169;&#22411;&#36873;&#25321;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#32773;&#22238;&#25253;&#21516;&#26102;&#26368;&#23567;&#21270;&#25945;&#23398;&#25104;&#26412;&#30340;&#31034;&#33539;&#36827;&#34892;&#20010;&#24615;&#21270;&#25945;&#23398;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#25945;&#23398;&#26041;&#24335;&#27604;&#20256;&#32479;&#19981;&#21487;&#30693;&#25945;&#23398;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.17275</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#24515;&#26234;&#35770;&#30340;&#25928;&#29992;&#23548;&#21521;&#33258;&#36866;&#24212;&#25945;&#23398;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind. (arXiv:2309.17275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#36125;&#21494;&#26031;&#24515;&#26234;&#35770;&#26426;&#21046;&#35774;&#35745;&#20102;&#25945;&#24072;&#20195;&#29702;&#65292;&#20854;&#36890;&#36807;&#26500;&#24314;&#23398;&#20064;&#32773;&#20869;&#37096;&#29366;&#24577;&#27169;&#22411;&#24182;&#21033;&#29992;&#27169;&#22411;&#36873;&#25321;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#32773;&#22238;&#25253;&#21516;&#26102;&#26368;&#23567;&#21270;&#25945;&#23398;&#25104;&#26412;&#30340;&#31034;&#33539;&#36827;&#34892;&#20010;&#24615;&#21270;&#25945;&#23398;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#25945;&#23398;&#26041;&#24335;&#27604;&#20256;&#32479;&#19981;&#21487;&#30693;&#25945;&#23398;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#31168;&#30340;&#25945;&#24072;&#24635;&#26159;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#24773;&#20917;&#26469;&#36827;&#34892;&#35299;&#37322;&#12290;&#35748;&#30693;&#31185;&#23398;&#23478;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#24314;&#27169;&#20026;&#22312;&#29702;&#24615;&#21407;&#21017;&#19979;&#36827;&#34892;&#30340;&#65306;&#25945;&#24072;&#23613;&#21487;&#33021;&#22320;&#26368;&#22823;&#21270;&#23398;&#20064;&#32773;&#30340;&#25928;&#29992;&#21516;&#26102;&#26368;&#23567;&#21270;&#25945;&#23398;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#20154;&#31867;&#25945;&#24072;&#20284;&#20046;&#20250;&#24314;&#31435;&#36215;&#23398;&#20064;&#32773;&#20869;&#37096;&#29366;&#24577;&#30340;&#24515;&#26234;&#27169;&#22411;&#65292;&#36825;&#19968;&#33021;&#21147;&#34987;&#31216;&#20026;&#24515;&#26234;&#35770;&#65288;Theory of Mind&#65292;ToM&#65289;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20511;&#37492;&#36125;&#21494;&#26031;ToM&#26426;&#21046;&#26469;&#35774;&#35745;&#31867;&#20154;&#25945;&#24072;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#20687;&#20154;&#31867;&#19968;&#26679;&#26681;&#25454;&#23398;&#20064;&#32773;&#26469;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24102;&#26377;ToM&#30340;&#25945;&#24072;&#20195;&#29702;&#26681;&#25454;&#35266;&#23519;&#24314;&#31435;&#36215;&#23398;&#20064;&#32773;&#20869;&#37096;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#36873;&#25321;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#32773;&#22238;&#25253;&#21516;&#26102;&#26368;&#23567;&#21270;&#25945;&#23398;&#25104;&#26412;&#30340;&#31034;&#33539;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#25945;&#25480;&#30340;&#23398;&#20064;&#32773;&#27604;&#20197;&#23398;&#20064;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#25945;&#25480;&#30340;&#23398;&#20064;&#32773;&#26356;&#21152;&#39640;&#25928;&#12290;&#24403;&#25945;&#24072;&#23545;&#23398;&#20064;&#32773;&#30340;&#27169;&#22411;&#19982;&#23454;&#38469;&#23398;&#20064;&#32773;&#30340;&#29366;&#24577;&#26356;&#21152;&#19968;&#33268;&#26102;&#65292;&#36825;&#31181;&#25928;&#26524;&#21464;&#24471;&#26356;&#21152;&#26174;&#33879;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good teachers always tailor their explanations to the learners. Cognitive scientists model this process under the rationality principle: teachers try to maximise the learner's utility while minimising teaching costs. To this end, human teachers seem to build mental models of the learner's internal state, a capacity known as Theory of Mind (ToM). Inspired by cognitive science, we build on Bayesian ToM mechanisms to design teacher agents that, like humans, tailor their teaching strategies to the learners. Our ToM-equipped teachers construct models of learners' internal states from observations and leverage them to select demonstrations that maximise the learners' rewards while minimising teaching costs. Our experiments in simulated environments demonstrate that learners taught this way are more efficient than those taught in a learner-agnostic way. This effect gets stronger when the teacher's model of the learner better aligns with the actual learner's state, either using a more accurate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17262</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#25928;&#29575;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#26088;&#22312;&#20272;&#35745;&#30001;&#32473;&#23450;&#31574;&#30053;&#960;&#33719;&#24471;&#30340;&#38543;&#26426;&#22238;&#25253;&#30340;&#23436;&#25972;&#20998;&#24067;&#65288;&#34920;&#31034;&#20026;&#951;^&#960;&#65289;&#12290;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#26500;&#36896;&#20102;&#20272;&#35745;&#22120;&#951;^&#960;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^(2p)(1-&#947;)^(2p+2)))&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#20445;&#35777;&#20272;&#35745;&#22120;&#951;^&#960;&#21644;&#30495;&#23454;&#20998;&#24067;&#951;^&#960;&#20043;&#38388;&#30340;p-Wasserstein&#36317;&#31163;&#23567;&#20110;&#949;&#30340;&#27010;&#29575;&#24456;&#39640;&#12290;&#36825;&#24847;&#21619;&#30528;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#21487;&#20197;&#20197;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#30340;&#26041;&#24335;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^2(1-&#947;)^4))&#30340;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#30830;&#20445;Kolmogorov&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.  We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}}\right)$ suffices to ensure the Kolmogorov metric and total variation m
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;Thirring&#27169;&#22411;&#20013;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#22495;&#27874;&#35299;&#21644;&#21442;&#25968;&#21457;&#29616;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#23616;&#22495;&#27874;&#35299;&#65292;&#21253;&#25324;&#23396;&#23376;&#12289;&#21628;&#21560;&#23376;&#21644;&#27969;&#27667;&#27874;&#12290;&#36890;&#36807;&#25193;&#23637;PINNs&#21644;&#22495;&#20998;&#35299;&#26041;&#27861;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#39640;&#38454;&#23616;&#22495;&#27874;&#35299;&#30340;&#21160;&#24577;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#20462;&#25913;&#30028;&#38754;&#32447;&#21644;&#24341;&#20837;&#30028;&#38754;&#26465;&#20214;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.17240</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#22495;&#27874;&#21644;&#36890;&#36807;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;Thirring&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Data-driven localized waves and parameter discovery in the massive Thirring model via extended physics-informed neural networks with interface zones. (arXiv:2309.17240v1 [nlin.PS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;Thirring&#27169;&#22411;&#20013;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#22495;&#27874;&#35299;&#21644;&#21442;&#25968;&#21457;&#29616;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#23616;&#22495;&#27874;&#35299;&#65292;&#21253;&#25324;&#23396;&#23376;&#12289;&#21628;&#21560;&#23376;&#21644;&#27969;&#27667;&#27874;&#12290;&#36890;&#36807;&#25193;&#23637;PINNs&#21644;&#22495;&#20998;&#35299;&#26041;&#27861;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#39640;&#38454;&#23616;&#22495;&#27874;&#35299;&#30340;&#21160;&#24577;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#20462;&#25913;&#30028;&#38754;&#32447;&#21644;&#24341;&#20837;&#30028;&#38754;&#26465;&#20214;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#22495;&#27874;&#35299;&#21644;&#22312;&#22823;Thirring&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21457;&#29616;&#12290;&#36890;&#36807;&#31934;&#30830;&#27169;&#25311;&#21644;&#30456;&#23545;&#35823;&#24046;&#12289;&#32477;&#23545;&#35823;&#24046;&#30340;&#23545;&#27604;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#65292;&#21253;&#25324;&#26126;&#20142;/&#26263;&#28129;&#22411;&#23396;&#23376;&#65292;&#21628;&#21560;&#23376;&#21644;&#27969;&#27667;&#27874;&#12290;&#23545;&#20110;&#39640;&#38454;&#23616;&#22495;&#27874;&#35299;&#65292;&#25105;&#20204;&#37319;&#29992;&#25193;&#23637;PINNs&#65288;XPINNs&#65289;&#21644;&#22495;&#20998;&#35299;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#21160;&#24577;&#34892;&#20026;&#30340;&#23436;&#25972;&#22270;&#20687;&#65292;&#22914;&#23396;&#23376;&#30896;&#25758;&#65292;&#21628;&#21560;&#23376;&#25391;&#33633;&#21644;&#27969;&#27667;&#27874;&#21472;&#21152;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;XPINNs&#20013;&#30340;&#30028;&#38754;&#32447;&#20462;&#25913;&#20026;&#19968;&#20010;&#23567;&#30340;&#30028;&#38754;&#21306;&#22495;&#65292;&#24182;&#24341;&#20837;&#20266;&#21021;&#20540;&#12289;&#27531;&#24046;&#21644;&#26799;&#24230;&#26465;&#20214;&#20316;&#20026;&#19982;&#21508;&#20010;&#31070;&#32463;&#32593;&#32476;&#30456;&#37051;&#30340;&#30028;&#38754;&#26465;&#20214;&#12290;&#28982;&#21518;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#36825;&#31181;&#20462;&#25913;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#35299;&#65292;&#21253;&#25324;&#26126;&#20142;-&#26126;&#20142;&#30340;&#23396;&#23376;&#65292;&#26263;&#28129;-&#26263;&#28129;&#30340;&#23396;&#23376;&#65292;&#21644;&#26263;&#28129;-&#26126;&#20142;&#30340;&#23396;&#23376;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study data-driven localized wave solutions and parameter discovery in the massive Thirring (MT) model via the deep learning in the framework of physics-informed neural networks (PINNs) algorithm. Abundant data-driven solutions including soliton of bright/dark type, breather and rogue wave are simulated accurately and analyzed contrastively with relative and absolute errors. For higher-order localized wave solutions, we employ the extended PINNs (XPINNs) with domain decomposition to capture the complete pictures of dynamic behaviors such as soliton collisions, breather oscillations and rogue-wave superposition. In particular, we modify the interface line in domain decomposition of XPINNs into a small interface zone and introduce the pseudo initial, residual and gradient conditions as interface conditions linked adjacently with individual neural networks. Then this modified approach is applied successfully to various solutions ranging from bright-bright soliton, dark-da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.17234</link><description>&lt;p&gt;
LLM-&#36777;&#35770;: &#20351;&#29992;&#20132;&#20114;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21830;&#28216;&#25103;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#26469;&#35299;&#20915;&#21487;&#33021;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#24773;&#20917;&#30340;&#29616;&#23454;&#20219;&#21153;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#26377;&#38480;&#30340;&#29702;&#35299;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#30001;&#20110;&#35848;&#21028;&#21644;&#22949;&#21327;&#26159;&#25105;&#20204;&#26085;&#24120;&#27807;&#36890;&#21644;&#21512;&#20316;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#12289;&#22810;&#38382;&#39064;&#30340;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#35848;&#21028;&#28216;&#25103;&#27979;&#35797;&#24179;&#21488;&#65292;&#38590;&#24230;&#21487;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#20855;&#22791;&#24378;&#22823;&#30340;&#31639;&#26415;&#12289;&#25512;&#29702;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#21516;&#26102;&#26080;&#32541;&#22320;&#25972;&#21512;&#23427;&#20204;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#36827;&#34892;&#35848;&#21028;&#24182;&#25345;&#32493;&#36798;&#25104;&#25104;&#21151;&#20132;&#26131;&#12290;&#25105;&#20204;&#29992;&#22810;&#20010;&#25351;&#26631;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;GPT-4&#19982;&#21407;&#25991;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#25104;&#21151;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#20943;&#23569;&#20102;&#20998;&#24067;&#22806;&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.17230</link><description>&lt;p&gt;
&#20266;&#29305;&#24449;&#22810;&#26679;&#24615;&#25913;&#21892;&#20102;&#23545;&#20998;&#24067;&#22806;&#27867;&#21270;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Spurious Feature Diversification Improves Out-of-distribution Generalization. (arXiv:2309.17230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#25104;&#21151;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#20943;&#23569;&#20102;&#20998;&#24067;&#22806;&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#27867;&#21270;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#12290;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#25554;&#20540;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;OOD&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#31181;&#24120;&#29992;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#21363;WiSE-FT&#25104;&#21151;&#22320;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#36825;&#23545;&#20110;&#20854;OOD&#30340;&#26377;&#25928;&#24615;&#36129;&#29486;&#37325;&#22823;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22823;&#37327;&#20266;&#29305;&#24449;&#30340;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#39044;&#27979;&#20102;&#19978;&#36848;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22522;&#20110;&#38598;&#25104;&#30340;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#65292;&#20943;&#23569;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional
&lt;/p&gt;</description></item><item><title>MORPH&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#65292;MORPH&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20248;&#21270;&#24182;&#20445;&#25345;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#25509;&#36817;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.17227</link><description>&lt;p&gt;
MORPH&#65306;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35774;&#35745;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy. (arXiv:2309.17227v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17227
&lt;/p&gt;
&lt;p&gt;
MORPH&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#65292;MORPH&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20248;&#21270;&#24182;&#20445;&#25345;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#25509;&#36817;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MORPH&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#19982;&#22823;&#22810;&#25968;&#21327;&#21516;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#65292;MORPH&#20381;&#36182;&#20110;&#27491;&#22312;&#20248;&#21270;&#30340;&#30828;&#20214;&#27169;&#22411;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#38598;&#25104;&#21040;&#26377;&#25928;&#30340;&#20248;&#21270;&#20363;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20195;&#29702;&#30828;&#20214;&#27169;&#22411;&#65292;&#23427;&#22987;&#32456;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#29992;RL&#21644;&#38271;&#26399;&#25511;&#21046;&#31574;&#30053;&#26377;&#25928;&#22320;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;MORPH&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#30830;&#20445;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#19982;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#23613;&#21487;&#33021;&#25509;&#36817;&#65292;&#21516;&#26102;&#20173;&#28982;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#30340;2D&#20280;&#25163;&#21644;3D&#22810;&#25351;&#25805;&#32437;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;FP8&#32447;&#24615;&#23618;&#30340;&#32553;&#25918;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#27599;&#20010;&#24352;&#37327;&#30340;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#23610;&#24230;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17224</link><description>&lt;p&gt;
&#20351;&#29992;8&#20301;&#28014;&#28857;&#25968;&#35757;&#32451;&#21644;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;FP8&#32447;&#24615;&#23618;&#30340;&#32553;&#25918;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#27599;&#20010;&#24352;&#37327;&#30340;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#23610;&#24230;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FP8&#26684;&#24335;&#27491;&#22312;&#21463;&#21040;&#38738;&#30544;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#26029;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#32553;&#25918;&#20197;&#38450;&#27490;&#30001;&#20110;&#36739;&#39640;&#31934;&#24230;&#26684;&#24335;&#30340;&#21160;&#24577;&#33539;&#22260;&#30340;&#20943;&#23569;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#23613;&#31649;&#20851;&#20110;&#36873;&#25321;INT&#26684;&#24335;&#30340;&#36825;&#20123;&#32553;&#25918;&#22240;&#23376;&#30340;&#25991;&#29486;&#24456;&#22810;&#65292;&#20294;&#23545;&#20110;FP8&#26469;&#35828;&#65292;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#27599;&#20010;&#24352;&#37327;&#23610;&#24230;&#30340;FP8&#32447;&#24615;&#23618;&#32553;&#25918;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;FP8&#35757;&#32451;&#21644;&#39564;&#35777;GPT&#21644;Llama 2&#31561;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27169;&#22411;&#22823;&#23567;&#33539;&#22260;&#20174;111M&#21040;70B&#19981;&#31561;&#12290;&#20026;&#20102;&#20415;&#20110;&#29702;&#35299;FP8&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#38468;&#24102;&#20102;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#27599;&#20010;&#24352;&#37327;&#23610;&#24230;&#20998;&#24067;&#30340;&#22270;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.
&lt;/p&gt;</description></item><item><title>RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17215</link><description>&lt;p&gt;
RSAM&#65306;&#20351;&#29992;Riemannian Sharpness-aware Minimization&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17215
&lt;/p&gt;
&lt;p&gt;
RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#26377;&#26395;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20043;&#21069;&#23558;&#20960;&#20309;&#21407;&#29702;&#24212;&#29992;&#20110;&#20248;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#23558;Sharpness-Aware Minimization (SAM)&#20248;&#21270;&#22120;&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#12290;&#20026;&#20102;&#25903;&#25345;&#27969;&#24418;&#19978;&#30340;&#8220;sharpness&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#27969;&#24418;&#19978;&#30340;sharpness&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#25551;&#36848;&#27969;&#24418;sharpness&#19982;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#32570;&#21475;&#19978;&#38480;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;Riemannian Sharpness-Aware Minimization (RSAM)&#12290;&#20026;&#20102;&#23637;&#31034;RSAM&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;&#21644;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HASTE&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#21363;&#21487;&#23454;&#26102;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#29305;&#24449;&#22270;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17211</link><description>&lt;p&gt;
&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#22312;CNN&#20013;&#23454;&#29616;&#21363;&#26102;&#22797;&#26434;&#24230;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing. (arXiv:2309.17211v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HASTE&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#21363;&#21487;&#23454;&#26102;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#29305;&#24449;&#22270;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22312;&#19981;&#22826;&#22823;&#31243;&#24230;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#20943;&#23569;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#26041;&#27861;&#35201;&#27714;&#36827;&#34892;&#31934;&#35843;&#25110;&#29305;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#20445;&#30041;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;FLOPs&#20043;&#38388;&#21512;&#29702;&#25240;&#34935;&#12290;&#36825;&#24341;&#20837;&#20102;&#35745;&#31639;&#24320;&#38144;&#30340;&#39069;&#22806;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HASTE&#65288;Hashing for Tractable Efficiency&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#21644;&#26080;&#38656;&#25968;&#25454;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#24120;&#35268;&#21367;&#31215;&#27169;&#22359;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#21363;&#26102;&#38477;&#20302;&#32593;&#32476;&#30340;&#27979;&#35797;&#25512;&#29702;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#22270;&#20013;&#30340;&#20887;&#20313;&#65292;&#25105;&#20204;&#33021;&#22815;&#22823;&#24133;&#21387;&#32553;&#28508;&#22312;&#29305;&#24449;&#22270;&#32780;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reduce the computational cost of convolutional neural networks (CNNs) for usage on resource-constrained devices, structured pruning approaches have shown promising results, drastically reducing floating-point operations (FLOPs) without substantial drops in accuracy. However, most recent methods require fine-tuning or specific training procedures to achieve a reasonable trade-off between retained accuracy and reduction in FLOPs. This introduces additional cost in the form of computational overhead and requires training data to be available. To this end, we propose HASTE (Hashing for Tractable Efficiency), a parameter-free and data-free module that acts as a plug-and-play replacement for any regular convolution module. It instantly reduces the network's test-time inference cost without requiring any training or fine-tuning. We are able to drastically compress latent feature maps without sacrificing much accuracy by using locality-sensitive hashing (LSH) to detect redundancies in the c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26550;&#26500;&#39044;&#27979;&#20154;&#31867;&#26410;&#26469;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;&#27169;&#22411;&#22312;&#24120;&#35265;&#30340;&#39044;&#27979;&#22522;&#20934;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#35299;&#20915;&#20102;&#21382;&#21490;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20195;&#29702;&#23548;&#33268;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17209</link><description>&lt;p&gt;
&#33021;&#22815;&#30475;&#35265;&#30340;&#26426;&#22120;&#20154;&#65306;&#21033;&#29992;&#20154;&#20307;&#23039;&#21183;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robots That Can See: Leveraging Human Pose for Trajectory Prediction. (arXiv:2309.17209v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26550;&#26500;&#39044;&#27979;&#20154;&#31867;&#26410;&#26469;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;&#27169;&#22411;&#22312;&#24120;&#35265;&#30340;&#39044;&#27979;&#22522;&#20934;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#35299;&#20915;&#20102;&#21382;&#21490;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20195;&#29702;&#23548;&#33268;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#23478;&#24237;&#21644;&#21150;&#20844;&#23460;&#31561;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#39044;&#27979;&#25152;&#26377;&#20154;&#31867;&#30340;&#36816;&#21160;&#23545;&#20110;&#23454;&#29616;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#31354;&#38388;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#19981;&#36981;&#24490;&#20005;&#26684;&#30340;&#36816;&#21160;&#35268;&#21017;&#65292;&#32780;&#19988;&#36890;&#24120;&#23384;&#22312;&#22810;&#20010;&#34987;&#36974;&#25377;&#30340;&#20837;&#21475;&#28857;&#65292;&#22914;&#25296;&#35282;&#21644;&#38376;&#65292;&#22312;&#36825;&#20123;&#22320;&#26041;&#23481;&#26131;&#21457;&#29983;&#31361;&#28982;&#30456;&#36935;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20174;&#21253;&#25324;&#20154;&#31867;&#20301;&#32622;&#12289;&#22836;&#37096;&#26041;&#21521;&#21644;&#19977;&#32500;&#39592;&#39612;&#20851;&#38190;&#28857;&#22312;&#20869;&#30340;&#36755;&#20837;&#29305;&#24449;&#20013;&#65292;&#39044;&#27979;&#20154;&#31867;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#25152;&#24471;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#26410;&#26469;&#20154;&#31867;&#36712;&#36857;&#39044;&#27979;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#39044;&#27979;&#22522;&#20934;&#21644;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#20154;&#31867;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21382;&#21490;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20195;&#29702;&#26159;&#38169;&#35823;&#30340;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#65292;&#24182;&#23637;&#31034;&#20102;&#34917;&#20805;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the motion of all humans in dynamic environments such as homes and offices is critical to enable safe and effective robot navigation. Such spaces remain challenging as humans do not follow strict rules of motion and there are often multiple occluded entry points such as corners and doors that create opportunities for sudden encounters. In this work, we present a Transformer based architecture to predict human future trajectories in human-centric environments from input features including human positions, head orientations, and 3D skeletal keypoints from onboard in-the-wild sensory information. The resulting model captures the inherent uncertainty for future human trajectory prediction and achieves state-of-the-art performance on common prediction benchmarks and a human tracking dataset captured from a mobile robot adapted for the prediction task. Furthermore, we identify new agents with limited historical data as a major contributor to error and demonstrate the complementa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#21457;&#29616;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#20197;60-70%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#19988;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.17197</link><description>&lt;p&gt;
&#22522;&#20110;&#20083;&#33146;DCE-MRI&#29305;&#24449;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features. (arXiv:2309.17197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#21457;&#29616;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#20197;60-70%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#19988;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#20351;&#29992;&#21463;&#20445;&#25252;&#23646;&#24615;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#65292;&#20294;&#26159;&#21033;&#29992;&#25163;&#24037;&#29305;&#24449;&#30340;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20063;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#26159;&#21033;&#29992;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE-MRI&#65289;&#39044;&#27979;&#32959;&#30244;&#20998;&#23376;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#30830;&#23454;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;RF&#27169;&#22411;&#21487;&#20197;&#20197;60-70&#65285;&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#35757;&#32451;&#30340;RF&#27169;&#22411;&#20284;&#20046;&#20250;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#26576;&#31181;&#31243;&#24230;&#30340;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60-70% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting bet
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17196</link><description>&lt;p&gt;
ResBit: &#22522;&#20110;&#27531;&#24046;&#20301;&#21521;&#37327;&#30340;&#31163;&#25955;&#20540;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#29420;&#28909;&#32534;&#30721;&#21521;&#37327;&#19968;&#30452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#32500;&#24230;&#38543;&#30528;&#35201;&#34920;&#31034;&#30340;&#31163;&#25955;&#25968;&#25454;&#32447;&#24615;&#22686;&#21152;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35270;&#20026;&#31354;&#38388;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20301;&#24207;&#21015;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21363;Analog Bits&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#35201;&#34920;&#31034;&#30340;&#31867;&#21035;&#31867;&#22411;&#25968;&#37327;&#19981;&#19968;&#23450;&#26159;2&#30340;&#24130;&#27425;&#65292;&#23548;&#33268;Analog Bits&#33021;&#22815;&#34920;&#31034;&#30340;&#33539;&#22260;&#19982;&#31867;&#21035;&#25968;&#25454;&#30340;&#33539;&#22260;&#23384;&#22312;&#24046;&#24322;&#12290;&#22914;&#26524;&#29983;&#25104;&#20102;&#36825;&#26679;&#30340;&#20540;&#65292;&#38382;&#39064;&#23601;&#26159;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#30340;&#31867;&#21035;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#20301;&#21521;&#37327;&#65288;ResBit&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618;&#30340;&#20301;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#20854;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;&#65292;&#25193;&#23637;&#20026;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;ReLU&#28608;&#27963;&#30340;FNN&#12290;</title><link>http://arxiv.org/abs/2309.17194</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#21464;&#37327;&#25237;&#24433;&#36827;&#34892;&#24191;&#20041;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#20854;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;&#65292;&#25193;&#23637;&#20026;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;ReLU&#28608;&#27963;&#30340;FNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;Rectified Linear Unit (ReLU)&#24120;&#22240;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#32780;&#21463;&#38738;&#30544;&#12290;&#21463;&#27973;&#23618;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476; (FNN) &#21644;&#21333;&#27425;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477; (PGD) &#31639;&#27861;&#20043;&#38388;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#12290;&#22312;&#36825;&#20010;&#35299;&#37322;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;ReLU&#65292;&#22914;&#20108;&#38454;&#38181; (SOC) &#25237;&#24433;&#65292;&#20174;&#32780;&#23558;&#20854;&#33258;&#28982;&#22320;&#25193;&#23637;&#20026;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#65292;&#36825;&#26159;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#25968;&#23398;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SOC&#25237;&#24433;&#28608;&#27963;&#30340;FNN&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20351;&#29992;ReLU&#30340;FNN&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#30740;&#31350;&#20102;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#35843;&#26597;&#36824;&#25506;&#35752;&#20102;&#22312;&#22810;&#20013;&#24515;&#21327;&#20316;&#20013;&#65292;&#19981;&#21516;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17192</link><description>&lt;p&gt;
&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#35843;&#26597;: &#23558;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#19982;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#30456;&#32467;&#21512;&#29992;&#20110;&#22810;&#20013;&#24515;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration. (arXiv:2309.17192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#30740;&#31350;&#20102;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#35843;&#26597;&#36824;&#25506;&#35752;&#20102;&#22312;&#22810;&#20013;&#24515;&#21327;&#20316;&#20013;&#65292;&#19981;&#21516;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#65292;&#22810;&#20010;&#20020;&#24202;&#20013;&#24515;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#38459;&#30861;&#20102;&#20174;&#22810;&#20013;&#24515;&#21327;&#20316;&#20013;&#24320;&#21457;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26420;&#32032;&#30340;&#26435;&#37325;&#36716;&#31227;&#26041;&#27861;&#22312;&#27809;&#26377;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20998;&#20139;&#20013;&#38388;&#27169;&#22411;&#26435;&#37325;&#65292;&#22240;&#27492;&#21487;&#20197;&#32469;&#36807;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#22312;&#27169;&#22411;&#20174;&#19968;&#20010;&#20013;&#24515;&#36716;&#31227;&#21040;&#19979;&#19968;&#20010;&#20013;&#24515;&#26102;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#36951;&#24536;&#38382;&#39064;&#12290;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23558;&#20256;&#32479;&#30340;&#39046;&#22495;/&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#12290;&#23545;&#20110;&#22810;&#20013;&#24515;&#21327;&#20316;&#65292;&#36824;&#36827;&#34892;&#20102;&#23545;&#19981;&#21516;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39046;&#22495;&#30693;&#35782;&#20849;&#20139;&#23545;&#22686;&#37327;&#36801;&#31227;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Due to data privacy constraints, data sharing among multiple clinical centers is restricted, which impedes the development of high performance deep learning models from multicenter collaboration. Naive weight transfer methods share intermediate model weights without raw data and hence can bypass data privacy restrictions. However, performance drops are typically observed when the model is transferred from one center to the next because of the forgetting problem. Incremental transfer learning, which combines peer-to-peer federated learning and domain incremental learning, can overcome the data privacy issue and meanwhile preserve model performance by using continual learning techniques. In this work, a conventional domain/task incremental learning framework is adapted for incremental transfer learning. A comprehensive survey on the efficacy of different regularization-based continual learning methods for multicenter collaboration is performed. The influences of data heterogeneity, class
&lt;/p&gt;</description></item><item><title>RECOMBINER&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#12289;&#22686;&#21152;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#21106;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#26469;&#35299;&#20915;COMBINER&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17182</link><description>&lt;p&gt;
RECOMBINER&#65306;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17182
&lt;/p&gt;
&lt;p&gt;
RECOMBINER&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#12289;&#22686;&#21152;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#21106;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#26469;&#35299;&#20915;COMBINER&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COMBINER&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#30340;&#20851;&#38190;&#25928;&#29575;&#38382;&#39064;&#65306;&#36991;&#20813;&#20102;&#37327;&#21270;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#36895;&#29575;-&#22833;&#30495;&#24615;&#33021;&#30340;&#30452;&#25509;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;COMBINER&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#65306;1&#65289;&#20351;&#29992;&#22240;&#24335;&#21270;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#36924;&#36817;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#65307;2&#65289;&#19981;&#33021;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#20559;&#31163;&#20840;&#23616;&#27169;&#24335;&#65307;3&#65289;&#20854;&#24615;&#33021;&#26131;&#21463;&#24314;&#27169;&#36873;&#25321;&#21644;&#21464;&#20998;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#40065;&#26834;&#21644;&#22686;&#24378;&#30340;COMBINER(RECOMBINER)&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65306;1&#65289;&#36890;&#36807;&#23545;INR&#26435;&#37325;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#65292;&#24182;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#65307;2&#65289;&#36890;&#36807;&#22686;&#21152;&#21487;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;INR&#65292;&#20351;&#20854;&#36866;&#24212;&#23616;&#37096;&#32454;&#33410;&#65307;3&#65289;&#23558;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#20998;&#21106;&#25104;...
&lt;/p&gt;
&lt;p&gt;
COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedZeN&#65292;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;Hessian&#20272;&#35745;&#30340;&#36229;&#32447;&#24615;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;Stiefel&#27969;&#24418;&#20013;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#21521;&#65292;&#20272;&#35745;&#20840;&#23616;&#30446;&#26631;&#30340;&#26354;&#29575;&#65292;&#24182;&#23454;&#29616;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2309.17174</link><description>&lt;p&gt;
FedZeN:&#36890;&#36807;&#22686;&#37327;Hessian&#20272;&#35745;&#23454;&#29616;&#36229;&#32447;&#24615;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation. (arXiv:2309.17174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedZeN&#65292;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;Hessian&#20272;&#35745;&#30340;&#36229;&#32447;&#24615;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;Stiefel&#27969;&#24418;&#20013;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#21521;&#65292;&#20272;&#35745;&#20840;&#23616;&#30446;&#26631;&#30340;&#26354;&#29575;&#65292;&#24182;&#23454;&#29616;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#19968;&#32452;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#32534;&#25490;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#23548;&#25968;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#32771;&#34385;&#20102;&#32852;&#37030;&#38646;&#38454;&#35774;&#32622;&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#21482;&#33021;&#36890;&#36807;&#39044;&#31639;&#25968;&#37327;&#30340;&#28857;&#35780;&#20272;&#26469;&#35775;&#38382;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20984;&#20248;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#38646;&#38454;&#31639;&#27861;&#26469;&#20272;&#35745;&#20840;&#23616;&#30446;&#26631;&#30340;&#26354;&#29575;&#65292;&#20197;&#23454;&#29616;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#35823;&#24046;&#33539;&#25968;&#32447;&#24615;&#25910;&#25947;&#30340;&#22686;&#37327;Hessian&#20272;&#35745;&#22120;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#32852;&#37030;&#38646;&#38454;&#35774;&#32622;&#65292;&#20174;Stiefel&#27969;&#24418;&#20013;&#38543;&#26426;&#37319;&#26679;&#25628;&#32034;&#26041;&#21521;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#26799;&#24230;&#21644;Hessian&#20272;&#35745;&#22120;&#37117;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20197;&#36890;&#20449;&#25928;&#29575;&#39640;&#19988;&#30465;&#36164;&#28304;&#30340;&#26041;&#24335;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed learning framework that allows a set of clients to collaboratively train a model under the orchestration of a central server, without sharing raw data samples. Although in many practical scenarios the derivatives of the objective function are not available, only few works have considered the federated zeroth-order setting, in which functions can only be accessed through a budgeted number of point evaluations. In this work we focus on convex optimization and design the first federated zeroth-order algorithm to estimate the curvature of the global objective, with the purpose of achieving superlinear convergence. We take an incremental Hessian estimator whose error norm converges linearly, and we adapt it to the federated zeroth-order setting, sampling the random search directions from the Stiefel manifold for improved performance. In particular, both the gradient and Hessian estimators are built at the central server in a communication-efficient and pr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#27604;&#36739;&#20998;&#26512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.17171</link><description>&lt;p&gt;
&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#20013;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain. (arXiv:2309.17171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17171
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20998;&#26512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#36890;&#29992;&#33521;&#35821;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22855;&#24187;&#25991;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;NER&#21487;&#20197;&#26816;&#27979;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;10&#20010;NER&#27169;&#22411;&#22312;7&#26412;&#40857;&#19982;&#22320;&#19979;&#22478;(D&amp;D)&#20882;&#38505;&#20070;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#35780;&#20272;&#29305;&#23450;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20070;&#31821;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#20570;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#35782;&#21035;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many NLP tasks, although well-resolved for general English, face challenges in specific domains like fantasy literature. This is evident in Named Entity Recognition (NER), which detects and categorizes entities in text. We analyzed 10 NER models on 7 Dungeons and Dragons (D&amp;D) adventure books to assess domain-specific performance. Using open-source Large Language Models, we annotated named entities in these books and evaluated each model's precision. Our findings indicate that, without modifications, Flair, Trankit, and Spacy outperform others in identifying named entities in the D&amp;D context.
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24403;&#21069;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#22909;&#30340;&#39044;&#27979;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.17161</link><description>&lt;p&gt;
&#24403;&#21069;&#30340;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Current Methods for Drug Property Prediction in the Real World. (arXiv:2309.17161v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17161
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24403;&#21069;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#22909;&#30340;&#39044;&#27979;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#39044;&#27979;&#33647;&#29289;&#23646;&#24615;&#23545;&#20110;&#22312;&#26114;&#36149;&#30340;&#20020;&#24202;&#35797;&#39564;&#20043;&#21069;&#38477;&#20302;&#39118;&#38505;&#12289;&#24555;&#36895;&#25214;&#21040;&#39640;&#27963;&#24615;&#21270;&#21512;&#29289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20852;&#36259;&#23548;&#33268;&#20102;&#22810;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#19981;&#21516;&#25991;&#20214;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24471;&#20986;&#30340;&#32467;&#35770;&#20063;&#19981;&#26131;&#20110;&#27604;&#36739;&#65292;&#22240;&#27492;&#20173;&#28982;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#25110;&#26041;&#27861;&#26368;&#21512;&#36866;&#12290;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#23558;&#20043;&#21069;&#30340;&#35768;&#22810;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29616;&#26377;&#23646;&#24615;&#31867;&#21035;&#12289;&#25968;&#25454;&#38598;&#21450;&#20854;&#19982;&#19981;&#21516;&#26041;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#33647;&#29289;&#24320;&#21457;&#20915;&#31574;&#21608;&#26399;&#20013;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#32780;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting drug properties is key in drug discovery to enable de-risking of assets before expensive clinical trials, and to find highly active compounds faster. Interest from the Machine Learning community has led to the release of a variety of benchmark datasets and proposed methods. However, it remains unclear for practitioners which method or approach is most suitable, as different papers benchmark on different datasets and methods, leading to varying conclusions that are not easily compared. Our large-scale empirical study links together numerous earlier works on different datasets and methods; thus offering a comprehensive overview of the existing property classes, datasets, and their interactions with different methods. We emphasise the importance of uncertainty quantification and the time and therefore cost of applying these methods in the drug development decision-making cycle. We discover that the best method depends on the dataset, and that engineered features with classical 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25163;&#20889;&#25968;&#25454;&#21644;&#35745;&#31639;&#30456;&#20851;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#32769;&#40836;&#20154;&#32676;&#36827;&#34892;&#24180;&#40836;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17156</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#25163;&#20889;&#25351;&#26631;&#36827;&#34892;&#24180;&#40836;&#32676;&#20307;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
Age Group Discrimination via Free Handwriting Indicators. (arXiv:2309.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17156
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25163;&#20889;&#25968;&#25454;&#21644;&#35745;&#31639;&#30456;&#20851;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#32769;&#40836;&#20154;&#32676;&#36827;&#34892;&#24180;&#40836;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#22686;&#38271;&#65292;&#32769;&#24369;&#32676;&#20307;&#30340;&#24739;&#30149;&#29575;&#39044;&#35745;&#23558;&#22686;&#21152;&#65292;&#32473;&#21307;&#30103;&#31995;&#32479;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#32769;&#24369;&#26159;&#19968;&#31181;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#32508;&#21512;&#24449;&#65292;&#20854;&#29305;&#24449;&#26159;&#20581;&#24247;&#36880;&#28176;&#19979;&#38477;&#65292;&#23545;&#21387;&#21147;&#30340;&#33030;&#24369;&#24615;&#22686;&#21152;&#65292;&#27515;&#20129;&#39118;&#38505;&#22686;&#21152;&#12290;&#32769;&#24369;&#23545;&#20844;&#20849;&#21355;&#29983;&#36896;&#25104;&#20102;&#37325;&#22823;&#36127;&#25285;&#65292;&#38477;&#20302;&#20102;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#26222;&#36941;&#25509;&#21463;&#30340;&#35780;&#20272;&#32769;&#24369;&#31243;&#24230;&#30340;&#26041;&#27861;&#21644;&#26631;&#20934;&#21270;&#23450;&#20041;&#65292;&#36825;&#31361;&#26174;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#32570;&#21475;&#12290;&#37492;&#20110;&#36825;&#19968;&#32570;&#21475;&#21644;&#26089;&#26399;&#39044;&#38450;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#25163;&#20889;&#36827;&#34892;&#29983;&#24577;&#23398;&#35780;&#20272;&#65292;&#20197;&#23545;&#19981;&#21516;&#24180;&#40836;&#32676;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#20998;&#26512;&#20102;&#26469;&#33258;80&#21517;&#19981;&#21516;&#24180;&#40836;&#32452;&#65288;20-40&#23681;&#12289;41-60&#23681;&#12289;61-70&#23681;&#21644;70+&#23681;&#65289;&#30340;&#20581;&#24247;&#21442;&#19982;&#32773;&#30340;&#38750;&#22270;&#20687;&#25163;&#20889;&#25968;&#25454;&#12290;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#35745;&#31639;&#20102;14&#20010;&#19982;&#25163;&#21183;&#21644;&#38663;&#39076;&#30456;&#20851;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#20116;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#36825;&#20123;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing global elderly population is expected to increase the prevalence of frailty, posing significant challenges to healthcare systems. Frailty, a syndrome associated with ageing, is characterised by progressive health decline, increased vulnerability to stressors and increased risk of mortality. It represents a significant burden on public health and reduces the quality of life of those affected. The lack of a universally accepted method to assess frailty and a standardised definition highlights a critical research gap. Given this lack and the importance of early prevention, this study presents an innovative approach using an instrumented ink pen to ecologically assess handwriting for age group classification. Content-free handwriting data from 80 healthy participants in different age groups (20-40, 41-60, 61-70 and 70+) were analysed. Fourteen gesture- and tremor-related indicators were computed from the raw data and used in five classification tasks. These tasks included discr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#23558;&#32447;&#24615;VAR&#36807;&#31243;&#21644;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.17154</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#35299;&#37322;&#22810;&#26102;&#38388;&#24207;&#21015;&#38750;&#32447;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Efficient Interpretable Nonlinear Modeling for Multiple Time Series. (arXiv:2309.17154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#23558;&#32447;&#24615;VAR&#36807;&#31243;&#21644;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#22797;&#26434;&#24230;&#19982;&#32447;&#24615;&#21521;&#37327;&#33258;&#22238;&#24402;&#65288;VAR&#65289;&#27169;&#22411;&#30456;&#24403;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#20551;&#35774;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#26159;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#65306;&#39318;&#20808;&#26159;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;VAR&#36807;&#31243;&#65292;&#28982;&#21518;&#26159;&#19968;&#32452;&#21487;&#36870;&#19988;Lipschitz&#36830;&#32493;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#36825;&#20123;&#26144;&#23556;&#24212;&#29992;&#20110;&#27599;&#20010;&#20256;&#24863;&#22120;&#65292;&#21363;&#20174;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#21040;&#27979;&#37327;&#31354;&#38388;&#20013;&#30340;&#21464;&#37327;&#30340;&#20998;&#37327;&#26144;&#23556;&#12290;VAR&#31995;&#25968;&#35782;&#21035;&#25552;&#20379;&#20102;&#25152;&#36848;&#21464;&#37327;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#25299;&#25169;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#20998;&#37327;&#30340;&#38750;&#32447;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;VAR&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive linear and nonlinear models based on kernel machines or deep neural networks have been used to discover dependencies among time series. This paper proposes an efficient nonlinear modeling approach for multiple time series, with a complexity comparable to linear vector autoregressive (VAR) models while still incorporating nonlinear interactions among different time-series variables. The modeling assumption is that the set of time series is generated in two steps: first, a linear VAR process in a latent space, and second, a set of invertible and Lipschitz continuous nonlinear mappings that are applied per sensor, that is, a component-wise mapping from each latent variable to a variable in the measurement space. The VAR coefficient identification provides a topology representation of the dependencies among the aforementioned variables. The proposed approach models each component-wise nonlinearity using an invertible neural network and imposes sparsity on the VAR coefficients to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#26469;&#21453;&#39539;&#20043;&#21069;&#19981;&#21487;&#20449;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#19968;&#28857;&#12290;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#26159;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.17144</link><description>&lt;p&gt;
&#21407;&#22411;&#29983;&#25104;: &#38024;&#23545;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#31283;&#20581;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability. (arXiv:2309.17144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#26469;&#21453;&#39539;&#20043;&#21069;&#19981;&#21487;&#20449;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#19968;&#28857;&#12290;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#26159;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#26080;&#20851;&#12289;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#26356;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#29983;&#25104;&#23548;&#33268;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#21453;&#39539;&#20102;&#20043;&#21069;&#22768;&#31216;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#19981;&#21487;&#20449;&#30340;&#35266;&#28857;&#65292;&#21407;&#22240;&#26159;&#30001;&#20110;&#20854;&#20869;&#37096;&#28608;&#27963;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#25105;&#20204;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#20123;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#26469;&#33719;&#24471;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#31361;&#20986;&#26174;&#31034;&#20986;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.17130</link><description>&lt;p&gt;
GRANDE: &#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#20173;&#28982;&#26159;&#22788;&#29702;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#28789;&#27963;&#24615;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26469;&#35828;&#65292;&#23384;&#22312;&#23545;&#29305;&#23450;&#20110;&#34920;&#26684;&#30340;&#26799;&#24230;&#26041;&#27861;&#30340;&#26174;&#33879;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#12290;GRANDE&#22522;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#31264;&#23494;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#30452;&#36890;&#25805;&#20316;&#31526;&#21644;&#21453;&#21521;&#20256;&#25773;&#19968;&#36215;&#20248;&#21270;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#65288;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#30340;&#19968;&#20010;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#65289;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GRANDE&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#21487;&#24494;&#20998;&#38899;&#39057;&#25928;&#26524;&#30340;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20165;&#35201;&#27714;&#25152;&#32771;&#34385;&#30340;&#21442;&#25968;&#20855;&#26377;&#36830;&#32493;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.17125</link><description>&lt;p&gt;
&#38750;&#21487;&#24494;&#20998;&#38899;&#39057;&#25928;&#26524;&#30340;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Style Transfer for Non-differentiable Audio Effects. (arXiv:2309.17125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#21487;&#24494;&#20998;&#38899;&#39057;&#25928;&#26524;&#30340;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20165;&#35201;&#27714;&#25152;&#32771;&#34385;&#30340;&#21442;&#25968;&#20855;&#26377;&#36830;&#32493;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#38899;&#39057;&#25928;&#26524;&#34987;&#38899;&#39057;&#24037;&#31243;&#24072;&#24191;&#27867;&#20351;&#29992;&#26469;&#25913;&#21464;&#38899;&#39057;&#25968;&#25454;&#30340;&#22768;&#23398;&#21644;&#26102;&#38388;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25928;&#26524;&#21487;&#33021;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#36825;&#20351;&#24471;&#23545;&#20110;&#21021;&#23398;&#32773;&#32780;&#35328;&#23398;&#20064;&#22256;&#38590;&#65292;&#23545;&#20110;&#19987;&#19994;&#20154;&#22763;&#32780;&#35328;&#38480;&#21046;&#20102;&#21019;&#36896;&#21147;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#21162;&#21147;&#33268;&#21147;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#26469;&#33719;&#21462;&#38899;&#39057;&#25928;&#26524;&#30340;&#20302;&#32423;&#21442;&#25968;&#37197;&#32622;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#21644;&#21442;&#32771;&#38899;&#36712;&#20043;&#38388;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#65292;&#31216;&#20043;&#20026;&#39118;&#26684;&#36801;&#31227;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#30340;&#26159;&#19981;&#28789;&#27963;&#30340;&#40657;&#30418;&#25216;&#26415;&#65292;&#25110;&#32773;&#35201;&#27714;&#25152;&#32771;&#34385;&#30340;&#25928;&#26524;&#22312;&#33258;&#21160;&#24494;&#20998;&#26694;&#26550;&#20013;&#23454;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#38899;&#39057;&#21046;&#20316;&#39118;&#26684;&#21305;&#37197;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#20123;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#25928;&#26524;&#65292;&#20165;&#35201;&#27714;&#25152;&#32771;&#34385;&#30340;&#21442;&#25968;&#20855;&#26377;&#36830;&#32493;&#30340;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#39118;&#26684;&#21305;&#37197;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital audio effects are widely used by audio engineers to alter the acoustic and temporal qualities of audio data. However, these effects can have a large number of parameters which can make them difficult to learn for beginners and hamper creativity for professionals. Recently, there have been a number of efforts to employ progress in deep learning to acquire the low-level parameter configurations of audio effects by minimising an objective function between an input and reference track, commonly referred to as style transfer. However, current approaches use inflexible black-box techniques or require that the effects under consideration are implemented in an auto-differentiation framework. In this work, we propose a deep learning approach to audio production style matching which can be used with effects implemented in some of the most widely used frameworks, requiring only that the parameters under consideration have a continuous domain. Further, our method includes style matching fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;DiffChest&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#36890;&#36807;&#35780;&#20272;DiffChest&#22312;&#35782;&#21035;&#19982;&#27835;&#30103;&#30456;&#20851;&#30340;&#28151;&#28102;&#22240;&#32032;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#36739;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17123</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#25918;&#23556;&#22270;&#20687;&#35299;&#35835;&#20013;&#30340;&#24739;&#32773;&#29305;&#23450;&#28151;&#28102;&#22240;&#32032;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining. (arXiv:2309.17123v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;DiffChest&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#36890;&#36807;&#35780;&#20272;DiffChest&#22312;&#35782;&#21035;&#19982;&#27835;&#30103;&#30456;&#20851;&#30340;&#28151;&#28102;&#22240;&#32032;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#36739;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#33258;&#21160;&#21270;&#35786;&#26029;&#36741;&#21161;&#31995;&#32479;&#20013;&#30340;&#35823;&#23548;&#24615;&#27169;&#24335;&#23545;&#20110;&#30830;&#20445;&#20854;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#30446;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25216;&#26415;&#26080;&#27861;&#22312;&#35786;&#26029;&#27700;&#24179;&#19978;&#21487;&#35270;&#21270;&#28151;&#28102;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;DiffChest&#65292;&#24182;&#22312;&#32654;&#22269;&#21644;&#27431;&#27954;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;194,956&#21517;&#24739;&#32773;&#30340;515,704&#24352;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;DiffChest&#22312;&#24739;&#32773;&#29305;&#23450;&#32423;&#21035;&#19978;&#35299;&#37322;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#21487;&#35270;&#21270;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35780;&#20272;DiffChest&#35782;&#21035;&#19982;&#27835;&#30103;&#30456;&#20851;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#33021;&#21147;&#26102;&#65292;&#35780;&#20272;&#32773;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#22823;&#22810;&#25968;&#24433;&#20687;&#32467;&#26524;&#30340;Fleiss' Kappa&#20540;&#20026;0.8&#25110;&#26356;&#39640;&#12290;&#28151;&#28102;&#22240;&#32032;&#30340;&#25429;&#25417;&#20934;&#30830;&#29575;&#20026;11.1%&#21040;100%&#19981;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20248;&#21270;&#20102;&#27169;&#22411;&#20197;&#25429;&#25417;&#26368;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting misleading patterns in automated diagnostic assistance systems, such as those powered by Artificial Intelligence, is critical to ensuring their reliability, particularly in healthcare. Current techniques for evaluating deep learning models cannot visualize confounding factors at a diagnostic level. Here, we propose a self-conditioned diffusion model termed DiffChest and train it on a dataset of 515,704 chest radiographs from 194,956 patients from multiple healthcare centers in the United States and Europe. DiffChest explains classifications on a patient-specific level and visualizes the confounding factors that may mislead the model. We found high inter-reader agreement when evaluating DiffChest's capability to identify treatment-related confounders, with Fleiss' Kappa values of 0.8 or higher across most imaging findings. Confounders were accurately captured with 11.1% to 100% prevalence rates. Furthermore, our pretraining process optimized the model to capture the most relev
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#32454;&#32990;&#31354;&#38388;&#26469;&#22686;&#24378;&#36229;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#32454;&#32990;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#20026;&#26377;&#25928;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.17116</link><description>&lt;p&gt;
Sheaf Hypergraph Networks. (arXiv:2309.17116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Sheaf Hypergraph Networks. (arXiv:2309.17116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#32454;&#32990;&#31354;&#38388;&#26469;&#22686;&#24378;&#36229;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#32454;&#32990;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#20026;&#26377;&#25928;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#20851;&#31995;&#22312;&#33258;&#28982;&#30028;&#20013;&#21313;&#20998;&#26222;&#36941;&#65292;&#35768;&#22810;&#29616;&#35937;&#28041;&#21450;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#20004;&#20004;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25552;&#21319;&#39640;&#38454;&#22788;&#29702;&#33021;&#21147;&#21487;&#20197;&#21152;&#36895;&#21508;&#20010;&#38656;&#35201;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#32454;&#32990;&#31354;&#38388;&#23545;&#36229;&#22270;&#36827;&#34892;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#23398;&#26500;&#36896;&#65292;&#22312;&#32500;&#25345;&#23616;&#37096;&#39640;&#38454;&#36830;&#36890;&#24615;&#30340;&#21516;&#26102;&#20026;&#20256;&#32479;&#36229;&#22270;&#28155;&#21152;&#39069;&#22806;&#30340;&#32467;&#26500;&#12290;&#21463;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;&#20004;&#31181;&#29420;&#29305;&#30340;&#32454;&#32990;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#24418;&#24335;&#65306;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#23558;&#32454;&#32990;&#31354;&#38388;&#24341;&#20837;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#27604;&#26631;&#20934;&#30340;&#36229;&#22270;&#25193;&#25955;&#25552;&#20379;&#20102;&#26356;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20026;&#26377;&#25928;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17113</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17113
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#30830;&#23450;&#20449;&#24687;&#30456;&#20851;&#30340;&#20851;&#31995;&#65306;&#35201;&#20040;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#20302;&#32423;&#26435;&#37325;&#23398;&#20064;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#20851;&#31995;&#20381;&#36182;&#38142;&#65292;&#31216;&#20026;&#20803;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#22312;&#23384;&#22312;&#22823;&#37327;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;&#65289;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#21518;&#19968;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22522;&#20110;&#23569;&#37327;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#36335;&#24452;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#20803;&#36335;&#24452;&#30340;&#22686;&#37327;&#26500;&#24314;&#20013;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#27491;&#30830;&#35782;&#21035;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#22312;&#21327;&#20316;&#23398;&#20064;&#30340;&#24773;&#26223;&#20013;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#22522;&#20934;&#27979;&#35797;&#65292;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.17097</link><description>&lt;p&gt;
Collaborative Learning&#26041;&#27861;&#22312;&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation. (arXiv:2309.17097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#22312;&#21327;&#20316;&#23398;&#20064;&#30340;&#24773;&#26223;&#20013;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#22522;&#20934;&#27979;&#35797;&#65292;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#36890;&#24120;&#34987;&#20998;&#21106;&#25104;&#22810;&#20010;&#21307;&#38498;&#30340;&#20013;&#23567;&#22411;&#38598;&#21512;&#65292;&#24182;&#19988;&#30001;&#20110;&#38544;&#31169;&#35268;&#23450;&#30340;&#38480;&#21046;&#65292;&#35775;&#38382;&#36825;&#20123;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#32473;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#20197;&#25968;&#25454;&#20026;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#36825;&#20801;&#35768;&#21307;&#38498;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35299;&#20915;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21327;&#20316;&#22330;&#26223;&#19979;&#30340;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#38382;&#39064;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#65288;CBM&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#21253;&#25324;&#26631;&#31614;&#34701;&#21512;&#25216;&#26415;&#22312;&#20869;&#30340;CBM&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;CBM&#23558;&#26469;&#33258;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#29702;&#24819;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;&#32852;&#37030;&#24378;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare data is often split into medium/small-sized collections across multiple hospitals and access to it is encumbered by privacy regulations. This brings difficulties to use them for the development of machine learning and deep learning models, which are known to be data-hungry. One way to overcome this limitation is to use collaborative learning (CL) methods, which allow hospitals to work collaboratively to solve a task, without the need to explicitly share local data.  In this paper, we address a prostate segmentation problem from MRI in a collaborative scenario by comparing two different approaches: federated learning (FL) and consensus-based methods (CBM).  To the best of our knowledge, this is the first work in which CBM, such as label fusion techniques, are used to solve a problem of collaborative learning. In this setting, CBM combine predictions from locally trained models to obtain a federated strong learner with ideally improved robustness and predictive variance proper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;DeltaXplainer&#22312;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17095</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#35268;&#21017;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#30340;&#21160;&#24577;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dynamic Interpretability for Model Comparison via Decision Rules. (arXiv:2309.17095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;DeltaXplainer&#22312;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22823;&#22810;&#34987;&#29992;&#26469;&#30740;&#31350;&#21644;&#38416;&#26126;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#34987;&#35774;&#35745;&#25104;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21644;&#35299;&#37322;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#24046;&#24322;&#30340;&#25361;&#25112;&#65292;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#12289;&#30417;&#25511;&#21644;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35780;&#20272;DeltaXplainer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) methods have mostly been built to investigate and shed light on single machine learning models and are not designed to capture and explain differences between multiple models effectively. This paper addresses the challenge of understanding and explaining differences between machine learning models, which is crucial for model selection, monitoring and lifecycle management in real-world applications. We propose DeltaXplainer, a model-agnostic method for generating rule-based explanations describing the differences between two binary classifiers. To assess the effectiveness of DeltaXplainer, we conduct experiments on synthetic and real-world datasets, covering various model comparison scenarios involving different types of concept drift.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#35268;&#27169;&#25193;&#23637;&#30740;&#31350;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#30772;&#22351;&#37325;&#24314;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17089</link><description>&lt;p&gt;
&#22826;&#22823;&#20102;&#65292;&#25152;&#20197;&#22833;&#36133;&#20102;&#65311;-- &#23454;&#29616;&#31070;&#32463;&#24314;&#27169;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Too Big, so Fail? -- Enabling Neural Construction Methods to Solve Large-Scale Routing Problems. (arXiv:2309.17089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17089
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#35268;&#27169;&#25193;&#23637;&#30740;&#31350;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#30772;&#22351;&#37325;&#24314;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;NP&#38590;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;&#20854;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#26159;&#39034;&#24207;&#31070;&#32463;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#24120;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#35757;&#32451;&#25104;&#26412;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#26377;&#38480;&#30340;&#23454;&#20363;&#22823;&#23567;&#19978;&#36827;&#34892;&#35757;&#32451;&#65288;&#20363;&#22914;100&#20010;&#39038;&#23458;&#65289;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36828;&#36828;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#65288;&#20363;&#22914;2000&#20010;&#39038;&#23458;&#65289;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#35268;&#27169;&#25193;&#23637;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#20063;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36229;&#36234;&#65292;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#23454;&#20363;&#19978;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30772;&#22351;&#37325;&#24314;&#21407;&#21017;&#65292;&#20132;&#26367;&#22320;&#23436;&#20840;&#30772;&#22351;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#37096;&#37096;&#20998;&#65292;&#28982;&#21518;&#37325;&#26032;&#21019;&#24314;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#20307;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20687;POMO&#36825;&#26679;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#27704;&#36828;&#19981;&#24212;&#29992;&#20110;&#20840;&#23616;&#38382;&#39064;&#65292;&#32780;&#20165;&#29992;&#20110;&#37325;&#24314;&#27493;&#39588;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years new deep learning approaches to solve combinatorial optimization problems, in particular NP-hard Vehicle Routing Problems (VRP), have been proposed. The most impactful of these methods are sequential neural construction approaches which are usually trained via reinforcement learning. Due to the high training costs of these models, they usually are trained on limited instance sizes (e.g. serving 100 customers) and later applied to vastly larger instance size (e.g. 2000 customers). By means of a systematic scale-up study we show that even state-of-the-art neural construction methods are outperformed by simple heuristics, failing to generalize to larger problem instances. We propose to use the ruin recreate principle that alternates between completely destroying a localized part of the solution and then recreating an improved variant. In this way, neural construction methods like POMO are never applied to the global problem but just in the reconstruction step, which only i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21512;&#36866;&#30340;&#35843;&#21046;&#21644;&#32534;&#30721;&#26041;&#26696;(MCS)&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#20391;&#38142;&#36890;&#20449;&#20013;&#30340;MCS&#33258;&#36866;&#24212;&#65292;&#24182;&#23637;&#31034;&#20102;&#36739;&#20256;&#32479;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#36807;&#23454;&#38469;&#34892;&#39542;&#27979;&#35797;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#22806;&#24320;&#25918;&#12290;</title><link>http://arxiv.org/abs/2309.17086</link><description>&lt;p&gt;
&#20174;&#32463;&#39564;&#27979;&#37327;&#21040;&#22686;&#24378;&#25968;&#25454;&#36895;&#29575;&#65306;&#19968;&#31181;&#24212;&#29992;&#20110;&#20391;&#38142;&#36890;&#20449;&#20013;&#30340;MCS&#33258;&#36866;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication. (arXiv:2309.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21512;&#36866;&#30340;&#35843;&#21046;&#21644;&#32534;&#30721;&#26041;&#26696;(MCS)&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#20391;&#38142;&#36890;&#20449;&#20013;&#30340;MCS&#33258;&#36866;&#24212;&#65292;&#24182;&#23637;&#31034;&#20102;&#36739;&#20256;&#32479;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#36807;&#23454;&#38469;&#34892;&#39542;&#27979;&#35797;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#22806;&#24320;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;C-V2X&#20391;&#38142;&#20013;&#32570;&#20047;&#21453;&#39304;&#20449;&#36947;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#35843;&#21046;&#21644;&#32534;&#30721;&#26041;&#26696;&#65288;MCS&#65289;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#36710;&#36742;&#21040;&#19968;&#20999;&#65288;V2X&#65289;&#36890;&#20449;&#20013;&#23545;&#25968;&#25454;&#36895;&#29575;&#35201;&#27714;&#26356;&#39640;&#30340;&#26368;&#26032;&#20351;&#29992;&#26696;&#20363;&#65292;&#38656;&#35201;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;MCS&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#21512;&#36866;&#30340;MCS&#32423;&#21035;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20998;&#20301;&#25968;&#39044;&#27979;&#65292;&#24182;&#23558;&#20854;&#19982;&#19981;&#21516;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#35780;&#20272;&#39044;&#27979;&#20855;&#26377;&#26368;&#39640;&#21487;&#36798;&#25968;&#25454;&#36895;&#29575;&#30340;MCS&#32423;&#21035;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#30340;&#36873;&#25321;MCS&#32423;&#21035;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#27604;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#20110;&#30740;&#31350;&#30340;&#26356;&#22823;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#36807;&#24191;&#27867;&#30340;&#34892;&#39542;&#27979;&#35797;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of a feedback channel in the C-V2X sidelink, finding a suitable modulation and coding scheme (MCS) is a difficult task. However, recent use cases for vehicle-to-everything (V2X) communication with higher demands on data rate necessitate choosing the MCS adaptively. In this paper, we propose a machine learning approach to predict suitable MCS levels. Additionally, we propose the use of quantile prediction and evaluate it in combination with different algorithms for the task of predicting the MCS level with the highest achievable data rate. Thereby, we show significant improvements over conventional methods of choosing the MCS level. Using a machine learning approach, however, requires larger real-world data sets than are currently publicly available for research. For this reason, this paper presents a data set that was acquired in extensive drive tests, and that we make publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#19982;&#38543;&#26426;&#37327;&#23376;&#21270;&#30456;&#36830;&#25509;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#38543;&#26426;&#37327;&#23376;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.17082</link><description>&lt;p&gt;
&#22312;&#26684;&#28857;&#22330;&#35770;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38543;&#26426;&#37327;&#23376;&#21270;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Stochastic Quantization in Lattice Field Theory. (arXiv:2309.17082v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#19982;&#38543;&#26426;&#37327;&#23376;&#21270;&#30456;&#36830;&#25509;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#38543;&#26426;&#37327;&#23376;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#21644;&#38543;&#26426;&#37327;&#23376;&#21270;&#65288;SQ&#65289;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;&#36890;&#36807;&#36817;&#20284;&#28385;&#36275;Langevin&#26041;&#31243;&#30340;&#38543;&#26426;&#36807;&#31243;&#30340;&#21453;&#28436;&#65292;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#26469;&#23454;&#29616;DM&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;DM&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#26469;&#29983;&#25104;&#20108;&#32500;$\phi^4$&#29702;&#35770;&#20013;&#30340;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;DM&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#31639;&#27861;&#22312;&#20020;&#30028;&#21306;&#22495;&#36935;&#21040;&#20020;&#30028;&#20943;&#24930;&#26102;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#26395;&#22312;&#26684;&#28857;&#22330;&#35770;&#27169;&#25311;&#20013;&#28608;&#21457;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#22823;&#22411;&#25968;&#25454;&#38598;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we establish a direct connection between generative diffusion models (DMs) and stochastic quantization (SQ). The DM is realized by approximating the reversal of a stochastic process dictated by the Langevin equation, generating samples from a prior distribution to effectively mimic the target distribution. Using numerical simulations, we demonstrate that the DM can serve as a global sampler for generating quantum lattice field configurations in two-dimensional $\phi^4$ theory. We demonstrate that DMs can notably reduce autocorrelation times in the Markov chain, especially in the critical region where standard Markov Chain Monte-Carlo (MCMC) algorithms experience critical slowing down. The findings can potentially inspire further advancements in lattice field theory simulations, in particular in cases where it is expensive to generate large ensembles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;3D&#32593;&#26684;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38236;&#20687;&#37325;&#37327;&#23545;&#31216;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#25552;&#39640;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17076</link><description>&lt;p&gt;
&#38236;&#20687;&#37325;&#37327;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;3D&#32593;&#26684;&#20998;&#21106;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications. (arXiv:2309.17076v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;3D&#32593;&#26684;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38236;&#20687;&#37325;&#37327;&#23545;&#31216;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#25552;&#39640;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#32593;&#26684;&#20998;&#21106;&#26159;&#20855;&#26377;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20154;&#20307;&#20855;&#26377;&#23545;&#31216;&#21644;&#19968;&#20123;&#22120;&#23448;&#20301;&#32622;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#26399;&#22312;&#25191;&#34892;&#29983;&#29289;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26059;&#36716;&#21644;&#21453;&#36716;&#19981;&#21464;&#30340;&#23618;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25191;&#34892;3D&#32593;&#26684;&#20998;&#21106;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#30149;&#29702;&#34880;&#31649;&#32467;&#26500;&#65288;&#21160;&#33033;&#30244;&#65289;&#21644;&#20256;&#32479;&#35299;&#21078;&#32467;&#26500;&#65288;&#24515;&#23460;&#30340;&#20869;&#33180;&#21644;&#22806;&#33180;&#65289;&#30340;3D&#32593;&#26684;&#20998;&#21106;&#38382;&#39064;&#12290;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#34987;&#32534;&#30721;&#20026;&#20174;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20013;&#37319;&#26679;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#32593;&#26684;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#22686;&#21152;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#21069;&#25552;&#26159;&#31070;&#32463;&#32593;&#32476;&#33267;&#23569;&#20855;&#26377;&#19977;&#20010;&#21367;&#31215;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D mesh segmentation is an important task with many biomedical applications. The human body has bilateral symmetry and some variations in organ positions. It allows us to expect a positive effect of rotation and inversion invariant layers in convolutional neural networks that perform biomedical segmentations. In this study, we show the impact of weight symmetry in neural networks that perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation for pathological vessel structures (aneurysms) and conventional anatomical structures (endocardium and epicardium of ventricles). Local geometrical features are encoded as sampling from the signed distance function, and the neural network performs prediction for each mesh node. We show that weight symmetry gains from 1 to 3% of additional accuracy and allows decreasing the number of trainable parameters up to 8 times without suffering the performance loss if neural networks have at least three convolutional layers. This also work
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;</title><link>http://arxiv.org/abs/2309.17053</link><description>&lt;p&gt;
&#20851;&#20110;Weisfeiler-Leman&#27979;&#35797;&#22312;&#22270;&#24418;&#27169;&#24335;&#21442;&#25968;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;$k$&#32500;Weisfeiler-Leman&#65288;$k$WL&#65289;&#27979;&#35797;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;$k$WL&#27979;&#35797;&#26159;&#19968;&#31181;&#24191;&#20026;&#35748;&#21487;&#30340;&#29992;&#20110;&#39564;&#35777;&#22270;&#21516;&#26500;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#36830;&#25509;&#37325;&#26032;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;$k$WL&#27979;&#35797;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30340;&#29305;&#23450;&#22270;&#23646;&#24615;&#30340;&#20852;&#36259;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#30340;&#20013;&#24515;&#26159;&#30830;&#23450;&#26368;&#23567;&#32500;&#24230;$k$&#65292;&#20351;&#24471;$k$WL&#21487;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26368;&#23567;$k$&#31216;&#20026;&#36825;&#20010;&#27169;&#24335;&#35745;&#25968;&#38382;&#39064;&#30340;WL&#32500;&#24230;&#12290;&#36825;&#20010;&#35843;&#26597;&#20256;&#32479;&#19978;&#25506;&#35752;&#19982;&#22270;&#26696;&#30456;&#20851;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#35745;&#25968;&#38382;&#39064;&#65306;&#23376;&#22270;&#35745;&#25968;&#21644;&#35825;&#23548;&#23376;&#22270;&#35745;&#25968;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#26368;&#21021;&#30475;&#36215;&#26469;&#26159;&#20855;&#26377;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#29420;&#31435;&#25361;&#25112;&#65292;&#20294;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#26159;&#19968;&#20010;&#26356;&#20840;&#38754;&#38382;&#39064;&#30340;&#30456;&#20114;&#20851;&#32852;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17048</link><description>&lt;p&gt;
&#20851;&#20110;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#36830;&#32493;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26159;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#19981;&#21463;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#23545;&#25239;&#26679;&#26412;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#36825;&#31181;&#25913;&#36827;&#26159;&#20197;&#33258;&#28982;&#26679;&#26412;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#20154;&#25552;&#20986;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#35266;&#28857;&#65292;&#20551;&#35774;&#30340;&#36830;&#32493;&#24615;&#19982;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36830;&#32493;&#20989;&#25968;&#19981;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#31283;&#20581;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#35856;&#27874;&#21644;ho&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;YOLOv5&#27169;&#22411;&#24555;&#36895;&#35782;&#21035;&#25991;&#26723;&#24067;&#23616;&#21644;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17033</link><description>&lt;p&gt;
&#20351;&#29992;YOLOv5&#24067;&#23616;&#26816;&#27979;&#25581;&#31034;&#25991;&#20214;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Unveiling Document Structures with YOLOv5 Layout Detection. (arXiv:2309.17033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;YOLOv5&#27169;&#22411;&#24555;&#36895;&#35782;&#21035;&#25991;&#26723;&#24067;&#23616;&#21644;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25968;&#23383;&#29615;&#22659;&#20013;&#24191;&#27867;&#23384;&#22312;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#32473;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#24102;&#26469;&#35768;&#22810;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#25552;&#21462;&#25216;&#26415;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#29992;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;YOLOv5&#65288;&#19968;&#31181;&#23574;&#31471;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65289;&#24555;&#36895;&#35782;&#21035;&#25991;&#26723;&#24067;&#23616;&#21644;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#8220;&#23545;&#35937;&#8221;&#27010;&#24565;&#65292;&#21253;&#25324;&#27573;&#33853;&#12289;&#34920;&#26684;&#12289;&#29031;&#29255;&#21644;&#20854;&#20182;&#32452;&#25104;&#37096;&#20998;&#31561;&#21508;&#31181;&#20803;&#32032;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#33258;&#20027;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#25991;&#26723;&#24067;&#23616;&#24182;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current digital environment is characterized by the widespread presence of data, particularly unstructured data, which poses many issues in sectors including finance, healthcare, and education. Conventional techniques for data extraction encounter difficulties in dealing with the inherent variety and complexity of unstructured data, hence requiring the adoption of more efficient methodologies. This research investigates the utilization of YOLOv5, a cutting-edge computer vision model, for the purpose of rapidly identifying document layouts and extracting unstructured data.  The present study establishes a conceptual framework for delineating the notion of "objects" as they pertain to documents, incorporating various elements such as paragraphs, tables, photos, and other constituent parts. The main objective is to create an autonomous system that can effectively recognize document layouts and extract unstructured data, hence improving the effectiveness of data extraction.  In the con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17016</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#26080;&#20998;&#24067;&#38480;&#21046;&#30340;&#24179;&#22343;&#20809;&#28369;&#24230;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#30001;Ashlagi&#31561;&#20154;&#65288;2021&#65289;&#25552;&#20986;&#65292;&#29992;&#20110;&#34913;&#37327;&#20989;&#25968;&#30456;&#23545;&#20110;&#20219;&#24847;&#26410;&#30693;&#28508;&#22312;&#20998;&#24067;&#30340;"&#26377;&#25928;"&#20809;&#28369;&#24230;&#12290;&#26368;&#36817;&#30340;Hanneke&#31561;&#20154;&#65288;2023&#65289;&#30340;&#30740;&#31350;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#24179;&#22343;&#20809;&#28369;&#20989;&#25968;&#30340;&#32039;&#23494;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#25928;&#21487;&#23454;&#29616;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30446;&#21069;&#22312;&#26222;&#36941;&#26080;&#20559;&#65288;&#21363;&#26377;&#22122;&#22768;&#65289;&#24773;&#20917;&#19979;&#23578;&#32570;&#20047;&#31867;&#20284;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23436;&#20840;&#22635;&#34917;&#20102;&#36825;&#20123;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#26080;&#20559;&#35774;&#32622;&#20013;&#30340;&#24179;&#22343;&#20809;&#28369;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#20998;&#24067;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25152;&#24471;&#21040;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20197;&#25968;&#25454;&#30340;&#20869;&#22312;&#20960;&#20309;&#24418;&#29366;&#20026;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#20840;&#26377;&#30028;&#24230;&#37327;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#35299;&#20915;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#19981;&#21487;&#29702;&#35299;&#21644;&#32570;&#20047;&#31995;&#32479;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17011</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24863;&#30693;&#33258;&#21160;&#36716;&#25442;&#22686;&#24378;&#29305;&#24449;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Feature Cognition Enhancement via Interaction-Aware Automated Transformation. (arXiv:2309.17011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#35299;&#20915;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#19981;&#21487;&#29702;&#35299;&#21644;&#32570;&#20047;&#31995;&#32479;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#31354;&#38388;&#23545;&#20110;&#38477;&#20302;&#32500;&#25968;&#28798;&#38590;&#12289;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12289;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#20197;&#21450;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#32463;&#20856;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;AutoFE&#65289;&#30340;&#36827;&#23637;&#22312;&#35299;&#20915;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20363;&#22914;&#23545;&#22823;&#37327;&#21171;&#21160;&#21644;&#32463;&#39564;&#30340;&#20381;&#36182;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23884;&#20837;&#21018;&#24615;&#29305;&#24449;&#31354;&#38388;&#37325;&#24314;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#20135;&#29983;&#28508;&#22312;&#30340;&#38590;&#20197;&#29702;&#35299;&#21644;&#19981;&#21512;&#24120;&#29702;&#30340;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#65292;&#28304;&#20110;&#24573;&#35270;&#19987;&#23478;&#32423;&#30340;&#35748;&#30693;&#36807;&#31243;&#65307;2&#65289;&#32570;&#20047;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#32531;&#24930;&#20197;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating an effective representation space is crucial for mitigating the curse of dimensionality, enhancing model generalization, addressing data sparsity, and leveraging classical models more effectively. Recent advancements in automated feature engineering (AutoFE) have made significant progress in addressing various challenges associated with representation learning, issues such as heavy reliance on intensive labor and empirical experiences, lack of explainable explicitness, and inflexible feature space reconstruction embedded into downstream tasks. However, these approaches are constrained by: 1) generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes; 2) lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space. To address these, we introduce an interaction-aware reinforced generation perspective. We redefine feature sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TPP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19978;&#19979;&#25991;&#20107;&#20214;&#23884;&#20837;&#12289;&#26102;&#38388;&#20449;&#24687;&#21644;&#39046;&#22495;&#29305;&#24449;&#26469;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#20013;&#20107;&#20214;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17009</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#30340;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#20013;&#20107;&#20214;&#38598;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain. (arXiv:2309.17009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TPP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19978;&#19979;&#25991;&#20107;&#20214;&#23884;&#20837;&#12289;&#26102;&#38388;&#20449;&#24687;&#21644;&#39046;&#22495;&#29305;&#24449;&#26469;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#20013;&#20107;&#20214;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#22312;&#39044;&#27979;&#25110;&#39044;&#27979;&#20107;&#20214;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#39044;&#27979;&#21516;&#26102;&#21457;&#29983;&#30340;&#22810;&#20010;&#20107;&#20214;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#30149;&#20154;&#24448;&#24448;&#20250;&#21516;&#26102;&#22240;&#20026;&#22810;&#31181;&#30149;&#24773;&#34987;&#36865;&#36827;&#21307;&#38498;&#65292;&#31867;&#20284;&#22320;&#65292;&#20154;&#20204;&#36141;&#20080;&#19981;&#27490;&#19968;&#21482;&#32929;&#31080;&#65292;&#32780;&#19988;&#21516;&#26102;&#21457;&#29983;&#22810;&#20010;&#26032;&#38395;&#20107;&#20214;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20107;&#20214;&#19981;&#20197;&#31163;&#25955;&#30340;&#26102;&#38388;&#38388;&#38548;&#21457;&#29983;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#20013;&#39044;&#27979;&#20107;&#20214;&#38598;&#20173;&#28982;&#23384;&#22312;&#30528;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#31616;&#21333;&#22320;&#25193;&#23637;&#29616;&#26377;&#30340;TPP&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#22788;&#29702;&#25351;&#25968;&#32423;&#25968;&#37327;&#30340;&#20107;&#20214;&#65292;&#25110;&#32773;&#24573;&#30053;&#20107;&#20214;&#38598;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TPP&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#20107;&#20214;&#23884;&#20837;&#12289;&#26102;&#38388;&#20449;&#24687;&#21644;&#39046;&#22495;&#29305;&#24449;&#26469;&#24314;&#27169;&#26102;&#38388;&#20107;&#20214;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naive approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;&#24456;&#23567;&#27604;&#20363;&#65292;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#65292;&#24182;&#19988;&#36825;&#20123;&#38169;&#35823;&#20449;&#24687;&#20250;&#34987;&#27169;&#22411;&#36755;&#20986;&#20256;&#25773;&#12290;&#38754;&#23545;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.17007</link><description>&lt;p&gt;
&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#26131;&#21463;&#26377;&#38024;&#23545;&#24615;&#30340;&#38169;&#35823;&#20449;&#24687;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;&#24456;&#23567;&#27604;&#20363;&#65292;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#65292;&#24182;&#19988;&#36825;&#20123;&#38169;&#35823;&#20449;&#24687;&#20250;&#34987;&#27169;&#22411;&#36755;&#20986;&#20256;&#25773;&#12290;&#38754;&#23545;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21307;&#23398;&#39046;&#22495;&#25317;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#21307;&#23398;&#20449;&#24687;&#30340;&#25512;&#29702;&#65292;&#23545;&#26410;&#26469;&#30340;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#21307;&#23398;&#39046;&#22495;&#23384;&#22312;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#33030;&#24369;&#24615;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#22320;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;1.1&#65285;&#65292;&#25105;&#20204;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#19968;&#20010;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#12290;&#38169;&#35823;&#20449;&#24687;&#20250;&#22312;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20256;&#25773;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#27809;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;1,038&#20010;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#36825;&#31181;&#29305;&#27530;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#24212;&#29992;LLM&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#36825;&#20984;&#26174;&#20102;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#38656;&#35201;&#37319;&#21462;&#24378;&#26377;&#21147;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#36827;&#34892;&#24443;&#24213;&#30340;&#39564;&#35777;&#26426;&#21046;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35775;&#38382;&#36827;&#34892;&#20005;&#26684;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.17002</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#39044;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20808;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#30340;&#24615;&#36136;&#65292;&#24182;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#30340;ImageNet-1K&#21644;YFCC15M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20419;&#36827;&#39046;&#22495;&#20869;&#30340;&#36716;&#31227;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20998;&#24067;&#65307;&#28982;&#32780;&#65292;&#23427;&#24635;&#26159;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#39044;&#35757;&#32451;&#20013;&#30340;&#22122;&#22768;&#20250;&#19981;&#21516;&#22320;&#22609;&#36896;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#26469;&#20351;&#29305;&#24449;&#31354;&#38388;&#36798;&#21040;&#26144;&#23556;&#24182;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20851;&#27880;&#20102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25391;&#21160;&#25968;&#25454;&#26469;&#39044;&#27979;&#25925;&#38556;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17001</link><description>&lt;p&gt;
&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Bearing Fault Classification Approaches. (arXiv:2309.17001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17001
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20851;&#27880;&#20102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25391;&#21160;&#25968;&#25454;&#26469;&#39044;&#27979;&#25925;&#38556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#36724;&#25215;&#23384;&#22312;&#20110;&#21508;&#20010;&#34892;&#19994;&#30340;&#26059;&#36716;&#35774;&#22791;&#20013;&#65292;&#24182;&#19988;&#23545;&#39640;&#25928;&#36816;&#33829;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21450;&#26102;&#26816;&#27979;&#21644;&#20934;&#30830;&#39044;&#27979;&#36724;&#25215;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#24847;&#22806;&#26426;&#22120;&#20572;&#26426;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25913;&#36827;&#32500;&#25252;&#35745;&#21010;&#65292;&#20174;&#32780;&#36991;&#20813;&#25439;&#22833;&#20135;&#33021;&#12290;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#20256;&#24863;&#22120;&#23545;&#36825;&#20123;&#35774;&#22791;&#30340;&#20581;&#24247;&#29366;&#20917;&#36827;&#34892;&#35268;&#27169;&#21270;&#30417;&#27979;&#65292;&#24182;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#39044;&#27979;&#25925;&#38556;&#12290;&#36890;&#36807;&#21152;&#36895;&#36816;&#34892;&#33267;&#25925;&#38556;&#30340;&#36807;&#31243;&#20013;&#37319;&#38598;&#25391;&#21160;&#25968;&#25454;&#65292;&#25110;&#32773;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#25925;&#38556;&#26469;&#37319;&#38598;&#25391;&#21160;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#24037;&#20316;&#26465;&#20214;&#19979;&#65288;&#22914;&#36716;&#36895;&#12289;&#36724;&#25215;&#36127;&#36733;&#12289;&#36724;&#25215;&#25925;&#38556;&#31867;&#22411;&#21644;&#25968;&#25454;&#37319;&#38598;&#39057;&#29575;&#65289;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#25391;&#21160;&#25968;&#25454;&#24320;&#21457;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rolling bearing fault diagnosis has garnered increased attention in recent years owing to its presence in rotating machinery across various industries, and an ever increasing demand for efficient operations. Prompt detection and accurate prediction of bearing failures can help reduce the likelihood of unexpected machine downtime and enhance maintenance schedules, averting lost productivity. Recent technological advances have enabled monitoring the health of these assets at scale using a variety of sensors, and predicting the failures using modern Machine Learning (ML) approaches including deep learning architectures. Vibration data has been collected using accelerated run-to-failure of overloaded bearings, or by introducing known failure in bearings, under a variety of operating conditions such as rotating speed, load on the bearing, type of bearing fault, and data acquisition frequency. However, in the development of bearing failure classification models using vibration data there is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16992</link><description>&lt;p&gt;
Segment Anything Model&#23545;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#20855;&#26377;&#33391;&#22909;&#30340;&#25945;&#23548;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#29305;&#24449;&#30340;&#26816;&#27979;&#21644;&#25551;&#36848;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#25551;&#36848;&#8220;&#20219;&#20309;&#22330;&#26223;&#8221;&#21644;&#8220;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#8221;&#30340;&#20851;&#38190;&#28857;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#33719;&#24471;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAMFeat&#26469;&#24341;&#20837;SAM&#65288;segment anything model&#65289;&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#28608;&#21457;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#65288;PSRD&#65289;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#23558;SAM&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35821;&#20041;&#20449;&#24687;&#36890;&#36807;&#29305;&#24449;&#20851;&#31995;&#33976;&#39311;&#21040;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#39640;&#36890;&#36807;&#35821;&#20041;&#21306;&#20998;&#25913;&#21892;&#26412;&#22320;&#29305;&#24449;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#22312;&#21508;&#31181;RL&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16984</link><description>&lt;p&gt;
&#19968;&#31181;&#20316;&#20026;&#20016;&#23500;&#39640;&#25928;&#30340;&#31574;&#30053;&#31867;&#21035;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#22312;&#21508;&#31181;RL&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#22312;&#24314;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20174;&#22270;&#20687;&#29983;&#25104;&#21040;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#20250;&#24456;&#24930;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#22312;&#20855;&#26377;&#36845;&#20195;&#37319;&#26679;&#30340;RL&#20013;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#31574;&#30053;&#34920;&#31034;&#65292;&#21363;&#19968;&#33268;&#24615;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#28436;&#21592;-&#35780;&#35770;&#23478;&#39118;&#26684;&#30340;&#31639;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#20856;&#22411;&#30340;RL&#35774;&#32622;&#65306;&#31163;&#32447;&#12289;&#31163;&#32447;&#21040;&#22312;&#32447;&#21644;&#22312;&#32447;&#12290;&#23545;&#20110;&#31163;&#32447;RL&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23545;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;RL&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#25193;&#25955;&#31574;&#30053;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#24615;&#33021;&#21487;&#27604;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#29978;&#33267;&#27604;&#25193;&#25955;&#31574;&#30053;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16977</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;DRL&#25511;&#21046;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#24212;&#29992;&#20102;&#19968;&#31181;&#29616;&#26377;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#22122;&#22768;&#25552;&#21462;&#65292;&#20197;&#26126;&#30830;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#21487;&#38752;&#24615;&#65306;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26500;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#21516;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35780;&#20272;&#32593;&#32476;&#30340;&#21442;&#25968;&#34987;&#26356;&#26032;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22522;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#24046;&#24322;&#35780;&#20272;&#29305;&#23450;&#29366;&#24577;&#19979;DRL&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Habana Gaudi&#22788;&#29702;&#22120;&#65292;&#23545;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#21152;&#36895;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#20840;&#38754;&#24615;&#33021;&#27604;&#36739;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16976</link><description>&lt;p&gt;
&#22312;Habana Gaudi&#22788;&#29702;&#22120;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#28145;&#20837;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors. (arXiv:2309.16976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Habana Gaudi&#22788;&#29702;&#22120;&#65292;&#23545;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#21152;&#36895;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#20840;&#38754;&#24615;&#33021;&#27604;&#36739;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#38754;&#20020;&#30528;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#19987;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#22914;Habana GAUDI&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;GAUDI&#25317;&#26377;&#30697;&#38453;&#20056;&#27861;&#24341;&#25806;&#65288;MME&#65289;&#21644;&#19968;&#31751;&#23436;&#20840;&#21487;&#32534;&#31243;&#30340;&#24352;&#37327;&#22788;&#29702;&#26680;&#24515;&#65288;TPC&#65289;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;GAUDI&#22788;&#29702;&#22120;&#21152;&#36895;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#35813;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;MME&#21644;TPC&#32452;&#20214;&#20043;&#38388;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20248;&#21270;MME&#21644;TPC&#21033;&#29992;&#29575;&#30340;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#30340;&#23454;&#29992;&#27934;&#23519;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;GAUDI&#26500;&#26550;&#30340;Transformer&#27169;&#22411;&#20248;&#21270;&#21644;&#24615;&#33021;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RO2O&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#24179;&#28369;&#24615;&#22686;&#24378;&#31163;&#32447;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#23398;&#20064;&#20013;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16973</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#24179;&#28369;&#24615;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#31163;&#32447;&#21040;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness. (arXiv:2309.16973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RO2O&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#24179;&#28369;&#24615;&#22686;&#24378;&#31163;&#32447;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#23398;&#20064;&#20013;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20197;&#36739;&#23569;&#30340;&#20114;&#21160;&#27425;&#25968;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65289;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#25506;&#32034;&#20449;&#24687;&#20016;&#23500;&#30340;&#36716;&#25442;&#65289;&#30456;&#32467;&#21512;&#12290;&#31163;&#32447;&#21040;&#22312;&#32447;&#65288;O2O&#65289;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#36827;&#31163;&#32447;&#35757;&#32451;&#20195;&#29702;&#30340;&#33539;&#20363;&#65292;&#20294;&#30001;&#20110;&#22312;&#32447;&#32463;&#39564;&#19982;&#31163;&#32447;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#24046;&#65292;&#22823;&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;O2O&#36866;&#24212;&#20013;&#24615;&#33021;&#19979;&#38477;&#24182;&#26080;&#27861;&#23454;&#29616;&#31283;&#23450;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Robust Offline-to-Online&#65288;RO2O&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#24179;&#28369;&#24615;&#22686;&#24378;&#31163;&#32447;&#31574;&#30053;&#65292;&#24182;&#20943;&#23569;&#22312;&#32447;&#36866;&#24212;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RO2O&#31639;&#27861;&#36890;&#36807;Q-ensemble&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#23454;&#29616;&#31574;&#30053;&#21644;&#20215;&#20540;&#30340;&#24179;&#28369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;RO2O&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
To obtain a near-optimal policy with fewer interactions in Reinforcement Learning (RL), a promising approach involves the combination of offline RL, which enhances sample efficiency by leveraging offline datasets, and online RL, which explores informative transitions by interacting with the environment. Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained agent within limited online interactions. However, due to the significant distribution shift between online experiences and offline data, most offline RL algorithms suffer from performance drops and fail to achieve stable policy improvement in O2O adaptation. To address this problem, we propose the Robust Offline-to-Online (RO2O) algorithm, designed to enhance offline policies through uncertainty and smoothness, and to mitigate the performance drop in online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty penalty and adversarial samples for policy and value smoothness, which enable RO2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#21046;&#22791;&#36895;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16972</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning. (arXiv:2309.16972v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#21046;&#22791;&#36895;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#36739;&#22823;&#65292;&#24182;&#19988;&#29616;&#26377;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#37319;&#29992;&#38454;&#26799;&#22411;&#22870;&#21169;&#20989;&#25968;&#65292;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#22312;&#26377;&#38480;&#26465;&#20214;&#19979;&#24456;&#38590;&#39640;&#20445;&#30495;&#24230;&#21046;&#22791;&#25152;&#38656;&#30340;&#30446;&#26631;&#37327;&#23376;&#24577;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#30340;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#12290;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#37327;&#23376;&#38376;&#31867;&#22411;&#21644;&#37327;&#23376;&#24577;&#28436;&#21270;&#26102;&#38388;&#30340;&#38480;&#21046;&#12290;&#22312;&#21046;&#22791;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#26435;&#24046;&#20998;&#21160;&#24577;&#22870;&#21169;&#20989;&#25968;&#65292;&#36741;&#21161;&#31639;&#27861;&#24555;&#36895;&#33719;&#24471;&#26368;&#22823;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#37319;&#21462;&#33258;&#36866;&#24212;&#949;-greedy&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#21462;&#24471;&#19968;&#23450;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16971</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20165;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#38754;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;FNO&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#29289;&#29702;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;FNO&#65288;MRA-FNO&#65289;&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#65292;&#23613;&#37327;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#65292;&#21516;&#26102;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#20998;&#36776;&#29575;FNO&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#39564;&#25512;&#29702;&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#25928;&#29992;&#25104;&#26412;&#27604;&#20316;&#20026;&#33719;&#21462;&#20989;&#25968;&#65292;&#22312;&#27599;&#19968;&#27493;&#33719;&#21462;&#26032;&#30340;&#26679;&#26412;&#21644;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30697;&#21305;&#37197;&#21644;&#30697;&#38453;&#34892;&#21015;&#24335;&#24341;&#29702;&#23454;&#29616;&#20102;&#21487;&#34892;&#65292;&#39640;&#25928;&#30340;&#25928;&#29992;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16970</link><description>&lt;p&gt;
&#20855;&#26377;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#26159;&#20998;&#26512;&#20915;&#31574;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#65288;MNLs&#65289;&#22240;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;ASU-DNN&#65289;&#30340;MNLs&#65292;&#24182;&#19988;&#22312;&#34892;&#20026;&#36873;&#25321;&#30340;&#39044;&#27979;&#31934;&#24230;&#19978;&#27604;&#20256;&#32479;MNLs&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#32467;&#26500;&#32780;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#19996;&#20140;&#25910;&#38598;&#30340;&#20986;&#34892;&#35843;&#26597;&#25968;&#25454;&#35780;&#20272;&#20102;&#20855;&#26377;GAUNet&#30340;MNL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;ASU-DNN&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26367;&#20195;&#23494;&#38053;&#26469;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16952</link><description>&lt;p&gt;
&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26367;&#20195;&#23494;&#38053;&#26469;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#38752;&#30340;&#29992;&#25143;&#21487;&#20197;&#28389;&#29992;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#21697;&#24182;&#21442;&#19982;&#22312;&#32447;&#30340;&#22403;&#22334;&#20449;&#24687;&#25110;&#34394;&#20551;&#23459;&#20256;&#27963;&#21160;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#26631;&#35760;&#38544;&#34255;&#20449;&#24687;&#26469;&#38450;&#27490;&#28389;&#29992;&#65292;&#24182;&#20351;&#29992;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#30340;&#26680;&#24515;&#23433;&#20840;&#23646;&#24615;&#26159;&#40065;&#26834;&#24615;&#65292;&#21363;&#25915;&#20987;&#32773;&#21482;&#33021;&#36890;&#36807;&#22823;&#24133;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#26469;&#36867;&#36991;&#26816;&#27979;&#12290;&#35780;&#20272;&#40065;&#26834;&#24615;&#38656;&#35201;&#20026;&#29305;&#23450;&#30340;&#27700;&#21360;&#31639;&#27861;&#35774;&#35745;&#33258;&#36866;&#24212;&#25915;&#20987;&#12290;&#35780;&#20272;&#27700;&#21360;&#31639;&#27861;&#21450;&#20854;&#65288;&#33258;&#36866;&#24212;&#65289;&#25915;&#20987;&#26102;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#30830;&#23450;&#33258;&#36866;&#24212;&#25915;&#20987;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#65292;&#21363;&#23427;&#26159;&#26368;&#20339;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30446;&#26631;&#20989;&#25968;&#24182;&#23558;&#33258;&#36866;&#24212;&#25915;&#20987;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#21019;&#24314;&#21487;&#24494;&#20998;&#30340;&#26367;&#20195;&#23494;&#38053;&#26469;&#26412;&#22320;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#20197;&#20248;&#21270;&#25915;&#20987;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in online spam or disinformation campaigns. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. A challenge when evaluating watermarking algorithms and their (adaptive) attacks is to determine whether an adaptive attack is optimal, i.e., it is the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.16951</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27700;&#36136;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#36164;&#28304;&#26159;&#20154;&#31867;&#29983;&#35745;&#21644;&#32463;&#27982;&#36827;&#27493;&#30340;&#22522;&#30784;&#65292;&#19982;&#20844;&#20849;&#20581;&#24247;&#21644;&#29615;&#22659;&#31119;&#31049;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#12290;&#20934;&#30830;&#39044;&#27979;&#27700;&#36136;&#26159;&#25913;&#21892;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#23545;&#25239;&#27745;&#26579;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#22810;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;XGBoost&#65292;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#32654;&#22269;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;LightGBM&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;MLP&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#23545;&#29305;&#24449;&#32553;&#25918;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#31354;&#38388;&#32771;&#34385;&#22240;&#32032;&#26041;&#38754;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#25152;&#21462;&#24471;&#30340;&#20248;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21487;&#24863;&#30693;&#24863;&#27979;&#26426;&#65288;NeuIM&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#30913;&#30636;&#24577;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#32531;&#24930;&#24863;&#24212;&#26426;&#21160;&#21147;&#23398;&#30340;&#25429;&#25417;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.16943</link><description>&lt;p&gt;
&#29289;&#29702;&#21487;&#24863;&#30693;&#24863;&#27979;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Induction Machine Modelling. (arXiv:2309.16943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21487;&#24863;&#30693;&#24863;&#27979;&#26426;&#65288;NeuIM&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#30913;&#30636;&#24577;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#32531;&#24930;&#24863;&#24212;&#26426;&#21160;&#21147;&#23398;&#30340;&#25429;&#25417;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#24863;&#24212;&#26426;&#65288;NeuIM&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#21487;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#23454;&#29616;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#30913;&#30636;&#24577;&#27169;&#25311;&#12290;&#20854;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#24418;&#25104;&#20102;&#33021;&#22815;&#22312;&#30456;&#22495;&#20013;&#34920;&#31034;&#24863;&#24212;&#26426;&#30340;NeuIM&#65307;&#65288;2&#65289;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#24555;&#36895;&#21644;&#32531;&#24930;&#24863;&#24212;&#26426;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#21487;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65307;&#21644;&#65288;3&#65289;&#19968;&#31181;&#25968;&#25454;-&#29289;&#29702;&#32467;&#21512;&#30340;&#28151;&#21512;NeuIM&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#24773;&#20917;&#12290;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;NeuIM&#30340;&#25928;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20043;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This rapid communication devises a Neural Induction Machine (NeuIM) model, which pilots the use of physics-informed machine learning to enable AI-based electromagnetic transient simulations. The contributions are threefold: (1) a formation of NeuIM to represent the induction machine in phase domain; (2) a physics-informed neural network capable of capturing fast and slow IM dynamics even in the absence of data; and (3) a data-physics-integrated hybrid NeuIM approach which is adaptive to various levels of data availability. Extensive case studies validate the efficacy of NeuIM and in particular, its advantage over purely data-driven approaches.
&lt;/p&gt;</description></item><item><title>G4SATBench&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#19982;&#25628;&#32034;&#22411;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#36824;&#25581;&#31034;&#20102;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.16941</link><description>&lt;p&gt;
G4SATBench: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#21644;&#25913;&#36827;SAT&#27714;&#35299;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks. (arXiv:2309.16941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16941
&lt;/p&gt;
&lt;p&gt;
G4SATBench&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#19982;&#25628;&#32034;&#22411;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#36824;&#25581;&#31034;&#20102;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20316;&#20026;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;(SAT)&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20026;&#20256;&#32479;&#30340;&#22238;&#28335;&#25110;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#25991;&#29486;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#21644;&#20844;&#27491;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G4SATBench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#24314;&#31435;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#22312;G4SATBench&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;7&#20010;&#38382;&#39064;&#21644;3&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#19979;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#25506;&#32034;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#29702;&#35299;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36824;&#23558;&#23427;&#20204;&#30340;&#27714;&#35299;&#36807;&#31243;&#19982;&#22522;&#20110;&#25628;&#32034;&#30340;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers
&lt;/p&gt;</description></item><item><title>PC-Adapter&#26159;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20445;&#30041;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#21644;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16936</link><description>&lt;p&gt;
PC-Adapter: &#22522;&#20110;&#25311;&#21512;&#20266;&#26631;&#31614;&#30340;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label. (arXiv:2309.16936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16936
&lt;/p&gt;
&lt;p&gt;
PC-Adapter&#26159;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20445;&#30041;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#21644;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29289;&#20307;&#23610;&#24230;&#65292;&#20256;&#24863;&#22120;&#35282;&#24230;&#21644;&#33258;&#36974;&#25377;&#24341;&#36215;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#65292;&#29702;&#35299;&#20174;&#30495;&#23454;&#19990;&#30028;&#20013;&#25429;&#33719;&#30340;&#28857;&#20113;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#33258;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#26368;&#26032;&#23398;&#20064;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#31616;&#27905;&#32780;&#24378;&#22823;&#22320;&#23545;&#28857;&#20113;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#28857;&#20113;&#25968;&#25454;&#22312;&#39046;&#22495;&#20559;&#31227;&#22330;&#26223;&#19979;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20445;&#25345;&#28304;&#39046;&#22495;&#30340;&#20840;&#23616;&#20960;&#20309;&#24418;&#29366;&#20449;&#24687;&#20197;&#21450;&#30446;&#26631;&#20266;&#26631;&#31614;&#36235;&#21183;&#23545;&#28304;&#26631;&#31614;&#20998;&#24067;&#26377;&#20559;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35266;&#23519;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#37197;&#22120;&#24341;&#23548;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;PC-Adapter&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36866;&#37197;&#22120;&#20445;&#30041;&#28304;&#39046;&#22495;&#30340;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;&#21478;&#19968;&#20010;&#37197;&#22791;&#20102;&#22270;&#21367;&#31215;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#26377;&#25928;&#20943;&#23569;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propo
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.16932</link><description>&lt;p&gt;
&#23545;&#31216;&#24615;&#23548;&#33268;&#23398;&#20064;&#30340;&#32467;&#26500;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24120;&#35265;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#23545;&#31216;&#24615;&#22312;&#24403;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#24403;&#26435;&#37325;&#34928;&#20943;&#25110;&#26799;&#24230;&#22122;&#22768;&#36739;&#22823;&#26102;&#65292;&#36825;&#31181;&#32422;&#26463;&#23558;&#25104;&#20026;&#39318;&#36873;&#35299;&#12290;&#20316;&#20026;&#30452;&#25509;&#25512;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#23548;&#33268;&#31232;&#30095;&#24615;&#65292;&#26059;&#36716;&#23545;&#31216;&#24615;&#23548;&#33268;&#20302;&#31209;&#24615;&#65292;&#32622;&#25442;&#23545;&#31216;&#24615;&#23548;&#33268;&#21516;&#36136;&#38598;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29702;&#35770;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#21508;&#31181;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#23545;&#31216;&#24615;&#35774;&#35745;&#21487;&#24494;&#20998;&#23454;&#26045;&#30828;&#24615;&#32422;&#26463;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16928</link><description>&lt;p&gt;
&#23398;&#20064;&#25509;&#21463;&#24110;&#21161;&#65306;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39640;&#32423;&#27010;&#24565;&#26500;&#24314;&#21644;&#35299;&#37322;&#31070;&#32463;&#26550;&#26500;&#30340;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20854;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#27530;&#23646;&#24615;&#26159;&#23427;&#20204;&#20801;&#35768;&#27010;&#24565;&#24178;&#39044;&#65292;&#29992;&#25143;&#21487;&#20197;&#32416;&#27491;&#34987;&#38169;&#35823;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24178;&#39044;&#26377;&#25928;&#24615;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#24178;&#39044;&#27010;&#24565;&#30340;&#39034;&#24207;&#20197;&#21450;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#28304;&#20110;CBM&#22312;&#35757;&#32451;&#26102;&#32570;&#20047;&#27169;&#22411;&#36866;&#24212;&#27010;&#24565;&#24178;&#39044;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;IntCEMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CBM&#30340;&#26032;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#27979;&#35797;&#26102;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#31574;&#30053;&#65292;&#20174;&#20013;&#21487;&#20197;&#37319;&#26679;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#21487;&#20197;&#22686;&#21152;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.16923</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#24335;&#36830;&#36890;&#24615;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mode Connectivity and Data Heterogeneity of Federated Learning. (arXiv:2309.16923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#21487;&#20197;&#22686;&#21152;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#35753;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26356;&#26032;&#20043;&#38388;&#30340;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#30740;&#31350;&#36739;&#23569;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#26356;&#26032;&#28418;&#31227;&#21040;&#20102;&#21738;&#37324;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27169;&#24335;&#36830;&#36890;&#24615;&#36827;&#34892;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#27169;&#24335;&#36830;&#36890;&#24615;&#21487;&#20197;&#34913;&#37327;&#22312;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21442;&#25968;&#36335;&#24452;&#19978;&#24615;&#33021;&#30340;&#21464;&#21270;&#65288;&#21363;&#36830;&#36890;&#24615;&#65289;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#20351;&#24471;&#19981;&#21516;&#36335;&#24452;&#19978;&#30340;&#36830;&#36890;&#24615;&#26356;&#21152;&#30456;&#20284;&#65292;&#24418;&#25104;&#20102;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#26356;&#22810;&#30340;&#20302;&#35823;&#24046;&#37325;&#21472;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#32447;&#24615;&#36830;&#25509;&#20004;&#20010;&#20840;&#23616;&#27169;&#24335;&#26102;&#23384;&#22312;&#36830;&#36890;&#24615;&#38556;&#30861;&#65292;&#32780;&#32771;&#34385;&#38750;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#21518;&#36830;&#36890;&#24615;&#38556;&#30861;&#28040;&#22833;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#22343;&#22330;&#29702;&#35770;&#25110;dropout&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to train a model while keeping their data private collaboratively. Previous studies have shown that data heterogeneity between clients leads to drifts across client updates. However, there are few studies on the relationship between client and global modes, making it unclear where these updates end up drifting. We perform empirical and theoretical studies on this relationship by utilizing mode connectivity, which measures performance change (i.e., connectivity) along parametric paths between different modes. Empirically, reducing data heterogeneity makes the connectivity on different paths more similar, forming more low-error overlaps between client and global modes. We also find that a barrier to connectivity occurs when linearly connecting two global modes, while it disappears with considering non-linear mode connectivity. Theoretically, we establish a quantitative bound on the global-mode connectivity using mean-field theory or dropou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16918</link><description>&lt;p&gt;
ACGAN-GNNExplainer&#65306;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36741;&#21161;&#26465;&#20214;&#29983;&#25104;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#20915;&#31574;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;GNN&#35299;&#37322;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20381;&#36182;&#24615;&#65292;&#23545;&#26410;&#35265;&#36807;&#30340;&#22270;&#30340;&#19968;&#33324;&#24615;&#19981;&#36275;&#65292;&#21487;&#33021;&#20135;&#29983;&#26080;&#25928;&#30340;&#35299;&#37322;&#20197;&#21450;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#20805;&#20998;&#30340;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#35299;&#37322;&#22120;ACGAN-GNNExplainer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#36816;&#29992;&#37492;&#21035;&#22120;&#26469;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#30830;&#20445;&#35299;&#37322;&#30340;&#20445;&#30495;&#24230;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35780;&#20272;&#20998;&#21035;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
&lt;/p&gt;</description></item><item><title>ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.16916</link><description>&lt;p&gt;
ONNXExplainer:&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16916
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#20570;&#20986;&#26576;&#20123;&#20915;&#31574;&#19982;&#25512;&#29702;&#24615;&#33021;&#19968;&#26679;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20854;&#20013;Shapley&#20540;&#26368;&#21463;&#27426;&#36814;&#12290;SHAP&#21253;&#26159;&#35299;&#37322;&#20351;&#29992;TensorFlow&#25110;PyTorch&#23454;&#29616;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;Shapley&#20540;&#30340;&#39046;&#20808;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#36328;&#24179;&#21488;&#25903;&#25345;&#12289;&#19968;&#27425;&#24615;&#37096;&#32626;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ONNXExplainer&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ONNX&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;Shapley&#20540;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;ONNXExplainer&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#19968;&#27425;&#24615;&#37096;&#32626;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#20844;&#24179;&#27604;&#36739;&#30446;&#30340;&#65292;&#25105;&#20204;&#36824;&#22312;TensorFlow&#21644;PyTorch&#20013;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;RecAD&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16896</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse for Anomaly Detection in Multivariate Time Series. (arXiv:2309.16896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;RecAD&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#36890;&#24120;&#34920;&#31034;&#20020;&#30028;&#20107;&#20214;&#65292;&#20363;&#22914;&#31995;&#32479;&#25925;&#38556;&#25110;&#22806;&#37096;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#26377;&#25928;&#20043;&#22806;&#65292;&#25512;&#33616;&#24322;&#24120;&#32531;&#35299;&#34892;&#21160;&#22312;&#23454;&#36341;&#20013;&#20063;&#24456;&#37325;&#35201;&#20294;&#30740;&#31350;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65292;&#21363;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#20415;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;&#31216;&#20026;RecAD&#65292;&#21487;&#20197;&#25512;&#33616;&#32763;&#36716;&#24322;&#24120;&#26102;&#38388;&#27493;&#39588;&#30340;&#34917;&#25937;&#34892;&#21160;&#12290;&#23545;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in multivariate time series has received extensive study due to the wide spectrum of applications. An anomaly in multivariate time series usually indicates a critical event, such as a system fault or an external attack. Therefore, besides being effective in anomaly detection, recommending anomaly mitigation actions is also important in practice yet under-investigated. In this work, we focus on algorithmic recourse in time series anomaly detection, which is to recommend fixing actions on abnormal time series with a minimum cost so that domain experts can understand how to fix the abnormal behavior. To this end, we propose an algorithmic recourse framework, called RecAD, which can recommend recourse actions to flip the abnormal time steps. Experiments on two synthetic and one real-world datasets show the effectiveness of our framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.16883</link><description>&lt;p&gt;
&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;Lipschitz-&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22122;&#22768;&#36755;&#20837;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#21322;&#24452;&#26159;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36275;&#22815;&#35748;&#35777;&#21322;&#24452;&#30340;&#39640;&#25928;&#20998;&#31867;&#22120;&#21602;&#65311;&#38543;&#26426;&#24179;&#28369;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#33719;&#24471;&#24179;&#28369;&#19988;&#26356;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;&#21478;&#22806;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#21363;&#20854;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#23545;&#24179;&#28369;&#20998;&#31867;&#22120;&#21644;&#32463;&#39564;&#26041;&#24046;&#30340;&#21452;&#37325;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#65292;&#20197;&#20415;&#36890;&#36807;Bernst&#30340;&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;&#26469;&#21033;&#29992;&#22522;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28040;&#24687;&#20256;&#25773;&#36890;&#36807;&#26102;&#38388;&#65288;MPTT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20445;&#30041;&#24207;&#21015;&#20381;&#36182;&#24615;&#12290;MPTT&#36890;&#36807;&#24322;&#27493;&#31649;&#29702;&#21021;&#22987;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#37319;&#29992;&#19977;&#31181;&#31574;&#30053;&#36807;&#28388;&#36807;&#26399;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;RNN&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16882</link><description>&lt;p&gt;
&#28040;&#24687;&#22312;&#26102;&#38388;&#20013;&#30340;&#20256;&#25773;&#65306;&#19968;&#31181;&#29992;&#20110;&#20445;&#30041;&#26102;&#24207;&#24314;&#27169;&#20013;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling. (arXiv:2309.16882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28040;&#24687;&#20256;&#25773;&#36890;&#36807;&#26102;&#38388;&#65288;MPTT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20445;&#30041;&#24207;&#21015;&#20381;&#36182;&#24615;&#12290;MPTT&#36890;&#36807;&#24322;&#27493;&#31649;&#29702;&#21021;&#22987;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#37319;&#29992;&#19977;&#31181;&#31574;&#30053;&#36807;&#28388;&#36807;&#26399;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;RNN&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26159;&#31185;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#28982;&#32780;&#22312;&#20351;&#29992;&#20256;&#32479;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#31574;&#30053;&#65288;&#20551;&#35774;&#26679;&#26412;&#29420;&#31435;&#21516;&#20998;&#24067;&#19988;RNN&#30340;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#20026;&#38646;&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;RNN&#65289;&#26102;&#65292;&#32463;&#24120;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20551;&#35774;&#24573;&#35270;&#20102;&#26679;&#26412;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28040;&#24687;&#20256;&#25773;&#36890;&#36807;&#26102;&#38388;&#65288;MPTT&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#20837;&#38271;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#23545;&#20110;&#26377;&#29366;&#24577;&#35299;&#20915;&#26041;&#26696;&#26356;&#24555;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;MPTT&#21033;&#29992;&#20004;&#20010;&#20869;&#23384;&#27169;&#22359;&#24322;&#27493;&#31649;&#29702;RNN&#30340;&#21021;&#22987;&#38544;&#34255;&#29366;&#24577;&#65292;&#20419;&#36827;&#26679;&#26412;&#20043;&#38388;&#30340;&#26080;&#32541;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;epochs&#26399;&#38388;&#20351;&#29992;&#22810;&#26679;&#30340;&#23567;&#25209;&#37327;&#12290;MPTT&#36827;&#19968;&#27493;&#23454;&#26045;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#36807;&#28388;&#36807;&#26399;&#30340;&#20449;&#24687;&#24182;&#20445;&#30041;&#38544;&#34255;&#29366;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#21021;&#22987;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#26041;&#20415;RNN&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling, a crucial area in science, often encounters challenges when training Machine Learning (ML) models like Recurrent Neural Networks (RNNs) using the conventional mini-batch training strategy that assumes independent and identically distributed (IID) samples and initializes RNNs with zero hidden states. The IID assumption ignores temporal dependencies among samples, resulting in poor performance. This paper proposes the Message Propagation Through Time (MPTT) algorithm to effectively incorporate long temporal dependencies while preserving faster training times relative to the stateful solutions. MPTT utilizes two memory modules to asynchronously manage initial hidden states for RNNs, fostering seamless information exchange between samples and allowing diverse mini-batches throughout epochs. MPTT further implements three policies to filter outdated and preserve essential information in the hidden states to generate informative initial hidden states for RNNs, facilitati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;&#25513;&#34109;&#25928;&#24212;&#21644;&#29983;&#25104;&#25928;&#24212;&#22312;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16878</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;&#25513;&#34109;&#25928;&#24212;&#21644;&#29983;&#25104;&#25928;&#24212;&#22312;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#25239;&#25200;&#21160;&#24182;&#19981;&#20813;&#30123;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#23545;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#20026;&#20309;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26681;&#26412;&#21407;&#22240;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#25506;&#32034;&#23545;&#25239;&#25200;&#21160;&#20013;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#31181;&#25915;&#20987;&#31639;&#27861;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#20013;&#34920;&#29616;&#20986;&#30340;&#20004;&#31181;&#19981;&#21516;&#25928;&#24212;&#12290;&#22312;&#38750;&#23450;&#21521;&#25915;&#20987;&#20013;&#65292;&#25513;&#34109;&#25928;&#24212;&#26356;&#20026;&#26174;&#33879;&#65292;&#32780;&#22312;&#23450;&#21521;&#25915;&#20987;&#20013;&#65292;&#29983;&#25104;&#25928;&#24212;&#26356;&#20026;&#24120;&#35265;&#12290;&#36890;&#36807;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#30772;&#22351;&#30446;&#26631;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#23384;&#22312;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m
&lt;/p&gt;</description></item><item><title>LEF&#26159;&#19968;&#31181;&#29992;&#20110;LiDAR 3D&#29289;&#20307;&#26816;&#27979;&#30340;&#36831;&#21040;&#26089;&#26399;&#26102;&#38388;&#34701;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#24863;&#30693;&#30340;&#28508;&#22312;&#23884;&#20837;&#34701;&#21512;&#21040;&#26089;&#26399;&#38454;&#27573;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#23039;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.16870</link><description>&lt;p&gt;
LEF: LiDAR 3D&#29289;&#20307;&#26816;&#27979;&#30340;&#36831;&#21040;&#26089;&#26399;&#26102;&#38388;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection. (arXiv:2309.16870v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16870
&lt;/p&gt;
&lt;p&gt;
LEF&#26159;&#19968;&#31181;&#29992;&#20110;LiDAR 3D&#29289;&#20307;&#26816;&#27979;&#30340;&#36831;&#21040;&#26089;&#26399;&#26102;&#38388;&#34701;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#24863;&#30693;&#30340;&#28508;&#22312;&#23884;&#20837;&#34701;&#21512;&#21040;&#26089;&#26399;&#38454;&#27573;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#26102;&#38388;LiDAR&#28857;&#20113;&#36827;&#34892;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#36831;&#21040;&#26089;&#26399;&#24490;&#29615;&#29305;&#24449;&#34701;&#21512;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#23558;&#30446;&#26631;&#24863;&#30693;&#30340;&#28508;&#22312;&#23884;&#20837;&#34701;&#21512;&#21040;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#19982;&#30452;&#25509;&#20174;&#21407;&#22987;&#28857;&#23398;&#20064;&#30456;&#27604;&#65292;&#36825;&#31181;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#23039;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#24490;&#29615;&#30340;&#26041;&#24335;&#36827;&#34892;&#36831;&#21040;&#26089;&#26399;&#29305;&#24449;&#34701;&#21512;&#12290;&#36825;&#36890;&#36807;&#22312;&#26102;&#38388;&#26657;&#20934;&#21644;&#23545;&#40784;&#30340;&#31232;&#30095;&#26609;&#29366;&#20196;&#29260;&#19978;&#26045;&#21152;&#22522;&#20110;&#31383;&#21475;&#30340;&#27880;&#24847;&#21147;&#22359;&#26469;&#23454;&#29616;&#12290;&#21033;&#29992;&#40479;&#30640;&#22270;&#21069;&#26223;&#26609;&#29366;&#20998;&#21106;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#38656;&#35201;&#34701;&#21512;&#21040;&#24403;&#21069;&#24103;&#20013;&#30340;&#31232;&#30095;&#21382;&#21490;&#29305;&#24449;&#25968;&#37327;&#20943;&#23569;&#20102;10&#20493;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#38271;&#24230;&#30340;FrameDrop&#35757;&#32451;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#38656;&#27714;&#35843;&#25972;&#24103;&#38271;&#24230;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#37319;&#29992;&#30340;Waymo Open Dataset&#21644;d&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
We propose a late-to-early recurrent feature fusion scheme for 3D object detection using temporal LiDAR point clouds. Our main motivation is fusing object-aware latent embeddings into the early stages of a 3D object detector. This feature fusion strategy enables the model to better capture the shapes and poses for challenging objects, compared with learning from raw points directly. Our method conducts late-to-early feature fusion in a recurrent manner. This is achieved by enforcing window-based attention blocks upon temporally calibrated and aligned sparse pillar tokens. Leveraging bird's eye view foreground pillar segmentation, we reduce the number of sparse history features that our model needs to fuse into its current frame by 10$\times$. We also propose a stochastic-length FrameDrop training technique, which generalizes the model to variable frame lengths at inference for improved performance without retraining. We evaluate our method on the widely adopted Waymo Open Dataset and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36890;&#36807;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#32467;&#21512;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#25311;&#21512;&#21040;2-3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16859</link><description>&lt;p&gt;
&#21069;&#35328;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20808;&#39564;&#29992;&#20110;&#23569;&#26679;&#26412;&#36229;&#39640;&#20998;&#36776;&#29575;&#20154;&#33080;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis. (arXiv:2309.16859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36890;&#36807;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#32467;&#21512;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#25311;&#21512;&#21040;2-3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NeRFs&#24050;&#32463;&#23454;&#29616;&#20102;&#21253;&#25324;&#22797;&#26434;&#30340;&#22836;&#21457;&#21644;&#30382;&#32932;&#22312;&#20869;&#30340;&#20154;&#33080;&#39640;&#24230;&#30495;&#23454;&#30340;&#21512;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#35270;&#22270;&#36755;&#20837;&#22270;&#20687;&#65292;&#20351;&#35813;&#36807;&#31243;&#23545;&#30828;&#20214;&#35201;&#27714;&#36739;&#39640;&#19988;&#32321;&#29712;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#20808;&#39564;&#27169;&#22411;&#30001;&#19968;&#20010;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;NeRF&#32452;&#25104;&#65292;&#32463;&#36807;&#22312;&#24050;&#30693;&#25668;&#20687;&#26426;&#26631;&#23450;&#30340;&#22810;&#26679;&#20154;&#31867;&#20302;&#20998;&#36776;&#29575;&#22810;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#12290;&#23613;&#31649;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;2&#25110;3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.16858</link><description>&lt;p&gt;
Transductive Learning&#30340;&#23574;&#38160;&#27867;&#21270;&#65306;&#19968;&#31181;Transductive Local Rademacher Complexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16858
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20256;&#32479;&#30340;local rademacher complexity (LRC)&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;transductive&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;LRC&#26041;&#27861;&#22312;&#24402;&#32435;&#35774;&#32622;&#20013;&#30340;&#20998;&#26512;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rademacher complex&#30340;&#23616;&#37096;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;transductive learning&#38382;&#39064;&#65292;&#24182;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#19982;LRC&#30340;&#21457;&#23637;&#31867;&#20284;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29420;&#31435;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#24320;&#22987;&#26500;&#24314;TLRC&#65292;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
&lt;/p&gt;</description></item><item><title>&#22312;&#29289;&#32852;&#32593;&#20013;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#21033;&#29992;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#36229;&#20010;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.16854</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#36229;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Applications of Federated Learning in IoT for Hyper Personalisation. (arXiv:2309.16854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16854
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#21033;&#29992;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#36229;&#20010;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#20159;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#37096;&#32626;&#65292;&#21033;&#29992;&#26356;&#24555;&#30340;&#20114;&#32852;&#32593;&#21644;&#35775;&#38382;&#26356;&#22810;&#33410;&#28857;&#30340;&#26426;&#20250;&#12290;&#36825;&#20123;&#35774;&#22791;&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#24182;&#27809;&#26377;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#12290;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#36229;&#20010;&#24615;&#21270;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Billions of IoT devices are being deployed, taking advantage of faster internet, and the opportunity to access more endpoints. Vast quantities of data are being generated constantly by these devices but are not effectively being utilised. Using FL training machine learning models over these multiple clients without having to bring it to a central server. We explore how to use such a model to implement ultra levels of personalization unlike before
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#30340;&#20559;&#31227;&#33539;&#22260;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#20943;&#23569;&#20102;10&#20493;&#20197;&#19978;&#65292;&#24182;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2309.16849</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#30340;&#20559;&#31227;&#33539;&#22260;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#20943;&#23569;&#20102;10&#20493;&#20197;&#19978;&#65292;&#24182;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35270;&#39057;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#65292;&#26377;&#25928;&#35745;&#31639;&#27880;&#24847;&#21147;&#22270;&#23545;&#20110;&#35270;&#39057;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;&#38750;&#23616;&#37096;&#25628;&#32034;&#23545;&#20110;&#27599;&#20010;&#26597;&#35810;&#28857;&#21608;&#22260;&#30340;&#31383;&#21475;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#20294;&#31383;&#21475;&#30340;&#22823;&#23567;&#26080;&#27861;&#23481;&#32435;&#36816;&#21160;&#12290;&#38271;&#31243;&#36816;&#21160;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#39044;&#27979;&#19982;&#27599;&#20010;&#26597;&#35810;&#20301;&#32622;&#30340;&#20559;&#31227;&#37327;&#26368;&#30456;&#20284;&#30340;&#20851;&#38190;&#22352;&#26631;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#39044;&#27979;&#27492;&#20559;&#31227;&#30340;&#20809;&#27969;&#22330;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#20063;&#26159;&#22914;&#27492;&#12290;&#23567;&#30340;&#31354;&#38388;&#19981;&#20934;&#30830;&#24615;&#20250;&#20005;&#37325;&#24433;&#21709;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#19982;&#39044;&#27979;&#30340;&#20559;&#31227;&#37327;&#33539;&#22260;&#30456;&#32467;&#21512;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#65292;&#23427;&#22312;&#39044;&#27979;&#30340;&#20559;&#31227;&#21608;&#22260;&#25191;&#34892;&#19968;&#20010;&#23567;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21407;&#22320;&#35745;&#31639;&#28040;&#32791;&#30340;&#20869;&#23384;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#23569;&#20102;10&#20493;&#65292;&#36895;&#24230;&#24555;&#20102;3&#20493;&#20197;&#19978;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#32416;&#27491;&#20102;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Efficiently computing attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial err
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#33719;&#24471;&#30340;&#21442;&#25968;&#21487;&#20197;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#65292;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#21270;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#65292;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.16846</link><description>&lt;p&gt;
&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#33021;&#25913;&#36827;&#38543;&#26426;&#29305;&#24449;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#33719;&#24471;&#30340;&#21442;&#25968;&#21487;&#20197;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#65292;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#21270;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#65292;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#19982;&#39640;&#26031;&#27169;&#22411;&#28176;&#36827;&#31561;&#25928;&#12290;&#31561;&#25928;&#27169;&#22411;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#21457;&#25381;&#30340;&#37325;&#35201;&#20294;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#8220;&#21442;&#25968;&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#32473;&#23450;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39640;&#26031;&#27169;&#22411;&#33719;&#21462;&#30340;&#21442;&#25968;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#30340;&#20004;&#20010;&#31034;&#20363;&#31867;&#65292;&#20363;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#34987;&#20248;&#21270;&#20197;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#12290;&#25105;&#20204;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65288;&#22914;CIFAR10&#65289;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#20110;wid&#12290;
&lt;/p&gt;
&lt;p&gt;
Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#24120;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#25968;&#25454;&#28857;&#21040;&#20854;&#25152;&#23646;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#36229;&#36807;&#21040;&#20219;&#20309;&#20854;&#20182;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#30340;$\alpha$&#20493;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#27867;&#21270;&#21040;&#36229;&#36807;&#24179;&#22343;&#36317;&#31163;&#30340;&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16840</link><description>&lt;p&gt;
&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#24120;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Constant Approximation for Individual Preference Stable Clustering. (arXiv:2309.16840v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#24120;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#25968;&#25454;&#28857;&#21040;&#20854;&#25152;&#23646;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#36229;&#36807;&#21040;&#20219;&#20309;&#20854;&#20182;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#30340;$\alpha$&#20493;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#27867;&#21270;&#21040;&#36229;&#36807;&#24179;&#22343;&#36317;&#31163;&#30340;&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#24615;&#26159;&#30001;Ahmadi&#31561;&#20154;&#65288;ICML 2022&#65289;&#24341;&#20837;&#30340;&#19968;&#31181;&#21463;&#21040;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#21551;&#21457;&#30340;&#33258;&#28982;&#32858;&#31867;&#30446;&#26631;&#12290;&#22914;&#26524;&#27599;&#20010;&#25968;&#25454;&#28857;&#21040;&#20854;&#25152;&#23646;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#36229;&#36807;&#21040;&#20219;&#20309;&#20854;&#20182;&#32858;&#31867;&#30340;&#24179;&#22343;&#36317;&#31163;&#30340;$\alpha$&#20493;&#65292;&#21017;&#31216;&#32858;&#31867;&#20026;$\alpha$-&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30830;&#23450;&#25968;&#25454;&#38598;&#26159;&#21542;&#23384;&#22312;$1$-&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#32858;&#31867;&#26159; NP-Hard &#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#26159;&#21542;&#24635;&#26159;&#23384;&#22312;$ o(n) $-&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#32858;&#31867;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#21482;&#20445;&#35777;&#20102;$O(n)$-&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#29702;&#35299;&#19978;&#30340;&#24046;&#36317;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#24230;&#37327;&#65292;&#24635;&#26159;&#23384;&#22312;$O(1)$-&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#32858;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#36755;&#20986;&#36825;&#26679;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36229;&#36807;&#24179;&#22343;&#36317;&#31163;&#30340;&#20010;&#20307;&#20559;&#22909;&#31283;&#23450;&#30340;&#27867;&#21270;&#65292;&#24182;&#22312;&#32771;&#34385;&#32858;&#31867;&#20869;&#37096;&#21644;&#32858;&#31867;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#21644;&#26368;&#23567;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#39640;&#25928;&#12289;&#36817;&#20046;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is $\alpha$-IP stable if the average distance of every data point to its own cluster is at most $\alpha$ times the average distance to any other cluster. Unfortunately, determining if a dataset admits a $1$-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an $o(n)$-IP stable clustering always \emph{exists}, as the prior state of the art only guaranteed an $O(n)$-IP stable clustering. We close this gap in understanding and show that an $O(1)$-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#20013;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#32858;&#21512;&#27969;&#31243;&#21518;&#26399;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#33719;&#24471;&#21518;&#32493;&#27169;&#22411;&#39044;&#27979;&#30340;&#32852;&#21512;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#21516;&#26102;&#65292;&#36824;&#33021;&#22815;&#25253;&#21578;&#27969;&#31243;&#20013;&#27599;&#20010;&#32452;&#20214;&#30340;&#20869;&#37096;&#12289;&#25968;&#25454;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.16831</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21644;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Propagation and Attribution of Uncertainty in Medical Imaging Pipelines. (arXiv:2309.16831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#20013;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#32858;&#21512;&#27969;&#31243;&#21518;&#26399;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#33719;&#24471;&#21518;&#32493;&#27169;&#22411;&#39044;&#27979;&#30340;&#32852;&#21512;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#21516;&#26102;&#65292;&#36824;&#33021;&#22815;&#25253;&#21578;&#27969;&#31243;&#20013;&#27599;&#20010;&#32452;&#20214;&#30340;&#20869;&#37096;&#12289;&#25968;&#25454;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20027;&#35201;&#30740;&#31350;&#20102;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#21333;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#20013;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#32858;&#21512;&#27969;&#31243;&#21518;&#26399;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#33719;&#24471;&#21518;&#32493;&#27169;&#22411;&#39044;&#27979;&#30340;&#32852;&#21512;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#21035;&#25253;&#21578;&#27969;&#31243;&#20013;&#27599;&#20010;&#32452;&#20214;&#30340;&#20869;&#37096;&#12289;&#25968;&#25454;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#27969;&#31243;&#20174;&#19979;&#37319;&#26679;&#30340;&#33041;&#37096;&#21644;&#33181;&#30422;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#22270;&#20687;&#37325;&#24314;&#65292;&#28982;&#21518;&#39044;&#27979;&#22270;&#20687;&#20013;&#30340;&#23450;&#37327;&#20449;&#24687;&#65292;&#20363;&#22914;&#33041;&#23481;&#31215;&#12289;&#33181;&#30422;&#36793;&#32536;&#25110;&#24739;&#32773;&#24615;&#21035;&#31561;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#26174;&#31034;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation, which provides a means of building explainable neural networks for medical imaging applications, have mostly been studied for single deep learning models that focus on a specific task. In this paper, we propose a method to propagate uncertainty through cascades of deep learning models in medical imaging pipelines. This allows us to aggregate the uncertainty in later stages of the pipeline and to obtain a joint uncertainty measure for the predictions of later models. Additionally, we can separately report contributions of the aleatoric, data-based, uncertainty of every component in the pipeline. We demonstrate the utility of our method on a realistic imaging pipeline that reconstructs undersampled brain and knee magnetic resonance (MR) images and subsequently predicts quantitative information from the images, such as the brain volume, or knee side or patient's sex. We quantitatively show that the propagated uncertainty is correlated with input uncertainty and com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.16829</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis of the derivative-free loss method for solving PDEs. (arXiv:2309.16829v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#19968;&#31867;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#37319;&#29992;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#65292;&#32467;&#21512;&#38543;&#26426;&#34892;&#36208;&#32773;&#21450;&#20854;&#23545;&#24212;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#20013;&#19982;&#26102;&#38388;&#38388;&#38548;&#30456;&#20851;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#34892;&#36208;&#32773;&#22823;&#23567;&#23545;&#35745;&#31639;&#25928;&#29575;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#37319;&#26679;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#25165;&#33021;&#35757;&#32451;&#32593;&#32476;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#35828;&#26126;&#65292;&#22312;&#26102;&#38388;&#38388;&#38548;&#30340;&#26368;&#20248;&#19979;&#30028;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#23613;&#21487;&#33021;&#23567;&#30340;&#34892;&#36208;&#32773;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#20998;&#26512;&#30340;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#32531;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#31070;&#32463;&#28608;&#27963;&#36793;&#30028;&#26469;&#32531;&#35299;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#33719;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#20998;&#26512;&#25903;&#25345;&#65292;&#34920;&#26126;&#22522;&#20110;&#26368;&#22823;&#36793;&#30028;&#30340;&#21518;&#35757;&#32451;&#27491;&#21017;&#21270;&#26174;&#33879;&#32531;&#35299;&#20102;&#38750;&#24694;&#24847;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16827</link><description>&lt;p&gt;
DNN&#20998;&#31867;&#22120;&#20013;&#30340;&#21518;&#35757;&#32451;&#36807;&#25311;&#21512;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Post-Training Overfitting Mitigation in DNN Classifiers. (arXiv:2309.16827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#32531;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#31070;&#32463;&#28608;&#27963;&#36793;&#30028;&#26469;&#32531;&#35299;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#33719;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#20998;&#26512;&#25903;&#25345;&#65292;&#34920;&#26126;&#22522;&#20110;&#26368;&#22823;&#36793;&#30028;&#30340;&#21518;&#35757;&#32451;&#27491;&#21017;&#21270;&#26174;&#33879;&#32531;&#35299;&#20102;&#38750;&#24694;&#24847;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#31867;&#22120;&#20013;&#38750;&#24694;&#24847;&#36807;&#25311;&#21512;&#30340;&#24050;&#30693;&#26469;&#28304;&#21253;&#25324;&#65306;i&#65289;&#22823;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65307;ii&#65289;&#35757;&#32451;&#38598;&#22810;&#26679;&#24615;&#19981;&#36275;&#65307;iii&#65289;&#36807;&#24230;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#20063;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#20855;&#26377;&#24322;&#24120;&#22823;&#30340;&#20998;&#31867;&#36793;&#30028;&#36317;&#31163;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#31867;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20801;&#35768;&#22823;&#20449;&#21495;&#22312;DNN&#20013;&#20256;&#25773;&#30340;&#65288;&#26080;&#30028;&#65289;ReLU&#28608;&#27963;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#35757;&#32451;&#32531;&#35299;&#26041;&#27861;&#65288;&#19981;&#38656;&#35201;&#20102;&#35299;&#35757;&#32451;&#38598;&#25110;&#35757;&#32451;&#36807;&#31243;&#65289;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#24178;&#20928;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#38480;&#21046;&#31070;&#32463;&#28608;&#27963;&#26469;&#35299;&#20915;&#21518;&#38376;&#38382;&#39064;&#12290;&#22312;&#23436;&#21892;&#35813;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#29305;&#23450;&#38408;&#20540;&#28608;&#27963;&#20197;&#38480;&#21046;&#26368;&#22823;&#36793;&#30028;&#65288;MMs&#65289;&#65292;&#20174;&#32780;&#22312;&#21518;&#38376;&#32531;&#35299;&#20013;&#33719;&#24471;&#24615;&#33021;&#25910;&#30410;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#25903;&#25345;&#35813;&#32531;&#35299;&#26041;&#27861;&#30340;&#20998;&#26512;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26368;&#22823;&#36793;&#30028;&#30340;&#21518;&#35757;&#32451;&#27491;&#21017;&#21270;&#22823;&#22823;&#32531;&#35299;&#20102;&#38750;&#24694;&#24847;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-known (non-malicious) sources of overfitting in deep neural net (DNN) classifiers include: i) large class imbalances; ii) insufficient training-set diversity; and iii) over-training. In recent work, it was shown that backdoor data-poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. Thus, an effective post-training (with no knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. Improving upon that work, we threshold activations specifically to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. We also provide some analytical support for this mitigation approach. Most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious ove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.16825</link><description>&lt;p&gt;
FENDA-FL&#65306;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#20811;&#26381;&#20020;&#24202;&#29615;&#22659;&#20013;&#25968;&#25454;&#23396;&#31435;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#22312;&#20020;&#24202;&#24212;&#29992;&#30340;FL&#30740;&#31350;&#20013;&#20570;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#23558;FENDA&#26041;&#27861;&#65288;Kim&#31561;&#20154;&#65292;2016&#65289;&#25193;&#23637;&#21040;FL&#30340;&#26041;&#27861;&#12290;&#22312;FLamby&#22522;&#20934;&#65288;du Terrail&#31561;&#20154;&#65292;2022a&#65289;&#21644;GEMINI&#25968;&#25454;&#38598;&#65288;Verma&#31561;&#20154;&#65292;2017&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;FL&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#21407;&#26377;&#30340;FLamby&#22522;&#20934;&#19978;&#34920;&#31034;&#20986;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#22522;&#20934;&#20197;&#21253;&#25324;&#20010;&#24615;&#21270;FL&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;FL&#26816;&#26597;&#28857;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#29615;&#22659;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;$Q$-learning&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#22810;Bellman&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20351;&#20854;&#25104;&#20026;&#21387;&#32553;&#26144;&#23556;&#30340;&#26465;&#20214;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#21487;&#20197;&#24471;&#21040;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24120;&#29992;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16819</link><description>&lt;p&gt;
&#29992;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22810;Bellman&#31639;&#23376;&#23545;$Q$-learning&#30340;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Bellman operator for convergence of $Q$-learning with linear function approximation. (arXiv:2309.16819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;$Q$-learning&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#22810;Bellman&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20351;&#20854;&#25104;&#20026;&#21387;&#32553;&#26144;&#23556;&#30340;&#26465;&#20214;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#21487;&#20197;&#24471;&#21040;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24120;&#29992;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;$Q$-learning&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;Bellman&#31639;&#23376;&#12290;&#36890;&#36807;&#25506;&#32034;&#35813;&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#25237;&#24433;&#30340;&#22810;Bellman&#31639;&#23376;&#21464;&#20026;&#21387;&#32553;&#26144;&#23556;&#26102;&#30340;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#27604;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#25237;&#24433;&#30340;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#29615;&#22659;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence of $Q$-learning with linear function approximation. Our key contribution is the introduction of a novel multi-Bellman operator that extends the traditional Bellman operator. By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive, providing improved fixed-point guarantees compared to the Bellman operator. To leverage these insights, we propose the multi $Q$-learning algorithm with linear function approximation. We demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator, yielding solutions of arbitrary accuracy. Finally, we validate our approach by applying it to well-known environments, showcasing the effectiveness and applicability of our findings.
&lt;/p&gt;</description></item><item><title>PROSE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22810;&#27169;&#24577;&#36755;&#20837;&#21040;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#21644;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16816</link><description>&lt;p&gt;
PROSE: &#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#39044;&#27979;&#36816;&#31639;&#31526;&#21644;&#31526;&#21495;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers. (arXiv:2309.16816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16816
&lt;/p&gt;
&lt;p&gt;
PROSE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22810;&#27169;&#24577;&#36755;&#20837;&#21040;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#21644;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#20026;&#21508;&#31181;&#31185;&#23398;&#35745;&#31639;&#20219;&#21153;&#25552;&#20379;&#20102;&#31283;&#20581;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#23454;&#26102;&#39044;&#27979;&#12289;&#21453;&#38382;&#39064;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#20195;&#29702;&#27169;&#25311;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#23558;&#21160;&#21147;&#23398;&#31995;&#32479;&#23884;&#20837;&#21040;&#32593;&#32476;&#20013;&#65306;&#23398;&#20064;&#21333;&#20010;&#35299;&#31639;&#31526;&#65288;&#21363;&#20174;&#36755;&#20837;&#21442;&#25968;&#21270;&#20989;&#25968;&#26144;&#23556;&#21040;&#35299;&#30340;&#26144;&#23556;&#65289;&#25110;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#65288;&#21363;&#30456;&#23545;&#20110;&#29366;&#24577;&#21464;&#37327;&#30340;&#26500;&#25104;&#27169;&#22411;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20250;&#24471;&#21040;&#30456;&#21516;&#22522;&#30784;&#25968;&#25454;&#25110;&#20989;&#25968;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#19968;&#32452;&#24494;&#20998;&#26041;&#31243;&#32463;&#24120;&#20855;&#26377;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23547;&#27714;&#22312;&#24191;&#27867;&#30340;&#26041;&#31243;&#20013;&#33719;&#24471;&#19968;&#20010;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#39044;&#27979;&#36816;&#31639;&#31526;&#21644;&#31526;&#21495;&#34920;&#36798;&#24335;&#65288;PROSE&#65289;&#65292;&#23427;&#23398;&#20064;&#20174;&#22810;&#27169;&#24577;&#36755;&#20837;&#21040;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#26144;&#23556;&#65292;&#33021;&#22815;&#29983;&#25104;&#25968;&#20540;&#39044;&#27979;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Approximating nonlinear differential equations using a neural network provides a robust and efficient tool for various scientific computing tasks, including real-time predictions, inverse problems, optimal controls, and surrogate modeling. Previous works have focused on embedding dynamical systems into networks through two approaches: learning a single solution operator (i.e., the mapping from input parametrized functions to solutions) or learning the governing system of equations (i.e., the constitutive model relative to the state variables). Both of these approaches yield different representations for the same underlying data or function. Additionally, observing that families of differential equations often share key characteristics, we seek one network representation across a wide range of equations. Our method, called Predicting Operators and Symbolic Expressions (PROSE), learns maps from multimodal inputs to multimodal outputs, capable of generating both numerical predictions and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;GraB-sampler&#30340;Python&#24211;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#20351;&#29992;GraB&#31639;&#27861;&#30340;&#20415;&#21033;&#65292;&#24182;&#25552;&#20986;&#20102;5&#20010;&#19981;&#21516;&#30340;&#21464;&#20307;&#12290;&#26368;&#20339;&#24615;&#33021;&#32467;&#26524;&#35777;&#26126;&#20102;GraB-sampler&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#26174;&#23384;&#20351;&#29992;&#19978;&#30340;&#24320;&#38144;&#38750;&#24120;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.16809</link><description>&lt;p&gt;
GraB-sampler: &#29992;&#20110;PyTorch&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#26368;&#20248;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25968;&#25454;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch. (arXiv:2309.16809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;GraB-sampler&#30340;Python&#24211;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#20351;&#29992;GraB&#31639;&#27861;&#30340;&#20415;&#21033;&#65292;&#24182;&#25552;&#20986;&#20102;5&#20010;&#19981;&#21516;&#30340;&#21464;&#20307;&#12290;&#26368;&#20339;&#24615;&#33021;&#32467;&#26524;&#35777;&#26126;&#20102;GraB-sampler&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#26174;&#23384;&#20351;&#29992;&#19978;&#30340;&#24320;&#38144;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#36880;&#20010;&#26679;&#26412;&#26799;&#24230;&#35299;&#20915;&#32858;&#38598;&#38382;&#39064;&#65292;&#22312;&#32447;&#26799;&#24230;&#24179;&#34913;&#65288;GraB&#65289;&#31639;&#27861;&#36890;&#36807;&#36138;&#24515;&#22320;&#36873;&#25321;&#31034;&#20363;&#25490;&#24207;&#34987;&#35777;&#26126;&#26159;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20445;&#35777;&#20248;&#20110;&#38543;&#26426;&#37325;&#25490;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#26377;&#25928;&#30340;GraB&#23454;&#29616;&#20379;&#31038;&#21306;&#36731;&#26494;&#20351;&#29992;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;Python&#24211;&#8220;GraB-sampler&#8221;&#65292;&#20351;&#24471;&#31038;&#21306;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;GraB&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;5&#20010;&#21464;&#20307;&#30340;GraB&#31639;&#27861;&#12290;GraB-sampler&#30340;&#26368;&#20339;&#24615;&#33021;&#32467;&#26524;&#22312;&#21482;&#22686;&#21152;8.7%&#30340;&#35757;&#32451;&#26102;&#38388;&#24320;&#38144;&#21644;0.85%&#30340;&#23792;&#20540;&#26174;&#23384;&#20351;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20877;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online Gradient Balancing (GraB) algorithm greedily choosing the examples ordering by solving the herding problem using per-sample gradients is proved to be the theoretically optimal solution that guarantees to outperform Random Reshuffling. However, there is currently no efficient implementation of GraB for the community to easily use it.  This work presents an efficient Python library, $\textit{GraB-sampler}$, that allows the community to easily use GraB algorithms and proposes 5 variants of the GraB algorithm. The best performance result of the GraB-sampler reproduces the training loss and test accuracy results while only in the cost of 8.7% training time overhead and 0.85% peak GPU memory usage overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16808</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31890;&#24230;&#65306;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#27491;&#23556;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#20272;&#35745;&#23621;&#27665;&#31038;&#21306;&#30340;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#19990;&#30028;&#19978;&#35768;&#22810;&#22320;&#21306;&#32570;&#20047;&#26377;&#20851;&#23621;&#27665;&#31119;&#31049;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#36965;&#24863;&#33719;&#21462;&#30340;&#39640;&#31354;&#24433;&#20687;&#65292;&#22914;&#21355;&#26143;&#25110;&#39134;&#26426;&#65292;&#21487;&#20197;&#20316;&#20026;&#31397;&#35270;&#22320;&#38754;&#19978;&#29983;&#27963;&#29366;&#20917;&#30340;&#31383;&#21475;&#65292;&#24182;&#24110;&#21161;&#22635;&#34917;&#31038;&#21306;&#20449;&#24687;&#31232;&#32570;&#30340;&#22320;&#26041;&#65292;&#36739;&#23567;&#22320;&#29702;&#23610;&#24230;&#30340;&#20272;&#35745;&#38656;&#35201;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#26816;&#27979;&#27169;&#24335;&#65292;&#20174;&#32780;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20854;&#20182;&#20449;&#24687;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;&#30417;&#30563;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#35270;&#35273;&#35789;&#34955;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65289;&#22914;&#20309;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#20013;&#20272;&#35745;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution 
&lt;/p&gt;</description></item><item><title>Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16797</link><description>&lt;p&gt;
Promptbreeder: &#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#23454;&#29616;&#33258;&#25105;&#21442;&#29031;&#30340;&#33258;&#25105;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16797
&lt;/p&gt;
&lt;p&gt;
Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;Chain-of-Thought Prompting&#36825;&#26679;&#30340;&#27969;&#34892;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#31574;&#30053;&#36890;&#24120;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Promptbreeder&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#25105;&#21442;&#29031;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#29992;&#20110;&#36827;&#21270;&#21644;&#35843;&#25972;&#32473;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;Promptbreeder&#36890;&#36807;LLM&#39537;&#21160;&#65292;&#23545;&#19968;&#32452;&#20219;&#21153;&#25552;&#31034;&#36827;&#34892;&#31361;&#21464;&#65292;&#24182;&#22312;&#35757;&#32451;&#38598;&#19978;&#35780;&#20272;&#20854;&#36866;&#24212;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#26159;&#30001;LLM&#29983;&#25104;&#24182;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#33258;&#25105;&#25913;&#36827;&#30340;&#31361;&#21464;&#25552;&#31034;&#26469;&#25511;&#21046;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;Promptbreeder&#19981;&#20165;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36824;&#25913;&#36827;&#20102;&#25913;&#36827;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#25552;&#31034;&#12290;Promptbreeder&#22312;&#24120;&#29992;&#30340;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;Chain-of-Thought&#21644;Plan-and-Solve Prompting&#31561;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16781</link><description>&lt;p&gt;
&#38271;&#36755;&#20837;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Hallucination Reduction in Long Input Text Summarization. (arXiv:2309.16781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#27169;&#22411;&#29983;&#25104;&#19981;&#34987;&#36755;&#20837;&#28304;&#25991;&#26723;&#25903;&#25345;&#30340;&#20449;&#24687;&#30340;&#29616;&#35937;&#12290;&#24187;&#35273;&#32473;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#38271;&#31687;&#31185;&#23398;&#30740;&#31350;&#25991;&#26723;&#21450;&#20854;&#25688;&#35201;&#30340;PubMed&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;Longformer Encoder-Decoder (LED)&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#21152;&#20837;&#20102;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#65288;JAENS&#65289;&#25216;&#26415;&#65292;&#20197;&#26368;&#23567;&#21270;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#19979;&#25351;&#26631;&#26469;&#34913;&#37327;&#23454;&#20307;&#32423;&#21035;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65306;&#28304;&#31934;&#30830;&#24230;&#21644;&#30446;&#26631;F1&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LED&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#25968;&#25454;&#36807;&#28388;&#25216;&#26415;&#22522;&#20110;&#19968;&#20123;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.16779</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16779
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23545;&#35937;&#30340;&#26368;&#20339;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65288;&#24555;&#36895;&#20294;&#28508;&#22312;&#23481;&#26131;&#20986;&#29616;&#24555;&#25463;&#23398;&#20064;&#65289;&#36824;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#36739;&#24930;&#20294;&#28508;&#22312;&#26356;&#31283;&#20581;&#65289;&#65311;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36716;&#21270;&#20026;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#21028;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#22235;&#20010;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65306;&#23427;&#20204;&#26174;&#31034;&#20986;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#65288;&#23545;&#20110;Imagen&#36798;&#21040;99%&#65289;&#65292;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#23427;&#20204;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30446;&#21069;&#27169;&#25311;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65292;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35268;&#27169;&#23545;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;DNN&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#29983;&#29289;&#36807;&#31243;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16773</link><description>&lt;p&gt;
&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35268;&#27169;&#23545;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;DNN&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#29983;&#29289;&#36807;&#31243;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31361;&#30772;&#26159;&#36890;&#36807;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25552;&#21319;&#32780;&#38750;&#26032;&#30340;&#35745;&#31639;&#33539;&#20363;&#30340;&#21457;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35268;&#27169;&#26159;&#21542;&#20063;&#33021;&#23545;&#29992;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27169;&#22411;&#20135;&#29983;&#31867;&#20284;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#21644;&#31995;&#32479;&#21270;&#30340;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#12289;&#25968;&#25454;&#35757;&#32451;&#38598;&#21644;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#34920;&#22411;&#21270;&#23398;&#31454;&#25216;&#22330;&#65288;Pheno-CA&#65289;&#22522;&#20934;&#36827;&#34892;&#31934;&#30830;&#24230;&#27979;&#35797;&#65306;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#39640;&#20869;&#28085;&#31579;&#36873;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#24320;&#21457;&#20219;&#21153;&#38598;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;Pheno-CA&#20219;&#21153;&#20013;&#26126;&#30830;&#34987;&#30417;&#30563;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#21069;&#20307;&#20219;&#21153;&#65292;&#21363;&#36870;&#29983;&#29289;&#36807;&#31243;&#65288;IBP&#65289;&#65292;&#23427;&#35774;&#35745;&#25104;&#31867;&#20284;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#22240;&#26524;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30830;&#23454;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#24341;&#20837;IBP&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#24403;&#20351;&#29992;IBP&#39044;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;Pheno-CA&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#21644;&#23398;&#20064;&#26041;&#24335;&#30456;&#20851;&#30340;&#22240;&#32032;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16750</link><description>&lt;p&gt;
&#30524;&#20013;&#35760;&#24518;&#65306;&#25193;&#25955;&#27169;&#22411;&#21644;&#20851;&#32852;&#35760;&#24518;&#20043;&#38388;&#30340;&#31070;&#31192;&#30456;&#20284;&#20043;&#22788;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#25968;&#23398;&#25551;&#36848;&#26377;&#24456;&#22810;&#31181;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#20154;&#20204;&#24456;&#38590;&#23545;&#20854;&#24037;&#20316;&#21407;&#29702;&#36827;&#34892;&#31616;&#21333;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#21160;&#21147;&#31995;&#32479;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;DMs&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19982;&#20854;&#39640;&#24230;&#30456;&#20851;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#33021;&#37327;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#30340;&#25968;&#23398;&#32852;&#31995;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;AMs&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20854;&#34892;&#20026;&#19982;&#21435;&#22122;DMs&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#19968;&#20010;Lyapunov&#33021;&#37327;&#20989;&#25968;&#65292;&#22312;&#20854;&#19978;&#21487;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#20197;&#21435;&#22122;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#33021;&#37327;AMs&#30340;40&#24180;&#21382;&#21490;&#65292;&#20174;&#26368;&#21021;&#30340;Hopfield&#32593;&#32476;&#24320;&#22987;&#65292;&#24182;&#35752;&#35770;&#20102;&#36890;&#36807;&#25551;&#36848;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#31243;&#24230;&#25581;&#31034;&#20986;&#26469;&#30340;AMs&#21644;DMs&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#28798;&#23475;&#39044;&#27979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2309.16747</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#28798;&#23475;&#39044;&#27979;&#30340;&#22810;&#27169;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework. (arXiv:2309.16747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#28798;&#23475;&#39044;&#27979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#30340;&#21152;&#21095;&#65292;&#20934;&#30830;&#30340;&#20840;&#29699;&#35268;&#27169;&#28798;&#23475;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#32039;&#36843;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;"&#27946;&#27700;"&#21644;"&#28369;&#22369;"&#30340;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#27668;&#35937;&#21644;&#22320;&#24418;&#22240;&#32032;&#26377;&#20851;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#35774;&#35745;&#65292;&#24182;&#37319;&#29992;&#31574;&#30053;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22686;&#24378;&#30340;&#31243;&#24230;&#21462;&#20915;&#20110;&#27599;&#31181;&#28798;&#23475;&#30340;&#20855;&#20307;&#24615;&#36136;&#21644;&#20854;&#29420;&#29305;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
As climate change intensifies, the urgency for accurate global-scale disaster predictions grows. This research presents a novel multimodal disaster prediction framework, combining weather statistics, satellite imagery, and textual insights. We particularly focus on "flood" and "landslide" predictions, given their ties to meteorological and topographical factors. The model is meticulously crafted based on the available data and we also implement strategies to address class imbalance. While our findings suggest that integrating multiple data sources can bolster model performance, the extent of enhancement differs based on the specific nature of each disaster and their unique underlying causes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16746</link><description>&lt;p&gt;
&#38544;&#24615;&#39640;&#26031;&#36807;&#31243;&#34920;&#31034;&#20219;&#24847;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#22330;
&lt;/p&gt;
&lt;p&gt;
Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16746
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26159;&#29992;&#20110;&#23398;&#20064;&#26410;&#30693;&#20989;&#25968;&#21644;&#37327;&#21270;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#34892;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;GPs&#65292;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#30340;&#26631;&#37327;&#21644;&#21521;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#20986;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#21160;&#21147;&#31995;&#32479;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20247;&#22810;&#39046;&#22495;&#20013;&#30340;&#24179;&#28369;&#27969;&#24418;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#25968;&#25454;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;RVGP&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28508;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#30340;GP&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19982;&#20999;&#21521;&#19995;&#20851;&#32852;&#30340;&#36830;&#25509;Laplacian&#30340;&#29305;&#24449;&#20989;&#25968;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#36825;&#20123;&#29305;&#24449;&#20989;&#25968;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#30340;&#24120;&#35265;&#25968;&#25454;&#36817;&#20284;&#20013;&#36731;&#26494;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RVGP&#22312;&#27969;&#24418;&#19978;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#65292;&#20351;&#24471;&#20854;&#33021;&#22815;&#22312;&#20445;&#30041;&#22855;&#24322;&#24615;&#30340;&#21516;&#26102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;RVGP&#26469;&#37325;&#26500;&#39640;&#23494;&#24230;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#26469;&#36827;&#34892;&#19968;&#31867;&#20998;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36890;&#36807;&#21033;&#29992;&#22686;&#24191;Lagrange&#20056;&#23376;&#27861;&#65288;AL-FPGM&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#27491;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;</title><link>http://arxiv.org/abs/2309.16745</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#19968;&#31867;&#20998;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of One Class Classification-SVMs. (arXiv:2309.16745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#26469;&#36827;&#34892;&#19968;&#31867;&#20998;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36890;&#36807;&#21033;&#29992;&#22686;&#24191;Lagrange&#20056;&#23376;&#27861;&#65288;AL-FPGM&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#27491;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#36827;&#34892;&#19968;&#31867;&#20998;&#31867;&#12290;&#22312;&#24120;&#35265;&#30340;&#20108;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#38656;&#35201;&#21516;&#26102;&#23384;&#22312;&#27491;&#20363;&#21644;&#36127;&#20363;&#25165;&#33021;&#24320;&#21457;&#20986;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24182;&#19981;&#28385;&#36275;&#36825;&#20010;&#26465;&#20214;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20174;&#32431;&#27491;&#20363;&#36755;&#20837;&#20013;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21452;&#36719;&#36793;&#30028;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22686;&#24191;Lagrange&#20056;&#23376;&#27861;&#65288;AL-FPGM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24555;&#36895;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;FPGM&#21482;&#38656;&#35201;&#19968;&#38454;&#23548;&#25968;&#65292;&#23545;&#20110;&#21452;&#36719;&#36793;&#30028;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#26469;&#35828;&#65292;&#20027;&#35201;&#26159;&#35745;&#31639;&#30697;&#38453;&#21521;&#37327;&#20056;&#31215;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;AL-FPGM&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#24191;&#27867;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the use of a highly effective training method to conduct one-class classification. The existence of both positive and negative examples in the training data is necessary to develop an effective classifier in common binary classification scenarios. Unfortunately, this criteria is not met in many domains. Here, there is just one class of examples. Classification algorithms that learn from solely positive input have been created to deal with this setting. In this paper, an effective algorithm for dual soft-margin one-class SVM training is presented. Our approach makes use of the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method. The FPGM requires only first derivatives, which for the dual soft margin OCC-SVM means computing mainly a matrix-vector product. Therefore, AL-FPGM, being computationally inexpensive, may complement existing quadratic programming solvers for training large SVMs. We extensively validate our approach over real-world 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#26032;&#20896;&#21518;&#24739;&#32773;&#38271;&#26399;&#32958;&#25439;&#20260;&#30340;&#39118;&#38505;&#65292;&#36890;&#36807;&#26089;&#26399;&#35782;&#21035;&#21644;&#24178;&#39044;&#25913;&#21892;&#20020;&#24202;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16744</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#26032;&#20896;&#21518;&#24739;&#32773;&#30340;&#38271;&#26399;&#32958;&#25439;&#20260;
&lt;/p&gt;
&lt;p&gt;
Predicting Long-term Renal Impairment in Post-COVID-19 Patients with Machine Learning Algorithms. (arXiv:2309.16744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#26032;&#20896;&#21518;&#24739;&#32773;&#38271;&#26399;&#32958;&#25439;&#20260;&#30340;&#39118;&#38505;&#65292;&#36890;&#36807;&#26089;&#26399;&#35782;&#21035;&#21644;&#24178;&#39044;&#25913;&#21892;&#20020;&#24202;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#30123;&#24773;&#23545;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#25105;&#20204;&#32487;&#32493;&#24212;&#23545;&#20854;&#21518;&#26524;&#65292;&#26032;&#20896;&#21518;&#24182;&#21457;&#30151;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#65292;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#38271;&#26399;&#20581;&#24247;&#24433;&#21709;&#65292;&#32958;&#25439;&#20260;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#20197;&#20234;&#25289;&#20811;&#19981;&#21516;&#22320;&#21306;&#22312;2021&#24180;&#33267;2023&#24180;&#26399;&#38388;&#30340;821&#21517;&#26032;&#20896;&#21518;&#24739;&#32773;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#38271;&#26399;&#32958;&#25439;&#20260;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21487;&#33021;&#36890;&#36807;&#21450;&#26089;&#35782;&#21035;&#21644;&#24178;&#39044;&#32958;&#25439;&#20260;&#39118;&#38505;&#24739;&#32773;&#65292;&#20174;&#32780;&#25913;&#21892;&#20020;&#24202;&#32467;&#26524;&#65292;&#38761;&#26032;&#26032;&#20896;&#21518;&#24739;&#32773;&#30340;&#25252;&#29702;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#20197;&#21450;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#35780;&#20272;&#24182;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#26032;&#20896;&#21518;&#24739;&#32773;&#38271;&#26399;&#32958;&#25439;&#20260;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has had far-reaching implications for global public health. As we continue to grapple with its consequences, it becomes increasingly clear that post-COVID-19 complications are a significant concern. Among these complications, renal impairment has garnered particular attention due to its potential long-term health impacts. This study, conducted with a cohort of 821 post-COVID-19 patients from diverse regions of Iraq across the years 2021, 2022, and 2023, endeavors to predict the risk of long-term renal impairment using advanced machine learning algorithms. Our findings have the potential to revolutionize post-COVID-19 patient care by enabling early identification and intervention for those at risk of renal impairment, ultimately improving clinical outcomes. This research encompasses comprehensive data collection and preprocessing, feature selection, and the development of predictive models using various machine learning algorithms. The study's objectives are to ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#39640;&#36890;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#32423;&#24182;&#34892;&#24615;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#25968;&#25454;&#20197;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#35757;&#32451;&#20648;&#22791;&#27744;&#26469;&#20943;&#36731;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;2&#23567;&#26102;&#20869;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20986;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47%&#12289;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#30340;&#28909;&#26041;&#31243;&#20195;&#29702;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.16743</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#38598;&#21512;&#36816;&#34892;&#20013;&#39640;&#36890;&#37327;&#35757;&#32451;&#28145;&#24230;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#39640;&#36890;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#32423;&#24182;&#34892;&#24615;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#25968;&#25454;&#20197;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#35757;&#32451;&#20648;&#22791;&#27744;&#26469;&#20943;&#36731;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;2&#23567;&#26102;&#20869;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20986;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47%&#12289;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#30340;&#28909;&#26041;&#31243;&#20195;&#29702;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#24544;&#23454;&#20294;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#27169;&#25311;&#12290;&#36825;&#20123;&#28145;&#24230;&#20195;&#29702;&#36890;&#24120;&#36890;&#36807;&#21516;&#19968;&#35299;&#31639;&#22120;&#29983;&#25104;&#30340;&#26377;&#38480;&#37327;&#30340;&#25968;&#25454;&#26377;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#20010;&#32423;&#21035;&#30340;&#24182;&#34892;&#24615;&#26469;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#12290;&#35757;&#32451;&#20648;&#22791;&#27744;&#22312;&#26368;&#22823;&#21270;GPU&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#65292;&#20943;&#36731;&#20102;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22312;2&#23567;&#26102;&#20869;&#33021;&#22815;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#23436;&#20840;&#36830;&#25509;&#32593;&#32476;&#20316;&#20026;&#28909;&#26041;&#31243;&#30340;&#20195;&#29702;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#31163;&#32447;&#36807;&#31243;&#65292;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47&#65285;&#65292;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#21644;&#39044;&#27979;COVID-19&#21518;&#32508;&#21512;&#30151;&#20013;&#30340;&#22810;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#23545;&#36825;&#31181;&#30142;&#30149;&#30340;&#35748;&#35782;&#65292;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#35782;&#21035;&#39118;&#38505;&#20010;&#20307;&#24182;&#21450;&#26102;&#24178;&#39044;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16736</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#35748;&#30693;COVID-19&#21518;&#22810;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Cognizance of Post-COVID-19 Multi-Organ Dysfunction through Machine Learning Analysis. (arXiv:2309.16736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#21644;&#39044;&#27979;COVID-19&#21518;&#32508;&#21512;&#30151;&#20013;&#30340;&#22810;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#23545;&#36825;&#31181;&#30142;&#30149;&#30340;&#35748;&#35782;&#65292;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#35782;&#21035;&#39118;&#38505;&#20010;&#20307;&#24182;&#21450;&#26102;&#24178;&#39044;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20849;&#32435;&#20837;&#20102;&#26469;&#33258;&#20234;&#25289;&#20811;&#21508;&#20010;&#22478;&#24066;&#30340;466&#21517;&#24739;&#32773;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#21644;&#39044;&#27979;COVID-19&#21518;&#32508;&#21512;&#30151;&#65288;&#36890;&#24120;&#31216;&#20026;&#38271;&#26399;COVID&#65289;&#20013;&#30340;&#22810;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#12290;COVID-19&#21518;&#32508;&#21512;&#30151;&#20250;&#23548;&#33268;&#21508;&#31181;&#21508;&#26679;&#25345;&#32493;&#24615;&#30151;&#29366;&#24433;&#21709;&#19981;&#21516;&#30340;&#22120;&#23448;&#31995;&#32479;&#65292;&#23545;&#21307;&#30103;&#20445;&#20581;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#23545;&#36825;&#31181;&#22797;&#26434;&#30149;&#30151;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#27835;&#30103;&#12290;&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#12289;&#27169;&#22411;&#24320;&#21457;&#21644;&#39564;&#35777;&#20197;&#21450;&#22312;&#35813;&#39046;&#22495;&#24320;&#23637;&#30740;&#31350;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25552;&#39640;&#25105;&#20204;&#23545;COVID-19&#21518;&#32508;&#21512;&#30151;&#30340;&#35748;&#30693;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21487;&#20197;&#35782;&#21035;&#20986;&#22788;&#20110;&#39118;&#38505;&#20013;&#30340;&#20010;&#20307;&#24182;&#21450;&#26102;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#65292;&#28508;&#22312;&#22320;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the year 2022, a total of 466 patients from various cities across Iraq were included in this study. This research paper focuses on the application of machine learning techniques to analyse and predict multi-organ dysfunction in individuals experiencing Post-COVID-19 Syndrome, commonly known as Long COVID. Post-COVID-19 Syndrome presents a wide array of persistent symptoms affecting various organ systems, posing a significant challenge to healthcare. Leveraging the power of artificial intelligence, this study aims to enhance early detection and management of this complex condition. The paper outlines the importance of data collection and preprocessing, feature selection and engineering, model development and validation, and ethical considerations in conducting research in this field. By improving our understanding of Post-COVID-19 Syndrome through machine learning, healthcare providers can identify at-risk individuals and offer timely interventions, potentially improving patient outc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16733</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#38887;&#24615;&#65306;&#20998;&#26512;&#21644;&#21152;&#22266;&#25216;&#26415;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30446;&#21069;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26159;&#26368;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#20043;&#19968;&#65292;&#22914;&#35270;&#35273;&#12289;&#33258;&#20027;&#31995;&#32479;&#31561;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#20154;&#20204;&#23545;ML&#24212;&#29992;&#22312;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#24433;&#21709;&#19979;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20570;&#20986;&#20102;&#22823;&#37327;&#36129;&#29486;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#27425;&#28145;&#20837;&#30340;&#22238;&#39038;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;ML&#25216;&#26415;&#20043;&#19968;&#65289;&#23545;&#25239;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#30340;&#24050;&#26377;&#30693;&#35782;&#65292;&#28165;&#26224;&#22320;&#21576;&#29616;&#20102;&#36825;&#19968;&#25991;&#29486;&#27969;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25991;&#31456;&#22522;&#20110;2019&#24180;1&#26376;&#33267;2023&#24180;3&#26376;&#38388;&#21457;&#34920;&#30340;163&#31687;&#31185;&#23398;&#35770;&#25991;&#65292;&#37319;&#29992;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#35835;&#21644;&#31361;&#20986;&#30740;&#31350;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#29305;&#28857;&#65292;&#20174;&#24037;&#20316;&#30340;&#20027;&#35201;&#33539;&#22260;&#12289;&#37319;&#29992;&#30340;&#25925;&#38556;&#21644;&#38169;&#35823;&#27169;&#22411;&#31561;&#22810;&#20010;&#21442;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#30142;&#30149;&#30340;&#24433;&#21709;&#24182;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#12290;&#26368;&#20248;&#27169;&#22411;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#31639;&#27861;&#65292;&#20854;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20020;&#24202;&#25928;&#30410;&#21644;&#25311;&#21512;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16730</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explainable machine learning-based prediction model for diabetic nephropathy. (arXiv:2309.16730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#30142;&#30149;&#30340;&#24433;&#21709;&#24182;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#12290;&#26368;&#20248;&#27169;&#22411;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#31639;&#27861;&#65292;&#20854;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20020;&#24202;&#25928;&#30410;&#21644;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#31958;&#23615;&#30149;&#32958;&#30149;&#65288;DN&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;DN&#30340;&#24739;&#30149;&#29575;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;2018&#24180;4&#26376;&#33267;2019&#24180;4&#26376;&#22823;&#36830;&#21307;&#31185;&#22823;&#23398;&#38468;&#23646;&#31532;&#20108;&#21307;&#38498;&#30340;548&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#32477;&#23545;&#25910;&#32553;&#21644;&#36873;&#25321;&#31639;&#23376;&#65288;LASSO&#65289;&#22238;&#24402;&#27169;&#22411;&#21644;10&#25240;&#20132;&#21449;&#39564;&#35777;&#36873;&#25321;&#20102;&#26368;&#20248;&#30340;38&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#24182;&#36890;&#36807;AUC-ROC&#26354;&#32447;&#12289;&#20915;&#31574;&#26354;&#32447;&#21644;&#26657;&#20934;&#26354;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;Shapley Additive exPlanations&#65288;SHAP&#65289;&#26041;&#27861;&#26469;&#37327;&#21270;&#26368;&#20248;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#25928;&#24212;&#12290;XGB&#27169;&#22411;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#65292;&#26368;&#39640;AUC&#20540;&#20026;0.966&#12290;XGB&#27169;&#22411;&#30340;&#20020;&#24202;&#20928;&#25928;&#30410;&#20063;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#25311;&#21512;&#24230;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach. The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). We select the optimal 38 features through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. We compare four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree and logistic regression, by AUC-ROC curves, decision curves, calibration curves. We quantify feature importance and interaction effects in the optimal predictive model by Shapley Additive exPlanations (SHAP) method. The XGB model has the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gains more clinical net benefits than others and the fitting degree is better. In addition, there are s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(SimPINNs)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34701;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#20223;&#30495;&#25968;&#25454;&#30340;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#65292;SimPINNs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#20110;&#26631;&#20934;PINNs&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16729</link><description>&lt;p&gt;
SimPINNs: &#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#24615;&#33021;&#30340;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems. (arXiv:2309.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(SimPINNs)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34701;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#20223;&#30495;&#25968;&#25454;&#30340;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#65292;SimPINNs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#20110;&#26631;&#20934;PINNs&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#26681;&#25454;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#28508;&#22312;&#30340;&#27491;&#21521;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#38750;&#32447;&#24615;&#34892;&#20026;&#19988;&#26410;&#30693;&#21442;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#26126;&#26174;&#23567;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#20102;&#35266;&#27979;&#25968;&#25454;&#21644;&#30001;&#24050;&#30693;(&#36817;&#20284;)&#29289;&#29702;&#27169;&#22411;&#29983;&#25104;&#30340;&#20223;&#30495;&#25968;&#25454;&#12290;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#26631;&#20934;PINNs&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to solve inverse problems by leveraging deep learning techniques. The objective is to infer unknown parameters that govern a physical system based on observed data. We focus on scenarios where the underlying forward model demonstrates pronounced nonlinear behaviour, and where the dimensionality of the unknown parameter space is substantially smaller than that of the observations. Our proposed method builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model. Experimental results on an orbit restitution problem demonstrate that our approach surpasses the performance of standard PINNs, providing improved accuracy and robustness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#30693;&#24773;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16725</link><description>&lt;p&gt;
&#35299;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#38745;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#30340;&#29289;&#29702;&#30693;&#24773;&#35299;&#65306;&#19968;&#20010;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study. (arXiv:2309.16725v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#30693;&#24773;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31119;&#20811;-&#26222;&#26391;&#20811;&#65288;FP&#65289;&#26041;&#31243;&#26159;&#19968;&#31181;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#21709;&#24212;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#28436;&#21270;&#12290;FP&#26041;&#31243;&#30340;&#31934;&#30830;&#35299;&#20165;&#23545;&#26377;&#38480;&#30340;&#21160;&#21147;&#31995;&#32479;&#23376;&#38598;&#26377;&#25928;&#12290;&#23545;&#20110;&#36739;&#22823;&#20294;&#20173;&#28982;&#26159;&#19968;&#23567;&#37096;&#20998;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#21322;&#35299;&#26512;&#26041;&#27861;&#12290;&#32780;&#20256;&#32479;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#21644;&#26377;&#38480;&#24046;&#20998;&#65292;&#38656;&#35201;&#23558;&#35745;&#31639;&#22495;&#21010;&#20998;&#20026;&#31163;&#25955;&#28857;&#30340;&#32593;&#26684;&#65292;&#23545;&#20110;&#39640;&#32500;&#31995;&#32479;&#32780;&#35328;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#29289;&#29702;&#30693;&#24773;&#23398;&#20064;&#20026;&#20256;&#32479;&#30340;&#35745;&#31639;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#26377;&#21147;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#30340;&#12289;&#20855;&#26377;&#29289;&#29702;&#30693;&#24773;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;FP&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fokker-Planck (FP) equation is a linear partial differential equation which governs the temporal and spatial evolution of the probability density function (PDF) associated with the response of stochastic dynamical systems. An exact analytical solution of the FP equation is only available for a limited subset of dynamical systems. Semi-analytical methods are available for larger, yet still a small subset of systems, while traditional computational methods; e.g. Finite Elements and Finite Difference require dividing the computational domain into a grid of discrete points, which incurs significant computational costs for high-dimensional systems. Physics-informed learning offers a potentially powerful alternative to traditional computational schemes. To evaluate its potential, we present a data-free, physics-informed neural network (PINN) framework to solve the FP equation for a class of nonlinear stochastic dynamical systems. In particular, through several examples concerning the sto
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22235;&#36275;&#27493;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22909;&#30340;&#31639;&#27861;&#33021;&#22815;&#19982;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30456;&#23218;&#32654;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16718</link><description>&lt;p&gt;
&#20026;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22235;&#36275;&#27493;&#24577;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Real-World Quadrupedal Locomotion Benchmark for Offline Reinforcement Learning. (arXiv:2309.16718v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22235;&#36275;&#27493;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22909;&#30340;&#31639;&#27861;&#33021;&#22815;&#19982;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30456;&#23218;&#32654;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#35757;&#32451;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#25928;&#29575;&#20302;&#19979;&#25110;&#19981;&#21487;&#38752;&#65292;&#23588;&#20854;&#23545;&#20110;&#22235;&#36275;&#26426;&#22120;&#20154;&#26356;&#26159;&#22914;&#27492;&#12290;&#20174;&#39044;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#21516;&#26102;&#65292;&#28789;&#27963;&#31283;&#23450;&#30340;&#22235;&#36275;&#27493;&#24577;&#26426;&#22120;&#20154;&#34892;&#36208;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#26377;&#28508;&#21147;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#65292;&#20294;&#30446;&#21069;&#30340;&#29942;&#39048;&#22312;&#20110;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#29992;&#20110;&#25361;&#25112;&#24615;&#29616;&#23454;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#22235;&#36275;&#27493;&#24577;&#25968;&#25454;&#38598;&#19978;&#23545;11&#31181;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#32463;&#20856;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;(MPC)&#25910;&#38598;&#30340;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#22522;&#20934;&#27979;&#35797;&#24120;&#29992;&#30340;&#26080;&#27169;&#22411;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34920;&#29616;&#26368;&#22909;&#30340;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#26041;&#38754;&#36229;&#36234;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online reinforcement learning (RL) methods are often data-inefficient or unreliable, making them difficult to train on real robotic hardware, especially quadruped robots. Learning robotic tasks from pre-collected data is a promising direction. Meanwhile, agile and stable legged robotic locomotion remains an open question in their general form. Offline reinforcement learning (ORL) has the potential to make breakthroughs in this challenging field, but its current bottleneck lies in the lack of diverse datasets for challenging realistic tasks. To facilitate the development of ORL, we benchmarked 11 ORL algorithms in the realistic quadrupedal locomotion dataset. Such dataset is collected by the classic model predictive control (MPC) method, rather than the model-free online RL method commonly used by previous benchmarks. Extensive experimental results show that the best-performing ORL algorithms can achieve competitive performance compared with the model-free RL, and even surpass it in som
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16713</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#35821;&#20041;&#36890;&#20449;&#19982;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;&#20559;&#36828;&#22320;&#21306;&#20803;&#23431;&#23449;&#29992;&#25143;&#30340;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#24179;&#34913;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#20943;&#23569;&#19978;&#34892;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#20041;&#27169;&#22411;&#35268;&#27169;&#12289;&#20449;&#36947;&#20998;&#37197;&#12289;&#20256;&#36755;&#21151;&#29575;&#21644;&#26080;&#20154;&#26426;&#36712;&#36857;&#19978;&#20570;&#20986;&#20915;&#31574;&#12290;&#21464;&#37327;&#34987;&#21010;&#20998;&#20026;&#31163;&#25955;&#31867;&#22411;&#21644;&#36830;&#32493;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#20197;&#29983;&#25104;&#32452;&#21512;&#21160;&#20316;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#26377;&#25928;&#25552;&#39640;&#19978;&#34892;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#25928;&#29575;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Mask R-CNN&#21644;&#20960;&#20309;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#20986;&#20102;&#33258;&#21160;&#26816;&#27979;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30028;&#22336;&#22320;&#31821;&#36793;&#30028;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21475;&#34955;&#30340;&#31616;&#21270;&#31639;&#27861;&#26469;&#25913;&#21892;&#24037;&#20316;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16708</link><description>&lt;p&gt;
&#20351;&#29992;Mask R-CNN&#33258;&#21160;&#26816;&#27979;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#30028;&#22336;&#22320;&#31821;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Automatic Cadastral Boundary Detection of Very High Resolution Images Using Mask R-CNN. (arXiv:2309.16708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16708
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Mask R-CNN&#21644;&#20960;&#20309;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#20986;&#20102;&#33258;&#21160;&#26816;&#27979;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30028;&#22336;&#22320;&#31821;&#36793;&#30028;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21475;&#34955;&#30340;&#31616;&#21270;&#31639;&#27861;&#26469;&#25913;&#21892;&#24037;&#20316;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21152;&#36895;&#21644;&#25913;&#36827;&#33258;&#21160;&#22320;&#31821;&#21046;&#22270;&#30340;&#26816;&#27979;&#38656;&#27714;&#24456;&#39640;&#12290;&#30001;&#20110;&#36825;&#20010;&#38382;&#39064;&#36824;&#22788;&#20110;&#36215;&#28857;&#38454;&#27573;&#65292;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#23578;&#26410;&#34987;&#32771;&#34385;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25913;&#21892;&#24037;&#20316;&#36136;&#37327;&#30340;&#20960;&#20309;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30001;&#20960;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ResNet-50&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;Mask R-CNN&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#19977;&#31181;&#20960;&#20309;&#21518;&#22788;&#29702;&#26041;&#27861;&#24212;&#29992;&#20110;&#31532;&#19968;&#37096;&#20998;&#30340;&#36755;&#20986;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24635;&#20307;&#36755;&#20986;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#35745;&#31639;&#20960;&#20309;&#26469;&#20171;&#32461;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#21475;&#34955;&#30340;&#31616;&#21270;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#24120;&#29992;&#30340;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#29575;&#31561;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a high demand for accelerating and improving the detection of automatic cadastral mapping. As this problem is in its starting point, there are many methods of computer vision and deep learning that have not been considered yet. In this paper, we focus on deep learning and provide three geometric post-processing methods that improve the quality of the work. Our framework includes two parts, each of which consists of a few phases. Our solution to this problem uses instance segmentation. In the first part, we use Mask R-CNN with the backbone of pre-trained ResNet-50 on the ImageNet dataset. In the second phase, we apply three geometric post-processing methods to the output of the first part to get better overall output. Here, we also use computational geometry to introduce a new method for simplifying lines which we call it pocket-based simplification algorithm. For evaluating the quality of our solution, we use popular formulas in this field which are recall, pre
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;DeepReceiver&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.16706</link><description>&lt;p&gt;
AIR: &#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery. (arXiv:2309.16706v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;DeepReceiver&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36890;&#24120;&#30001;&#19968;&#20010;&#20256;&#36755;&#22120;&#21644;&#19968;&#20010;&#25509;&#25910;&#22120;&#32452;&#25104;&#65292;&#20256;&#36755;&#22120;&#23558;&#20449;&#24687;&#20256;&#36755;&#65292;&#25509;&#25910;&#22120;&#20174;&#25509;&#25910;&#21040;&#30340;&#25197;&#26354;&#20449;&#21495;&#20013;&#24674;&#22797;&#21407;&#22987;&#20449;&#24687;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#29992;&#20110;&#25913;&#21892;&#22797;&#26434;&#20449;&#36947;&#29615;&#22659;&#19979;&#25509;&#25910;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#20026;&#20102;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;DeepReceiver&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#24102;&#26377;&#21151;&#29575;&#21644;&#23792;&#22343;&#27604; (PAPR) &#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#26681;&#25454;&#23545;&#25163;&#23545;DeepReceiver&#27169;&#22411;&#21644;/&#25110;&#27979;&#35797;&#26679;&#26412;&#30340;&#20102;&#35299;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#22330;&#26223;&#20013;&#65292;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wireless communications system usually consists of a transmitter which transmits the information and a receiver which recovers the original information from the received distorted signal. Deep learning (DL) has been used to improve the performance of the receiver in complicated channel environments and state-of-the-art (SOTA) performance has been achieved. However, its robustness has not been investigated. In order to evaluate the robustness of DL-based information recovery models under adversarial circumstances, we investigate adversarial attacks on the SOTA DL-based information recovery model, i.e., DeepReceiver. We formulate the problem as an optimization problem with power and peak-to-average power ratio (PAPR) constraints. We design different adversarial attack methods according to the adversary's knowledge of DeepReceiver's model and/or testing samples. Extensive experiments show that the DeepReceiver is vulnerable to the designed attack methods in all of the considered scenari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.16705</link><description>&lt;p&gt;
&#35299;&#30721;&#22270;&#20687;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#25361;&#25112;-&#21709;&#24212;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Google Bard&#36827;&#34892;&#20102;64&#20010;&#35270;&#35273;&#25361;&#25112;&#65292;&#26088;&#22312;&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25361;&#25112;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#8220;&#35270;&#35273;&#24773;&#22659;&#25512;&#29702;&#8221;&#65292;&#8220;&#35270;&#35273;&#25991;&#26412;&#25512;&#29702;&#8221;&#21644;&#8220;&#19979;&#19968;&#22330;&#26223;&#39044;&#27979;&#8221;&#31561;&#65292;&#20197;&#30830;&#23450;Bard&#22312;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Bard&#20542;&#21521;&#20110;&#26681;&#25454;&#22270;&#29255;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#22270;&#29255;&#20013;&#30340;&#32447;&#32034;&#26102;&#12290;&#19982;GPT4&#31561;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;Bard&#20284;&#20046;&#19981;&#20381;&#36182;&#20110;&#20687;Tesseract&#36825;&#26679;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#24211;&#65292;&#32780;&#26159;&#20687;Google Lens&#21644;Visual API&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#35782;&#21035;&#22797;&#26434;&#22270;&#29255;&#20013;&#30340;&#25991;&#26412;&#12290;&#26174;&#30528;&#30340;&#26159;&#65292;Bard&#21487;&#20197;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#35299;&#20915;ChatGPT&#26080;&#27861;&#29702;&#35299;&#30340;&#39564;&#35777;&#30721;&#65292;&#25512;&#33616;&#20351;&#29992;Tesseract&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Bard&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#26080;&#27861;&#37325;&#24314;&#25110;&#20462;&#25913;&#21407;&#22987;&#30340;&#35270;&#35273;&#23545;&#35937;&#26469;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;Detectron2&#12289;YOLOv8&#21644;SAM&#65289;&#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#65292;&#25214;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#30740;&#31350;&#26377;&#21161;&#20110;&#29702;&#35299;&#23391;&#21152;&#25289;&#25991;&#26723;&#20013;&#22797;&#26434;&#30340;&#24067;&#23616;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.16700</link><description>&lt;p&gt;
&#22522;&#20110;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#21644;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Framework and Model Analysis on Bengali Document Layout Analysis Dataset: BaDLAD. (arXiv:2309.16700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;Detectron2&#12289;YOLOv8&#21644;SAM&#65289;&#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#65292;&#25214;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#30740;&#31350;&#26377;&#21161;&#20110;&#29702;&#35299;&#23391;&#21152;&#25289;&#25991;&#26723;&#20013;&#22797;&#26434;&#30340;&#24067;&#23616;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#20808;&#36827;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;Detectron2&#12289;YOLOv8&#21644;SAM&#65289;&#20102;&#35299;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#12290;&#25105;&#20204;&#22312;&#30740;&#31350;&#20013;&#32771;&#23519;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#12290;Detectron2&#22312;&#26597;&#25214;&#21644;&#20998;&#31163;&#25991;&#26723;&#30340;&#19981;&#21516;&#37096;&#20998;&#65288;&#22914;&#25991;&#26412;&#26694;&#21644;&#27573;&#33853;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;YOLOv8&#22312;&#35782;&#21035;&#19981;&#21516;&#30340;&#34920;&#26684;&#21644;&#22270;&#29255;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;SAM&#65292;&#23427;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#26840;&#25163;&#30340;&#24067;&#23616;&#12290;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#65292;&#25105;&#20204;&#20102;&#35299;&#20102;&#21738;&#20010;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#35299;&#26512;&#23391;&#21152;&#25289;&#25991;&#26723;&#20013;&#22797;&#26434;&#30340;&#24067;&#23616;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;&#35821;&#35328;&#20063;&#26377;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on understanding Bengali Document Layouts using advanced computer programs: Detectron2, YOLOv8, and SAM. We looked at lots of different Bengali documents in our study. Detectron2 is great at finding and separating different parts of documents, like text boxes and paragraphs. YOLOv8 is good at figuring out different tables and pictures. We also tried SAM, which helps us understand tricky layouts. We tested these programs to see how well they work. By comparing their accuracy and speed, we learned which one is good for different types of documents. Our research helps make sense of complex layouts in Bengali documents and can be useful for other languages too.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#36716;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;TML&#65289;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#21644;&#24615;&#33021;&#65292;&#21457;&#29616;TML&#22312;&#39044;&#27979;&#31934;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#37117;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#25968;&#37327;&#36275;&#22815;&#26102;&#12290;</title><link>http://arxiv.org/abs/2309.16693</link><description>&lt;p&gt;
&#25193;&#23637;&#36716;&#21270;&#26426;&#22120;&#23398;&#20064;&#65306;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Extension of Transformational Machine Learning: Classification Problems. (arXiv:2309.16693v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#36716;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;TML&#65289;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#21644;&#24615;&#33021;&#65292;&#21457;&#29616;TML&#22312;&#39044;&#27979;&#31934;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#37117;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#25968;&#37327;&#36275;&#22815;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36716;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;TML&#65289;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#21644;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;TML&#22312;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#20849;&#21516;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#36229;&#36234;&#20256;&#32479;&#27169;&#22411;&#30340;&#22797;&#21512;&#27169;&#22411;&#12290;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#22797;&#26434;&#32791;&#26102;&#65292;TML&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#12289;&#25913;&#21892;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#27492;&#33021;&#22815;&#20026;&#27492;&#36807;&#31243;&#24102;&#26469;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#20010;&#20998;&#31867;&#22120;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#22240;&#27492;&#32771;&#34385;&#20102;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;TML&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#36817;&#20284;&#27491;&#30830;&#30340;&#20551;&#35774;&#12289;&#20811;&#26381;&#23616;&#37096;&#26368;&#20248;&#21644;&#25193;&#23637;&#21487;&#34920;&#31034;&#20989;&#25968;&#30340;&#31354;&#38388;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36229;&#36234;&#21482;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#25968;&#37327;&#36275;&#22815;&#26102;&#25165;&#33021;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application and performance of Transformational Machine Learning (TML) in drug discovery. TML, a meta learning algorithm, excels in exploiting common attributes across various domains, thus developing composite models that outperform conventional models. The drug discovery process, which is complex and time-consuming, can benefit greatly from the enhanced prediction accuracy, improved interpretability and greater generalizability provided by TML. We explore the efficacy of different machine learning classifiers, where no individual classifier exhibits distinct superiority, leading to the consideration of ensemble classifiers such as the Random Forest.  Our findings show that TML outperforms base Machine Learning (ML) as the number of training datasets increases, due to its capacity to better approximate the correct hypothesis, overcome local optima, and expand the space of representable functions by combining separate classifiers capabilities. However, this supe
&lt;/p&gt;</description></item><item><title>ecoBLE&#26159;&#19968;&#31181;&#22522;&#20110;LSTMP&#30340;&#34013;&#29273;&#20302;&#21151;&#32791;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#25152;&#26377;&#32452;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.16686</link><description>&lt;p&gt;
ecoBLE: &#19968;&#31181;&#20302;&#35745;&#31639;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#26694;&#26550;&#30340;&#34013;&#29273;&#20302;&#21151;&#32791;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ecoBLE: A Low-Computation Energy Consumption Prediction Framework for Bluetooth Low Energy. (arXiv:2309.16686v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16686
&lt;/p&gt;
&lt;p&gt;
ecoBLE&#26159;&#19968;&#31181;&#22522;&#20110;LSTMP&#30340;&#34013;&#29273;&#20302;&#21151;&#32791;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#25152;&#26377;&#32452;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34013;&#29273;&#20302;&#21151;&#32791; (BLE) &#26159;&#29289;&#32852;&#32593; (IoT) &#24212;&#29992;&#20013;&#30340;&#19968;&#31181;&#20107;&#23454;&#26631;&#20934;&#25216;&#26415;&#65292;&#25215;&#35834;&#26497;&#20302;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20302;&#33021;&#37327;&#28040;&#32791;&#21482;&#32771;&#34385;&#20102;&#26080;&#32447;&#30005;&#37096;&#20998;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#30828;&#20214;&#21644;&#36719;&#20214;&#32452;&#20214;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#30417;&#27979;&#21644;&#39044;&#27979;&#37096;&#32626;&#21518;&#30340;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#33021;&#37327;&#28040;&#32791;&#21487;&#20197;&#22823;&#22823;&#24110;&#21161;&#30830;&#20445;&#20302;&#33021;&#37327;&#28040;&#32791;&#65292;&#35745;&#31639;&#21097;&#20313;&#30005;&#27744;&#23551;&#21629;&#65292;&#39044;&#27979;&#33021;&#37327;&#25910;&#38598;&#33410;&#28857;&#25152;&#38656;&#30340;&#33021;&#28304;&#65292;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LSTMP&#30340;BLE&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#21450;&#22312;&#24191;&#27867;&#37319;&#29992;BLE&#30340;&#21307;&#30103;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#20197;&#26080;&#32447;&#30005;&#20026;&#28966;&#28857;&#30340;&#29702;&#35770;&#33021;&#37327;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#25152;&#26377;&#32452;&#20214;&#65292;&#21253;&#25324;&#26080;&#32447;&#30005;&#12289;&#20256;&#24863;&#22120;&#21644;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;(MCU)&#12290;
&lt;/p&gt;
&lt;p&gt;
Bluetooth Low Energy (BLE) is a de-facto technology for Internet of Things (IoT) applications, promising very low energy consumption. However, this low energy consumption accounts only for the radio part, and it overlooks the energy consumption of other hardware and software components. Monitoring and predicting the energy consumption of IoT nodes after deployment can substantially aid in ensuring low energy consumption, calculating the remaining battery lifetime, predicting needed energy for energy-harvesting nodes, and detecting anomalies. In this paper, we introduce a Long Short-Term Memory Projection (LSTMP)-based BLE energy consumption prediction framework together with a dataset for a healthcare application scenario where BLE is widely adopted. Unlike radio-focused theoretical energy models, our framework provides a comprehensive energy consumption prediction, considering all components of the IoT node, including the radio, sensor as well as microcontroller unit (MCU). Our measur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;TargetVAE&#65289;&#65292;&#36890;&#36807;&#34507;&#30333;&#36136;&#22810;&#27169;&#24577;&#32593;&#32476;&#65288;PMN&#65289;&#23558;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#34920;&#31034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#37197;&#20307;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25972;&#20010;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25429;&#25417;&#20854;&#39034;&#24207;&#12289;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.16685</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#37197;&#20307;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Target-aware Variational Auto-encoders for Ligand Generation with Multimodal Protein Representation Learning. (arXiv:2309.16685v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;TargetVAE&#65289;&#65292;&#36890;&#36807;&#34507;&#30333;&#36136;&#22810;&#27169;&#24577;&#32593;&#32476;&#65288;PMN&#65289;&#23558;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#34920;&#31034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#37197;&#20307;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25972;&#20010;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25429;&#25417;&#20854;&#39034;&#24207;&#12289;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#20102;&#35299;&#29305;&#23450;&#21475;&#34955;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#34507;&#30333;&#36136;&#30446;&#26631;&#30340;&#25972;&#20307;&#32467;&#26500;&#29983;&#25104;&#37197;&#20307;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#20943;&#23569;&#28508;&#22312;&#33647;&#29289;&#20505;&#36873;&#22312;&#27969;&#27700;&#32447;&#20013;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#34507;&#30333;&#36136;&#20248;&#21270;&#23450;&#21046;&#30340;&#32593;&#32476;&#65292;&#36825;&#26159;&#32321;&#29712;&#19988;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TargetVAE&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#30446;&#26631;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24418;Transformer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#65292;&#29983;&#25104;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#37197;&#20307;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#34920;&#31034;&#65288;&#20363;&#22914;&#27688;&#22522;&#37240;&#24207;&#21015;&#12289;3D&#32467;&#26500;&#65289;&#32479;&#19968;&#21040;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#21162;&#21147;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#34507;&#30333;&#36136;&#22810;&#27169;&#24577;&#32593;&#32476;&#65288;PMN&#65289;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#20174;&#25972;&#20010;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#23427;&#20204;&#30340;&#39034;&#24207;&#12289;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without knowledge of specific pockets, generating ligands based on the global structure of a protein target plays a crucial role in drug discovery as it helps reduce the search space for potential drug-like candidates in the pipeline. However, contemporary methods require optimizing tailored networks for each protein, which is arduous and costly. To address this issue, we introduce TargetVAE, a target-aware variational auto-encoder that generates ligands with high binding affinities to arbitrary protein targets, guided by a novel multimodal deep neural network built based on graph Transformers as the prior for the generative model. This is the first effort to unify different representations of proteins (e.g., sequence of amino-acids, 3D structure) into a single model that we name as Protein Multimodal Network (PMN). Our multimodal architecture learns from the entire protein structures and is able to capture their sequential, topological and geometrical information. We showcase the supe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29983;&#25104;&#37197;&#20307;&#26500;&#35937;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#32422;&#26463;&#21644;&#37197;&#20307;-&#30446;&#26631;&#20256;&#36882;&#20449;&#24687;&#22359;&#26469;&#35299;&#20915;&#26082;&#26377;&#27169;&#22411;&#29983;&#25104;&#26500;&#35937;&#32570;&#20047;&#32467;&#26500;&#21644;&#38543;&#26426;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16684</link><description>&lt;p&gt;
&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29983;&#25104;&#37197;&#20307;&#26500;&#35937;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Side Information for Ligand Conformation Generation using Diffusion-Based Approaches. (arXiv:2309.16684v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29983;&#25104;&#37197;&#20307;&#26500;&#35937;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#32422;&#26463;&#21644;&#37197;&#20307;-&#30446;&#26631;&#20256;&#36882;&#20449;&#24687;&#22359;&#26469;&#35299;&#20915;&#26082;&#26377;&#27169;&#22411;&#29983;&#25104;&#26500;&#35937;&#32570;&#20047;&#32467;&#26500;&#21644;&#38543;&#26426;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#20307;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#29983;&#25104;&#32570;&#20047;&#26377;&#24847;&#20041;&#30340;&#32467;&#26500;&#21644;&#38543;&#26426;&#24615;&#30340;&#26500;&#35937;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#24517;&#35201;&#30340;&#20391;&#38754;&#20449;&#24687;&#25152;&#33268;&#12290;&#36825;&#20123;&#20391;&#38754;&#20449;&#24687;&#21253;&#25324;&#30446;&#26631;&#34507;&#30333;&#30340;&#21270;&#23398;&#21644;&#20960;&#20309;&#29305;&#24449;&#65292;&#37197;&#20307;-&#30446;&#26631;&#21270;&#21512;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#37197;&#20307;&#30340;&#21270;&#23398;&#24615;&#36136;&#12290;&#22312;&#27809;&#26377;&#36825;&#20123;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#26500;&#35937;&#21487;&#33021;&#19981;&#36866;&#21512;&#36827;&#19968;&#27493;&#36873;&#25321;&#21644;&#35774;&#35745;&#26032;&#33647;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#24182;&#23558;&#28789;&#27963;&#32422;&#26463;&#24341;&#20837;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#37197;&#20307;&#26500;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#21463;&#21040;&#20449;&#24687;&#20256;&#36882;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37197;&#20307;-&#30446;&#26631;&#20256;&#36882;&#20449;&#24687;&#22359;&#65292;&#36825;&#20010;&#26426;&#21046;&#26377;&#21161;&#20110;&#20449;&#24687;&#30340;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ligand molecule conformation generation is a critical challenge in drug discovery. Deep learning models have been developed to tackle this problem, particularly through the use of generative models in recent years. However, these models often generate conformations that lack meaningful structure and randomness due to the absence of essential side information. Examples of such side information include the chemical and geometric features of the target protein, ligand-target compound interactions, and ligand chemical properties. Without these constraints, the generated conformations may not be suitable for further selection and design of new drugs. To address this limitation, we propose a novel method for generating ligand conformations that leverage side information and incorporate flexible constraints into standard diffusion models. Drawing inspiration from the concept of message passing, we introduce ligand-target massage passing block, a mechanism that facilitates the exchange of info
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;Solo12&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#24378;&#22823;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#25511;&#21046;&#22120;&#12290;&#36825;&#31181;&#25511;&#21046;&#22120;&#33021;&#22815;&#22312;&#36981;&#24490;&#32473;&#23450;&#36895;&#24230;&#21442;&#32771;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#33021;&#37327;&#28040;&#32791;&#12289;&#31283;&#20581;&#24615;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.16683</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;Solo12&#22235;&#36275;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning. (arXiv:2309.16683v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;Solo12&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#24378;&#22823;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#25511;&#21046;&#22120;&#12290;&#36825;&#31181;&#25511;&#21046;&#22120;&#33021;&#22815;&#22312;&#36981;&#24490;&#32473;&#23450;&#36895;&#24230;&#21442;&#32771;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#33021;&#37327;&#28040;&#32791;&#12289;&#31283;&#20581;&#24615;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#26426;&#22120;&#20154;&#38656;&#35201;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#20197;&#22312;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20805;&#20998;&#21457;&#25381;&#20854;&#26426;&#21160;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;Solo12&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#20851;&#33410;&#38459;&#25239;&#21442;&#32771;&#12290;&#24471;&#21040;&#30340;&#25511;&#21046;&#31574;&#30053;&#22312;&#36981;&#24490;&#32473;&#23450;&#36895;&#24230;&#21442;&#32771;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#33021;&#37327;&#28040;&#32791;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#23398;&#20064;&#36807;&#31243;&#21644;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#36801;&#31227;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Solo12&#26426;&#22120;&#20154;&#26159;&#19968;&#20010;&#36866;&#21512;&#32467;&#21512;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#24320;&#28304;&#24179;&#21488;&#65292;&#22240;&#20026;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#36731;&#26494;&#22320;&#36801;&#31227;&#21040;&#24182;&#37096;&#32626;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped robots require robust and general locomotion skills to exploit their mobility potential in complex and challenging environments. In this work, we present the first implementation of a robust end-to-end learning-based controller on the Solo12 quadruped. Our method is based on deep reinforcement learning of joint impedance references. The resulting control policies follow a commanded velocity reference while being efficient in its energy consumption, robust and easy to deploy. We detail the learning procedure and method for transfer on the real robot. In our experiments, we show that the Solo12 robot is a suitable open-source platform for research combining learning and control because of the easiness in transferring and deploying learned controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#37329;&#34701;&#20132;&#26131;&#65292;&#24182;&#32771;&#34385;&#24773;&#32490;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#24120;&#35265;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24212;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16679</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22312;&#32447;&#24773;&#32490;&#20998;&#26512;&#36827;&#34892;&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management. (arXiv:2309.16679v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#37329;&#34701;&#20132;&#26131;&#65292;&#24182;&#32771;&#34385;&#24773;&#32490;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#24120;&#35265;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24212;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#26159;&#25351;&#22312;&#19968;&#31995;&#21015;&#37329;&#34701;&#36164;&#20135;&#65288;&#22914;&#32929;&#31080;&#12289;&#25351;&#25968;&#22522;&#37329;&#12289;&#22806;&#27719;&#25110;&#21152;&#23494;&#36135;&#24065;&#65289;&#19978;&#20998;&#37197;&#36164;&#37329;&#24182;&#36827;&#34892;&#20132;&#26131;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#21033;&#28070;&#21516;&#26102;&#26368;&#23567;&#21270;&#20132;&#26131;&#25805;&#20316;&#25152;&#36896;&#25104;&#30340;&#25439;&#22833;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#20197;&#26469;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33258;&#21160;&#21270;&#37329;&#34701;&#20132;&#26131;&#23601;&#26159;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#37329;&#34701;&#20132;&#26131;&#20013;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#35265;&#35299;&#65292;&#20998;&#21035;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#19982;&#20132;&#26131;&#36164;&#20135;&#30456;&#20851;&#30340;&#24773;&#32490;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#24212;&#30340;&#30740;&#31350;&#30740;&#31350;&#35770;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#27492;&#31867;&#37329;&#34701;&#26234;&#33021;&#20307;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#24517;&#35201;&#30340;&#30693;&#35782;&#65292;&#20197;&#36991;&#20813;&#36825;&#20123;&#38382;&#39064;&#24182;&#23558;&#35752;&#35770;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial portfolio management describes the task of distributing funds and conducting trading operations on a set of financial assets, such as stocks, index funds, foreign exchange or cryptocurrencies, aiming to maximize the profit while minimizing the loss incurred by said operations. Deep Learning (DL) methods have been consistently excelling at various tasks and automated financial trading is one of the most complex one of those. This paper aims to provide insight into various DL methods for financial trading, under both the supervised and reinforcement learning schemes. At the same time, taking into consideration sentiment information regarding the traded assets, we discuss and demonstrate their usefulness through corresponding research studies. Finally, we discuss commonly found problems in training such financial agents and equip the reader with the necessary knowledge to avoid these problems and apply the discussed methods in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.15730</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15730
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#38142;&#25509;&#23646;&#24615;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#26102;&#38388;&#22270;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20004;&#20010;&#24230;&#37327;&#65292;&#21487;&#20197;&#37327;&#21270;&#25968;&#25454;&#38598;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20840;&#23616;&#21160;&#24577;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#20998;&#26512;&#25105;&#20204;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#22522;&#32447;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#65292;&#23548;&#33268;&#26080;&#27861;&#23545;&#26102;&#38388;&#22270;&#32593;&#32476;&#36827;&#34892;&#25490;&#24207;&#30340;&#39044;&#27979;&#23436;&#20840;&#39281;&#21644;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33151;&#24335;&#36816;&#21160;&#30340;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#23545;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15462</link><description>&lt;p&gt;
DTC: &#28145;&#24230;&#36319;&#36394;&#25511;&#21046;--&#19968;&#31181;&#32479;&#19968;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#21151;&#33021;&#21644;&#40065;&#26834;&#30340;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion. (arXiv:2309.15462v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33151;&#24335;&#36816;&#21160;&#30340;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#23545;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33151;&#24335;&#36816;&#21160;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#38656;&#35201;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26469;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#33151;&#24335;&#31995;&#32479;&#20351;&#29992;&#36870;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#20248;&#21270;&#25511;&#21046;&#12290;&#36825;&#31181;&#23618;&#27425;&#21270;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22240;&#20854;&#30452;&#35266;&#30340;&#25104;&#26412;&#20989;&#25968;&#35843;&#25972;&#12289;&#20934;&#30830;&#30340;&#35268;&#21010;&#20197;&#21450;&#36229;&#36807;&#21313;&#24180;&#30340;&#24191;&#27867;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#28145;&#21051;&#29702;&#35299;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#19981;&#21305;&#37197;&#21644;&#20551;&#35774;&#30340;&#36829;&#21453;&#26159;&#25925;&#38556;&#25805;&#20316;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#21487;&#33021;&#22952;&#30861;&#25104;&#21151;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#36716;&#25442;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20223;&#30495;&#30340;&#24378;&#21270;&#23398;&#20064;&#20135;&#29983;&#20102;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#40065;&#26834;&#24615;&#21644;&#24674;&#22797;&#33021;&#21147;&#30340;&#36816;&#21160;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#37117;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#36825;&#31181;&#29615;&#22659;&#20013;&#21512;&#27861;&#30340;&#33853;&#33050;&#28857;&#24456;&#23569;&#65292;&#27604;&#22914;&#32570;&#21475;&#25110;&#36339;&#30707;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation and may hinder successful sim-to-real transfer. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achie
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15214</link><description>&lt;p&gt;
&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15214
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20174;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#36827;&#34892;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#21315;&#31859;&#23610;&#24230;&#25968;&#20540;&#27169;&#25311;&#65292;&#24182;&#39537;&#21160;&#36739;&#31895;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21315;&#31859;&#23610;&#24230;&#38477;&#23610;&#24230;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#21488;&#28286;&#30340;&#21306;&#22495;&#39640;&#20998;&#36776;&#29575;&#22825;&#27668;&#27169;&#22411;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#22522;&#30784;&#19979;&#36827;&#34892;&#20102;&#26465;&#20214;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22823;&#20998;&#36776;&#29575;&#27604;&#29575;&#65288;25km&#33267;2km&#65289;&#65292;&#19981;&#21516;&#23610;&#24230;&#19978;&#28041;&#21450;&#30340;&#19981;&#21516;&#29289;&#29702;&#36807;&#31243;&#20197;&#21450;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39044;&#27979;&#36890;&#36947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#26041;&#27861;&#65288;ResDiff&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;UNet&#65289;&#22238;&#24402;&#22312;&#31532;&#19968;&#27493;&#39044;&#27979;&#24179;&#22343;&#20540;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#31532;&#20108;&#27493;&#39044;&#27979;&#27531;&#24046;&#12290;\textit{ResDiff}&#22312;&#22359;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;CRPS&#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25216;&#33021;&#12290;ResDiff&#39044;&#27979;&#30340;&#20809;&#35889;&#21644;&#20998;&#24067;&#24544;&#23454;&#22320;&#24674;&#22797;&#20102;&#35843;&#33410;&#26377;&#23475;&#39118;&#21644;&#38632;&#30340;&#37325;&#35201;&#24130;&#24459;&#20851;&#31995;&#12290;&#32479;&#19968;&#30340;&#22825;&#27668;&#29616;&#35937;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
&lt;/p&gt;</description></item><item><title>BiSinger&#26159;&#19968;&#20010;&#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#34920;&#31034;&#12289;&#34701;&#21512;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#24320;&#28304;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.14089</link><description>&lt;p&gt;
BiSinger: &#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BiSinger: Bilingual Singing Voice Synthesis. (arXiv:2309.14089v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14089
&lt;/p&gt;
&lt;p&gt;
BiSinger&#26159;&#19968;&#20010;&#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#34920;&#31034;&#12289;&#34701;&#21512;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#24320;&#28304;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#22312;&#27468;&#22768;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22810;&#35821;&#31181;&#21512;&#25104;&#27468;&#22768;&#27169;&#22411;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BiSinger&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#30340;&#21452;&#35821;&#27969;&#34892;&#27468;&#22768;&#21512;&#25104;&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;&#31995;&#32479;&#38656;&#35201;&#20998;&#21035;&#38024;&#23545;&#27599;&#31181;&#35821;&#35328;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#26080;&#27861;&#20934;&#30830;&#22320;&#34920;&#31034;&#27721;&#35821;&#21644;&#33521;&#35821;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27721;&#35821;&#21644;&#33521;&#35821;&#27468;&#22768;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;CMU&#23383;&#20856;&#21644;&#26144;&#23556;&#35268;&#21017;&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#27468;&#22768;&#36716;&#25442;&#25216;&#26415;&#34701;&#21512;&#20102;&#21333;&#35821;&#27468;&#22768;&#25968;&#25454;&#38598;&#65292;&#20197;&#29983;&#25104;&#21452;&#35821;&#27468;&#22768;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#21452;&#35821;&#35328;&#38899;&#25968;&#25454;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#25972;&#21512;&#20351;&#24471;&#21333;&#19968;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#26041;&#38754;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;&#26576;&#32593;&#22336;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#20013;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.14068</link><description>&lt;p&gt;
&#36719;&#28151;&#21512;&#38477;&#22122;&#65306;&#36229;&#36234;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#36798;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models. (arXiv:2309.14068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#20013;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#22312;&#35832;&#22914;&#22270;&#20687;&#21512;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#26377;&#19968;&#31181;&#36235;&#21183;&#35777;&#26126;&#65288;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65289;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36817;&#20284;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#26041;&#38754;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#19968;&#20123;&#29616;&#26377;&#29702;&#35770;&#20445;&#35777;&#25152;&#20570;&#30340;&#20551;&#35774;&#36807;&#20110;&#24378;&#22823;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#38477;&#22122;&#20013;&#23384;&#22312;&#26080;&#30028;&#35823;&#24046;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#34920;&#36798;&#21147;&#24378;&#19988;&#39640;&#25928;&#30340;&#21453;&#21521;&#38477;&#22122;&#27169;&#22411;&#12290;SMD&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#20801;&#35768;&#25193;&#25955;&#27169;&#22411;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#32780;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SMD&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#65289;&#65292;&#23588;&#20854;&#22312;&#23569;&#37327;&#21453;&#21521;&#38477;&#22122;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually have an expressive bottleneck in backward denoising and some assumption made by existing theoretical guarantees is too strong. Based on this finding, we prove that diffusion models have unbounded errors in both local and global denoising. In light of our theoretical studies, we introduce soft mixture denoising (SMD), an expressive and efficient model for backward denoising. SMD not only permits diffusion models to well approximate any Gaussian mixture distributions in theory, but also is simple and efficient for implementation. Our experiments on multiple image datasets show that SMD significantly improves different types of diffusion models (e.g., DDPM), espeically in the situation of few backw
&lt;/p&gt;</description></item><item><title>Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.13860</link><description>&lt;p&gt;
Fast-HuBERT: &#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13860
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21508;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#65289;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#38754;&#20020;&#30528;&#20849;&#21516;&#30340;&#22256;&#22659;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#28145;&#20837;&#23398;&#26415;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;Fast-HuBERT&#12290;&#25152;&#25552;&#20986;&#30340;Fast-HuBERT&#21487;&#20197;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#20351;&#29992;8&#20010;V100 GPU&#22312;1.1&#22825;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#21407;&#22987;&#23454;&#29616;&#65292;&#21152;&#36895;&#20102;5.2&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Fast-HuBERT&#20013;&#30340;&#20004;&#31181;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20043;&#21069;&#24037;&#20316;&#20013;&#25253;&#21578;&#30340;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23558;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.13405</link><description>&lt;p&gt;
&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition. (arXiv:2309.13405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23558;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22823;&#35268;&#27169;&#30340;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26725;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#38408;&#20540;&#26679;&#26412;&#21327;&#26041;&#24046;&#22270;&#19978;&#30340;&#20960;&#20010;&#36739;&#23567;&#35268;&#27169;&#30340;&#23376;&#38382;&#39064;&#36827;&#34892;&#26725;&#22359;&#20998;&#35299;&#26469;&#31561;&#25928;&#20248;&#21270;&#65292;&#20197;&#21450;&#23545;&#24212;&#20110;&#26725;&#30340;&#26465;&#30446;&#30340;&#19968;&#32452;&#26126;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#31616;&#21333;&#32780;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#19968;&#20010;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#30340;&#21487;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#26174;&#33879;&#25913;&#36827;&#20102;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#35758;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#22522;&#20934;&#30456;&#27604;&#65292;&#36895;&#24230;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two ($\text{MTP}_2$). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a \emph{bridge-block decomposition} on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to \emph{bridges}. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22312;&#35821;&#38899;&#24212;&#29992;&#20013;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;BitFit&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#21482;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.10930</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#29992;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training for Speech. (arXiv:2309.10930v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22312;&#35821;&#38899;&#24212;&#29992;&#20013;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;BitFit&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#21482;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#65288;TTT&#65289;&#22312;&#22788;&#29702;&#35821;&#38899;&#24212;&#29992;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#20998;&#24067;&#20559;&#31227;&#24341;&#20837;&#21040;&#26631;&#20934;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#20363;&#22914;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#32490;&#26816;&#27979;&#65292;&#24182;&#25506;&#35752;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22914;&#20309;&#24110;&#21161;&#35843;&#25972;&#21040;&#36825;&#20123;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#30001;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#30340;&#33258;&#28982;&#21464;&#21270;&#65288;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65289;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20248;&#21270;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#20248;&#21270;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#36873;&#25321;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#21442;&#25968;&#23376;&#38598;&#65289;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#20110;&#27599;&#20010;&#26679;&#26412;&#37117;&#26377;&#33258;&#24049;&#30340;&#19968;&#32452;&#21442;&#25968;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#19981;&#21487;&#25193;&#23637;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;BitFit&#8212;&#8212;&#19968;&#31181;&#20165;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#25991;&#26412;&#24212;&#29992;&#20013;&#25552;&#20986;&#30340;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;&#20316;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the application of Test-Time Training (TTT) as a solution to handling distribution shifts in speech applications. In particular, we introduce distribution-shifts to the test datasets of standard speech-classification tasks -- for example, speaker-identification and emotion-detection -- and explore how Test-Time Training (TTT) can help adjust to the distribution-shift. In our experiments that include distribution shifts due to background noise and natural variations in speech such as gender and age, we identify some key-challenges with TTT including sensitivity to optimization hyperparameters (e.g., number of optimization steps and subset of parameters chosen for TTT) and scalability (e.g., as each example gets its own set of parameters, TTT is not scalable). Finally, we propose using BitFit -- a parameter-efficient fine-tuning algorithm proposed for text applications that only considers the bias parameters for fine-tuning -- as a solution to the aforementioned c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.07156</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#30561;&#30496;&#65306;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#19982;&#27169;&#22411;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#29983;&#29702;&#36807;&#31243;&#65292;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#20934;&#30830;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35782;&#21035;&#21487;&#33021;&#30340;&#30561;&#30496;&#38556;&#30861;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#23558;&#30561;&#30496;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;&#20998;&#31867;&#36807;&#31243;&#22522;&#20110;&#23545;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20998;&#26512;&#12290;&#24314;&#35758;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#32452;&#25104;&#65306;&#21033;&#29992;SE-ResNet&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21033;&#29992;Bi-LSTM&#21333;&#20803;&#22534;&#26632;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#24471;&#21040;&#35777;&#23454;&#65292;&#20998;&#21035;&#26159;SLeepEDF-20&#12289;SleepEDF-78&#21644;SHHS&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;87.5&#65285;&#12289;83.9&#65285;&#21644;87.8&#65285;&#65292;&#24182;&#19988;&#22312;&#23439;F1&#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#20026;82.5&#12289;78.9&#21644;81.9&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.03179</link><description>&lt;p&gt;
SLiMe: &#20687;&#25105;&#19968;&#26679;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03179
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;SD&#65289;&#65292;&#22312;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#23545;&#24212;&#21644;3D&#24418;&#29366;&#29983;&#25104;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20986;SLiMe&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#23545;&#22270;&#20687;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;SLiMe&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24352;&#35757;&#32451;&#22270;&#20687;&#21450;&#20854;&#20998;&#21106;&#25513;&#33180;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SD&#20808;&#39564;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#21152;&#26435;&#32047;&#31215;&#33258;&#27880;&#24847;&#21147;&#22270;&#8221;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20248;&#21270;&#31283;&#23450;&#25193;&#25955;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#20351;&#24471;&#27599;&#20010;&#23884;&#20837;&#21482;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20998;&#21106;&#21306;&#22495;&#12290;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#28982;&#21518;&#22312;&#27880;&#24847;&#21147;&#22270;&#20013;&#31361;&#20986;&#26174;&#31034;&#20998;&#21106;&#21306;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#36825;&#20351;&#24471;SLiMe&#21487;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
&lt;/p&gt;</description></item><item><title>FTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#12289;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23398;&#20064;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#38544;&#34255;&#29305;&#24449;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65292;&#20351;&#25915;&#20987;&#20855;&#26377;&#38544;&#34109;&#24615;&#12290;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00127</link><description>&lt;p&gt;
FTA: &#20855;&#26377;&#28789;&#27963;&#35302;&#21457;&#22120;&#30340;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (arXiv:2309.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00127
&lt;/p&gt;
&lt;p&gt;
FTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#12289;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23398;&#20064;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#38544;&#34255;&#29305;&#24449;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65292;&#20351;&#25915;&#20987;&#20855;&#26377;&#38544;&#34109;&#24615;&#12290;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21518;&#38376;&#25915;&#20987;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36890;&#29992;&#35302;&#21457;&#22120;&#25110;&#35821;&#20041;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#34987;&#26576;&#20123;&#38450;&#24481;&#26426;&#21046;&#65288;&#22914;&#33539;&#25968;&#21098;&#35009;&#65292;&#27604;&#36739;&#23616;&#37096;&#26356;&#26032;&#20043;&#38388;&#30340;&#21442;&#25968;&#24046;&#24322;&#65289;&#36731;&#26131;&#26816;&#27979;&#21644;&#36807;&#28388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#20855;&#26377;&#28789;&#27963;&#35302;&#21457;&#22120;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#35302;&#21457;&#22120;&#20989;&#25968;&#65292;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#19968;&#20010;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#24182;&#21516;&#26102;&#35753;&#35302;&#21457;&#22120;&#27169;&#24335;&#21253;&#21547;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#30340;&#38544;&#34255;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#21487;&#20197;&#22312;&#19981;&#21516;&#36718;&#27425;&#20013;&#19981;&#26029;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#35843;&#25972;&#21040;&#20840;&#23616;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65288;&#35302;&#21457;&#22120;&#27169;&#24335;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#26144;&#23556;&#65289;&#65292;&#25105;&#20204;&#20351;&#25105;&#20204;&#30340;&#25915;&#20987;&#20855;&#26377;&#33258;&#28982;&#30340;&#38544;&#34109;&#24615;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current backdoor attacks against federated learning (FL) strongly rely on universal triggers or semantic patterns, which can be easily detected and filtered by certain defense mechanisms such as norm clipping, comparing parameter divergences among local updates. In this work, we propose a new stealthy and robust backdoor attack with flexible triggers against FL defenses. To achieve this, we build a generative trigger function that can learn to manipulate the benign samples with an imperceptible flexible trigger pattern and simultaneously make the trigger pattern include the most significant hidden features of the attacker-chosen label. Moreover, our trigger generator can keep learning and adapt across different rounds, allowing it to adjust to changes in the global model. By filling the distinguishable difference (the mapping between the trigger pattern and target label), we make our attack naturally stealthy. Extensive experiments on real-world datasets verify the effectiveness and st
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.15605</link><description>&lt;p&gt;
&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#23545;&#20248;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#27979;&#37327;&#31713;&#25913;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#22810;&#20010;&#27979;&#37327;&#32467;&#26524;&#65292;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#20551;&#35937;&#65292;&#32780;&#19981;&#26159;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#25991;&#26412;&#36755;&#20837;&#21644;&#27979;&#37327;&#32467;&#26524;&#65292;&#26088;&#22312;&#30830;&#23450;&#26576;&#20010;&#32467;&#26524;&#26159;&#21542;&#21457;&#29983;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#37327;&#32467;&#26524;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#37117;&#34920;&#26126;&#32467;&#26524;&#21457;&#29983;&#30340;&#31034;&#20363;&#26159;&#21542;&#30830;&#23454;&#21457;&#29983;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#36825;&#26159;&#30001;&#20110;&#27979;&#37327;&#31713;&#25913;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31616;&#21333;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#22312;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#25105;&#20204;&#24863;&#21040;&#20852;&#22859;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12029</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#23610;&#24230;&#19981;&#21464;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12029
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#24179;&#34913;&#20173;&#28982;&#26159;MTL&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#22833;/&#26799;&#24230;&#23610;&#24230;&#30340;&#19981;&#24179;&#34913;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#25240;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;SI-MTL&#65289;&#26041;&#27861;&#65292;&#20174;&#25439;&#22833;&#21644;&#26799;&#24230;&#35282;&#24230;&#32531;&#35299;&#20102;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SI-MTL&#21253;&#21547;&#23545;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#30340;&#23545;&#25968;&#21464;&#25442;&#65292;&#20197;&#30830;&#20445;&#22312;&#25439;&#22833;&#27700;&#24179;&#19978;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#26799;&#24230;&#24179;&#34913;&#26041;&#27861;SI-G&#65292;&#23427;&#23558;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#20026;&#19982;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#19968;&#33268;&#35777;&#26126;&#20102;SI-G&#30340;&#26377;&#25928;&#24615;&#21644;SI-MTL&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11819</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#28151;&#28102;&#22120;&#35299;&#20915;&#26377;&#20559;&#35265;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11819
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25968;&#25454;&#24314;&#27169;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#19978;&#65292;&#30001;&#20110;EHR&#30340;&#22797;&#26434;&#28508;&#22312;&#32467;&#26500;&#21644;&#28508;&#22312;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#32780;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#20445;&#25345;&#27169;&#22411;&#25972;&#20307;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20581;&#24247;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#35266;&#23519;&#25968;&#25454;&#20043;&#22806;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20844;&#24179;&#32437;&#21521;&#21307;&#30103;&#21435;&#28151;&#28102;&#22120;&#65288;FLMD&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#24314;&#27169;&#12290;&#21463;&#21040;&#21435;&#28151;&#28102;&#22120;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;FLMD&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;FLMD&#25429;&#25417;&#20102;&#27599;&#27425;&#25509;&#35302;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#65292;&#36825;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#36229;&#20986;&#35266;&#23519;&#21040;&#30340;EHR&#20043;&#22806;&#30340;&#28508;&#22312;&#21307;&#30103;&#22240;&#32032;&#65292;&#22914;&#24739;&#32773;&#22522;&#22240;&#22411;&#21644;&#29983;&#27963;&#20064;&#24815;&#12290;&#36825;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.06203</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#26500;&#24314;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#31995;&#32479;&#35774;&#35745;&#32773;&#26080;&#27861;&#39044;&#27979;&#24182;&#26126;&#30830;&#35774;&#35745;&#20986;&#26426;&#22120;&#20154;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#22312;&#39640;&#24230;&#21463;&#25511;&#30340;&#29615;&#22659;&#20043;&#22806;&#23481;&#26131;&#20986;&#29616;&#25925;&#38556;&#12290;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#19982;&#20854;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#36890;&#24120;&#36935;&#21040;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#35768;&#22810;&#24212;&#29992;&#25152;&#38656;&#30340;&#22522;&#26412;&#24863;&#30693;&#21644;&#25805;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;&#20179;&#24211;&#29289;&#27969;&#21644;&#23478;&#24237;&#20154;&#24037;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#27169;&#25311;&#21151;&#33021;&#23884;&#20837;&#21040;&#36825;&#20010;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.05021</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65311;&#29702;&#35770;&#20998;&#26512;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05021
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#20110;&#20854;&#32423;&#32852;&#32467;&#26500;&#65292;&#21363;&#21435;&#22122;&#27169;&#22359;&#38142;&#24335;&#20256;&#25773;&#21644;&#25918;&#22823;&#20102;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26399;&#26395;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#22240;&#20026;&#35768;&#22810;&#39034;&#24207;&#27169;&#22411;&#65292;&#22914;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#65292;&#26159;&#19981;&#20250;&#20986;&#29616;&#38169;&#35823;&#20256;&#25773;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#24402;&#32467;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#20010;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26377;&#28145;&#21051;&#35265;&#35299;&#30340;&#36716;&#31227;&#26041;&#31243;&#65292;&#34920;&#26126;&#27169;&#22359;&#26080;&#27861;&#20174;&#36755;&#20837;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#29978;&#33267;&#20250;&#23558;&#39069;&#22806;&#30340;&#38169;&#35823;&#20256;&#25773;&#21040;&#19979;&#19968;&#20010;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30452;&#25509;&#23548;&#33268;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#26126;&#30830;&#20943;&#23569;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04553</link><description>&lt;p&gt;
&#20174;&#20551;&#21040;&#30495;&#65288;FFR&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#19982;&#21512;&#25104;&#25968;&#25454;&#30456;&#20851;&#24615;&#38169;&#35823;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#30001;&#20110;&#35757;&#32451;&#38598;&#30340;&#19981;&#24179;&#34913;&#23548;&#33268;&#30340;&#30456;&#20851;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#26576;&#20123;&#32676;&#20307;&#65288;&#22914;&#22899;&#24615;&#65289;&#22312;&#26576;&#20123;&#31867;&#21035;&#65288;&#22914;&#31243;&#24207;&#21592;&#65289;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#20026;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#65292;&#20174;&#32780;&#24179;&#34913;&#35757;&#32451;&#38598;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#23398;&#20064;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#20943;&#23569;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;1&#65289;&#25105;&#20204;&#22312;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;2&#65289;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#22312;&#31532;&#19968;&#27493;&#20013;&#25105;&#20204;&#23398;&#20064;&#21040;&#20102;&#25269;&#25239;&#20559;&#24046;&#30340;&#31283;&#20581;&#29305;&#24449;&#65292;&#22312;&#31532;&#20108;&#27493;&#20013;&#20943;&#36731;&#20102;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15503</link><description>&lt;p&gt;
Federated Learning&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Applicability of Federated Learning to Official Statistics. (arXiv:2307.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15503
&lt;/p&gt;
&lt;p&gt;
Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;FL&#27169;&#22411;&#24615;&#33021;&#19982;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#21305;&#37197;&#31243;&#24230;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;FL&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#25345;&#26377;&#32773;&#30340;&#38544;&#31169;&#65292;&#20174;&#32780;&#20415;&#20110;&#33719;&#21462;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#24182;&#26368;&#32456;&#25552;&#21319;&#23448;&#26041;&#32479;&#35745;&#25968;&#25454;&#12290;&#36890;&#36807;&#27169;&#25311;&#19977;&#31181;&#19981;&#21516;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#33719;&#24471;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#20351;&#29992;&#26696;&#20363;&#22522;&#20110;&#21307;&#30103;&#20445;&#38505;&#25968;&#25454;&#38598;&#12289;&#32454;&#39063;&#31890;&#29289;&#27745;&#26579;&#25968;&#25454;&#38598;&#21644;&#31227;&#21160;&#26080;&#32447;&#20449;&#21495;&#35206;&#30422;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#23448;&#26041;&#32479;&#35745;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#27599;&#20010;&#27169;&#25311;&#20013;&#38598;&#20013;&#24335;&#21644;FL&#31639;&#27861;&#24615;&#33021;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#19977;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;FL&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#19982;&#38598;&#20013;&#24335;&#27169;&#22411;&#22522;&#20934;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#21644;&#23427;&#20204;&#23545;&#27169;&#25311;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the potential of Federated Learning (FL) for official statistics and shows how well the performance of FL models can keep up with centralized learning methods. At the same time, its utilization can safeguard the privacy of data holders, thus facilitating access to a broader range of data and ultimately enhancing official statistics. By simulating three different use cases, important insights on the applicability of the technology are gained. The use cases are based on a medical insurance data set, a fine dust pollution data set and a mobile radio coverage data set - all of which are from domains close to official statistics. We provide a detailed analysis of the results, including a comparison of centralized and FL algorithm performances for each simulation. In all three use cases, we were able to train models via FL which reach a performance very close to the centralized model benchmarks. Our key observations and their implications for transferring the simulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.09955</link><description>&lt;p&gt;
XSkill&#65306;&#36328;&#20307;&#29616;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31034;&#33539;&#35270;&#39057;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24191;&#27867;&#25968;&#25454;&#28304;&#65292;&#24182;&#19988;&#26159;&#34920;&#36798;&#25152;&#38656;&#34892;&#20026;&#30340;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#35270;&#39057;&#20013;&#25552;&#21462;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#25216;&#33021;&#38754;&#20020;&#30528;&#20307;&#29616;&#24046;&#24322;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#20307;&#29616;&#24046;&#36317;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;XSkill&#65292;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#21517;&#20026;&#25216;&#33021;&#21407;&#22411;&#30340;&#36328;&#20307;&#29616;&#34920;&#31034;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#25216;&#33021;&#34920;&#31034;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#20154;&#31867;&#25552;&#31034;&#35270;&#39057;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26469;&#23436;&#25104;&#26410;&#35265;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#20419;&#36827;&#20102;&#26410;&#35265;&#20219;&#21153;&#30340;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08945</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#26469;&#20943;&#32531;&#26631;&#31614;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#22686;&#21152;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#20559;&#35265;&#22312;&#21253;&#25324;&#21307;&#30103;&#12289;&#25307;&#32856;&#21644;&#20869;&#23481;&#23457;&#26680;&#22312;&#20869;&#30340;&#37325;&#35201;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#23481;&#26131;&#24102;&#26377;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#26631;&#31614;&#20559;&#35265;&#30340;&#23384;&#22312;&#24050;&#32463;&#22312;&#27010;&#24565;&#19978;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20294;&#32570;&#20047;&#24212;&#23545;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#8212;&#8212;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#65288;DeCoLe&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#23558;DeCoLe&#24212;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#25361;&#25112;&#30340;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#25104;&#21151;&#35782;&#21035;&#20986;&#20559;&#35265;&#26631;&#31614;&#24182;&#36229;&#36234;&#31454;&#20105;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03811</link><description>&lt;p&gt;
&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#26144;&#23556;&#21040;&#22120;&#20214;&#24615;&#33021;&#30340;&#37197;&#26041;&#22270;
&lt;/p&gt;
&lt;p&gt;
Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22312;&#31215;&#26497;&#23547;&#27714;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#30340;&#32452;&#21512;&#26448;&#26009;&#65288;&#22914;&#37197;&#26041;&#65289;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#39046;&#22495;&#24863;&#30693;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#32452;&#21512;&#25104;&#37197;&#26041;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#23547;&#25214;&#30446;&#26631;&#24212;&#29992;&#30340;&#26032;&#21270;&#21512;&#29289;&#65292;&#20294;&#26159;&#22312;&#20174;&#31934;&#36873;&#21270;&#23398;&#31354;&#38388;&#20013;&#35782;&#21035;&#20986;&#21512;&#36866;&#30340;&#8220;&#37197;&#26041;&#8221;&#26041;&#38754;&#20173;&#28982;&#20027;&#35201;&#26159;&#23454;&#39564;&#39537;&#21160;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#36136;&#12290;&#22810;&#20010;GCN&#24182;&#34892;&#32452;&#35013;&#65292;&#24182;&#26681;&#25454;&#37197;&#26041;&#20013;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#30452;&#35266;&#22320;&#23545;&#37197;&#26041;&#25104;&#20998;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#28982;&#21518;&#26681;&#25454;&#30456;&#24212;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#23545;&#25152;&#24471;&#30340;&#20998;&#23376;&#25551;&#36848;&#31526;&#36827;&#34892;&#32553;&#25918;&#65292;&#25509;&#19979;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26234;&#33021;&#28388;&#27874;&#22120;&#36741;&#21161;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;SFDANN&#65289;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#28388;&#27874;&#22120;&#65292;&#22312;&#26102;&#39057;&#22495;&#20013;&#21160;&#24577;&#21152;&#24378;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.01429</link><description>&lt;p&gt;
&#26234;&#33021;&#28388;&#27874;&#22120;&#36741;&#21161;&#30340;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#24037;&#19994;&#22330;&#26223;&#20013;&#25925;&#38556;&#35786;&#26029;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios. (arXiv:2307.01429v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26234;&#33021;&#28388;&#27874;&#22120;&#36741;&#21161;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;SFDANN&#65289;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#28388;&#27874;&#22120;&#65292;&#22312;&#26102;&#39057;&#22495;&#20013;&#21160;&#24577;&#21152;&#24378;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21151;&#25928;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#30340;&#24037;&#20917;&#12289;&#19981;&#21516;&#30340;&#21333;&#20301;&#20043;&#38388;&#25110;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#36716;&#31227;&#36816;&#34892;&#32463;&#39564;&#21644;&#25925;&#38556;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#65292;&#26410;&#30693;&#30340;&#22122;&#22768;&#27700;&#24179;&#21644;&#31867;&#22411;&#20250;&#21152;&#21095;&#39046;&#22495;&#23545;&#40784;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26234;&#33021;&#28388;&#27874;&#22120;&#36741;&#21161;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;SFDANN&#65289;&#30340;UDA&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22312;&#26102;&#39057;&#22495;&#20013;&#23558;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21160;&#24577;&#22320;&#21152;&#24378;&#65292;&#23454;&#29616;&#39046;&#22495;&#23545;&#40784;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#23567;&#27874;&#21253;&#21464;&#25442;&#32593;&#32476;&#65288;LWPT&#65289;&#21644;&#20256;&#32479;&#30340;&#23567;&#27874;&#21253;&#21464;&#25442;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of unsupervised domain adaptation (UDA)-based fault diagnosis methods has shown significant efficacy in industrial settings, facilitating the transfer of operational experience and fault signatures between different operating conditions, different units of a fleet or between simulated and real data. However, in real industrial scenarios, unknown levels and types of noise can amplify the difficulty of domain alignment, thus severely affecting the diagnostic performance of deep learning models. To address this issue, we propose an UDA method called Smart Filter-Aided Domain Adversarial Neural Network (SFDANN) for fault diagnosis in noisy industrial scenarios. The proposed methodology comprises two steps. In the first step, we develop a smart filter that dynamically enforces similarity between the source and target domain data in the time-frequency domain. This is achieved by combining a learnable wavelet packet transform network (LWPT) and a traditional wavelet packet tra
&lt;/p&gt;</description></item><item><title>ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00398</link><description>&lt;p&gt;
ProbVLM: &#20923;&#32467;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00398
&lt;/p&gt;
&lt;p&gt;
ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#25104;&#21151;&#22320;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26631;&#20934;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#36807;&#31243;&#65292;&#23558;&#22270;&#20687;&#25110;&#25991;&#26412;&#26679;&#26412;&#26144;&#23556;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21521;&#37327;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65306;&#30001;&#20110;&#22810;&#20010;&#26679;&#26412;&#65288;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#21487;&#20197;&#25277;&#35937;&#20986;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#30456;&#21516;&#27010;&#24565;&#65292;&#30830;&#23450;&#24615;&#23884;&#20837;&#19981;&#21453;&#26144;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProbVLM&#65292;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20107;&#21518;&#26041;&#24335;&#22312;&#39044;&#35757;&#32451;&#30340;VLM&#20013;&#36890;&#36807;&#20869;&#37096;/&#22806;&#37096;&#27169;&#24577;&#23545;&#40784;&#20272;&#35745;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#12290;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;COCO&#12289;Flickr&#12289;CUB&#21644;Oxford-flowers&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20004;&#20010;VLM&#65288;CLIP&#21644;BLIP&#65289;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#20102;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26657;&#20934;&#65292;&#24182;&#34920;&#26126;ProbVLM&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20852;&#36259;&#28857;&#20043;&#38388;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35775;&#38382;&#37327;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.15927</link><description>&lt;p&gt;
&#20174;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#20013;&#23398;&#20064;&#21160;&#24577;&#22270;&#20197;&#20934;&#30830;&#39044;&#27979;&#20852;&#36259;&#28857;&#30340;&#35775;&#38382;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting. (arXiv:2306.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20852;&#36259;&#28857;&#20043;&#38388;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35775;&#38382;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#22320;&#21306;&#39044;&#27979;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#35775;&#38382;&#37327;&#23545;&#20110;&#35268;&#21010;&#21644;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28041;&#21450;&#39046;&#22495;&#21253;&#25324;&#22478;&#24066;&#35268;&#21010;&#12289;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#30740;&#31350;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;POIs&#20043;&#38388;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#25581;&#31034;POIs&#20043;&#38388;&#30340;&#28508;&#22312;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#35775;&#38382;&#37327;&#12290;&#19982;&#20854;&#20182;&#20165;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;BysGNN&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#36890;&#36807;&#32467;&#21512;&#25152;&#26377;&#32972;&#26223;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#21495;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the number of visits to Points-of-Interest (POI) in an urban area is critical for planning and decision-making for various application domains, from urban planning and transportation management to public health and social studies. Although this forecasting problem can be formulated as a multivariate time-series forecasting task, the current approaches cannot fully exploit the ever-changing multi-context correlations among POIs. Therefore, we propose Busyness Graph Neural Network (BysGNN), a temporal graph neural network designed to learn and uncover the underlying multi-context correlations between POIs for accurate visit forecasting. Unlike other approaches where only time-series data is used to learn a dynamic graph, BysGNN utilizes all contextual information and time-series data to learn an accurate dynamic graph representation. By incorporating all contextual, temporal, and spatial signals, we observe a significant improvement in our forecasting accuracy over state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14861</link><description>&lt;p&gt;
&#21033;&#29992;&#20219;&#21153;&#32467;&#26500;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#36776;&#21035;&#24615;&#30340;&#29702;&#35770;&#65292;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#20219;&#21153;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#21518;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#24773;&#20917;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#24378;&#22823;&#21644;&#26356;&#26377;&#29992;&#30340;&#32467;&#26524;&#12290;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24674;&#22797;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#35268;&#33539;&#34920;&#31034;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;</title><link>http://arxiv.org/abs/2306.13253</link><description>&lt;p&gt;
&#25552;&#21069;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65306;&#30740;&#31350;&#25484;&#25569;&#25216;&#33021;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#39044;&#27979;&#65292;&#35813;&#29616;&#35937;&#26159;&#23436;&#32654;&#27010;&#25324;&#22312;&#20986;&#29616;&#36807;&#25311;&#21512;&#25110;&#35760;&#24518;&#36857;&#35937;&#20043;&#21518;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#25165;&#20986;&#29616;&#12290;&#25253;&#21578;&#31216;&#65292;&#21482;&#26377;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#19979;&#25165;&#33021;&#35266;&#23519;&#21040;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#36825;&#20351;&#24471;&#30830;&#23450;&#23548;&#33268;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29702;&#35299;&#21069;&#28010;&#28526;&#38656;&#35201;&#22823;&#37327;&#30340;&#36845;&#20195;&#36718;&#25968;&#65292;&#22240;&#27492;&#23547;&#25214;&#23548;&#33268;&#23427;&#30340;&#36229;&#21442;&#25968;&#26159;&#24456;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22823;&#37327;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#22312;&#21069;&#20960;&#36718;&#20013;&#20986;&#29616;&#26576;&#20123;&#25391;&#33633;&#65292;&#37027;&#20040;&#21487;&#20197;&#26399;&#26395;&#22312;&#27169;&#22411;&#35757;&#32451;&#26356;&#38271;&#26102;&#38388;&#21518;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07261</link><description>&lt;p&gt;
&#21462;&#28040;&#19971;&#24180;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19971;&#24180;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#35823;&#24046;&#29575;&#30456;&#31561;&#12290;&#36825;&#39033;&#24037;&#20316;&#21551;&#21160;&#20102;&#25968;&#30334;&#31687;&#35770;&#25991;&#65292;&#22768;&#31216;&#33021;&#22815;&#25913;&#36827;&#21518;&#22788;&#29702;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#25968;&#21315;&#20010;&#27169;&#22411;&#35780;&#20272;&#30340;&#23454;&#35777;&#35780;&#20272;&#26469;&#35780;&#20272;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#35770;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#22256;&#25200;&#20102;&#20197;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#19968;&#20010;&#19982;&#20351;&#29992;&#19981;&#21516;&#30340;&#26080;&#32422;&#26463;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#26377;&#20851;&#12290;&#21478;&#19968;&#20010;&#28041;&#21450;&#23454;&#29616;&#19981;&#21516;&#30340;&#32422;&#26463;&#25918;&#26494;&#27700;&#24179;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21462;&#28040;&#22788;&#29702;&#65292;&#22823;&#33268;&#23545;&#24212;&#20110;&#21518;&#22788;&#29702;&#30340;&#21453;&#28436;&#12290;&#21462;&#28040;&#22788;&#29702;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#20351;&#29992;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#21644;&#25918;&#26494;&#32423;&#21035;&#30340;&#26041;&#27861;&#12290;&#35299;&#35835;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02747</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#28304;&#34920;&#31034;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21463;&#21040;&#22797;&#26434;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35797;&#22270;&#26126;&#30830;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#21464;&#21270;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#38750;&#24179;&#31283;&#24615;&#21487;&#20197;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#20013;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20256;&#25773;&#21644;&#32047;&#31215;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22797;&#26434;&#24615;&#24182;&#24433;&#21709;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#12290;COREP&#20027;&#35201;&#37319;&#29992;&#24341;&#23548;&#26356;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#19968;&#31181;&#31283;&#23450;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#31216;&#20026;&#22240;&#26524;&#36215;&#28304;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#34917;&#20805;&#20102;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.01904</link><description>&lt;p&gt;
&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Stability Gap in Continual Learning. (arXiv:2306.01904v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20154;&#20204;&#35748;&#20026;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#20351;&#32593;&#32476;&#26356;&#26032;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#38556;&#30861;&#26159;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#21363;&#22312;&#26356;&#26032;&#26032;&#25968;&#25454;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#28982;&#21518;&#25165;&#24471;&#20197;&#24674;&#22797;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#32531;&#35299;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#20551;&#35774;&#20197;&#20102;&#35299;&#20854;&#20135;&#29983;&#21407;&#22240;&#12290;&#36825;&#20351;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#31283;&#23450;&#24615;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#25152;&#38656;&#30340;&#32593;&#32476;&#26356;&#26032;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21487;&#33021;&#25512;&#21160;&#36830;&#32493;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, deep neural networks are retrained from scratch as a dataset grows in size. Given the computational expense for retraining networks, it has been argued that continual learning could make updating networks more efficient. An obstacle to achieving this goal is the stability gap, which refers to an observation that when updating on new data, performance on previously learned data degrades before recovering. Addressing this problem would enable learning new data with fewer network updates, resulting in increased computational efficiency. We study how to mitigate the stability gap. We test a variety of hypotheses to understand why the stability gap occurs. This leads us to discover a method that vastly reduces this gap. In large-scale class incremental learning experiments, we are able to significantly reduce the number of network updates needed for continual learning. Our work has the potential to advance the state-of-the-art in continual learning for real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01474</link><description>&lt;p&gt;
&#36890;&#29992;&#31561;&#21464;Transformer&#65306;&#29992;&#20110;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#35768;&#22810;&#36807;&#31243;&#28041;&#21450;&#19981;&#21516;&#20998;&#23376;&#20043;&#38388;&#30340;&#21508;&#31181;3D&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#19982;&#34507;&#30333;&#36136;&#65292;&#34507;&#30333;&#36136;&#19982;&#23567;&#20998;&#23376;&#31561;&#12290;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26469;&#23398;&#20064;&#26222;&#36866;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20998;&#23376;&#36890;&#24120;&#20197;&#19981;&#21516;&#31890;&#24230;&#34920;&#31034;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#23558;3D&#20998;&#23376;&#36890;&#29992;&#34920;&#31034;&#20026;&#38598;&#21512;&#30340;&#20960;&#20309;&#22270;&#24418;&#22270;&#65292;&#19982;&#20256;&#32479;&#21333;&#23618;&#34920;&#31034;&#24418;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#34920;&#31034;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#31561;&#21464;Transformer&#65288;GET&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#31232;&#30095;&#22359;&#32423;&#21644;&#23494;&#38598;&#21407;&#23376;&#32423;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GET&#30001;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#32452;&#25104;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#20197;&#28385;&#36275;3D&#19990;&#30028;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;GET&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2306.00740</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#32622;&#20449;&#29616;&#35937;&#21450;&#20854;&#23545;&#26657;&#20934;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00740
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23649;&#27425;&#34920;&#29616;&#20986;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20272;&#35745;&#19981;&#20339;&#30340;&#24773;&#20917;&#8212;&#8212;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#22312;&#38169;&#35823;&#26102;&#32463;&#24120;&#36807;&#24230;&#33258;&#20449;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#20197;&#20462;&#25913;&#35757;&#32451;&#26041;&#26696;&#21644;&#35757;&#32451;&#21518;&#26657;&#20934;&#31243;&#24207;&#30340;&#24418;&#24335;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#30340;&#37325;&#35201;&#38556;&#30861;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#24456;&#22810;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20013;&#37117;&#20250;&#20986;&#29616;&#65288;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36825;&#31181;&#29616;&#35937;&#20986;&#29616;&#26102;&#65292;&#22312;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#30340;&#22823;&#31867;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#21363;&#20351;&#22312;&#24212;&#29992;&#26657;&#20934;&#21518;&#20063;&#19981;&#33021;&#33719;&#24471;&#27604;&#38543;&#26426;&#26356;&#22909;&#30340;&#28176;&#36817;&#26657;&#20934;&#27169;&#22411;&#65288;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19473</link><description>&lt;p&gt;
&#23545;&#25968;&#20985;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38142;
&lt;/p&gt;
&lt;p&gt;
Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26159;&#19968;&#31181;&#20174;&#26410;&#26631;&#20934;&#21270;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#36890;&#29992;&#31639;&#27861;&#31867;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;MCMC&#38754;&#20020;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65306;(i)&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#22312;&#30001;&#23567;&#27010;&#29575;&#22359;&#38548;&#24320;&#30340;&#21306;&#22495;&#20013;&#38598;&#20013;;(ii)&#23545;&#25968;&#20985;&#24615;&#30340;&#23567;&#27010;&#29575;&#22359;&#26412;&#36523;&#36890;&#24120;&#23384;&#22312;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#35770;&#23494;&#24230;&#20989;&#25968;&#30340;&#26368;&#23567;&#20551;&#35774;&#26159;&#20160;&#20040;&#65292;&#20174;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#36890;&#36807;&#31561;&#22122;&#22768;&#27979;&#37327;&#30340;&#32047;&#31215;&#65292;&#20174;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#24207;&#21015;&#12290;&#35813;&#26500;&#36896;&#36319;&#36394;&#20102;&#26679;&#26412;&#21382;&#21490;&#65292;&#22240;&#27492;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#32780;&#35328;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#65292;&#20294;&#21382;&#21490;&#20165;&#20197;&#32463;&#39564;&#22343;&#20540;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20869;&#23384;&#21360;&#36857;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#25512;&#24191;&#20102;&#27493;&#34892;&#36339;&#36291;&#37319;&#26679;&#65288;1&#65289;&#12290;"&#36208;"&#38454;&#27573;&#21464;&#25104;&#20102;&#23545;&#25968;&#20985;&#38142;&#30340;(&#38750;&#39532;&#23572;&#21487;&#22827;)&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36827;&#34892;&#24555;&#36895;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#20013;&#36895;&#24230;&#22826;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18944</link><description>&lt;p&gt;
&#31070;&#32463;PDE&#20195;&#29702;&#30340;&#24555;&#36895;&#21160;&#24577;1D&#21187;&#28982;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates. (arXiv:2305.18944v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36827;&#34892;&#24555;&#36895;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#20013;&#36895;&#24230;&#22826;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#38081;&#25176;&#24335;&#26680;&#32858;&#21464;&#35774;&#22791;&#20013;&#30340;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#23545;&#20110;&#24212;&#23545;&#20854;&#28909;&#37327;&#21644;&#31890;&#23376;&#36890;&#37327;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#25311;&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#31561;&#31163;&#23376;&#20307;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#65292;&#30446;&#21069;&#21482;&#26377;&#31616;&#21333;&#30340;&#36817;&#20284;&#26041;&#27861;&#36895;&#24230;&#36275;&#22815;&#24555;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;PDE&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#31181;&#24555;&#36895;&#27169;&#25311;&#22120;&#30340;&#32570;&#20047;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20351;&#29992;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#20195;&#29702;&#27169;&#22411;&#36817;&#20284;&#28436;&#21270;&#21442;&#32771;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#23436;&#25972;&#31354;&#38388;&#35299;&#20915;&#26041;&#26696;&#30340;&#26102;&#38388;&#27493;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;DIV1D&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#21363;&#20174;X&#28857;&#65288;&#19978;&#28216;&#65289;&#21040;&#30446;&#26631;&#22788;&#30340;1D&#28909;&#27969;&#31649;&#30340;DIV1D&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;TCV&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#65292;&#20854;&#21160;&#24577;&#26159;&#30001;&#19978;&#28216;&#23494;&#24230;&#22350;&#21475;&#24341;&#36215;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24555;&#36895;&#26242;&#24577;&#30340;&#25506;&#32034;&#24615;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural PDE surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D's domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. Stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#35777;&#26126;&#20102;GD&#31639;&#27861;&#30340;&#19968;&#33324;&#24615;&#20445;&#35777;&#65292;&#20026;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#25512;&#23548;&#20986;&#20102;&#36807;&#37327;&#39118;&#38505;&#29575;&#65292;&#25193;&#23637;&#20102;&#20197;&#24448;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16891</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#35777;&#26126;&#20102;GD&#31639;&#27861;&#30340;&#19968;&#33324;&#24615;&#20445;&#35777;&#65292;&#20026;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#25512;&#23548;&#20986;&#20102;&#36807;&#37327;&#39118;&#38505;&#29575;&#65292;&#25193;&#23637;&#20102;&#20197;&#24448;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#26041;&#27861;&#65292;&#23545;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#27867;&#21270;&#36827;&#34892;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#38544;&#34255;&#23618;NN&#19978;&#65292;&#24182;&#27809;&#26377;&#35299;&#20915;&#19981;&#21516;&#32593;&#32476;&#32553;&#25918;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;GD&#22312;&#22810;&#23618;NN&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#20197;&#24448;&#30340;&#24037;&#20316;&#12290;&#23545;&#20110;&#21452;&#23618;NN&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#19968;&#33324;&#30340;&#32593;&#32476;&#32553;&#25918;&#21442;&#25968;&#19979;&#24314;&#31435;&#30340;&#65292;&#25918;&#23485;&#20102;&#20197;&#21069;&#30340;&#26465;&#20214;&#12290;&#23545;&#20110;&#19977;&#23618;NN&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#31574;&#30053;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20960;&#20046;&#21327;&#21516;&#32422;&#26463;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#19968;&#33324;&#24615;&#21457;&#29616;&#30340;&#30452;&#25509;&#24212;&#29992;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;GD&#31639;&#27861;&#22312;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#20013;&#30340;&#36807;&#37327;&#39118;&#38505;&#36895;&#29575;&#20026;$O(1/\sqrt{n})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\sqrt{n})$ for GD algorithms in both two-layer and thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.16791</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Kidger&#65292;Morrill&#31561;&#65292;2020&#65289;&#20174;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#39044;&#27979;&#32467;&#26524;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#36830;&#32493;&#36335;&#24452;&#30340;&#31163;&#25955;&#21270;&#65292;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#22330;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20381;&#36182;&#20110;&#36825;&#20010;&#36335;&#24452;&#12290;&#20351;&#29992;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20250;&#24341;&#20837;&#31163;&#25955;&#20559;&#24046;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20851;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27969;&#30340;&#36830;&#32493;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36924;&#36817;&#20559;&#24046;&#30452;&#25509;&#19982;&#30001;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#30456;&#20851;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#19978;&#30028;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36798;&#21040;&#30340;&#26399;&#26395;&#25439;&#22833;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
&lt;/p&gt;</description></item><item><title>&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;</title><link>http://arxiv.org/abs/2305.16038</link><description>&lt;p&gt;
$L_{2}$&#27491;&#21017;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38544;&#24615;SGD&#20559;&#24046;&#65306;&#20174;&#39640;&#31209;&#21040;&#20302;&#31209;&#30340;&#21333;&#21521;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16038
&lt;/p&gt;
&lt;p&gt;
&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65288;DLN&#65289;&#30340;$L_{2}$&#27491;&#21017;&#21270;&#25439;&#22833;&#20855;&#26377;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#23545;&#24212;&#20110;&#20855;&#26377;&#19981;&#21516;&#31209;&#30340;&#30697;&#38453;&#12290;&#22312;&#30697;&#38453;&#23436;&#25104;&#31561;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#25910;&#25947;&#21040;&#26368;&#23567;&#31209;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#35813;&#23616;&#37096;&#26368;&#23567;&#20540;&#20173;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#21487;&#20197;&#36731;&#26494;&#36991;&#20813;&#20302;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#36866;&#21512;&#25968;&#25454;&#65292;&#20294;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#20250;&#38519;&#20837;&#39640;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;SGD&#65292;&#24635;&#26159;&#26377;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#36339;&#36291;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#27010;&#29575;&#65292;&#20294;&#36339;&#22238;&#30340;&#27010;&#29575;&#20026;&#38646;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#38598;&#21512;$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$&#65292;&#20351;&#24471;$B_{r}$&#21253;&#21547;&#31209;$r$&#25110;&#26356;&#23569;&#30340;&#25152;&#26377;&#26368;&#23567;&#20540;&#65288;&#32780;&#19981;&#26159;&#26356;&#22810;&#65289;&#65292;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#23725;&#21442;&#25968;$\lambda$&#21644;&#23398;&#20064;&#29575;$\eta$&#65292;&#23427;&#20204;&#26159;&#21560;&#25910;&#30340;&#65306;SGD&#31163;&#24320;$B_{r}$&#30340;&#27010;&#29575;&#20026;0&#65292;&#20174;&#20219;&#20309;&#36215;&#28857;&#24320;&#22987;&#65292;SGD&#36827;&#20837;$B_{r}$&#30340;&#27010;&#29575;&#38750;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15927</link><description>&lt;p&gt;
&#29992;&#26368;&#20248;&#20256;&#36755;&#23398;&#20064;&#26377;&#21521;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15927
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#27010;&#29575;&#26377;&#21521;&#22270;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#32467;&#26500;&#20381;&#36182;&#24615;&#25110;&#27169;&#22411;&#31867;&#30340;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#37117;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#35266;&#28857;&#25480;&#26435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36816;&#20316;&#65292;&#32780;&#19981;&#20250;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20570;&#20986;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#25110;&#35785;&#35832;&#20110;&#40657;&#31665;&#21464;&#20998;&#36817;&#20284;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#23427;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24674;&#22797;&#22522;&#20934;&#21442;&#25968;&#65292;&#32780;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#24471;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15613</link><description>&lt;p&gt;
&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;
&lt;/p&gt;
&lt;p&gt;
Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;nD&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#28857;&#20113;&#20998;&#26512;&#20013;&#31561;&#21464;&#20110;&#27491;&#20132;&#36716;&#25442;&#65292;&#21033;&#29992;&#20102;&#36229;&#29699;&#20307;&#21644;&#24120;&#35268;n&#21333;&#24418;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36817;&#26399;&#21457;&#23637;&#30340;&#21487;&#25805;&#32437;3D&#29699;&#24418;&#31070;&#32463;&#20803;&#29702;&#35770;--&#22522;&#20110;&#29699;&#24418;&#20915;&#31574;&#38754;&#30340;SO&#65288;3&#65289;-&#31561;&#21464;&#28388;&#27874;&#22120;&#32452;&#65292;&#23558;&#35813;&#31070;&#32463;&#20803;&#25193;&#23637;&#21040;&#20102;nD&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22534;&#21472;&#22312;&#22810;&#23618;&#20013;&#12290;&#21033;&#29992;ModelNet40&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#28508;&#22312;&#23454;&#29992;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15196</link><description>&lt;p&gt;
&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS
&lt;/p&gt;
&lt;p&gt;
Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#26159;N-BEATS&#30340;&#38750;&#24179;&#20961;&#25193;&#23637;&#65292;&#37319;&#29992;&#20102;&#21452;&#37325;&#27531;&#24046;&#21472;&#21152;&#21407;&#21017;&#65288;Oreshkin&#31561;&#20154;[42]&#65289;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22260;&#32469;&#30528;&#30001;N-BEATS&#27599;&#20010;&#22534;&#26632;&#30340;&#27531;&#24046;&#21644;&#29305;&#24449;&#25552;&#21462;&#31639;&#23376;&#30340;&#22797;&#26434;&#32452;&#21512;&#20135;&#29983;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65288;Sinkhorn&#36317;&#31163;&#65289;&#23558;&#23427;&#20204;&#22534;&#21472;&#22320;&#23545;&#40784;&#12290;&#35757;&#32451;&#25439;&#22833;&#30001;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;&#21363;&#39044;&#27979;&#25439;&#22833;&#65289;&#21644;Sinkhorn&#36317;&#31163;&#35745;&#31639;&#30340;&#23545;&#40784;&#25439;&#22833;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#28304;&#25968;&#25454;&#24207;&#21015;&#20013;&#22534;&#21472;&#22320;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;N-BEATS&#30340;&#21487;&#35299;&#37322;&#35774;&#35745;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13849</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#20272;&#35745;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#21644;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#26679;&#26412;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33391;&#22909;&#27491;&#21017;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#20102;&#37325;&#22823;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;&#12289;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#26550;&#26500;&#30340;&#25913;&#21160;&#35201;&#27714;&#26368;&#23567;&#12290;&#20026;&#20102;&#24471;&#21040;&#36866;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#26031;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31867;&#20869;&#34920;&#31034;&#20998;&#20026;&#22810;&#20010;&#39640;&#26031;&#12290;&#20855;&#26377;&#38750;&#39640;&#26031;&#34920;&#31034;&#30340;&#31867;&#21035;&#34987;&#33258;&#21160;&#35782;&#21035;&#24182;&#21160;&#24577;&#32858;&#31867;&#20026;&#22810;&#20010;&#22823;&#27010;&#29575;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#31867;&#21035;&#12290;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13289</link><description>&lt;p&gt;
&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#26679;&#26412;&#22797;&#26434;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;DRO&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20027;&#21160;&#25506;&#32034;&#12290;&#35813;&#38382;&#39064;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#24754;&#35266;&#30340;&#24577;&#24230;&#23545;&#24453;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24809;&#32602;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#29366;&#24577;-&#34892;&#20026;&#23545;&#30340;&#22870;&#21169;&#26469;&#20445;&#23432;&#20272;&#35745;&#20540;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#22522;&#20110;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32479;&#35745;&#21512;&#29702;&#30340;&#36716;&#31227;&#26680;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#23547;&#25214;&#22312;&#35813;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#38669;&#22827;&#19969;&#39118;&#26684;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#26679;&#30495;&#23454;&#30340;&#36716;&#31227;&#26680;&#20197;&#39640;&#27010;&#29575;&#20301;&#20110;&#35813;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#23454;&#29616;$\epsilon$&#30340;&#27425;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mat&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.12715</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65306;&#23398;&#20064;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;ILL&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#23558;&#31934;&#30830;&#26631;&#31614;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19982;&#20197;&#21069;&#35797;&#22270;&#20174;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;ILL&#26694;&#26550;&#32771;&#34385;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#24378;&#21152;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#20219;&#20309;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ILL&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20197;&#21450;&#36825;&#20123;&#37197;&#32622;&#30340;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#26631;&#24535;&#30528;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
&lt;/p&gt;</description></item><item><title>Elektrum&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#37238;&#21453;&#24212;&#65292;&#21033;&#29992;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;Elektrum&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#30340;&#27835;&#30103;&#24178;&#39044;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11917</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#19982;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#29702;&#35299;&#24207;&#21015;&#20381;&#36182;&#30340;&#37238;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Interpretable neural architecture search and transfer learning for understanding sequence dependent enzymatic reactions. (arXiv:2305.11917v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11917
&lt;/p&gt;
&lt;p&gt;
Elektrum&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#37238;&#21453;&#24212;&#65292;&#21033;&#29992;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;Elektrum&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#30340;&#27835;&#30103;&#24178;&#39044;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#33410;&#30340;&#37238;&#36884;&#24452;&#25511;&#21046;&#30528;&#32454;&#32990;&#36807;&#31243;&#65292;&#23427;&#20204;&#30340;&#22833;&#35843;&#21487;&#33021;&#23548;&#33268;&#30142;&#30149;&#12290;&#20026;&#36825;&#20123;&#36884;&#24452;&#21019;&#24314;&#39044;&#27979;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#36884;&#24452;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#32454;&#32990;&#21644;&#22522;&#22240;&#32452;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Elektrum&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21644;&#29983;&#29289;&#29289;&#29702;&#35299;&#37322;&#27169;&#22411;&#65292;&#30830;&#23450;&#29983;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#20307;&#22806;&#21160;&#21147;&#23398;&#27979;&#23450;&#24555;&#36895;&#20551;&#35774;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;&#21160;&#21147;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;KINN&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#21453;&#24212;&#36895;&#29575;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#36801;&#31227;&#23398;&#20064;&#27493;&#39588;&#65292;&#23558;KINN&#20316;&#20026;&#20013;&#20171;&#23618;&#25554;&#20837;&#26356;&#28145;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24494;&#35843;&#21453;&#24212;&#30456;&#20851;&#30340;&#20307;&#20869;&#32467;&#26524;&#30340;&#39044;&#27979;&#12290;Elektrum&#26377;&#25928;&#21033;&#29992;&#20102;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#25429;&#33719;&#32454;&#32990;&#32972;&#26223;&#30340;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;Elektrum&#24212;&#29992;&#20110;&#29702;&#35299;&#19982;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#32925;&#30149;&#30456;&#20851;&#30340;&#30899;&#27700;&#21270;&#21512;&#29289;&#21644;&#33026;&#36136;&#20195;&#35874;&#20013;&#28041;&#21450;&#30340;&#37238;&#21453;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;Elektrum&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;1&#65289;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#39044;&#27979;&#20307;&#20869;&#21453;&#24212;&#36895;&#29575;; (2) &#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;; &#20197;&#21450;&#65288;3&#65289;&#21457;&#29616;&#27835;&#30103;&#24178;&#39044;&#30340;&#28508;&#22312;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finely-tuned enzymatic pathways control cellular processes, and their dysregulation can lead to disease. Creating predictive and interpretable models for these pathways is challenging because of the complexity of the pathways and of the cellular and genomic contexts. Here we introduce Elektrum, a deep learning framework which addresses these challenges with data-driven and biophysically interpretable models for determining the kinetics of biochemical systems. First, it uses in vitro kinetic assays to rapidly hypothesize an ensemble of high-quality Kinetically Interpretable Neural Networks (KINNs) that predict reaction rates. It then employs a novel transfer learning step, where the KINNs are inserted as intermediary layers into deeper convolutional neural networks, fine-tuning the predictions for reaction-dependent in vivo outcomes. Elektrum makes effective use of the limited, but clean in vitro data and the noisy, yet plentiful in vivo data that captures cellular context. We apply Ele
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>GETMusic&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#25193;&#25955;&#27169;&#22411;GETDiff&#12290;GETScore&#20351;&#29992;&#26631;&#35760;&#34920;&#31034;&#38899;&#31526;&#65292;&#23558;&#23427;&#20204;&#26377;&#24207;&#22320;&#32452;&#32455;&#36215;&#26469;&#65292;&#32780;GETDiff&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#12290;</title><link>http://arxiv.org/abs/2305.10841</link><description>&lt;p&gt;
GETMusic&#65306;&#20351;&#29992;&#32479;&#19968;&#30340;&#34920;&#31034;&#21644;&#25193;&#25955;&#26694;&#26550;&#29983;&#25104;&#20219;&#24847;&#38899;&#20048;&#26354;&#30446;
&lt;/p&gt;
&lt;p&gt;
GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework. (arXiv:2305.10841v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10841
&lt;/p&gt;
&lt;p&gt;
GETMusic&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#25193;&#25955;&#27169;&#22411;GETDiff&#12290;GETScore&#20351;&#29992;&#26631;&#35760;&#34920;&#31034;&#38899;&#31526;&#65292;&#23558;&#23427;&#20204;&#26377;&#24207;&#22320;&#32452;&#32455;&#36215;&#26469;&#65292;&#32780;GETDiff&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#26088;&#22312;&#21019;&#24314;&#38899;&#31526;&#65292;&#20026;&#29992;&#25143;&#21019;&#20316;&#38899;&#20048;&#25552;&#20379;&#24110;&#21161;&#65292;&#20363;&#22914;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#30446;&#26631;&#20048;&#22120;&#36712;&#36947;&#25110;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#28304;&#36712;&#36947;&#36827;&#34892;&#21019;&#20316;&#12290;&#32771;&#34385;&#21040;&#28304;&#36712;&#36947;&#21644;&#30446;&#26631;&#36712;&#36947;&#20043;&#38388;&#30340;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#24615;&#32452;&#21512;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#30340;&#32479;&#19968;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#20316;&#21697;&#30001;&#20110;&#38899;&#20048;&#34920;&#31034;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETMusic&#65288;`GET'&#20195;&#34920;GEnerate music Tracks&#65289;&#30340;&#32479;&#19968;&#34920;&#31034;&#21644;&#25193;&#25955;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#19968;&#20010;&#21517;&#20026;GETDiff&#30340;&#25193;&#25955;&#22411;&#27169;&#22411;&#12290;GETScore&#23558;&#38899;&#31526;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#22312;&#20108;&#32500;&#32467;&#26500;&#20013;&#20117;&#28982;&#26377;&#24207;&#22320;&#32452;&#32455;&#23427;&#20204;&#65292;&#36712;&#36947;&#22402;&#30452;&#22534;&#21472;&#24182;&#27700;&#24179;&#22320;&#38543;&#26102;&#38388;&#25512;&#36827;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36712;&#36947;&#38543;&#26426;&#34987;&#36873;&#20026;&#30446;&#26631;&#25110;&#28304;&#12290;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrumental tracks from scratch, or based on user-provided source tracks. Considering the diverse and flexible combination between source and target tracks, a unified model capable of generating any arbitrary tracks is of crucial necessity. Previous works fail to address this need due to inherent constraints in music representations and model architectures. To address this need, we propose a unified representation and diffusion framework named GETMusic (`GET' stands for GEnerate music Tracks), which includes a novel music representation named GETScore, and a diffusion model named GETDiff. GETScore represents notes as tokens and organizes them in a 2D structure, with tracks stacked vertically and progressing horizontally over time. During training, tracks are randomly selected as either the target or source. In the forward process, target tracks are corrupted by masking
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09807</link><description>&lt;p&gt;
&#20851;&#20110;transformer&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#21487;&#36801;&#31227;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26597;&#35810;&#23545;&#27169;&#22411;&#23398;&#20064;&#26368;&#26377;&#30410;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#20110;&#24494;&#35843;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#19981;&#28165;&#26970;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#20027;&#21160;&#23398;&#20064;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36866;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#31215;&#26497;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#22312;&#20351;&#29992;&#19981;&#21516;PLM&#35757;&#32451;&#26102;&#33021;&#21542;&#20445;&#25345;AL&#25910;&#30410;&#12290;&#25105;&#20204;&#23558;AL&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#19982;&#19981;&#21516;PLMs&#26597;&#35810;&#21040;&#30340;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#34920;&#26126;&#20855;&#26377;&#31867;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;AL&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#33719;&#21462;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#26356;&#21463;&#21040;AL&#26041;&#27861;&#30340;&#36873;&#25321;&#32780;&#38750;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#21453;&#38382;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#23454;&#26102;&#20272;&#35745;&#21644;&#35745;&#31639;&#19982;&#36870;&#38382;&#39064;&#35299;&#30456;&#20851;&#30340;&#24863;&#20852;&#36259;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.08324</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#29992;&#20110;&#21453;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented Uncertainty Quantification for Inverse Problems via Variational Encoder-Decoder Networks. (arXiv:2304.08324v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#21453;&#38382;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#23454;&#26102;&#20272;&#35745;&#21644;&#35745;&#31639;&#19982;&#36870;&#38382;&#39064;&#35299;&#30456;&#20851;&#30340;&#24863;&#20852;&#36259;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#21453;&#38382;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#19982;&#26631;&#20934;&#30340;&#36870;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#8220;&#30446;&#26631;&#23548;&#21521;&#8221;&#30340;&#65292;&#30446;&#26631;&#26159;&#20272;&#35745;&#19982;&#36870;&#38382;&#39064;&#30340;&#35299;&#30340;&#20989;&#25968;&#26377;&#20851;&#30340;&#19968;&#20123;&#24863;&#20852;&#36259;&#37327;&#65288;QoI&#65289;&#65292;&#32780;&#19981;&#26159;&#35299;&#26412;&#36523;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#19982;QoI&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#24863;&#20852;&#36259;&#65292;&#22240;&#27492;&#21033;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#65292;&#21253;&#25324;&#39044;&#27979;&#31639;&#23376;&#21644;&#25506;&#32034;&#21518;&#39564;&#30340;&#25216;&#26415;&#12290;&#36825;&#21487;&#33021;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#32447;&#24615;&#12289;&#21487;&#33021;&#26410;&#30693;&#30340;&#31639;&#23376;&#21644;&#38750;&#26631;&#20934;&#30340;&#20808;&#39564;&#20551;&#35774;&#12290;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21363;VED&#32593;&#32476;&#65292;&#26469;&#25551;&#36848;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#35268;&#27169;&#36870;&#38382;&#39064;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#23545;QoI&#36827;&#34892;&#23454;&#26102;&#30340;&#30446;&#26631;&#23548;&#21521;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we describe a new approach that uses variational encoder-decoder (VED) networks for efficient goal-oriented uncertainty quantification for inverse problems. Contrary to standard inverse problems, these approaches are \emph{goal-oriented} in that the goal is to estimate some quantities of interest (QoI) that are functions of the solution of an inverse problem, rather than the solution itself. Moreover, we are interested in computing uncertainty metrics associated with the QoI, thus utilizing a Bayesian approach for inverse problems that incorporates the prediction operator and techniques for exploring the posterior. This may be particularly challenging, especially for nonlinear, possibly unknown, operators and nonstandard prior assumptions. We harness recent advances in machine learning, i.e., VED networks, to describe a data-driven approach to large-scale inverse problems. This enables a real-time goal-oriented uncertainty quantification for the QoI. One of the advantages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLF&#65289;&#26041;&#27861;&#19982;Actor-Critic&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#20445;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#22522;&#20110;&#26469;&#33258;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22791;&#29992;&#25511;&#21046;&#22120;&#65292;&#20197;&#38450;RL&#25511;&#21046;&#22120;&#26080;&#27861;&#25552;&#20379;&#31283;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#35299;&#20915;&#37327;&#23376;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36816;&#29992;&#23481;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.02613</link><description>&lt;p&gt;
&#37327;&#23376;&#26368;&#20248;&#25511;&#21046;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantum Algorithms for Quantum Optimal Control. (arXiv:2304.02613v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#35299;&#20915;&#37327;&#23376;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36816;&#29992;&#23481;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;&#31639;&#27861;&#35299;&#20915;&#37327;&#23376;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#36895;&#24230;&#24555;&#20986;&#25351;&#25968;&#20493;&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#23547;&#25214;&#25511;&#21046;&#21464;&#37327;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#26102;&#38388;T&#26102;&#21463;&#26102;&#21464;&#34203;&#23450;&#35860;&#26041;&#31243;&#25511;&#21046;&#30340;&#31995;&#32479;&#30340;&#26576;&#20010;&#29289;&#29702;&#37327;&#12290;&#36825;&#31181;&#25511;&#21046;&#38382;&#39064;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#26377;&#30528;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#26102;&#21464;&#21704;&#23494;&#39039;&#27169;&#25311;&#26041;&#27861;&#21644;&#24555;&#36895;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#26469;&#33258;&#21508;&#20010;&#27493;&#39588;&#30340;&#24635;&#35823;&#24046;&#65292;&#22914;&#25511;&#21046;&#20989;&#25968;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#12289;&#25968;&#20540;&#31215;&#20998;&#21644;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#31639;&#27861;&#38656;&#35201;&#23481;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present efficient quantum algorithms that are exponentially faster than classical algorithms for solving the quantum optimal control problem. This problem involves finding the control variable that maximizes a physical quantity at time $T$, where the system is governed by a time-dependent Schr\"odinger equation. This type of control problem also has an intricate relation with machine learning. Our algorithms are based on a time-dependent Hamiltonian simulation method and a fast gradient-estimation algorithm. We also provide a comprehensive error analysis to quantify the total error from various steps, such as the finite-dimensional representation of the control function, the discretization of the Schr\"odinger equation, the numerical quadrature, and optimization. Our quantum algorithms require fault-tolerant quantum computers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31215;&#26497;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;PAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#38656;&#35201;&#30693;&#36947;&#27491;&#35270;&#22270;&#30340;&#38480;&#21046;&#12290;PAL&#19981;&#20165;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36824;&#33021;&#22815;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#24182;&#25903;&#25345;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.15256</link><description>&lt;p&gt;
&#20027;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;: &#21482;&#38656;&#23569;&#37327;&#20302;&#25104;&#26412;&#30340;&#20851;&#31995;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need. (arXiv:2303.15256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31215;&#26497;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;PAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#38656;&#35201;&#30693;&#36947;&#27491;&#35270;&#22270;&#30340;&#38480;&#21046;&#12290;PAL&#19981;&#20165;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36824;&#33021;&#22815;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#24182;&#25903;&#25345;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SSL&#38656;&#35201;&#26500;&#24314;&#24050;&#30693;&#35821;&#20041;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#21363;&#27491;&#35270;&#22270;&#12290;&#36825;&#31181;&#38656;&#27714;&#26159;SSL&#30340;&#20027;&#35201;&#38480;&#21046;&#65292;&#36890;&#24120;&#36890;&#36807;&#20020;&#26102;&#31574;&#30053;&#26469;&#22788;&#29702;&#65292;&#20363;&#22914;&#23545;&#30456;&#21516;&#36755;&#20837;&#24212;&#29992;&#24050;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31215;&#26497;&#20027;&#21160;&#23398;&#20064;&#65288;PAL&#65289;&#23558;&#36825;&#20010;&#21407;&#21017;&#24418;&#24335;&#21270;&#21644;&#25512;&#24191;&#65292;&#20854;&#20013;&#19968;&#20010;oracle&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;PAL&#23454;&#29616;&#20102;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#25581;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#22270;&#30340;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;SSL&#65292;&#21487;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;oracle&#25193;&#23637;&#21040;&#22788;&#29702;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#23427;&#20026;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#65288;&#22914;&#35266;&#27979;&#30340;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#31639;&#27861;&#65292;&#32780;&#26080;&#38656;&#26356;&#25913;&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#12290;&#31532;&#19977;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#23545;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#21435;&#22122;&#22120;&#30340;Plug &amp; Play&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#22270;&#20687;&#30340;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11217</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#36870;&#38382;&#39064;&#36827;&#34892;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Inverse problem regularization with hierarchical variational autoencoders. (arXiv:2303.11217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#23545;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#21435;&#22122;&#22120;&#30340;Plug &amp; Play&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#22270;&#20687;&#30340;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#23618;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#20316;&#20026;&#22270;&#20687;&#20808;&#39564;&#23545;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#21435;&#22122;&#22120;&#30340;Plug &amp; Play&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;VAE&#30340;&#29305;&#24615;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#21463;&#30410;&#20110;Plug-and-Play&#65288;PnP&#65289;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38480;&#20110;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;PnP-HVAE&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20219;&#24847;&#23610;&#23544;&#30340;&#33258;&#28982;&#22270;&#20687;&#30340;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PnP-HVAE&#26041;&#27861;&#22312;&#19982;&#22522;&#20110;&#21435;&#22122;&#22120;&#30340;Plug &amp; Play&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20854;&#20182;&#29616;&#26377;&#24674;&#22797;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose to regularize ill-posed inverse problems using a deep hierarchical variational autoencoder (HVAE) as an image prior. The proposed method synthesizes the advantages of i) denoiser-based Plug \&amp; Play approaches and ii) generative model based approaches to inverse problems. First, we exploit VAE properties to design an efficient algorithm that benefits from convergence guarantees of Plug-and-Play (PnP) methods. Second, our approach is not restricted to specialized datasets and the proposed PnP-HVAE model is able to solve image restoration problems on natural images of any size. Our experiments show that the proposed PnP-HVAE method is competitive with both SOTA denoiser-based PnP approaches, and other SOTA restoration methods based on generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08103</link><description>&lt;p&gt;
&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24212;&#29992;&#65288;&#22914;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65289;&#36890;&#24120;&#38754;&#20020;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05319</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#24211;&#19978;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;LM&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#24182;&#32463;&#24120;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#26412;&#30740;&#31350;&#27839;&#30528;&#20004;&#20010;&#37325;&#35201;&#26041;&#21521;&#30740;&#31350;&#20102;LM&#30340;&#23433;&#20840;&#24615;:(i)&#23433;&#20840;&#21152;&#22266;&#65292;&#26088;&#22312;&#22686;&#24378;LM&#22312;&#29983;&#25104;&#23433;&#20840;&#20195;&#30721;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;;(ii)&#23545;&#25239;&#27979;&#35797;&#65292;&#26088;&#22312;&#22312;&#23545;&#25239;&#24615;&#31435;&#22330;&#35780;&#20272;LM&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#39033;&#31216;&#20026;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#23433;&#20840;&#20219;&#21153;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#23558;&#19968;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25351;&#23548;LM&#29983;&#25104;&#23433;&#20840;&#25110;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#30041;LM&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SVEN&#30340;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;SVEN&#21033;&#29992;&#23646;&#24615;&#29305;&#23450;&#30340;&#36830;&#32493;&#21521;&#37327;&#26469;&#24341;&#23548;&#31243;&#24207;&#29983;&#25104;&#36798;&#21040;&#32473;&#23450;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#20462;&#25913;LM&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#25237;&#24433;&#25439;&#22833;&#26469;&#20248;&#21270;&#36825;&#20123;&#36830;&#32493;&#21521;&#37327;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SVEN&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#24403;&#21069;&#30340;LM&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22312;&#27979;&#35797;&#26102;&#20462;&#25913;&#23427;&#20204;&#30340;&#36755;&#20837;&#32780;&#20445;&#30041;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#38656;&#35201;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#24212;&#24615;&#35843;&#24230;&#35268;&#21017;&#26469;&#35299;&#20915;&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#29983;&#20135;&#20013;&#30340;&#20013;&#26029;&#21644;&#20132;&#25442;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.02506</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33021;&#20013;&#26029;&#20132;&#25442;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#20013;&#30340;&#35843;&#24230;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning. (arXiv:2302.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#24212;&#24615;&#35843;&#24230;&#35268;&#21017;&#26469;&#35299;&#20915;&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#29983;&#20135;&#20013;&#30340;&#20013;&#26029;&#21644;&#20132;&#25442;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65288;ISBJSSP&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#23384;&#20648;&#33021;&#21147;&#19981;&#36275;&#21644;&#24847;&#22806;&#29983;&#20135;&#20013;&#26029;&#30340;&#38382;&#39064;&#65292;&#23454;&#38469;&#27169;&#25311;&#35768;&#22810;&#21046;&#36896;&#35268;&#21010;&#21644;&#29289;&#27969;&#24212;&#29992;&#12290;&#30001;&#20110;&#26426;&#22120;&#25925;&#38556;&#25110;&#32500;&#25252;&#30340;&#38543;&#26426;&#24178;&#25200;&#65292;&#24037;&#19994;&#29983;&#20135;&#29615;&#22659;&#36890;&#24120;&#36873;&#25321;&#37319;&#29992;&#35843;&#24230;&#35268;&#21017;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#23454;&#26102;&#37325;&#35843;&#24230;&#65292;&#32780;&#19981;&#26159;&#38656;&#35201;&#22312;&#38382;&#39064;&#26465;&#20214;&#21160;&#24577;&#21464;&#21270;&#26102;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#20026;&#20102;&#29983;&#25104;ISBJSSP&#38382;&#39064;&#30340;&#35843;&#24230;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20998;&#31163;&#22270;&#20844;&#24335;&#65292;&#20854;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#21487;&#25345;&#32493;&#36827;&#34892;&#21024;&#38500;&#21644;&#28155;&#21152;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#33021;&#22815;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#33258;&#36866;&#24212;&#35843;&#24230;&#22120;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#20013;&#26029;&#12289;&#20132;&#25442;&#31561;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a complex scheduling problem that is able to model many manufacturing planning and logistics applications realistically by addressing both the lack of storage capacity and unforeseen production interruptions. Subjected to random disruptions due to machine malfunction or maintenance, industry production settings often choose to adopt dispatching rules to enable adaptive, real-time re-scheduling, rather than traditional methods that require costly re-computation on the new configuration every time the problem condition changes dynamically. To generate dispatching rules for the ISBJSSP problem, we introduce a dynamic disjunctive graph formulation characterized by nodes and edges subjected to continuous deletions and additions. This formulation enables the training of an adaptive scheduler utilizing graph neural networks and reinforcement learning. Furthermore, a simulator is developed to simulate interruption, swapping, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#25554;&#34917;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#39550;&#39542;&#21592;&#25163;&#21183;&#27963;&#21160;&#30340;&#20934;&#30830;&#20998;&#31867;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12592</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#30340;&#38598;&#25104;&#23398;&#20064;&#22312;&#36974;&#25377;&#21644;&#32570;&#22833;&#20449;&#24687;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65306;&#26694;&#26550;&#21644;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21496;&#26426;&#25163;&#21183;&#35782;&#21035;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition. (arXiv:2301.12592v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#25554;&#34917;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#39550;&#39542;&#21592;&#25163;&#21183;&#27963;&#21160;&#30340;&#20934;&#30830;&#20998;&#31867;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#20026;&#38598;&#25104;&#23398;&#20064;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20197;&#21033;&#29992;&#20887;&#20313;&#21644;&#34917;&#20805;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#23433;&#20840;&#24212;&#29992;&#20013;&#36827;&#34892;&#36830;&#32493;&#21496;&#26426;&#29366;&#24577;&#30417;&#27979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#30001;&#36974;&#25377;&#12289;&#22122;&#22768;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#24341;&#36215;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#36825;&#20123;&#25968;&#25454;&#32570;&#21475;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22635;&#34917;&#20449;&#24687;&#32570;&#22833;&#30340;&#25554;&#34917;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#25163;&#21183;&#27963;&#21160;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#24182;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#21516;&#32452;&#21463;&#35797;&#32773;&#39564;&#35777;&#26102;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#26368;&#20339;&#21333;&#19968;&#25668;&#20687;&#22836;&#27169;&#22411;&#22312;&#20272;&#35745;&#25163;&#20013;&#29289;&#20307;&#30340;&#20301;&#32622;&#26102;&#65292;&#25105;&#20204;&#30340;&#22810;&#25668;&#20687;&#22836;&#26694;&#26550;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#19988;&#22312;&#36328;&#32452;&#39564;&#35777;&#26102;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-sensor frameworks provide opportunities for ensemble learning and sensor fusion to make use of redundancy and supplemental information, helpful in real-world safety applications such as continuous driver state monitoring which necessitate predictions even in cases where information may be intermittently missing. We define this problem of intermittent instances of missing information (by occlusion, noise, or sensor failure) and design a learning framework around these data gaps, proposing and analyzing an imputation scheme to handle missing information. We apply these ideas to tasks in camera-based hand activity classification for robust safety during autonomous driving. We show that a late-fusion approach between parallel convolutional neural networks can outperform even the best-placed single camera model in estimating the hands' held objects and positions when validated on within-group subjects, and that our multi-camera framework performs best on average in cross-group validat
&lt;/p&gt;</description></item><item><title>BQ-NCO&#36890;&#36807;&#23558;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#24182;&#20351;&#29992;&#21452;&#23556;&#32553;&#20943;&#26041;&#27861;&#26469;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.03313</link><description>&lt;p&gt;
BQ-NCO: &#39640;&#25928;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#21452;&#23556;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization. (arXiv:2301.03313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03313
&lt;/p&gt;
&lt;p&gt;
BQ-NCO&#36890;&#36807;&#23558;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#24182;&#20351;&#29992;&#21452;&#23556;&#32553;&#20943;&#26041;&#27861;&#26469;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#21551;&#21457;&#24335;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;(COPs)&#26032;&#39062;&#22320;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(MDPs)&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;COPs&#30340;&#20849;&#21516;&#23545;&#31216;&#24615;&#65292;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#12290;&#20174;&#30452;&#25509;&#30340;&#26500;&#36896;&#26041;&#27861;&#30340;MDP&#24418;&#24335;&#21270;&#24320;&#22987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;MDPs&#20013;&#21452;&#23556;&#32553;&#20943;(BQ)&#30340;&#36890;&#29992;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#20855;&#26377;&#36882;&#24402;&#24615;&#36136;&#30340;COPs&#65292;&#25105;&#20204;&#29305;&#21270;&#21452;&#23556;&#32553;&#20943;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20943;&#23567;&#30340;&#29366;&#24577;&#31354;&#38388;&#26469;&#21033;&#29992;&#36825;&#20123;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#24182;&#20419;&#36827;MDP&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26377;&#21407;&#21017;&#30340;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;BQ-MDP&#30340;&#26368;&#20248;&#31574;&#30053;&#23454;&#38469;&#19978;&#35299;&#20915;&#20102;&#30456;&#20851;&#30340;COPs&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27431;&#20960;&#37324;&#24471;&#26053;&#34892;&#21830;&#38382;&#39064;&#12289;&#38750;&#23545;&#31216;&#26053;&#34892;&#21830;&#38382;&#39064;&#12289;&#23481;&#37327;&#36710;&#36742;&#36335;&#30001;&#38382;&#39064;&#12289;&#23450;&#21521;&#38382;&#39064;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization remains a challenge. In this paper, we present a novel formulation of Combinatorial Optimization Problems (COPs) as Markov Decision Processes (MDPs) that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Starting from a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. Then, for COPs with a recursive nature, we specialize the bisimulation and show how the reduced state exploits the symmetries of these problems and facilitates MDP solving. Our approach is principled and we prove that an optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.00856</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#39118;&#38505;&#33258;&#36866;&#24212;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#22312;&#24037;&#31243;&#35774;&#35745;&#12289;&#32479;&#35745;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#30001;&#20110;&#22266;&#26377;&#30340;&#39118;&#38505;&#35268;&#36991;&#21644;&#23545;&#20551;&#35774;&#30340;&#27169;&#31946;&#24615;&#65292;&#36890;&#24120;&#36890;&#36807;&#21046;&#23450;&#21644;&#35299;&#20915;&#20351;&#29992;&#39118;&#38505;&#21644;&#30456;&#20851;&#27010;&#24565;&#30340;&#20445;&#23432;&#20248;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;25&#24180;&#26469;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20174;&#23427;&#20204;&#22312;&#37329;&#34701;&#24037;&#31243;&#39046;&#22495;&#30340;&#36215;&#27493;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23427;&#20204;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#24037;&#31243;&#21644;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#39118;&#38505;&#27979;&#24230;&#25166;&#26681;&#20110;&#20984;&#20998;&#26512;&#65292;&#20026;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#35745;&#31639;&#21644;&#29702;&#35770;&#20248;&#21183;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#20107;&#23454;&#65292;&#21015;&#20030;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#21442;&#32771;&#25991;&#29486;&#20379;&#36827;&#19968;&#27493;&#38405;&#35835;&#12290;&#35813;&#35843;&#26597;&#36824;&#22238;&#39038;&#20102;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#32852;&#31995;&#65292;&#25351;&#20986;&#20102;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#23450;&#20041;&#20102;&#30456;&#23545;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#30340;&#30446;&#30340;&#65292;&#24182;&#22312;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.00484</link><description>&lt;p&gt;
&#39640;&#25928;&#25511;&#21046;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control. (arXiv:2212.00484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#30340;&#30446;&#30340;&#65292;&#24182;&#22312;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#23454;&#29616;&#65292;&#20174;&#32479;&#35745;&#36716;&#25442;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#37325;&#35201;&#30340;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#20256;&#32479;&#25216;&#26415;&#21019;&#24314;&#36716;&#25442;&#25968;&#25454;&#38598;&#38750;&#24120;&#32791;&#26102;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#38454;&#27573;&#22806;&#65292;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#24046;&#20998;&#38544;&#31169;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#21066;&#24369;&#25968;&#25454;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#29305;&#21035;&#35299;&#20915;&#39640;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#27169;&#31946;&#39640;&#39118;&#38505;&#26696;&#20363;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#30340;&#25968;&#25454;&#25928;&#29992;&#12290;&#19982;17&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;&#20256;&#32479;&#21644;&#26368;&#26032;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30456;&#27604;&#65292;&#949;-PrivateSMOTE&#22312;&#38544;&#31169;&#39118;&#38505;&#21644;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user data privacy can be achieved via many methods, from statistical transformations to generative models. However, all of them have critical drawbacks. For example, creating a transformed data set using traditional techniques is highly time-consuming. Also, recent deep learning-based solutions require significant computational resources in addition to long training phases, and differentially private-based solutions may undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE, a technique designed for safeguarding against re-identification and linkage attacks, particularly addressing cases with a high re-identification risk. Our proposal combines synthetic data generation via noise-induced interpolation to obfuscate high-risk cases while maximising the data utility of the original data. Compared to multiple traditional and state-of-the-art privacy-preservation methods on 17 data sets, $\epsilon$-PrivateSMOTE achieves competitive results in privacy risk and b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.07533</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#12290;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#21327;&#21464;&#37327;&#30340;&#26435;&#37325;&#65292;&#20351;&#24471;&#25968;&#25454;&#30340;&#20998;&#24067;&#31867;&#20284;&#20110;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#24179;&#34913;&#26435;&#37325;&#65288;NBW&#65289;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#65292;&#20197;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102; $\alpha$-&#24046;&#24322;&#20316;&#20026;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#20854;&#22320;&#38754;&#23454;&#20917;&#20540;&#21644;&#26080;&#20559;&#23567;&#25209;&#37327;&#26799;&#24230;&#30340;&#20272;&#35745;&#22120;&#65292;&#32780;&#19988;&#23545;&#20110;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20197;&#19979;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#24179;&#34913;&#26435;&#37325;&#65306;&#25552;&#39640;&#24179;&#34913;&#26435;&#37325;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26816;&#26597;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;TiDAL&#65292;&#21033;&#29992;&#35757;&#32451;&#21160;&#21147;&#23398;&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36319;&#36394;&#38382;&#39064;&#65292;&#21033;&#29992;&#39044;&#27979;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2210.06788</link><description>&lt;p&gt;
TiDAL: &#23398;&#20064;&#20027;&#21160;&#23398;&#20064;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
TiDAL: Learning Training Dynamics for Active Learning. (arXiv:2210.06788v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;TiDAL&#65292;&#21033;&#29992;&#35757;&#32451;&#21160;&#21147;&#23398;&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36319;&#36394;&#38382;&#39064;&#65292;&#21033;&#29992;&#39044;&#27979;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#26368;&#26377;&#29992;&#30340;&#25968;&#25454;&#26679;&#26412;&#24182;&#23545;&#20854;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#25193;&#23637;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#23588;&#20854;&#26159;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#36873;&#25321;&#26368;&#19981;&#30830;&#23450;&#30340;&#26679;&#26412;&#65292;&#22312;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#32463;&#24120;&#24573;&#35270;&#35757;&#32451;&#21160;&#21147;&#23398;&#65288;TD&#65289;&#65292;&#21363;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#26102;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#23613;&#31649;&#25991;&#29486;&#30340;&#20854;&#20182;&#39046;&#22495;&#32463;&#39564;&#35777;&#26126;TD&#25552;&#20379;&#20102;&#34913;&#37327;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#21160;&#21147;&#23398;&#20027;&#21160;&#23398;&#20064;&#65288;TiDAL&#65289;&#65292;&#23427;&#21033;&#29992;TD&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#36319;&#36394;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;TD&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25152;&#20197;TiDAL&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#27979;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;TD&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;TiDAL&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to select the most useful data samples from an unlabeled data pool and annotate them to expand the labeled dataset under a limited budget. Especially, uncertainty-based methods choose the most uncertain samples, which are known to be effective in improving model performance. However, AL literature often overlooks training dynamics (TD), defined as the ever-changing model behavior during optimization via stochastic gradient descent, even though other areas of literature have empirically shown that TD provides important clues for measuring the sample uncertainty. In this paper, we propose a novel AL method, Training Dynamics for Active Learning (TiDAL), which leverages the TD to quantify uncertainties of unlabeled data. Since tracking the TD of all the large-scale unlabeled data is impractical, TiDAL utilizes an additional prediction module that learns the TD of labeled data. To further justify the design of TiDAL, we provide theoretical and empirical evidence t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;Neyman-Pearson&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;COVID-19&#24739;&#32773;&#25968;&#25454;&#20013;&#20248;&#20808;&#30830;&#23450;&#20005;&#37325;&#30142;&#30149;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#24739;&#32773;&#30340;&#29983;&#29289;&#29305;&#24449;&#26469;&#39044;&#27979;&#20005;&#37325;&#31243;&#24230;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#38169;&#35823;&#20248;&#20808;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.02197</link><description>&lt;p&gt;
COVID-19&#24739;&#32773;&#25968;&#25454;&#20013;&#20248;&#20808;&#30830;&#23450;&#20005;&#37325;&#30142;&#30149;&#31867;&#21035;&#30340;&#20998;&#23618;Neyman-Pearson&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neyman-Pearson Classification for Prioritizing Severe Disease Categories in COVID-19 Patient Data. (arXiv:2210.02197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;Neyman-Pearson&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;COVID-19&#24739;&#32773;&#25968;&#25454;&#20013;&#20248;&#20808;&#30830;&#23450;&#20005;&#37325;&#30142;&#30149;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#24739;&#32773;&#30340;&#29983;&#29289;&#29305;&#24449;&#26469;&#39044;&#27979;&#20005;&#37325;&#31243;&#24230;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#38169;&#35823;&#20248;&#20808;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#20174;&#26080;&#30151;&#29366;&#21040;&#38656;&#35201;&#20303;&#38498;&#19981;&#31561;&#12290;&#20102;&#35299;&#23548;&#33268;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#26426;&#21046;&#23545;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#21644;&#38477;&#20302;&#27515;&#20129;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#33719;&#24471;&#36825;&#31181;&#29702;&#35299;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#31867;&#21035;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#24739;&#32773;&#30340;&#29983;&#29289;&#29305;&#24449;&#26469;&#39044;&#27979;&#24739;&#32773;&#30340;&#20005;&#37325;&#31243;&#24230;&#31867;&#21035;&#12290;&#22312;&#36825;&#20010;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#26377;&#21033;&#20110;&#20248;&#20808;&#35782;&#21035;&#26356;&#20005;&#37325;&#30340;&#31867;&#21035;&#24182;&#25511;&#21046;&#8220;&#27425;&#32423;&#20998;&#31867;&#8221;&#38169;&#35823;&#65292;&#21363;&#23558;&#24739;&#32773;&#38169;&#35823;&#20998;&#31867;&#20026;&#26356;&#36731;&#24494;&#30340;&#31867;&#21035;&#12290;Neyman-Pearson (NP)&#20998;&#31867;&#33539;&#24335;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20248;&#20808;&#22788;&#29702;&#25351;&#23450;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;NP&#36807;&#31243;&#35201;&#20040;&#36866;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#35201;&#20040;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#26080;&#27861;&#39640;&#27010;&#29575;&#25511;&#21046;&#20248;&#20808;&#38169;&#35823;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;NP (H-NP)&#26694;&#26550;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 has a spectrum of disease severity, ranging from asymptomatic to requiring hospitalization. Understanding the mechanisms driving disease severity is crucial for developing effective treatments and reducing mortality rates. One way to gain such understanding is using a multi-class classification framework, in which patients' biological features are used to predict patients' severity classes. In this severity classification problem, it is beneficial to prioritize the identification of more severe classes and control the "under-classification" errors, in which patients are misclassified into less severe categories. The Neyman-Pearson (NP) classification paradigm has been developed to prioritize the designated type of error. However, current NP procedures are either for binary classification or do not provide high probability controls on the prioritized errors in multi-class classification. Here, we propose a hierarchical NP (H-NP) framework and an umbrella algorithm that generall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#35843;&#35856;&#65288;PIT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#30340;&#20840;&#23616;&#29366;&#24577;&#21644;&#30005;&#33655;&#24577;&#12290;&#20854;&#20013;&#65292;PIT&#30340;&#31532;&#19968;&#20010;&#27169;&#22359;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#23548;&#33322;&#65292;&#31532;&#20108;&#20010;&#27169;&#22359;&#20351;&#29992;&#19968;&#31995;&#21015;&#27979;&#37327;&#26469;&#35843;&#33410;&#30005;&#33655;&#29366;&#24577;&#12290;&#36825;&#20123;&#24037;&#20855;&#20855;&#26377;&#30452;&#35266;&#12289;&#21487;&#38752;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2209.03837</link><description>&lt;p&gt;
&#20351;&#29992;&#23556;&#32447;&#35843;&#35856;&#38453;&#21015;&#65306;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#37327;&#23376;&#28857;&#30005;&#33655;&#24577;&#35843;&#35856;
&lt;/p&gt;
&lt;p&gt;
Tuning arrays with rays: Physics-informed tuning of quantum dot charge states. (arXiv:2209.03837v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#35843;&#35856;&#65288;PIT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#30340;&#20840;&#23616;&#29366;&#24577;&#21644;&#30005;&#33655;&#24577;&#12290;&#20854;&#20013;&#65292;PIT&#30340;&#31532;&#19968;&#20010;&#27169;&#22359;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#23548;&#33322;&#65292;&#31532;&#20108;&#20010;&#27169;&#22359;&#20351;&#29992;&#19968;&#31995;&#21015;&#27979;&#37327;&#26469;&#35843;&#33410;&#30005;&#33655;&#29366;&#24577;&#12290;&#36825;&#20123;&#24037;&#20855;&#20855;&#26377;&#30452;&#35266;&#12289;&#21487;&#38752;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38376;&#23450;&#20041;&#30340;&#37327;&#23376;&#28857;&#65288;QDs&#65289;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#39044;&#35745;&#33021;&#22815;&#23454;&#29616;&#35268;&#27169;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25163;&#21160;&#26657;&#20934;&#36825;&#20123;&#31995;&#32479;&#30340;&#36127;&#25285;&#21464;&#24471;&#19981;&#21512;&#29702;&#65292;&#24517;&#39035;&#20351;&#29992;&#33258;&#21160;&#35843;&#35856;&#12290;&#26368;&#36817;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#33258;&#21160;&#35843;&#35856;&#19981;&#21516;QD&#21442;&#25968;&#30340;&#28436;&#31034;&#65292;&#20363;&#22914;&#31895;&#30053;&#30340;&#38376;&#33539;&#22260;&#12289;&#20840;&#23616;&#29366;&#24577;&#25299;&#25169;&#65288;&#20363;&#22914;&#21333;&#20010;QD&#12289;&#21452;&#37325;QD&#65289;&#12289;&#30005;&#33655;&#21644;&#38567;&#36947;&#32806;&#21512;&#65292;&#20197;&#21450;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#32452;&#30452;&#35266;&#12289;&#21487;&#38752;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#20840;&#23616;&#29366;&#24577;&#21644;&#30005;&#33655;&#65292;&#36825;&#20010;&#26694;&#26550;&#34987;&#31216;&#20026;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#35843;&#35856;&#65288;PIT&#65289;&#12290;PIT&#30340;&#31532;&#19968;&#20010;&#27169;&#22359;&#26159;&#19968;&#31181;&#22522;&#20110;&#34892;&#21160;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#29289;&#29702;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23548;&#33322;&#21040;&#30446;&#26631;&#20840;&#23616;&#29366;&#24577;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#20351;&#29992;&#19968;&#31995;&#21015;&#19968;&#32500;&#27979;&#37327;&#26469;&#35843;&#33410;&#21040;&#30446;&#26631;&#30005;&#33655;&#29366;&#24577;&#65292;&#39318;&#20808;&#23558;QDs&#23436;&#20840;&#25918;&#31354;&#65292;&#28982;&#21518;&#26657;&#20934;&#30005;&#23481;&#32806;&#21512;&#24182;&#23548;&#33322;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computers based on gate-defined quantum dots (QDs) are expected to scale. However, as the number of qubits increases, the burden of manually calibrating these systems becomes unreasonable and autonomous tuning must be used. There has been a range of recent demonstrations of automated tuning of various QD parameters such as coarse gate ranges, global state topology (e.g. single QD, double QD), charge, and tunnel coupling with a variety of methods. Here, we demonstrate an intuitive, reliable, and data-efficient set of tools for an automated global state and charge tuning in a framework deemed physics-informed tuning (PIT). The first module of PIT is an action-based algorithm that combines a machine learning classifier with physics knowledge to navigate to a target global state. The second module uses a series of one-dimensional measurements to tune to a target charge state by first emptying the QDs of charge, followed by calibrating capacitive couplings and navigating to the targ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#20294;&#19981;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21464;&#24418;&#31561;&#21464;&#40723;&#21169;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#35757;&#32451;&#22270;&#20687;&#21512;&#25104;&#32593;&#32476;&#21644;&#37197;&#20934;&#32593;&#32476;&#65292;&#24182;&#20801;&#35768;&#22312;&#38750;&#23545;&#40784;&#25968;&#25454;&#26465;&#20214;&#19979;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#36825;&#39033;&#24037;&#20316;&#38477;&#20302;&#20102;&#26032;&#30340;&#20020;&#24202;&#24212;&#29992;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.12491</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#23545;&#40784;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24418;&#31561;&#21464;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deformation equivariant cross-modality image synthesis with paired non-aligned training data. (arXiv:2208.12491v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#20294;&#19981;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21464;&#24418;&#31561;&#21464;&#40723;&#21169;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#35757;&#32451;&#22270;&#20687;&#21512;&#25104;&#32593;&#32476;&#21644;&#37197;&#20934;&#32593;&#32476;&#65292;&#24182;&#20801;&#35768;&#22312;&#38750;&#23545;&#40784;&#25968;&#25454;&#26465;&#20214;&#19979;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#36825;&#39033;&#24037;&#20316;&#38477;&#20302;&#20102;&#26032;&#30340;&#20020;&#24202;&#24212;&#29992;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#22810;&#20010;&#21307;&#23398;&#20020;&#24202;&#30456;&#20851;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#20801;&#35768;&#20351;&#29992;&#37197;&#23545;&#20294;&#19981;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#19981;&#23384;&#22312;&#31283;&#20581;&#19988;&#24615;&#33021;&#33391;&#22909;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#37197;&#23545;&#20294;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21464;&#24418;&#31561;&#21464;&#40723;&#21169;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#30001;&#22270;&#20687;&#21512;&#25104;&#32593;&#32476;&#30340;&#32852;&#21512;&#35757;&#32451;&#20197;&#21450;&#21333;&#29420;&#30340;&#37197;&#20934;&#32593;&#32476;&#32452;&#25104;&#65292;&#24182;&#20801;&#35768;&#22312;&#38750;&#23545;&#40784;&#25968;&#25454;&#26465;&#20214;&#19979;&#23545;&#36755;&#20837;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20801;&#35768;&#26356;&#38590;&#30340;&#25968;&#25454;&#38598;&#36731;&#26494;&#35757;&#32451;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#21512;&#25104;&#32593;&#32476;&#65292;&#38477;&#20302;&#20102;&#26032;&#30340;&#20020;&#24202;&#24212;&#29992;&#30340;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modality image synthesis is an active research topic with multiple medical clinically relevant applications. Recently, methods allowing training with paired but misaligned data have started to emerge. However, no robust and well-performing methods applicable to a wide range of real world data sets exist. In this work, we propose a generic solution to the problem of cross-modality image synthesis with paired but non-aligned data by introducing new deformation equivariance encouraging loss functions. The method consists of joint training of an image synthesis network together with separate registration networks and allows adversarial training conditioned on the input even with misaligned data. The work lowers the bar for new clinical applications by allowing effortless training of cross-modality image synthesis networks for more difficult data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#37327;&#23376;&#28789;&#24863;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27773;&#36710;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26631;&#20934;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#26399;&#26395;&#36816;&#34892;&#26102;&#38388;&#21644;&#36866;&#24212;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2205.04878</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;ResNet&#29992;&#20110;&#27773;&#36710;&#20998;&#31867;&#21450;&#20854;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum ResNet for car classification and its hyperparameter optimization. (arXiv:2205.04878v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#37327;&#23376;&#28789;&#24863;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27773;&#36710;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26631;&#20934;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#26399;&#26395;&#36816;&#34892;&#26102;&#38388;&#21644;&#36866;&#24212;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#35201;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#30001;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#32452;&#25104;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#26102;&#38388;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#35843;&#25972;&#27169;&#22411;&#36229;&#21442;&#25968;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#20110;&#37327;&#23376;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35266;&#23519;&#21040;&#38543;&#30528;&#25628;&#32034;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#21644;&#36866;&#24212;&#24230;&#30340;&#38477;&#20302;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27773;&#36710;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#28151;&#21512;&#37327;&#23376;ResNet&#27169;&#22411;&#30340;&#20840;&#38754;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition is one of the primary applications of machine learning algorithms. Nevertheless, machine learning models used in modern image recognition systems consist of millions of parameters that usually require significant computational time to be adjusted. Moreover, adjustment of model hyperparameters leads to additional overhead. Because of this, new developments in machine learning models and hyperparameter optimization techniques are required. This paper presents a quantum-inspired hyperparameter optimization technique and a hybrid quantum-classical machine learning model for supervised learning. We benchmark our hyperparameter optimization method over standard black-box objective functions and observe performance improvements in the form of reduced expected run times and fitness in response to the growth in the size of the search space. We test our approaches in a car image classification task and demonstrate a full-scale implementation of the hybrid quantum ResNet model w
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#21160;&#20316;&#38382;&#39064;&#20013;&#30340;&#24577;&#23494;&#24230;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#25968;&#20540;&#31215;&#20998;&#24102;&#26469;&#30340;&#35823;&#24046;&#32047;&#31215;&#12290;</title><link>http://arxiv.org/abs/2203.01243</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#24577;&#23494;&#24230;&#26041;&#27861;&#29992;&#20110;&#22797;&#26434;&#21160;&#20316;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Flow-based density of states for complex actions. (arXiv:2203.01243v2 [hep-lat] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01243
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#21160;&#20316;&#38382;&#39064;&#20013;&#30340;&#24577;&#23494;&#24230;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#25968;&#20540;&#31215;&#20998;&#24102;&#26469;&#30340;&#35823;&#24046;&#32047;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#26684;&#28857;&#35745;&#31639;&#20013;&#30340;&#36941;&#21382;&#24615;&#38382;&#39064;&#12290;&#32780;&#19988;&#24050;&#32463;&#27880;&#24847;&#21040;&#27969;&#21487;&#20197;&#29992;&#26469;&#35745;&#31639;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#33719;&#24471;&#30340;&#28909;&#21147;&#23398;&#37327;&#12290;&#36825;&#34920;&#26126;&#23427;&#20204;&#20063;&#36866;&#29992;&#20110;&#22797;&#26434;&#21160;&#20316;&#38382;&#39064;&#20013;&#30340;&#24577;&#23494;&#24230;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#27969;&#30340;&#37319;&#26679;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#23494;&#24230;&#65292;&#19982;&#20256;&#32479;&#30340;&#36890;&#36807;&#27979;&#37327;&#21644;&#31215;&#20998;&#20854;&#23545;&#25968;&#23548;&#25968;&#26469;&#37325;&#24314;&#23494;&#24230;&#30340;&#31574;&#30053;&#30456;&#21453;&#12290;&#36890;&#36807;&#35268;&#36991;&#36825;&#19968;&#36807;&#31243;&#65292;&#21487;&#20197;&#23436;&#20840;&#36991;&#20813;&#25968;&#20540;&#31215;&#20998;&#24341;&#20837;&#30340;&#35823;&#24046;&#32047;&#31215;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#30830;&#23450;&#25972;&#20307;&#24402;&#19968;&#21270;&#22240;&#23376;&#12290;&#22312;&#36825;&#20010;&#21407;&#29702;&#35777;&#26126;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20108;&#20998;&#37327;&#26631;&#37327;&#22330;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;$O(2)$&#23545;&#31216;&#24615;&#34987;&#19968;&#20010;&#34394;&#25968;&#22806;&#22330;&#26174;&#24335;&#30772;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging sampling algorithms based on normalizing flows have the potential to solve ergodicity problems in lattice calculations. Furthermore, it has been noted that flows can be used to compute thermodynamic quantities which are difficult to access with traditional methods. This suggests that they are also applicable to the density-of-states approach to complex action problems. In particular, flow-based sampling may be used to compute the density directly, in contradistinction to the conventional strategy of reconstructing it via measuring and integrating the derivative of its logarithm. By circumventing this procedure, the accumulation of errors from the numerical integration is avoided completely and the overall normalization factor can be determined explicitly. In this proof-of-principle study, we demonstrate our method in the context of two-component scalar field theory where the $O(2)$ symmetry is explicitly broken by an imaginary external field. First, we concentrate on the zero-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#22312;&#37051;&#23621;&#38388;&#36827;&#34892;&#20849;&#35782;&#22411;&#20132;&#20114;&#65292;&#24182;&#32771;&#34385;&#20102;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36830;&#25509;&#30697;&#38453;&#20026;&#31616;&#21333;&#38543;&#26426;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#25512;&#23548;&#20986;&#20102;&#32479;&#19968;&#24378;&#36830;&#25509;&#22270;&#24207;&#21015;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#24179;&#26041;&#22343;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2111.12665</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Bounds for Distributed Linear Stochastic Approximation. (arXiv:2111.12665v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#22312;&#37051;&#23621;&#38388;&#36827;&#34892;&#20849;&#35782;&#22411;&#20132;&#20114;&#65292;&#24182;&#32771;&#34385;&#20102;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36830;&#25509;&#30697;&#38453;&#20026;&#31616;&#21333;&#38543;&#26426;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#25512;&#23548;&#20986;&#20102;&#32479;&#19968;&#24378;&#36830;&#25509;&#22270;&#24207;&#21015;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#24179;&#26041;&#22343;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30001;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#39537;&#21160;&#65292;&#24182;&#20855;&#26377;&#19968;&#33324;&#30340;&#20849;&#35782;&#22411;&#20132;&#20114;&#12290;&#27599;&#20010;&#20195;&#29702;&#26681;&#25454;&#20854;&#26412;&#22320;&#38543;&#26426;&#36924;&#36817;&#36807;&#31243;&#28436;&#21270;&#65292;&#20854;&#20013;&#35813;&#36807;&#31243;&#20381;&#36182;&#20110;&#20854;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#20195;&#29702;&#20043;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#30001;&#19968;&#20010;&#26102;&#21464;&#26377;&#21521;&#22270;&#25551;&#36848;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#36807;&#22522;&#20110;&#20849;&#35782;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#20195;&#29702;&#38388;&#36830;&#25509;&#30001;&#21452;&#38543;&#26426;&#30697;&#38453;&#25551;&#36848;&#26102;&#30340;&#25910;&#25947;&#24615;&#65288;&#33267;&#23569;&#26399;&#26395;&#25910;&#25947;&#65289;&#65292;&#20294;&#23545;&#20110;&#36830;&#25509;&#30697;&#38453;&#20026;&#31616;&#21333;&#38543;&#26426;&#30697;&#38453;&#30340;&#24773;&#20917;&#20102;&#35299;&#36739;&#23569;&#12290;&#23545;&#20110;&#19982;&#38543;&#26426;&#36830;&#25509;&#30697;&#38453;&#30456;&#20851;&#30340;&#32479;&#19968;&#24378;&#36830;&#25509;&#22270;&#24207;&#21015;&#65292;&#26412;&#25991;&#25512;&#23548;&#20102;&#24179;&#26041;&#22343;&#35823;&#24046;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#24179;&#26041;&#22343;&#35823;&#24046;&#23450;&#20041;&#20026;&#31639;&#27861;&#36755;&#20986;&#19982;&#30456;&#20851;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#21807;&#19968;&#24179;&#34913;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction, in which each agent evolves according to its local stochastic approximation process which depends on the information from its neighbors. The interconnection structure among the agents is described by a time-varying directed graph. While the convergence of consensus-based stochastic approximation algorithms when the interconnection among the agents is described by doubly stochastic matrices (at least in expectation) has been studied, less is known about the case when the interconnection matrix is simply stochastic. For any uniformly strongly connected graph sequences whose associated interaction matrices are stochastic, the paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. For the case of inter
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#32593;&#32476;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#31639;&#27861;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270;TD&#22266;&#23450;&#28857;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2101.08862</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#32593;&#32476;&#25171;&#30772;&#33268;&#21629;&#19977;&#35282;&#65288;Reinforcement Learning&#65289;
&lt;/p&gt;
&lt;p&gt;
Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.08862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#32593;&#32476;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#31639;&#27861;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270;TD&#22266;&#23450;&#28857;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#33268;&#21629;&#19977;&#35282;&#8221;&#26159;&#25351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#12289;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#26102;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30446;&#26631;&#32593;&#32476;&#20316;&#20026;&#25171;&#30772;&#8220;&#33268;&#21629;&#19977;&#35282;&#8221;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#30446;&#26631;&#32593;&#32476;&#31283;&#23450;&#35757;&#32451;&#30340;&#24120;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#23558;&#24120;&#29992;&#30340; Polyak &#24179;&#22343;&#39118;&#26684;&#26356;&#26032;&#19982;&#20004;&#20010;&#25237;&#24433;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#31639;&#27861;&#20013;&#24212;&#29992;&#30446;&#26631;&#32593;&#32476;&#21644;&#23725;&#27491;&#21017;&#21270;&#65292;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270; TD &#22266;&#23450;&#28857;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#26159;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#65292;&#28085;&#30422;&#25919;&#31574;&#35780;&#20272;&#21644;&#25511;&#21046;&#65292;&#20197;&#21450;&#25240;&#25187;&#21644;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#30340;&#32447;&#24615;$Q$&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#38480;&#21046;&#24615;&#21644;&#21464;&#21270;&#34892;&#20026;&#31574;&#30053;&#19979;&#22343;&#25104;&#31435;&#65292;&#19981;&#38656;&#35201;&#21452;&#23618;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#31243;&#24207;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;-&#25968;&#25454;&#20998;&#31163;&#21644;&#36923;&#36753;&#26465;&#20214;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#24182;&#22312;&#20351;&#29992;while&#24490;&#29615;&#21644;&#31232;&#26377;&#35266;&#27979;&#30340;&#31243;&#24207;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2101.01502</link><description>&lt;p&gt;
&#25511;&#21046;-&#25968;&#25454;&#20998;&#31163;&#21644;&#36923;&#36753;&#26465;&#20214;&#20256;&#25773;&#29992;&#20110;&#27010;&#29575;&#31243;&#24207;&#30340;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Control-Data Separation and Logical Condition Propagation for Efficient Inference on Probabilistic Programs. (arXiv:2101.01502v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#31243;&#24207;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;-&#25968;&#25454;&#20998;&#31163;&#21644;&#36923;&#36753;&#26465;&#20214;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#24182;&#22312;&#20351;&#29992;while&#24490;&#29615;&#21644;&#31232;&#26377;&#35266;&#27979;&#30340;&#31243;&#24207;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#31243;&#24207;&#37319;&#26679;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20197;&#19968;&#31181;&#38750;&#24120;&#22797;&#26434;&#30340;&#26041;&#24335;&#23558;&#20004;&#20010;&#26368;&#26032;&#30340;&#24605;&#24819;&#8212;&#8212;&#25511;&#21046;-&#25968;&#25454;&#20998;&#31163;&#21644;&#36923;&#36753;&#26465;&#20214;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#20004;&#20010;&#24605;&#24819;&#30456;&#20114;&#22686;&#24378;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;Anglican&#19978;&#23454;&#29616;&#20102;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;while&#24490;&#29615;&#21644;&#31232;&#26377;&#35266;&#27979;&#30340;&#31243;&#24207;&#20013;&#23588;&#20854;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel sampling framework for probabilistic programs. The framework combines two recent ideas -- \emph{control-data separation} and \emph{logical condition propagation} -- in a nontrivial manner so that the two ideas boost the benefits of each other. We implemented our algorithm on top of Anglican. The experimental results demonstrate our algorithm's efficiency, especially for programs with while loops and rare observations.
&lt;/p&gt;</description></item></channel></rss>