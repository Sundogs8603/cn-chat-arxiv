<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#30028;&#26816;&#27979;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25277;&#35937;&#20248;&#21270;&#26694;&#26550;&#21644;&#20855;&#20307;&#31616;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#27861;&#35775;&#38382;&#36234;&#30028;&#25968;&#25454;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00865</link><description>&lt;p&gt;
&#20026;&#35299;&#20915;&#36234;&#30028;&#26816;&#27979;&#32780;&#20248;&#21270;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#30028;&#26816;&#27979;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25277;&#35937;&#20248;&#21270;&#26694;&#26550;&#21644;&#20855;&#20307;&#31616;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#27861;&#35775;&#38382;&#36234;&#30028;&#25968;&#25454;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#22609;&#36896;&#26159;&#19968;&#31867;&#26041;&#27861;&#65292;&#20854;&#22312;&#36234;&#30028;&#65288;OOD&#65289;&#26816;&#27979;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#25805;&#20316;&#29305;&#24449;&#34920;&#31034;&#26469;&#21306;&#20998;&#27491;&#24120;&#65288;ID&#65289;&#26679;&#26412;&#21644;&#36234;&#30028;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#35774;&#35745;&#30340;&#35268;&#21017;&#65292;&#38024;&#23545;&#20855;&#20307;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#36234;&#30028;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#25277;&#35937;&#30340;&#20248;&#21270;&#26694;&#26550;&#26469;&#30740;&#31350;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#31616;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29616;&#26377;&#30340;&#29305;&#24449;&#22609;&#36896;&#26041;&#27861;&#36817;&#20284;&#20110;&#35813;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#36234;&#30028;&#25968;&#25454;&#26159;&#26080;&#27861;&#35775;&#38382;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#20998;&#27573;&#24120;&#25968;&#22609;&#36896;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shapin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32479;&#35745;&#26694;&#26550;&#21644;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;&#23436;&#20840;&#20998;&#31867;&#19982;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#12290;</title><link>https://arxiv.org/abs/2402.00857</link><description>&lt;p&gt;
&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20013;&#30340;&#32047;&#31215;&#20934;&#30830;&#24230;&#38388;&#38548;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Early Time Classification with Accumulated Accuracy Gap Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32479;&#35745;&#26694;&#26550;&#21644;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;&#23436;&#20840;&#20998;&#31867;&#19982;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#26088;&#22312;&#22312;&#19981;&#22788;&#29702;&#23436;&#25972;&#36755;&#20837;&#27969;&#30340;&#24773;&#20917;&#19979;&#23545;&#29305;&#24449;&#27969;&#36827;&#34892;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#24212;&#29992;&#20998;&#31867;&#22120;&#21040;&#25972;&#20010;&#36755;&#20837;&#26102;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#39034;&#24207;&#20998;&#31867;&#22120;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#12290;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21017;&#22312;&#23436;&#20840;&#20998;&#31867;&#21644;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#19978;&#33719;&#24471;&#20102;&#26377;&#38480;&#26679;&#26412;&#30340;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#23398;&#20064;-&#27979;&#35797;&#26657;&#20934;&#26694;&#26550;&#65292;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#23454;&#20363;&#19978;&#23545;&#36825;&#20010;&#38388;&#38548;&#36827;&#34892;&#20102;&#24179;&#22343;&#25511;&#21046;&#12290;&#30001;&#20110;&#36825;&#31181;&#31639;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#26089;&#20572;&#26102;&#38388;&#20934;&#30830;&#24230;&#38388;&#38548;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#65292;&#20854;&#20013;&#20934;&#30830;&#24230;&#38388;&#38548;&#21463;&#21040;&#25511;&#21046;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#36866;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulnes
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>LTAU-FF&#26159;&#19968;&#31181;&#21033;&#29992;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;&#26469;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21644;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#38598;&#21512;&#34920;&#31034;&#21644;&#19981;&#30830;&#23450;&#24230;&#37327;&#65292;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.00853</link><description>&lt;p&gt;
LTAU-FF: &#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00853
&lt;/p&gt;
&lt;p&gt;
LTAU-FF&#26159;&#19968;&#31181;&#21033;&#29992;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;&#26469;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21644;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#38598;&#21512;&#34920;&#31034;&#21644;&#19981;&#30830;&#23450;&#24230;&#37327;&#65292;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38598;&#21512;&#26159;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#21407;&#23376;&#21147;&#22330;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#19981;&#30830;&#23450;&#24230;&#37327;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#38598;&#21512;&#20135;&#29983;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#36880;&#26679;&#26412;&#35823;&#24046;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65288;CDF&#65289;&#26469;&#39640;&#25928;&#34920;&#31034;&#27169;&#22411;&#38598;&#21512;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;&#36317;&#31163;&#30340;&#27169;&#22411;&#28508;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#19981;&#30830;&#23450;&#24230;&#37327;&#25351;&#26631;&#65288;&#31216;&#20026;LTAU&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#21516;&#26102;&#21457;&#25381;&#20102;&#38598;&#21512;&#25216;&#26415;&#30340;&#20248;&#21183;&#12290;&#20316;&#20026;&#21021;&#22987;&#27979;&#35797;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;LTAU-FF&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#34987;&#36731;&#26494;&#22320;&#26657;&#20934;&#20197;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test erro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#35889;&#21487;&#21152;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#29420;&#31435;&#26631;&#31614;&#30340;&#39069;&#22806;&#25968;&#25454;&#28857;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00851</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#27880;&#37322;&#30340;&#25289;&#26364;&#20809;&#35889;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#35889;&#21487;&#21152;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#29420;&#31435;&#26631;&#31614;&#30340;&#39069;&#22806;&#25968;&#25454;&#28857;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#25216;&#26415;&#20013;&#65292;&#25289;&#26364;&#20809;&#35889;&#27861;&#20316;&#20026;&#19968;&#31181;&#36807;&#31243;&#20998;&#26512;&#25216;&#26415;&#65288;PAT&#65289;&#24555;&#36895;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;&#32454;&#32990;&#23494;&#24230;&#12289;&#24213;&#29289;&#21644;&#20135;&#29289;&#27987;&#24230;&#12290;&#30001;&#20110;&#25289;&#26364;&#20809;&#35889;&#35760;&#24405;&#20102;&#20998;&#23376;&#30340;&#25391;&#21160;&#27169;&#24335;&#65292;&#22240;&#27492;&#21487;&#20197;&#38750;&#20405;&#20837;&#24615;&#22320;&#22312;&#19968;&#20010;&#20809;&#35889;&#20013;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#24120;&#65292;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#26159;&#20174;&#20809;&#35889;&#20013;&#25512;&#26029;&#24863;&#20852;&#36259;&#21464;&#37327;&#20449;&#24687;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#36807;&#31243;&#20197;&#20854;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#65292;&#20854;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#32771;&#34385;&#20809;&#26463;&#38169;&#20301;&#12289;&#20687;&#32032;&#25925;&#38556;&#25110;&#20854;&#20182;&#29289;&#36136;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#36807;&#31243;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20809;&#35889;&#30340;&#21487;&#21152;&#24615;&#26469;&#29983;&#25104;&#20174;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#30340;&#20855;&#26377;&#32479;&#35745;&#29420;&#31435;&#26631;&#31614;&#30340;&#39069;&#22806;&#25968;&#25454;&#28857;&#65292;&#20197;&#20415;&#35757;&#32451;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00839</link><description>&lt;p&gt;
X-CBA: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;CatBoosted Anomal-E&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23041;&#32961;&#26085;&#30410;&#22797;&#26434;&#30340;&#26102;&#20195;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#30340;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20026;&#35782;&#21035;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#21644;&#24322;&#24120;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;IDS&#20013;&#20351;&#29992;ML&#21644;DL&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#20219;&#36196;&#23383;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#12290;&#36825;&#31181;IDS&#30740;&#31350;&#20013;&#30340;&#36879;&#26126;&#24230;&#24046;&#36317;&#26174;&#33879;&#65292;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#38382;&#36131;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#31216;&#20026;X-CBA&#65292;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32467;&#26500;&#20248;&#21183;&#26469;&#26377;&#25928;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;GNN&#20026;&#22522;&#30784;&#30340;IDS&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#32593;&#32476;&#27969;&#37327;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#36824;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#26469;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
&lt;/p&gt;</description></item><item><title>ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00835</link><description>&lt;p&gt;
ALISON: &#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
ALISON: Fast and Effective Stylometric Authorship Obfuscation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00835
&lt;/p&gt;
&lt;p&gt;
ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#24230;&#65288;AA&#65289;&#21644;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#65288;AO&#65289;&#26159;&#38544;&#31169;&#30740;&#31350;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#20004;&#39033;&#31454;&#20105;&#20219;&#21153;&#12290;&#29616;&#20195;AA&#21033;&#29992;&#20316;&#32773;&#30340;&#19968;&#36143;&#20889;&#20316;&#39118;&#26684;&#65292;&#20351;&#29992;AA&#20998;&#31867;&#22120;&#23558;&#25991;&#26412;&#19982;&#20854;&#20316;&#32773;&#21305;&#37197;&#12290;AO&#26159;&#30456;&#24212;&#30340;&#23545;&#25239;&#24615;&#20219;&#21153;&#65292;&#26088;&#22312;&#20197;&#19968;&#31181;&#26041;&#24335;&#20462;&#25913;&#25991;&#26412;&#65292;&#20351;&#20854;&#35821;&#20041;&#24471;&#21040;&#20445;&#30041;&#65292;&#20294;AA&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#25512;&#26029;&#20854;&#20316;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#20808;&#36827;&#30340;AA&#26041;&#27861;&#24341;&#21457;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;AO&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#35757;&#32451;&#21644;&#28151;&#28102;&#36895;&#24230;&#36807;&#24930;&#65288;&#36890;&#24120;&#38656;&#35201;&#25968;&#23567;&#26102;&#65289;&#65292;&#20351;&#29992;&#36215;&#26469;&#20173;&#28982;&#19981;&#22826;&#23454;&#38469;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;AO&#26041;&#27861;ALISON&#65292;&#23427;&#65288;1&#65289;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;/&#28151;&#28102;&#26102;&#38388;&#65292;&#28436;&#31034;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;AO&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#30340;&#28151;&#28102;&#36895;&#24230;&#65292;&#65288;2&#65289;&#36890;&#36807;&#25915;&#20987;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#19977;&#31181;&#22522;&#20110;Transformer&#30340;AA&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#65292;&#36890;&#24120;&#27604;&#31454;&#20105;&#26041;&#27861;&#34920;&#29616;&#22909;15%&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;YANG&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39592;&#24178;&#32593;&#32476;&#20013;&#36827;&#34892;&#40657;&#27934;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22635;&#34917;&#20102;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.00831</link><description>&lt;p&gt;
YANG&#36741;&#21161;&#19979;&#30340;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#32479;&#19968;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;YANG&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39592;&#24178;&#32593;&#32476;&#20013;&#36827;&#34892;&#40657;&#27934;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22635;&#34917;&#20102;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20114;&#32852;&#32593;&#39592;&#24178;&#32593;&#32476;&#20013;&#35299;&#20915;&#40657;&#27934;&#25925;&#38556;&#30340;&#37325;&#35201;&#24615;&#19981;&#21487;&#24573;&#35270;&#65292;&#20294;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#20173;&#28982;&#32570;&#20047;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31227;&#21160;&#33258;&#32452;&#32593;(MANETs)&#19978;&#65292;&#32780;MANETs&#22312;&#21160;&#24577;&#12289;&#21327;&#35758;&#21644;&#25299;&#25169;&#19978;&#26377;&#30528;&#23436;&#20840;&#19981;&#21516;&#30340;&#25805;&#20316;&#65292;&#22240;&#27492;&#20854;&#30740;&#31350;&#32467;&#26524;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#40657;&#27934;&#25925;&#38556;&#26816;&#27979;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#38656;&#35201;&#32771;&#34385;&#22810;&#26679;&#30340;&#26465;&#20214;&#65292;&#36825;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#30340;&#32593;&#32476;&#25968;&#25454;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#21464;&#24471;&#24182;&#19981;&#30452;&#35266;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;Yet Another Next Generation (YANG)&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;(BHMM)&#20998;&#26512;&#26469;&#36827;&#34892;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#40657;&#27934;&#26816;&#27979;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#36873;&#25321;&#21644;&#20998;&#26512;&#20102;&#19982;&#40657;&#27934;&#26816;&#27979;&#30456;&#20851;&#30340;&#22235;&#20010;YANG&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;&#29575;&#19981;&#21464;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;RDO&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31354;&#38388;&#22495;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#23376;&#65288;NO&#65289;&#30340;&#24212;&#29992;&#38480;&#21046;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#20307;&#30340;PDE&#12290; (RDO is a novel framework that decouples the spatial domain of input and output, addressing the limitation of neural operators (NO) and enabling the resolution of PDEs with complex geometries.)</title><link>https://arxiv.org/abs/2402.00825</link><description>&lt;p&gt;
&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#20307;&#30340;PDE&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Resolution invariant deep operator network for PDEs with complex geometries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;&#29575;&#19981;&#21464;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;RDO&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31354;&#38388;&#22495;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#23376;&#65288;NO&#65289;&#30340;&#24212;&#29992;&#38480;&#21046;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#20307;&#30340;PDE&#12290; (RDO is a novel framework that decouples the spatial domain of input and output, addressing the limitation of neural operators (NO) and enabling the resolution of PDEs with complex geometries.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#65288;NO&#65289;&#26159;&#20855;&#26377;&#21151;&#33021;&#24615;&#36755;&#20986;&#30340;&#31163;&#25955;&#19981;&#21464;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#24615;&#31639;&#23376;&#12290;NO&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#36229;&#36807;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#36755;&#20837;&#20989;&#25968;&#30340;&#31354;&#38388;&#22495;&#38656;&#35201;&#19982;&#36755;&#20986;&#20989;&#25968;&#30456;&#21516;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26080;&#27861;&#36924;&#36817;&#23558;&#36793;&#30028;&#26465;&#20214;&#26144;&#23556;&#21040;PDE&#35299;&#30340;&#31639;&#23376;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#36776;&#29575;&#19981;&#21464;&#28145;&#24230;&#31639;&#23376;&#65288;RDO&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#23558;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31354;&#38388;&#22495;&#35299;&#32806;&#12290;RDO&#21463;&#21040;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONet&#65289;&#30340;&#21551;&#21457;&#65292;&#19982;DeepONet&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#23545;&#32593;&#32476;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26469;&#36866;&#24212;&#36755;&#20837;/&#36755;&#20986;&#30340;&#21464;&#21270;&#12290;RDO&#25509;&#21463;&#21151;&#33021;&#24615;&#36755;&#20837;&#65292;&#20854;&#36755;&#20986;&#20063;&#26159;&#21151;&#33021;&#24615;&#30340;&#65292;&#22240;&#27492;&#20445;&#25345;NO&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#36136;&#12290;&#23427;&#36824;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#20307;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator. NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods. However, the spatial domain of its input function needs to be identical to its output, which limits its applicability. For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution. To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output. RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet. RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO. It can also resolve PDEs with com
&lt;/p&gt;</description></item><item><title>SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.00823</link><description>&lt;p&gt;
SLIM: &#22810;&#21028;&#21035;&#22120;&#22312;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SLIM: Skill Learning with Multiple Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00823
&lt;/p&gt;
&lt;p&gt;
SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#33719;&#21462;&#21033;&#29992;&#29615;&#22659;&#30340;&#24213;&#23618;&#21160;&#24577;&#30340;&#26377;&#29992;&#34892;&#20026;&#12290;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#21487;&#33021;&#28041;&#21450;&#21040;&#29615;&#22659;&#20013;&#24456;&#22810;&#33258;&#30001;&#24230;&#65292;&#21333;&#32431;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26080;&#27861;&#20135;&#29983;&#26377;&#29992;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLIM&#65292;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#32773;&#26694;&#26550;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#26469;&#20248;&#38597;&#22320;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#21516;&#26102;&#20811;&#26381;&#22870;&#21169;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#24178;&#25200;&#65292;&#38459;&#30861;&#23545;&#26377;&#29992;&#25216;&#33021;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00816</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#34109;&#25216;&#26415;&#26159;&#23454;&#29616;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23631;&#34109;&#26041;&#27861;&#23384;&#22312;&#30456;&#24403;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#37096;&#32626;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#25110;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#23558;&#26356;&#36890;&#29992;&#30340;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#65288;AMBS&#65289;&#26694;&#26550;&#25193;&#23637;&#21040;&#36830;&#32493;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;Safety Gym&#20316;&#20026;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#23558;AMBS&#19982;&#27969;&#34892;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#20026;&#36830;&#32493;&#29615;&#22659;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24809;&#32602;&#25216;&#26415;&#65292;&#30452;&#25509;&#20462;&#25913;&#31574;&#30053;&#26799;&#24230;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#36739;&#22823;&#30340;&#26041;&#24046;&#21487;&#22686;&#21152;&#22122;&#22768;&#25233;&#21046;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00811</link><description>&lt;p&gt;
&#25193;&#25955;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#26041;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of the Variance of Diffusion-based Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#36739;&#22823;&#30340;&#26041;&#24046;&#21487;&#22686;&#21152;&#22122;&#22768;&#25233;&#21046;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#29992;&#20110;&#29983;&#25104;&#35821;&#38899;&#22686;&#24378;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#22312;&#26368;&#36817;&#30340;SGMSE+&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#28041;&#21450;&#21040;&#25511;&#21046;&#28436;&#21270;&#36807;&#31243;&#20013;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#36880;&#28176;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#21040;&#24178;&#20928;&#35821;&#38899;&#20449;&#21495;&#20013;&#12290;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#21462;&#20915;&#20110;&#36873;&#25321;&#29992;&#26469;&#25511;&#21046;&#28155;&#21152;&#29615;&#22659;&#21644;&#39640;&#26031;&#22122;&#22768;&#19979;&#28436;&#21270;&#36807;&#31243;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23427;&#25511;&#21046;&#30528;&#22122;&#22768;&#25233;&#21046;&#21644;&#35821;&#38899;&#22833;&#30495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36739;&#22823;&#30340;&#26041;&#24046;&#22686;&#21152;&#20102;&#22122;&#22768;&#25233;&#21046;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#22240;&#20026;&#20135;&#29983;&#20272;&#35745;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required.
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#36712;&#36857;&#25340;&#25509;&#31639;&#27861;&#21644;&#22870;&#21169;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#39640;&#22238;&#25253;&#36712;&#36857;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#22810;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.00807</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#25340;&#25509;&#23558;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#25552;&#28860;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#36712;&#36857;&#25340;&#25509;&#31639;&#27861;&#21644;&#22870;&#21169;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#39640;&#22238;&#25253;&#36712;&#36857;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#22810;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22823;&#27169;&#22411;&#35268;&#27169;&#22312;&#35745;&#31639;&#19978;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#22238;&#25253;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25340;&#25509;&#31639;&#27861;&#23558;&#23427;&#20204;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#29983;&#25104;&#22120;&#12290;&#23558;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#20960;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20449;&#21495;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20449;&#21495;&#36136;&#37327;&#25351;&#26631;&#21644;&#21435;&#22122;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#25968;&#25454;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.00803</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20449;&#21495;&#36136;&#37327;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Signal Quality Auditing for Time-series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20449;&#21495;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20449;&#21495;&#36136;&#37327;&#25351;&#26631;&#21644;&#21435;&#22122;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#25968;&#25454;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#65288;SQA&#65289;&#23545;&#20110;&#30417;&#27979;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#65288;PMx&#65289;&#24212;&#29992;&#29615;&#22659;&#20013;&#12290;SQA&#23545;&#20110;&#35299;&#20915;&#25968;&#25454;&#37319;&#38598;&#30828;&#20214;&#21644;&#36719;&#20214;&#30340;&#8220;&#38745;&#40664;&#25925;&#38556;&#8221;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#25925;&#38556;&#22914;&#26524;&#19981;&#27880;&#24847;&#65292;&#20250;&#35823;&#23548;&#25968;&#25454;&#29992;&#25143;&#65292;&#22686;&#21152;&#19981;&#27491;&#30830;&#20915;&#31574;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#29978;&#33267;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#23454;&#29616;&#20449;&#21495;&#36136;&#37327;&#25351;&#26631;&#65288;SQIs&#65289;&#29992;&#20110;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#32534;&#20889;&#20102;&#19968;&#31995;&#21015;SQIs&#65292;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#20449;&#21495;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29992;&#20110;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35797;&#22270;&#25913;&#21892;&#24050;&#32463;&#21463;&#25439;&#30340;&#20449;&#21495;&#36136;&#37327;&#65292;&#24182;&#22312;&#30456;&#20851;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#24037;&#20855;&#21253;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#24191;&#27867;&#33539;&#22260;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#24320;&#28304;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing "silent failures" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.00793</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#30340;&#21306;&#20998;&#65306;&#31639;&#27861;&#39044;&#27979;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#26469;&#21306;&#20998;&#37027;&#20123;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#20855;&#26377;&#20449;&#24687;&#30340;&#35775;&#38382;&#26435;&#38480;&#8212;&#8212;&#29305;&#21035;&#26159;&#20027;&#35266;&#20449;&#24687;&#8212;&#8212;&#32780;&#36825;&#20123;&#20449;&#24687;&#26159;&#31639;&#27861;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#32534;&#30721;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#65292;&#20165;&#22312;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#26377;&#25152;&#25913;&#21892;&#26102;&#25165;&#36873;&#25321;&#24615;&#22320;&#32435;&#20837;&#20154;&#31867;&#21453;&#39304;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31639;&#27861;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24448;&#24448;&#20248;&#20110;&#20154;&#31867;&#23545;&#24212;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#31867;&#21028;&#26029;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#21487;&#20197;&#39044;&#20808;&#30830;&#23450;&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;X&#23556;&#32447;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#23376;&#38598;&#22312;&#24739;&#32773;&#32676;&#20307;&#20013;&#21344;&#25454;&#20102;&#36817;30%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
&lt;/p&gt;</description></item><item><title>Graph-Mamba&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#26469;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00789</link><description>&lt;p&gt;
Graph-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#38271;&#31243;&#22270;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00789
&lt;/p&gt;
&lt;p&gt;
Graph-Mamba&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#26469;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22270;&#21464;&#25442;&#22120;&#20013;&#24191;&#27867;&#29992;&#20110;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#30001;&#20110;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22823;&#22411;&#22270;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#26368;&#36817;&#30340;&#35745;&#31639;&#25928;&#29575;&#25913;&#36827;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25110;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#22270;&#23376;&#37319;&#26679;&#36827;&#34892;&#27880;&#24847;&#21147;&#31232;&#30095;&#21270;&#23454;&#29616;&#65292;&#20294;&#22312;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65288;&#22914;Mamba&#65289;&#22240;&#20854;&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23558;SSM&#36866;&#24212;&#38750;&#24207;&#21015;&#22270;&#25968;&#25454;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22270;-Mamba&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#22522;&#20110;&#36755;&#20837;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#65292;&#20197;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#23581;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20197;&#22270;&#20026;&#20013;&#24515;&#30340;&#33410;&#28857;&#20248;&#20808;&#32423;&#21644;&#25490;&#21015;&#31574;&#30053;&#26469;&#22686;&#24378;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#25928;&#26524;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21644;&#26657;&#20934;&#24322;&#36136;&#26377;&#30028;&#29702;&#24615;&#24066;&#22330;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#22312;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#20013;&#25163;&#21160;&#23450;&#20041;&#34892;&#20026;&#35268;&#21017;&#30340;&#38656;&#35201;&#65292;&#24182;&#23558;&#26234;&#33021;&#20307;&#34920;&#31034;&#19982;&#32463;&#27982;&#21644;&#37329;&#34701;&#27169;&#22411;&#30456;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.00787</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21644;&#26657;&#20934;&#24322;&#36136;&#26377;&#30028;&#29702;&#24615;&#24066;&#22330;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21644;&#26657;&#20934;&#24322;&#36136;&#26377;&#30028;&#29702;&#24615;&#24066;&#22330;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#22312;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#20013;&#25163;&#21160;&#23450;&#20041;&#34892;&#20026;&#35268;&#21017;&#30340;&#38656;&#35201;&#65292;&#24182;&#23558;&#26234;&#33021;&#20307;&#34920;&#31034;&#19982;&#32463;&#27982;&#21644;&#37329;&#34701;&#27169;&#22411;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#26080;&#27861;&#19982;&#20256;&#32479;&#22343;&#34913;&#20998;&#26512;&#20860;&#23481;&#30340;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22312;ABM&#20013;&#25163;&#21160;&#23450;&#20041;&#34892;&#20026;&#35268;&#21017;&#12290;&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#38754;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26234;&#33021;&#20307;&#20204;&#21162;&#21147;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#25928;&#29992;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35268;&#21017;&#35268;&#23450;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#20197;&#23398;&#20064;&#20026;&#37325;&#28857;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#29702;&#24615;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#19982;&#24050;&#26377;&#30340;&#32463;&#27982;&#21644;&#37329;&#34701;&#27169;&#22411;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34920;&#36798;&#26041;&#24335;&#36829;&#32972;&#20102;ABM&#30340;&#26681;&#26412;&#21160;&#26426;&#65306;&#21487;&#20197;&#24314;&#27169;&#20174;&#26377;&#30028;&#29702;&#24615;&#21644;&#26234;&#33021;&#20307;&#24322;&#36136;&#24615;&#20013;&#20135;&#29983;&#30340;&#30495;&#23454;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26126;&#26174;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;MARL&#26694;&#26550;&#20013;&#34920;&#31034;&#24322;&#36136;&#22788;&#29702;&#21463;&#38480;&#20195;&#29702;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constraine
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#26435;&#37325;&#65292;&#23558;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#21040;&#23436;&#25104;&#30340;&#25152;&#26377;&#26631;&#35760;&#19978;&#65292;&#20174;&#32780;&#31264;&#23494;&#21270;&#20449;&#21495;&#24182;&#31361;&#20986;&#26174;&#31034;&#26368;&#37325;&#35201;&#30340;&#26631;&#35760;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.00782</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#33719;&#24471;&#30340;&#31264;&#23494;&#22870;&#21169;&#33258;&#30001;
&lt;/p&gt;
&lt;p&gt;
Dense Reward for Free in Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00782
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#26435;&#37325;&#65292;&#23558;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#21040;&#23436;&#25104;&#30340;&#25152;&#26377;&#26631;&#35760;&#19978;&#65292;&#20174;&#32780;&#31264;&#23494;&#21270;&#20449;&#21495;&#24182;&#31361;&#20986;&#26174;&#31034;&#26368;&#37325;&#35201;&#30340;&#26631;&#35760;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#33719;&#24471;&#30340;&#31264;&#23494;&#22870;&#21169;&#33258;&#30001;&#65288;RLHF&#65289;&#34987;&#35748;&#20026;&#26159;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#25351;&#31034;&#24182;&#20135;&#29983;&#26377;&#29992;&#21327;&#21161;&#30340;&#20851;&#38190;&#36827;&#23637;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#22238;&#31572;&#26597;&#35810;&#20043;&#21069;&#20174;LLM&#20013;&#29983;&#25104;&#23436;&#25104;&#65292;&#24182;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20026;&#23436;&#25972;&#30340;&#23436;&#25104;&#25351;&#23450;&#19968;&#20010;&#20998;&#25968;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#36807;&#31243;&#65292;LLM&#24517;&#39035;&#32463;&#21382;&#35768;&#22810;&#8220;&#21160;&#20316;&#8221;&#65288;&#36873;&#25321;&#21333;&#20010;&#26631;&#35760;&#65289;&#65292;&#24182;&#22312;&#19968;&#20010;episode&#32467;&#26463;&#26102;&#21482;&#25910;&#21040;&#19968;&#20010;&#21333;&#29420;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#36825;&#19968;&#35774;&#32622;&#34987;&#35748;&#20026;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#38590;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#21253;&#21547;&#30340;&#19981;&#20165;&#20165;&#26159;&#26631;&#37327;&#36755;&#20986;&#30340;&#26356;&#22810;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#20316;&#20026;transformer&#26550;&#26500;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#35745;&#31639;&#20102;&#19968;&#20010;&#23545;&#26631;&#35760;&#36827;&#34892;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#37325;&#26032;&#20998;&#37197;&#22870;&#21169;&#65292;&#20351;&#20449;&#21495;&#21464;&#24471;&#23494;&#38598;&#24182;&#31361;&#20986;&#26174;&#31034;&#26368;&#37325;&#35201;&#30340;&#26631;&#35760;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many "actions" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all witho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00776</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#35270;&#35273;&#36716;&#25442;&#22120;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#22522;&#20110;&#37327;&#23376;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23578;&#19981;&#33021;&#25191;&#34892;&#39640;&#32500;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#26368;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#37327;&#23376;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21306;&#20998;&#30005;&#23376;&#21644;&#20809;&#23376;&#22312;&#30005;&#30913;&#37327;&#33021;&#22120;&#20013;&#65289;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical anal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36816;&#31639;&#32593;&#32476;&#30340;&#32593;&#26684;&#36816;&#21160;&#27169;&#22411;&#65292;&#22312;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;&#30340;&#21452;&#35843;&#21644;&#32593;&#26684;&#36816;&#21160;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00774</link><description>&lt;p&gt;
&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#28145;&#24230;&#36816;&#31639;&#32593;&#32476;&#20013;&#30340;&#32593;&#26684;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Mesh motion in fluid-structure interaction with deep operator networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36816;&#31639;&#32593;&#32476;&#30340;&#32593;&#26684;&#36816;&#21160;&#27169;&#22411;&#65292;&#22312;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;&#30340;&#21452;&#35843;&#21644;&#32593;&#26684;&#36816;&#21160;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36816;&#31639;&#32593;&#32476;&#30340;&#32593;&#26684;&#36816;&#21160;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#22312;&#21452;&#35843;&#21644;&#32593;&#26684;&#36816;&#21160;&#27169;&#22411;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#26684;&#36816;&#21160;&#27169;&#22411;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#19982;&#21452;&#35843;&#21644;&#32593;&#26684;&#36816;&#21160;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
A mesh motion model based on deep operator networks is presented. The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails. The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems.
&lt;/p&gt;</description></item><item><title>AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00769</link><description>&lt;p&gt;
AnimateLCM: &#20351;&#29992;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#21152;&#36895;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#30340;&#21160;&#30011;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00769
&lt;/p&gt;
&lt;p&gt;
AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36830;&#36143;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#30340;&#21435;&#22122;&#36807;&#31243;&#20351;&#20854;&#35745;&#31639;&#23494;&#38598;&#19988;&#32791;&#26102;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#30340;&#27493;&#39588;&#33976;&#39311;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#20197;&#21450;&#20854;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#25104;&#21151;&#25193;&#23637;&#8212;&#8212;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCM&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnimateLCM&#65292;&#20801;&#35768;&#22312;&#26368;&#23567;&#30340;&#27493;&#39588;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#36825;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#31283;&#23450;&#30340;&#25193;&#25955;&#31038;&#21306;&#20013;&#30340;&#21363;&#25554;&#21363;&#29992;&#36866;&#37197;&#22120;&#30340;&#32452;&#21512;&#20197;&#23454;&#29616;&#21508;&#31181;&#20462;&#25913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#37197;&#22120;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#25216;&#26415;&#22312;&#32447;&#35843;&#25972;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#31283;&#23450;&#24615;&#21644;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00761</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20013;&#22312;&#32447;&#35843;&#25972;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#29702;&#35770;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#25216;&#26415;&#22312;&#32447;&#35843;&#25972;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#31283;&#23450;&#24615;&#21644;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#20027;&#35201;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#21644;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;DNNs&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#24182;&#22312;&#32447;&#37096;&#32626;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26799;&#24230;&#20248;&#21270;&#35757;&#32451;DNNs&#26080;&#27861;&#25552;&#20379;DNN&#30340;&#22266;&#26377;&#24615;&#33021;&#20445;&#35777;&#25110;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#21253;&#25324;&#25511;&#21046;&#22312;&#20869;&#30340;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;&#65292;&#32463;&#21382;&#20102;&#20174;&#35757;&#32451;&#20998;&#24067;&#21040;&#29616;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22495;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31283;&#23450;&#24615;&#21644;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#30340;&#25216;&#26415;&#22312;&#32447;&#26356;&#26032;DNN&#21442;&#25968;&#12290;&#25105;&#20204;&#23558;&#20840;&#36830;&#25509;&#21069;&#21521;DNNs&#24418;&#24335;&#21270;&#20026;&#36830;&#32493;&#26102;&#38388;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;.
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel las
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25104;&#21151;&#29983;&#25104;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;EuroPED&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#39564;&#35777;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00760</link><description>&lt;p&gt;
EuroPED-NN: &#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EuroPED-NN: Uncertainty aware surrogate model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25104;&#21151;&#29983;&#25104;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;EuroPED&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#39564;&#35777;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20808;&#39564;&#65288;BNN-NCP&#65289;&#25216;&#26415;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#23545;EuroPED&#31561;&#31163;&#23376;&#20307;&#24213;&#24231;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;JET-ILW&#24213;&#24231;&#25968;&#25454;&#24211;&#21644;&#21518;&#32493;&#27169;&#22411;&#35780;&#20272;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#31216;&#20026;EuroPED-NN&#12290;BNN-NCP&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#22909;&#36873;&#25321;&#65292;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#20379;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#31361;&#20986;&#26174;&#31034;&#20986;&#20998;&#24067;&#33539;&#22260;&#22806;&#65288;OOD&#65289;&#21306;&#22495;&#12290;&#36825;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;EuroPED-NN&#24050;&#32463;&#24471;&#21040;&#20102;&#29289;&#29702;&#39564;&#35777;&#65292;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#30005;&#23376;&#23494;&#24230;$n_e\!\left(\psi_{\text{pol}}=0.94\right)$&#38543;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;$I_p$&#30340;&#22686;&#21152;&#32780;&#21464;&#21270;&#65292;&#24182;&#39564;&#35777;&#20102;&#19982;EuroPED&#27169;&#22411;&#30456;&#20851;&#30340;$\Delta-\beta_{p,ped}$&#20851;&#31995;&#12290;&#36825;&#35777;&#23454;&#20102;&#20195;&#29702;&#27169;&#22411;&#25152;&#23398;&#21040;&#30340;&#24213;&#23618;&#29289;&#29702;&#23398;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties. This provides critical insights into model robustness and reliability. EuroPED-NN has been physically validated, first, analyzing electron density $n_e\!\left(\psi_{\text{pol}}=0.94\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\Delta-\beta_{p,ped}$ relation associated with the EuroPED model. Affirming the robustness of the underlying physics learned by the surrogate mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00751</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Algorithms for In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#26410;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#31934;&#30830;&#30340;&#21435;&#23398;&#20064;&#8212;&#8212;&#22312;&#27809;&#26377;&#20351;&#29992;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#27169;&#22411;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#22411;&#8212;&#8212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#20302;&#25928;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#36827;&#34892;&#20219;&#21153;&#36866;&#24212;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25928;&#31934;&#30830;&#21435;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21152;&#21040;LLM&#30340;&#25552;&#31034;&#21069;&#38754;&#65288;&#29992;&#20110;&#20219;&#21153;&#36866;&#24212;&#65289;&#65292;&#21517;&#20026;ERASE&#65292;&#23427;&#30340;&#21435;&#23398;&#20064;&#25805;&#20316;&#25104;&#26412;&#19982;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.00743</link><description>&lt;p&gt;
Transformer&#30340;&#22909;&#22788;&#65306;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;\citet{zhang2023trained,huang2023context}&#23545;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#26159;&#20182;&#20204;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#30340;&#36755;&#20837;$x_i$&#21644;&#36755;&#20986;$y_i$&#37117;&#34987;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#20196;&#29260;&#20013;&#65288;&#21363;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#23427;&#20204;&#21576;&#29616;&#20026;&#20004;&#20010;&#20196;&#29260;&#65288;&#21363;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;\cite{wibisono2023role}&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#30456;&#24212;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Transformer&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Transformer&#20013;&#36215;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#29992;&#30340;&#30830;&#20999;&#32452;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24102;&#26377;&#20004;&#23618;softmax&#65288;&#33258;&#25105;&#65289;&#27880;&#24847;&#21147;&#21644;&#21069;&#30651;&#24615;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;Transformer&#21487;&#20197;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22914;&#26524;$y_i$&#22312;&#20196;&#29260;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#26512;&#12290;&#20174;&#35814;&#32454;&#30340;&#20010;&#20307;&#36712;&#36857;&#21040;&#31232;&#30095;&#36712;&#36857;&#21644;&#32858;&#21512;&#36712;&#36857;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;&#31227;&#21160;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#30456;&#20851;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.00732</link><description>&lt;p&gt;
MobilityDL:&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
MobilityDL: A Review of Deep Learning From Trajectory Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#26512;&#12290;&#20174;&#35814;&#32454;&#30340;&#20010;&#20307;&#36712;&#36857;&#21040;&#31232;&#30095;&#36712;&#36857;&#21644;&#32858;&#21512;&#36712;&#36857;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;&#31227;&#21160;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#30456;&#20851;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#32467;&#21512;&#20102;&#26102;&#38388;&#24207;&#21015;&#12289;&#31354;&#38388;&#25968;&#25454;&#21644;&#65288;&#26377;&#26102;&#26159;&#38750;&#29702;&#24615;&#30340;&#65289;&#36816;&#21160;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20026;&#36712;&#36857;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#39318;&#20010;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#20843;&#20010;&#20855;&#20307;&#30340;&#31227;&#21160;&#24615;&#29992;&#20363;&#65292;&#26681;&#25454;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#38500;&#20102;&#23545;2018&#24180;&#20197;&#26469;&#25991;&#29486;&#30340;&#20840;&#38754;&#23450;&#37327;&#23457;&#26597;&#22806;&#65292;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#35813;&#39046;&#22495;&#26368;&#26032;&#24037;&#20316;&#30340;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;&#65292;&#23558;&#20854;&#32622;&#20110;&#31227;&#21160;&#24615;&#25968;&#25454;&#36830;&#32493;&#24615;&#30340;&#33539;&#30068;&#20013;&#65292;&#20174;&#35814;&#32454;&#30340;&#20010;&#20307;&#31227;&#21160;&#32773;&#23494;&#38598;&#36712;&#36857;&#65288;&#20934;&#36830;&#32493;&#36861;&#36394;&#25968;&#25454;&#65289;&#21040;&#31232;&#30095;&#36712;&#36857;&#65288;&#22914;&#31614;&#21040;&#25968;&#25454;&#65289;&#21644;&#32858;&#21512;&#36712;&#36857;&#65288;&#20154;&#32676;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25216;&#26415;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00728</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#22810;&#37325;&#24615;&#35780;&#20272;&#30340;&#22522;&#20110;Dropout&#30340;&#25289;&#32918;&#33945;&#38598;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25216;&#26415;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#37325;&#24615;&#26159;&#25351;&#20998;&#31867;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#65292;&#23427;&#20204;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#24615;&#33021;&#65292;&#20294;&#20026;&#21333;&#20010;&#26679;&#26412;&#29983;&#25104;&#20102;&#30456;&#20114;&#20914;&#31361;&#30340;&#36755;&#20986;&#12290;&#36825;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#24615;&#25490;&#38500;&#12289;&#38590;&#20197;&#35299;&#37322;&#30340;&#27495;&#35270;&#21644;&#19981;&#20844;&#24179;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22312;&#21487;&#33021;&#24222;&#22823;&#30340;&#20551;&#35774;&#31354;&#38388;&#20013;&#25506;&#32034;&#25152;&#26377;&#36825;&#20123;&#20960;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#21363;&#25289;&#32918;&#33945;&#38598;&#65292;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#23558;Dropout&#21442;&#25968;&#19982;&#25289;&#32918;&#33945;&#38598;&#30340;&#23646;&#24615;&#30456;&#36830;&#25509;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20215;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;MRI&#25195;&#25551;&#20013;&#20934;&#30830;&#22320;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00724</link><description>&lt;p&gt;
&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#30340;&#33258;&#21160;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automatic Segmentation of the Spinal Cord Nerve Rootlets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;MRI&#25195;&#25551;&#20013;&#20934;&#30830;&#22320;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35782;&#21035;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#23545;&#20110;&#25551;&#32472;&#33034;&#39635;&#30340;&#21151;&#33021;&#27963;&#21160;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;T2&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#20013;&#35821;&#20041;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#24320;&#25918;&#23384;&#21462;&#30340;MRI&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#65292;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#19977;&#32500;&#22810;&#31867;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20998;&#21106;C2-C8&#32972;&#38754;&#31070;&#32463;&#26681;&#20998;&#25903;&#12290;&#27599;&#20010;&#36755;&#20986;&#31867;&#21035;&#23545;&#24212;&#19968;&#20010;&#33034;&#39635;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#22330;&#22320;&#12289;&#19981;&#21516;&#20250;&#35805;&#21644;&#19981;&#21516;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#30340;Dice&#20998;&#25968;&#20026;0.67+-0.16&#65288;&#22343;&#20540;+-&#26631;&#20934;&#24046;&#65292;&#38024;&#23545;&#19981;&#21516;&#31070;&#32463;&#26681;&#20998;&#25903;&#27700;&#24179;&#65289;&#65292;&#34920;&#26126;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#20302;&#21378;&#21830;&#38388;&#21644;&#22330;&#22320;&#38388;&#30340;&#21464;&#24322;&#24615;&#65288;&#21464;&#24322;&#31995;&#25968;&lt;=1.41%&#65289;&#65292;&#20197;&#21450;&#20302;&#20250;&#35805;&#38388;&#21464;&#24322;&#24615;&#65288;&#21464;&#24322;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation &lt;= 1.41 %), as well as low inter-session variability (coefficient of variation &lt;= 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#21644;TD3&#32593;&#32476;&#30340;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#31181;&#39118;&#26684;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#32593;&#32476;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.00722</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#24310;&#36831;DDPG&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#20849;&#20139;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#21644;TD3&#32593;&#32476;&#30340;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#31181;&#39118;&#26684;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#32593;&#32476;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#65288;NST&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#20351;&#20803;&#32032;&#65288;&#36890;&#24120;&#20026;&#22270;&#20687;&#65289;&#37319;&#29992;&#21478;&#19968;&#20010;&#20803;&#32032;&#30340;&#22806;&#35266;&#25110;&#39118;&#26684;&#30340;&#31639;&#27861;&#12290;&#27599;&#20010;&#20803;&#32032;&#30001;&#20869;&#23481;&#21644;&#39118;&#26684;&#32452;&#25104;&#65306;&#20869;&#23481;&#21487;&#20197;&#27010;&#24565;&#19978;&#23450;&#20041;&#20026;&#20803;&#32032;&#30340;&#8220;&#26159;&#20160;&#20040;&#8221;&#65292;&#32780;&#39118;&#26684;&#21017;&#26159;&#20803;&#32032;&#30340;&#8220;&#22914;&#20309;&#8221;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;NST&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#39118;&#26684;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#65292;&#20363;&#22914;&#65292;&#30456;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#21487;&#20197;&#37319;&#29992;&#24868;&#24594;&#12289;&#24555;&#20048;&#12289;&#24179;&#38745;&#25110;&#24754;&#20260;&#30340;&#26041;&#24335;&#25191;&#34892;&#12290;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#25552;&#21462;&#21644;&#23450;&#20041;&#30446;&#26631;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#32593;&#32476;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23450;&#20041;&#30340;&#25439;&#22833;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#31227;TD3&#65288;NPST3&#65289;&#36890;&#36807;&#24341;&#20837;&#35757;&#32451;&#39118;&#26684;&#26469;&#25913;&#21464;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20197;&#22312;&#32447;&#25110;&#31163;&#32447;&#30340;&#26041;&#24335;&#23454;&#29616;&#33258;&#20027;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.00707</link><description>&lt;p&gt;
&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#19982;&#26368;&#36817;&#37051;
&lt;/p&gt;
&lt;p&gt;
Non-Exchangeable Conformal Language Generation with Nearest Neighbors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#35753;&#20154;&#20204;&#26816;&#26597;&#28508;&#22312;&#30340;&#38169;&#35273;&#21644;&#20351;&#31995;&#32479;&#26356;&#21487;&#38752;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20849;&#24418;&#39044;&#27979;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#65292;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20219;&#20309;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#20851;&#20110;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#39044;&#27979;&#30340;&#32467;&#26524;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#30830;&#20445;&#35206;&#30422;&#33539;&#22260;&#12290;&#32467;&#26524;--&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#26680;&#37319;&#26679;&#65292;&#26159;&#23545;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#29983;&#25104;&#30340;&#20849;&#24418;&#39044;&#27979;&#26694;&#26550;&#30340;&#19968;&#31181;&#26032;&#39062;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#27169;&#22411;&#30340;&#20107;&#21518;&#22788;&#29702;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26631;&#35760;&#32423;&#21035;&#12289;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#29983;&#25104;&#36136;&#37327;&#32467;&#26524;&#12290;&#36890;&#36807;&#21516;&#26102;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#26356;&#32039;&#23494;&#30340;&#39044;&#27979;&#38598;&#65292;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#22522;&#20110;&#33655;&#20848;&#30340;&#35843;&#26597;&#25968;&#25454;&#21644;&#30331;&#35760;&#25968;&#25454;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21644;&#26679;&#26412;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#32946;&#32467;&#26524;&#30340;&#20855;&#20307;&#20869;&#23481;&#12290;&#20182;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00705</link><description>&lt;p&gt;
&#32467;&#21512;&#33655;&#20848;&#35843;&#26597;&#21644;&#30331;&#35760;&#25968;&#25454;&#30340;&#25968;&#25454;&#25361;&#25112;&#65292;&#39044;&#27979;&#29983;&#32946;&#29575;&#65288;PreFer&#65289;
&lt;/p&gt;
&lt;p&gt;
Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#22522;&#20110;&#33655;&#20848;&#30340;&#35843;&#26597;&#25968;&#25454;&#21644;&#30331;&#35760;&#25968;&#25454;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21644;&#26679;&#26412;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#32946;&#32467;&#26524;&#30340;&#20855;&#20307;&#20869;&#23481;&#12290;&#20182;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#24050;&#32463;&#31215;&#32047;&#20102;&#22823;&#37327;&#26377;&#20851;&#29983;&#32946;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#21363;&#20154;&#20204;&#26159;&#21542;&#20197;&#21450;&#20309;&#26102;&#29983;&#32946;&#23376;&#22899;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20915;&#23450;&#22240;&#32032;&#21644;&#22522;&#26412;&#29702;&#35770;&#30340;&#39044;&#27979;&#33021;&#21147;&#24456;&#23569;&#22312;&#26032;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#26080;&#27861;&#31995;&#32479;&#22320;&#27604;&#36739;&#30740;&#31350;&#65292;&#38459;&#30861;&#20102;&#30693;&#35782;&#30340;&#35780;&#20272;&#21644;&#31215;&#32047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#19968;&#20010;&#25968;&#25454;&#38598;&#22522;&#20110;LISS&#38754;&#26495;&#65292;&#36825;&#26159;&#19968;&#20010;&#32437;&#21521;&#35843;&#26597;&#65292;&#21253;&#25324;&#20102;&#25968;&#21315;&#20010;&#20851;&#20110;&#21508;&#31181;&#20027;&#39064;&#30340;&#21464;&#37327;&#65292;&#21253;&#25324;&#20010;&#20307;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#12290;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#22522;&#20110;&#33655;&#20848;&#30331;&#35760;&#25968;&#25454;&#65292;&#32570;&#20047;&#24577;&#24230;&#25968;&#25454;&#65292;&#20294;&#21253;&#25324;&#20102;&#25968;&#30334;&#19975;&#33655;&#20848;&#23621;&#27665;&#29983;&#27963;&#36712;&#36857;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#24182;&#25551;&#36848;&#24863;&#20852;&#36259;&#30340;&#29983;&#32946;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35760;&#24405;&#21644;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#21644;&#24212;&#29992;&#24773;&#20917;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.00699</link><description>&lt;p&gt;
PeaTMOSS: &#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35760;&#24405;&#21644;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#21644;&#24212;&#29992;&#24773;&#20917;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#35757;&#32451;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#21644;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#27491;&#22312;&#37319;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;(PTMs)&#26469;&#36827;&#34892;&#21518;&#32493;&#24212;&#29992;&#12290;PTM&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#20165;&#35760;&#24405;&#20803;&#25968;&#25454;&#65292;&#36824;&#35760;&#24405;&#36825;&#20123;&#27169;&#22411;&#30340;&#21518;&#32493;&#24212;&#29992;&#12290;&#27809;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;MSR&#31038;&#21306;&#26080;&#27861;&#20840;&#38754;&#29702;&#35299;PTM&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;281,638&#20010;PTM&#30340;&#20803;&#25968;&#25454;&#21644;&#36229;&#36807;50&#20010;&#26376;&#19979;&#36733;&#37327;&#30340;&#25152;&#26377;PTM&#30340;&#35814;&#32454;&#24555;&#29031;(14,296&#20010;PTMs)&#65292;&#20197;&#21450;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;28,575&#20010;&#26469;&#33258;GitHub&#30340;&#24320;&#28304;&#36719;&#20214;&#20195;&#30721;&#24211;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;15,129&#20010;GitHub&#20195;&#30721;&#24211;&#21040;&#23427;&#20204;&#20351;&#29992;&#30340;2,530&#20010;PTMs&#30340;44,337&#20010;&#26144;&#23556;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#25552;&#31034;&#65292;&#20197;&#33258;&#21160;&#22320;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#21453;&#28436;&#30340;&#24418;&#24577;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#28436;&#20248;&#21270;&#24418;&#24577;&#23884;&#20837;&#24674;&#22797;&#36924;&#30495;&#30340;&#20154;&#33080;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#20197;&#24448;&#25216;&#26415;&#30456;&#31454;&#20105;&#24182;&#36229;&#36234;&#20854;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00695</link><description>&lt;p&gt;
&#36817;&#20284;&#20248;&#21270;&#27169;&#26495;&#21453;&#28436;&#30340;&#24418;&#24577;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Approximating Optimal Morphing Attacks using Template Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#21453;&#28436;&#30340;&#24418;&#24577;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#28436;&#20248;&#21270;&#24418;&#24577;&#23884;&#20837;&#24674;&#22797;&#36924;&#30495;&#30340;&#20154;&#33080;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#20197;&#24448;&#25216;&#26415;&#30456;&#31454;&#20105;&#24182;&#36229;&#36234;&#20854;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35770;&#25991;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#28436;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#26469;&#24674;&#22797;&#36924;&#30495;&#30340;&#20154;&#33080;&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#26495;&#21453;&#28436;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#28436;&#29702;&#35770;&#19978;&#30340;&#20248;&#21270;&#24418;&#24577;&#23884;&#20837;&#30340;&#26032;&#22411;&#28145;&#24230;&#24418;&#24577;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#28304;&#22270;&#20687;&#30340;&#20154;&#33080;&#23884;&#20837;&#27714;&#24179;&#22343;&#24471;&#21040;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20004;&#31181;&#21464;&#31181;&#65306;&#31532;&#19968;&#31181;&#21033;&#29992;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#23884;&#20837;&#36716;&#22270;&#20687;&#21453;&#28436;&#27169;&#22411;&#65292;&#32780;&#31532;&#20108;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;StyleGAN&#32593;&#32476;&#30340;&#21512;&#25104;&#32593;&#32476;&#22686;&#21152;&#24418;&#24577;&#30340;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#20174;&#22810;&#20010;&#28304;&#25968;&#25454;&#38598;&#29983;&#25104;&#24418;&#24577;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#23545;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#20013;&#22312;&#26377;&#25928;&#24615;&#26041;&#38754;&#19982;&#20197;&#24448;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24577;&#29983;&#25104;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#31454;&#20105;&#24182;&#32463;&#24120;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally 
&lt;/p&gt;</description></item><item><title>&#20197;&#24448;&#30340;&#26426;&#22120;&#20154;&#32534;&#31243;&#26041;&#27861;&#23545;&#38750;&#19987;&#23478;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21160;&#20316;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#36866;&#24212;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.00678</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#21487;&#36861;&#36394;&#24615;
&lt;/p&gt;
&lt;p&gt;
Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00678
&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#26426;&#22120;&#20154;&#32534;&#31243;&#26041;&#27861;&#23545;&#38750;&#19987;&#23478;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21160;&#20316;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#36866;&#24212;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20351;&#31995;&#32479;&#36866;&#24212;&#19982;&#38750;&#19987;&#23478;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#26426;&#22120;&#20154;&#27169;&#20223;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31034;&#33539;&#30452;&#25509;&#32534;&#31243;&#26469;&#31616;&#21270;&#21644;&#20943;&#23569;&#26426;&#22120;&#20154;&#32534;&#31243;&#30340;&#26102;&#38388;&#12290;&#22312;&#20256;&#32479;&#30340;&#26694;&#26550;&#20013;&#65292;&#21160;&#20316;&#34987;&#24314;&#27169;&#20026;&#20851;&#33410;&#25110;&#31515;&#21345;&#23572;&#31354;&#38388;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#32431;&#20960;&#20309;&#26041;&#27861;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#20854;&#20182;&#29305;&#24449;&#65292;&#27604;&#22914;&#35270;&#35273;&#29305;&#24449;&#12290;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;(CGDA)&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#23558;&#21160;&#20316;&#32534;&#30721;&#20026;&#21487;&#20197;&#20174;&#29615;&#22659;&#20013;&#25552;&#21462;&#30340;&#20219;&#20309;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#26080;&#20851;&#29305;&#24449;&#30340;&#32534;&#30721;&#65292;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#20851;&#33410;&#36712;&#36857;&#24517;&#39035;&#23436;&#20840;&#35745;&#31639;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;(EA)&#36827;&#34892;&#35745;&#31639;&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#36807;&#22810;&#30340;&#35780;&#20272;&#25165;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36825;&#20010;&#36827;&#21270;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#21518;&#23558;&#35780;&#20272;&#32467;&#26524;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#31639;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25511;&#21046;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#25442;&#12290;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#32593;&#32476;&#26469;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#21516;&#26102;&#32534;&#30721;&#20102;&#34892;&#20026;&#30340;&#30446;&#26631;&#21644;&#39118;&#26684;&#65292;&#20174;&#32780;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#32780;&#20445;&#25345;&#20854;&#20869;&#23481;&#19981;&#21464;&#12290;&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#29992;&#25143;&#28436;&#31034;&#23454;&#29616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39118;&#26684;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.00677</link><description>&lt;p&gt;
&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Neural Policy Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#31639;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25511;&#21046;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#25442;&#12290;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#32593;&#32476;&#26469;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#21516;&#26102;&#32534;&#30721;&#20102;&#34892;&#20026;&#30340;&#30446;&#26631;&#21644;&#39118;&#26684;&#65292;&#20174;&#32780;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#32780;&#20445;&#25345;&#20854;&#20869;&#23481;&#19981;&#21464;&#12290;&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#29992;&#25143;&#28436;&#31034;&#23454;&#29616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39118;&#26684;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#36716;&#25442;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#25552;&#20986;&#65306;&#32654;&#26415;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22266;&#23450;&#36712;&#36857;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#27599;&#20010;&#32593;&#32476;&#37117;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#22870;&#21169;&#65292;&#36890;&#24120;&#32534;&#30721;&#20102;&#19968;&#31181;&#34892;&#20026;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#25551;&#36848;&#20026;&#20869;&#23481;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#20351;&#24471;&#21487;&#20197;&#32534;&#30721;&#31532;&#20108;&#20010;&#20219;&#21153;&#65292;&#21487;&#20197;&#25551;&#36848;&#20026;&#39118;&#26684;&#12290;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#65288;NPST&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#25345;&#21518;&#32773;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#28145;&#24230; Q-Network &#26550;&#26500;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#36827;&#34892;&#20102;&#20004;&#32452;&#19981;&#21516;&#30340;&#29992;&#25143;&#28436;&#31034;&#65292;&#19968;&#32452;&#20026;&#20869;&#23481;&#65292;&#21478;&#19968;&#32452;&#20026;&#39118;&#26684;&#12290;&#19981;&#21516;&#30340;&#39118;&#26684;&#36890;&#36807;&#29992;&#25143;&#28436;&#31034;&#36827;&#34892;&#32534;&#30721;&#12290;&#29983;&#25104;&#30340;&#31574;&#30053;&#26159;&#36890;&#36807;&#23558;&#20869;&#23481;&#31574;&#30053;&#36755;&#36865;&#21040;&#19968;&#20010;&#29983;&#25104;&#22120;&#20013;&#26469;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.00676</link><description>&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#20154;&#32032;&#25551;&#65306;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#22312;&#20154;&#31867;&#32032;&#25551;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#26368;&#26032;&#30340;&#35748;&#30693;&#31185;&#23398;&#29702;&#35770;&#26041;&#27861;&#12290;&#33402;&#26415;&#29615;&#22659;&#34987;&#35748;&#30693;&#31185;&#23398;&#30028;&#35270;&#20026;&#20016;&#23500;&#12289;&#33258;&#28982;&#12289;&#22810;&#24863;&#23448;&#12289;&#22810;&#25991;&#21270;&#30340;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#12290;&#28145;&#24230;Q&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;DQN&#65289;&#26159;&#22312;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#26368;&#25104;&#21151;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;DQN&#26041;&#27861;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#29983;&#25104;&#22797;&#26434;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33402;&#26415;&#32472;&#30011;&#26426;&#22120;&#20154;&#24212;&#29992;&#20351;&#29992;&#31616;&#21333;&#30340;&#25511;&#21046;&#27861;&#21017;&#65292;&#38480;&#21046;&#20102;&#26694;&#26550;&#30340;&#36866;&#24212;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#32472;&#30011;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;DQN&#12290;&#30446;&#26631;&#26159;&#30740;&#31350;&#22914;&#20309;&#24341;&#20837;&#22797;&#26434;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control pol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#36135;&#36816;&#26041;&#24335;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#27604;&#36739;&#20102;&#20843;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#22120;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#26159;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26368;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.00659</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24314;&#27169;&#36135;&#36816;&#26041;&#24335;&#36873;&#25321;&#65306;&#22522;&#20110;&#21830;&#21697;&#27969;&#35843;&#26597;&#25968;&#25454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#36135;&#36816;&#26041;&#24335;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#27604;&#36739;&#20102;&#20843;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#22120;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#26159;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24314;&#27169;&#36135;&#36816;&#26041;&#24335;&#36873;&#25321;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20843;&#31181;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;K&#36817;&#37051;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;Boosting&#21644;Bagging&#65292;&#20197;&#21450;&#32463;&#20856;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;2012&#24180;&#21830;&#21697;&#27969;&#35843;&#26597;&#25968;&#25454;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#24182;&#20351;&#29992;&#20108;&#32423;&#25968;&#25454;&#26469;&#28304;&#30340;&#31354;&#38388;&#23646;&#24615;&#36827;&#34892;&#34917;&#20805;&#12290;&#26681;&#25454;&#39044;&#27979;&#20934;&#30830;&#24615;&#32467;&#26524;&#27604;&#36739;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#30740;&#31350;&#36824;&#30740;&#31350;&#20102;&#26679;&#26412;&#22823;&#23567;&#21644;&#35757;&#32451;&#27979;&#35797;&#25968;&#25454;&#20998;&#31163;&#27604;&#29575;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#20102;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#23450;&#36825;&#20123;&#21464;&#37327;&#22914;&#20309;&#24433;&#21709;&#36135;&#36816;&#26041;&#24335;&#36873;&#25321;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#34920;&#29616;&#26368;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38543;&#26426;&#26862;&#26519;&#20135;&#29983;&#20102;&#26368;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the usefulness of machine learning classifiers for modeling freight mode choice. We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model. US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources. The performance of the classifiers is compared based on prediction accuracy results. The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches. In addition, the importance of variables is estimated to determine how the variables influence freight mode choice. The results show that the tree-based ensemble classifiers perform the best. Specifically, Random Forest produces the most accura
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#12289;&#25552;&#21462;&#22320;&#29702;&#29305;&#24449;&#21644;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00654</link><description>&lt;p&gt;
&#25552;&#39640;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65306;&#22522;&#20110;2017&#24180;CFS PUF&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#12289;&#25552;&#21462;&#22320;&#29702;&#29305;&#24449;&#21644;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#25910;&#38598;&#20102;&#20004;&#36718;&#23454;&#39564;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#20840;&#22269;&#21830;&#21697;&#27969;&#21160;&#30340;&#36816;&#36755;&#29305;&#24449;&#65292;&#20998;&#21035;&#22312;2012&#24180;&#65288;&#21363;&#20844;&#20849;&#20351;&#29992;&#24494;&#25968;&#25454;&#65289;&#21644;2017&#24180;&#65288;&#21363;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#65289;&#21457;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#20449;&#24687;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#29702;&#35299;&#36135;&#36816;&#29289;&#27969;&#30340;&#35814;&#32454;&#27169;&#24335;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#19977;&#20010;&#20027;&#35201;&#25913;&#36827;&#65306;&#65288;1&#65289;&#20026;&#27599;&#20010;&#29420;&#31435;&#30340;&#21830;&#21697;/&#34892;&#19994;&#31867;&#21035;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#65307;&#65288;2&#65289;&#25552;&#21462;&#26377;&#29992;&#30340;&#22320;&#29702;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#27599;&#31181;&#36135;&#36816;&#27169;&#24335;&#22312;&#36215;&#28857;/&#32456;&#28857;&#21306;&#22495;&#20043;&#38388;&#30340;&#34893;&#29983;&#36317;&#31163;&#65307;&#65288;3&#65289;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22534;&#21472;&#25110;&#25237;&#31080;&#65292;&#23558;&#26412;&#22320;&#21644;&#32479;&#19968;&#27169;&#22411;&#30340;&#32467;&#26524;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24178;&#21069;&#39304;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30005;&#36335;&#28145;&#24230;&#21644;&#37327;&#23376;&#27604;&#29305;&#38656;&#27714;&#26041;&#38754;&#26356;&#20026;&#39640;&#25928;&#65292;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.00653</link><description>&lt;p&gt;
&#30456;&#24178;&#21069;&#39304;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Coherent Feed Forward Quantum Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24178;&#21069;&#39304;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30005;&#36335;&#28145;&#24230;&#21644;&#37327;&#23376;&#27604;&#29305;&#38656;&#27714;&#26041;&#38754;&#26356;&#20026;&#39640;&#25928;&#65292;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#26410;&#30693;&#39046;&#22495;&#12290;&#30446;&#21069;&#30340;QNN&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#21464;&#20998;&#30005;&#36335;&#25110;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#65292;&#22312;ansatz&#25110;&#32773;&#37327;&#23376;&#29305;&#24449;&#22270;&#19978;&#36827;&#34892;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#32416;&#32544;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22686;&#21152;&#20102;&#30005;&#36335;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36229;&#20986;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#65292;&#32780;&#19988;&#30001;&#20110;&#19982;&#20856;&#22411;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(FFNN)&#32467;&#26500;&#30340;&#20559;&#31163;&#65292;&#35823;&#23548;&#24615;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#26631;&#35760;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#37327;&#23376;&#27604;&#29305;&#38656;&#27714;&#22312;&#25968;&#25454;&#29305;&#24449;&#25968;&#37327;&#22686;&#21152;&#26102;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#20174;&#32780;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36896;&#25104;&#20102;&#25928;&#29575;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22320;&#36947;&#30340;QNN&#27169;&#22411;&#65292;&#23427;&#22312;&#21487;&#35843;&#25972;&#30340;&#20013;&#38388;&#23618;&#21644;&#33410;&#28857;&#19978;&#19982;&#20256;&#32479;&#30340;FFNN&#30456;&#21327;&#35843;&#65292;&#22312;&#27809;&#26377;&#20013;&#38388;&#27979;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25972;&#20010;&#27169;&#22411;&#26159;&#30456;&#24178;&#30340;&#12290;&#35813;&#27169;&#22411;&#20197;&#20854;&#20943;&#23569;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#37327;&#23376;&#27604;&#29305;&#38656;&#27714;&#33073;&#39062;&#32780;&#20986;&#65292;&#20351;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circ
&lt;/p&gt;</description></item><item><title>&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23398;&#20064;&#20805;&#20998;&#24179;&#28369;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#24863;&#30693;&#33539;&#24335;&#20013;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.00645</link><description>&lt;p&gt;
&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Spectrally Transformed Kernel Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00645
&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23398;&#20064;&#20805;&#20998;&#24179;&#28369;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#24863;&#30693;&#33539;&#24335;&#20013;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#31614;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#22522;&#30784;&#26680;&#65288;&#22914;&#949;-&#37051;&#23621;&#26680;&#25110;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#65289;&#20013;&#32534;&#30721;&#30340;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#23454;&#29616;&#19968;&#31181;&#24179;&#28369;&#24615;&#24418;&#24335;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#65288;STKR&#65289;&#30340;&#32463;&#20856;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31867;&#26032;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;STKR&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#36890;&#36807;&#20809;&#35889;&#21464;&#25442;&#65292;STKR&#21033;&#29992;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#25552;&#20379;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STKR&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34920;&#24449;&#19968;&#31181;"&#30446;&#26631;&#24179;&#28369;&#24615;"&#30340;&#36890;&#29992;&#31867;&#22411;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#20805;&#20998;&#24179;&#28369;&#30340;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;STKR&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;STKR&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#24863;&#30693;&#33539;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#30340;&#21464;&#25442;&#20989;&#25968;&#65292;&#32780;&#20808;&#21069;&#30340;&#24037;&#20316;&#22823;&#37096;&#20998;&#38480;&#20110;&#25512;&#23548;&#33539;&#24335;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#32479;&#35745;&#23646;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information. First, we show that STKR is a principled and general approach, by characterizing a universal type of "target smoothness", and proving that any sufficiently smooth function can be learned by STKR. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive stati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19982;&#20013;&#39118;&#24739;&#32773;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#20837;&#38498;&#21518;3&#20010;&#26376;&#30340;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00638</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#20013;&#39118;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Random Forest-Based Prediction of Stroke Outcome
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19982;&#20013;&#39118;&#24739;&#32773;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#20837;&#38498;&#21518;3&#20010;&#26376;&#30340;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#20013;&#39118;&#24739;&#32773;&#32467;&#26524;&#30456;&#20851;&#30340;&#20020;&#24202;&#12289;&#29983;&#21270;&#21644;&#31070;&#32463;&#25104;&#20687;&#22240;&#32032;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#38498;&#21518;3&#20010;&#26376;&#30340;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#27431;&#27954;&#19977;&#32423;&#21307;&#38498;&#20013;&#39118;&#21333;&#20803;&#20837;&#38498;&#30340;&#32570;&#34880;&#24615;&#20013;&#39118;&#65288;IS&#65289;&#21644;&#38750;&#21019;&#20260;&#24615;&#39045;&#20869;&#20986;&#34880;&#65288;ICH&#65289;&#24739;&#32773;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#26426;&#22120;&#23398;&#20064;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#30340;&#20027;&#35201;&#21464;&#37327;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#24739;&#32773;&#27515;&#20129;&#29575;/&#21457;&#30149;&#29575;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#24635;&#20043;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38543;&#26426;&#26862;&#26519;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#20013;&#39118;&#24739;&#32773;&#30340;&#38271;&#26399;&#32467;&#26524;&#39044;&#27979;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission. The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered. We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity. In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.00626</link><description>&lt;p&gt;
Vision-LLMs&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#21487;&#20197;&#33258;&#27450;&#27450;&#20154;
&lt;/p&gt;
&lt;p&gt;
Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#26032;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;LVLM&#23545;&#20110;&#28041;&#21450;&#23558;&#35823;&#23548;&#24615;&#25991;&#26412;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#30340;&#20174;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#21364;&#27809;&#26377;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25490;&#29256;&#25915;&#20987;&#20381;&#36182;&#20110;&#20174;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#21512;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#35823;&#23548;&#24615;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#21487;&#33021;&#19981;&#26159;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#35774;&#35745;&#30340;&#26032;&#39062;&#22522;&#20934;&#26469;&#27979;&#35797;LVLM&#23545;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#32780;&#26356;&#26377;&#25928;&#30340;&#25490;&#29256;&#25915;&#20987;&#65306;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#25552;&#31034;GPT-4V&#31561;&#27169;&#22411;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#33021;&#21147;&#25512;&#33616;&#19968;&#31181;&#25490;&#29256;&#25915;&#20987;&#26469;&#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#25915;&#20987;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
&lt;/p&gt;</description></item><item><title>&#20197;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#21644;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#65292;&#36827;&#34892;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21516;&#26102;&#20197;&#23616;&#37096;&#21464;&#37327;&#20026;&#20989;&#25968;&#20272;&#35745;&#24178;&#39044;&#20998;&#24067;&#24182;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.00623</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bayesian Causal Inference with Gaussian Process Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00623
&lt;/p&gt;
&lt;p&gt;
&#20197;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#21644;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#65292;&#36827;&#34892;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21516;&#26102;&#20197;&#23616;&#37096;&#21464;&#37327;&#20026;&#20989;&#25968;&#20272;&#35745;&#24178;&#39044;&#20998;&#24067;&#24182;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#21644;&#25512;&#26029;&#26159;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23427;&#26082;&#28041;&#21450;&#24314;&#27169;&#38382;&#39064;&#65292;&#20063;&#28041;&#21450;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#20250;&#23545;&#32852;&#21512;&#20998;&#24067;&#20570;&#20986;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#22914;&#32447;&#24615;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#65288;GPN&#65289;&#27169;&#22411;&#20013;&#65292;&#23545;&#20551;&#35774;&#24178;&#39044;&#25928;&#26524;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#21442;&#25968;&#22320;&#25551;&#36848;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#65292;&#24182;&#22312;&#19979;&#28216;&#21464;&#37327;&#19978;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#26469;&#36827;&#34892;GPN&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35745;&#31639;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23558;&#24178;&#39044;&#20998;&#24067;&#20272;&#35745;&#20026;&#23616;&#37096;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#26694;&#26550;&#25193;&#23637;&#21040;&#20102;&#19981;&#20165;&#20165;&#26159;&#24050;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#22240;&#26524;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. These are typically addressed by imposing strict assumptions on the joint distribution such as linearity. We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically. We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes. We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36719;&#36718;&#24275;&#31995;&#25968;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#32858;&#31867;&#20013;&#20248;&#21270;&#35813;&#31995;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#24418;&#25104;&#32039;&#20945;&#19988;&#20114;&#30456;&#20998;&#31163;&#30340;&#31751;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#35813;&#26041;&#27861;&#30340;&#33258;&#32534;&#30721;&#22120;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.00608</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#36718;&#24275;&#20998;&#25968;&#30340;&#28145;&#24230;&#32858;&#31867;&#65306;&#26397;&#21521;&#32039;&#20945;&#19988;&#20114;&#30456;&#20998;&#31163;&#30340;&#31751;
&lt;/p&gt;
&lt;p&gt;
Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36719;&#36718;&#24275;&#31995;&#25968;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#32858;&#31867;&#20013;&#20248;&#21270;&#35813;&#31995;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#24418;&#25104;&#32039;&#20945;&#19988;&#20114;&#30456;&#20998;&#31163;&#30340;&#31751;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#35813;&#26041;&#27861;&#30340;&#33258;&#32534;&#30721;&#22120;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#32780;&#26085;&#30410;&#37325;&#35201;&#12290;&#28145;&#24230;&#32858;&#31867;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#65292;&#26088;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#33021;&#21147;&#26469;&#25552;&#21319;&#32858;&#31867;&#24615;&#33021;&#12290;&#22823;&#37096;&#20998;&#28145;&#24230;&#32858;&#31867;&#30340;&#25991;&#29486;&#37117;&#33268;&#21147;&#20110;&#22312;&#26576;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#20869;&#37096;&#32858;&#31867;&#21464;&#24322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19982;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36719;&#36718;&#24275;&#65292;&#21363;&#36718;&#24275;&#31995;&#25968;&#30340;&#27010;&#29575;&#24418;&#24335;&#12290;&#36719;&#36718;&#24275;&#19982;&#20256;&#32479;&#30340;&#36718;&#24275;&#31995;&#25968;&#19968;&#26679;&#65292;&#22870;&#21169;&#32039;&#20945;&#19988;&#26126;&#26174;&#20998;&#31163;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#20013;&#20248;&#21270;&#36719;&#36718;&#24275;&#21487;&#20197;&#24341;&#23548;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24418;&#25104;&#32039;&#20945;&#19988;&#20114;&#30456;&#20998;&#31163;&#30340;&#31751;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#36825;&#31181;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InfoBoost&#65292;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#24178;&#25200;&#28304;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00607</link><description>&lt;p&gt;
&#20154;&#24037;&#21512;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30495;&#30340;&#19981;&#22914;&#30495;&#23454;&#25968;&#25454;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Synthetic Time-series Data Really not as Good as Real Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InfoBoost&#65292;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#24178;&#25200;&#28304;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12289;&#20559;&#35265;&#21644;&#33030;&#24369;&#24615;&#20197;&#21450;&#27867;&#21270;&#38382;&#39064;&#12290;&#25972;&#21512;&#36890;&#29992;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#21253;&#21547;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InfoBoost -- &#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#65292;&#36229;&#36234;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#22810;&#20010;&#28304;&#30340;&#33410;&#22863;&#20449;&#21495;&#12289;&#22122;&#22768;&#24178;&#25200;&#21644;&#36229;&#36807;&#37319;&#26679;&#31383;&#21475;&#33021;&#21147;&#30340;&#38271;&#21608;&#26399;&#29305;&#24449;&#30340;&#24178;&#25200;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#28145;&#24230;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#34920;&#31034;&#21644;&#32463;&#39564;&#22238;&#25918;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00580</link><description>&lt;p&gt;
&#20351;&#29992;&#31283;&#23450;&#34920;&#31034;&#21644;&#32463;&#39564;&#22238;&#25918;&#30340;&#36830;&#32493;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#34920;&#31034;&#21644;&#32463;&#39564;&#22238;&#25918;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#21482;&#33021;&#35775;&#38382;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26356;&#26032;&#22522;&#30784;&#27169;&#22411;&#26469;&#20445;&#25345;&#27169;&#22411;&#22312;&#22495;&#36716;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;UDA&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#21516;&#26102;&#35775;&#38382;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#25152;&#26377;&#20855;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#31283;&#23450;&#23398;&#20064;&#30340;&#20869;&#37096;&#20998;&#24067;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20869;&#37096;&#20998;&#24067;&#26159;&#36890;&#36807;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#21709;&#24212;&#26469;&#24314;&#27169;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#26469;&#24314;&#27169;&#36825;&#20010;&#20869;&#37096;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#39046;&#22495;&#30340;&#20869;&#37096;&#23398;&#20064;&#20998;&#24067;&#19982;&#20272;&#35745;&#30340;GMM&#36827;&#34892;&#21305;&#37197;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#39564;&#22238;&#25918;&#26469;&#20811;&#26381;&#29992;&#25143;&#20307;&#39564;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an algorithm for tackling the problem of unsupervised domain adaptation (UDA) in continual learning (CL) scenarios. The primary objective is to maintain model generalization under domain shift when new domains arrive continually through updating a base model when only unlabeled data is accessible in subsequent tasks. While there are many existing UDA algorithms, they typically require access to both the source and target domain datasets simultaneously. Conversely, existing CL approaches can handle tasks that all have labeled data. Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains. The internal distribution is modeled by network responses in hidden layer. We model this internal distribution using a Gaussian mixture model (GMM ) and update the model by matching the internally learned distribution of new domains to the estimated GMM. Additionally, we leverage experience replay to overcome the problem of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#28909;&#24102;&#24615;&#36136;&#33021;&#22815;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00576</link><description>&lt;p&gt;
&#28909;&#24102;&#20915;&#31574;&#36793;&#30028;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#23545;&#25239;&#25915;&#20987;&#26159;&#31283;&#20581;&#30340;
&lt;/p&gt;
&lt;p&gt;
Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#28909;&#24102;&#24615;&#36136;&#33021;&#22815;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28909;&#24102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#28909;&#24102;&#25237;&#24433;&#25176;&#27931;&#26031;&#20013;&#30340;&#19968;&#20010;&#38544;&#34255;&#23618;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#27573;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#28909;&#24102;&#24615;&#36136;&#65292;&#35813;&#23618;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20854;&#20915;&#31574;&#36793;&#30028;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#23433;&#20840;&#30340;&#26234;&#33021;&#23478;&#23621;&#35748;&#35777;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35748;&#35777;&#21327;&#35758;&#26080;&#27861;&#23454;&#29616;&#23433;&#20840;&#30456;&#20114;&#35748;&#35777;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#35774;&#22791;&#35748;&#35777;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20250;&#35805;&#23494;&#38053;&#27844;&#38706;&#12289;&#20882;&#20805;&#21644;&#30423;&#31363;&#26234;&#33021;&#35774;&#22791;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00568</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26234;&#33021;&#23478;&#23621;&#35748;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Secure Supervised Learning-Based Smart Home Authentication Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#23433;&#20840;&#30340;&#26234;&#33021;&#23478;&#23621;&#35748;&#35777;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35748;&#35777;&#21327;&#35758;&#26080;&#27861;&#23454;&#29616;&#23433;&#20840;&#30456;&#20114;&#35748;&#35777;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#35774;&#22791;&#35748;&#35777;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20250;&#35805;&#23494;&#38053;&#27844;&#38706;&#12289;&#20882;&#20805;&#21644;&#30423;&#31363;&#26234;&#33021;&#35774;&#22791;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#20960;&#21313;&#24180;&#26469;&#29289;&#32852;&#32593;&#21644;&#20449;&#24687;&#36890;&#20449;&#25216;&#26415;&#30340;&#31995;&#32479;&#36827;&#27493;&#65292;&#26234;&#33021;&#23478;&#23621;&#20855;&#22791;&#20026;&#29992;&#25143;&#25552;&#20379;&#23478;&#24237;&#26381;&#21153;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#35774;&#22791;&#25552;&#20379;&#30340;&#23478;&#24237;&#26381;&#21153;&#24110;&#21161;&#29992;&#25143;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#30446;&#26631;&#19978;&#36798;&#21040;&#26368;&#22823;&#21270;&#30340;&#33298;&#36866;&#27700;&#24179;&#12290;&#30001;&#20110;&#29992;&#25143;&#21644;&#26234;&#33021;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#19981;&#23433;&#20840;&#30340;&#36890;&#36947;&#36827;&#34892;&#36890;&#20449;&#65292;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#23481;&#26131;&#20986;&#29616;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#38656;&#35201;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#30340;&#35748;&#35777;&#21327;&#35758;&#65292;&#20351;&#24471;&#26234;&#33021;&#35774;&#22791;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#35774;&#22791;&#35748;&#35777;&#25104;&#20026;&#21487;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26234;&#33021;&#23478;&#23621;&#35748;&#35777;&#21327;&#35758;&#34987;&#21457;&#29616;&#26080;&#27861;&#23454;&#29616;&#23433;&#20840;&#30340;&#30456;&#20114;&#35748;&#35777;&#65292;&#22686;&#21152;&#20102;&#20250;&#35805;&#23494;&#38053;&#27844;&#38706;&#12289;&#20882;&#20805;&#21644;&#30423;&#31363;&#26234;&#33021;&#35774;&#22791;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#23433;&#20840;&#30340;&#26234;&#33021;&#23478;&#23621;&#35748;&#35777;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades. The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality. As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems. A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments. Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device. In this paper, Secure Supervised Learning-based Smart H
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#36817;&#20284;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#36825;&#20010;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#20856;&#22522;&#20989;&#25968;&#23637;&#24320;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#65292;&#36890;&#36807;&#37327;&#23376;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#26465;&#20214;&#26059;&#36716;&#21644;&#21704;&#36798;&#29595;&#21644;Swap&#27979;&#35797;&#26469;&#20272;&#35745;&#29305;&#24449;&#20540;&#21644;&#35780;&#20272;&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.00544</link><description>&lt;p&gt;
&#37327;&#23376;&#36741;&#21161;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Quantum-Assisted Hilbert-Space Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#36817;&#20284;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#36825;&#20010;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#20856;&#22522;&#20989;&#25968;&#23637;&#24320;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#65292;&#36890;&#36807;&#37327;&#23376;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#26465;&#20214;&#26059;&#36716;&#21644;&#21704;&#36798;&#29595;&#21644;Swap&#27979;&#35797;&#26469;&#20272;&#35745;&#29305;&#24449;&#20540;&#21644;&#35780;&#20272;&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20316;&#20989;&#25968;&#20808;&#39564;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#21487;&#20197;&#25429;&#25417;&#22122;&#22768;&#32479;&#35745;&#12289;&#20989;&#25968;&#24179;&#28369;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36805;&#36895;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#36817;&#20284;&#30340;&#37327;&#23376;&#31639;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#20856;&#22522;&#20989;&#25968;&#23637;&#24320;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#65292;&#21253;&#25324;&#37327;&#23376;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#26465;&#20214;&#26059;&#36716;&#21644;&#21704;&#36798;&#29595;&#21644;Swap&#27979;&#35797;&#12290;&#37327;&#23376;&#20027;&#25104;&#20998;&#20998;&#26512;&#29992;&#20110;&#20272;&#35745;&#29305;&#24449;&#20540;&#65292;&#26465;&#20214;&#26059;&#36716;&#21644;&#21704;&#36798;&#29595;&#21644;Swap&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are probabilistic models that are commonly used as functional priors in machine learning. Due to their probabilistic nature, they can be used to capture the prior information on the statistics of noise, smoothness of the functions, and training data uncertainty. However, their computational complexity quickly becomes intractable as the size of the data set grows. We propose a Hilbert space approximation-based quantum algorithm for Gaussian process regression to overcome this limitation. Our method consists of a combination of classical basis function expansion with quantum computing techniques of quantum principal component analysis, conditional rotations, and Hadamard and Swap tests. The quantum principal component analysis is used to estimate the eigenvalues while the conditional rotations and the Hadamard and Swap tests are employed to evaluate the posterior mean and variance of the Gaussian process. Our method provides polynomial computational complexity reductio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#23558;&#20851;&#38190;&#20449;&#24687;&#19982;&#26597;&#35810;&#21644;&#25968;&#20540;&#20998;&#31163;&#24182;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00534</link><description>&lt;p&gt;
&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27969;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Manifold Representation of the Key in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#23558;&#20851;&#38190;&#20449;&#24687;&#19982;&#26597;&#35810;&#21644;&#25968;&#20540;&#20998;&#31163;&#24182;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#36716;&#25442;&#22120;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27880;&#24847;&#21147;&#22359;&#23454;&#29616;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MSA&#65289;&#12290;&#22312;&#36825;&#20123;&#22359;&#20013;&#65292;&#26597;&#35810;&#12289;&#20851;&#38190;&#20449;&#24687;&#21644;&#25968;&#20540;&#36890;&#24120;&#32416;&#32544;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#21333;&#20010;&#20849;&#20139;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#20851;&#38190;&#20449;&#24687;&#20174;&#26597;&#35810;&#21644;&#25968;&#20540;&#20013;&#35299;&#32806;&#65292;&#24182;&#20026;&#20851;&#38190;&#20449;&#24687;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35299;&#32806;&#24182;&#36171;&#20104;&#20851;&#38190;&#20449;&#24687;&#27969;&#24418;&#32467;&#26500;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ViT-B&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#30340;top-1&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;0.87&#65285;&#65292;&#32780;Swin-T&#21017;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;0.52&#65285;&#65292;&#20351;&#29992;&#20102;&#20843;&#20010;&#27969;&#24418;&#20851;&#38190;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#22686;&#21152;&#20102;&#26356;&#22810;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#31616;&#21333;&#24615;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#25506;&#32034;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#25968;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#26469;&#35786;&#26029;&#21644;&#32531;&#35299;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#30149;&#24577;&#12290;&#20351;&#29992;&#39044;&#22788;&#29702;&#26469;&#25913;&#21892;&#26465;&#20214;&#25968;&#12290;&#22312;18&#20010;PDE&#38382;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;7&#20010;&#38382;&#39064;&#20013;&#23558;&#35823;&#24046;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2402.00531</link><description>&lt;p&gt;
&#39044;&#22788;&#29702;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Preconditioning for Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00531
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25968;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#26469;&#35786;&#26029;&#21644;&#32531;&#35299;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#30149;&#24577;&#12290;&#20351;&#29992;&#39044;&#22788;&#29702;&#26469;&#25913;&#21892;&#26465;&#20214;&#25968;&#12290;&#22312;18&#20010;PDE&#38382;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;7&#20010;&#38382;&#39064;&#20013;&#23558;&#35823;&#24046;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#35299;&#20915;&#21508;&#31181;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#30149;&#24577;&#24433;&#21709;&#20102;PINNs&#30340;&#25910;&#25947;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26465;&#20214;&#25968;&#20316;&#20026;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#26469;&#35786;&#26029;&#21644;&#32531;&#35299;PINNs&#20013;&#30340;&#35757;&#32451;&#30149;&#24577;&#12290;&#21463;&#32463;&#20856;&#25968;&#20540;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#26465;&#20214;&#25968;&#27979;&#37327;&#25935;&#24863;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#24378;&#35843;&#20854;&#22312;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23450;&#29702;&#65292;&#25581;&#31034;&#20102;&#26465;&#20214;&#25968;&#19982;PINNs&#30340;&#35823;&#24046;&#25511;&#21046;&#21644;&#25910;&#25947;&#24615;&#30340;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#22788;&#29702;&#26469;&#25913;&#21892;&#26465;&#20214;&#25968;&#30340;&#31639;&#27861;&#12290;&#23545;18&#20010;PDE&#38382;&#39064;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20854;&#20013;&#30340;7&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38169;&#35823;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;&#26465;&#20214;&#25968;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.00518</link><description>&lt;p&gt;
EE-Tuning:&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EE-Tuning&#65292;&#19968;&#31181;&#36731;&#37327;&#19988;&#32463;&#27982;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35757;&#32451;/&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#23436;&#25972;&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#24120;&#35265;&#26041;&#27861;&#19981;&#21516;&#65292;EE-Tuning&#36890;&#36807;&#22312;&#21442;&#25968;&#39640;&#25928;&#26041;&#24335;&#19979;&#22686;&#21152;&#39069;&#22806;&#30340;&#26089;&#26399;&#32456;&#27490;&#23618;&#65292;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#65288;&#21487;&#33021;&#26159;&#24494;&#35843;&#65289;&#30340;&#26631;&#20934;LLM&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#24615;&#33021;&#20248;&#21270;&#21644;&#23436;&#20840;&#20860;&#23481;3D&#24182;&#34892;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#23454;&#29616;&#20102;EE-Tuning&#30340;&#21331;&#36234;&#35757;&#32451;&#25928;&#29575;&#12290;&#31995;&#32479;&#23454;&#39564;&#35777;&#23454;&#20102;EE-Tuning&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;&#20026;&#20102;&#23558;&#26089;&#26399;&#32456;&#27490;LLMs&#25512;&#24191;&#21040;&#31038;&#21306;&#65292;&#25105;&#20204;&#22312;https://github.com/pan-x-c/EE-LLM&#19978;&#21457;&#24067;&#20102;EE-Tuning&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00515</link><description>&lt;p&gt;
&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20316;&#21453;&#24212;&#24615;&#26234;&#33021;&#20307;&#20197;&#22312;&#39640;&#24230;&#21160;&#33633;&#30340;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#24555;&#36895;&#23398;&#20064;&#24182;&#21709;&#24212;&#26032;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#34892;&#19994;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#22797;&#26434;&#30340;&#20851;&#32852;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#36235;&#21183;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#26368;&#22823;&#21270;&#26032;&#21046;&#23450;&#30340;&#25237;&#36164;&#32452;&#21512;&#30340;&#24635;&#22238;&#25253;&#65292;&#32780;&#24573;&#35270;&#20854;&#22312;&#20840;&#29699;&#25110;&#21306;&#22495;&#37096;&#38376;&#30340;&#21508;&#31181;&#24066;&#22330;&#26465;&#20214;&#21160;&#33633;&#19979;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASA&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21644;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#37319;&#29992;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20180;&#32454;&#21160;&#24577;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
&lt;/p&gt;</description></item><item><title>&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.00501</link><description>&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00501
&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;f&#20013;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#32473;&#20986;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#65288;ERM-$f$DR&#65289;&#30340;&#35299;&#27861;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#26368;&#20248;&#27979;&#24230;&#34987;&#35777;&#26126;&#26159;&#21807;&#19968;&#30340;&#12290;&#24182;&#32473;&#20986;&#20102;&#29305;&#23450;&#36873;&#25321;&#20989;&#25968;f&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#21033;&#29992;f-&#20998;&#24067;&#26063;&#30340;&#28789;&#27963;&#24615;&#65292;&#33719;&#24471;&#20102;&#20808;&#21069;&#23545;&#24120;&#35265;&#27491;&#21017;&#21270;&#36873;&#25321;&#30340;&#24050;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#21807;&#19968;&#35299;&#65288;Type-I&#21644;Type-II&#65289;&#12290;&#23545;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;ERM-$f$DR&#38382;&#39064;&#20013;&#20351;&#29992;f-&#20998;&#24067;&#26102;&#30340;&#20197;&#19979;&#23646;&#24615;&#65306;$i)$ f-&#20998;&#24067;&#27491;&#21017;&#21270;&#24378;&#21046;&#23558;&#35299;&#30340;&#25903;&#25345;&#19982;&#21442;&#32771;&#27979;&#24230;&#30340;&#25903;&#25345;&#37325;&#21512;&#65292;&#24341;&#20837;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#30340;&#35777;&#25454;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65307;$ii)$ &#20219;&#20309;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#37117;&#31561;&#20215;&#20110;&#21478;&#19968;&#31181;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique. Examples of the solution for particular choices of the function $f$ are presented. Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II). The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization 
&lt;/p&gt;</description></item><item><title>CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00450</link><description>&lt;p&gt;
CPT: &#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#33021; &#21147;&#36882;&#36827;&#24335;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
CPT: Competence-progressive Training Strategy for Few-shot Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00450
&lt;/p&gt;
&lt;p&gt;
CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#25104;&#21151;&#20173;&#28982;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#33410;&#28857;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#26631;&#31614;&#31232;&#30095;&#65292;&#24378;&#35843;&#20102;GNN&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#24773;&#33410;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#38480;&#21046;&#65306;&#38543;&#26426;&#21644;&#22343;&#21248;&#20219;&#21153;&#20998;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#20803;&#23398;&#20064;&#22120;&#36807;&#26089;&#22320;&#38754;&#20020;&#22797;&#26434;&#20219;&#21153;&#65292;&#38459;&#30861;&#20102;&#27491;&#24120;&#30340;&#23398;&#20064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20803;&#23398;&#20064;&#22120;&#24212;&#35813;&#20174;&#31616;&#21333;&#27010;&#24565;&#24320;&#22987;&#65292;&#36880;&#28176;&#36827;&#20837;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#23601;&#20687;&#20154;&#31867;&#23398;&#20064;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#38590;&#24230;&#19982;&#20803;&#23398;&#20064;&#22120;&#30340;&#36882;&#36827;&#33021;&#21147;&#30456;&#21305;&#37197;&#65292;&#22686;&#24378;&#20102;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00435</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A practical existence theorem for reduced order models based on convolutional autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#38477;&#38454;&#24314;&#27169;&#39046;&#22495;&#36234;&#21457;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#31070;&#32463;&#31639;&#23376;&#12289;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#31561;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#26102;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38477;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;CNN&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#26550;&#26500;&#65292;&#36890;&#24120;&#20197;&#19975;&#33021;&#36924;&#36817;&#23450;&#29702;&#30340;&#24418;&#24335;&#38472;&#36848;&#12290;&#23588;&#20854;&#26159;&#65292;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20026;&#35774;&#35745;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#65292;&#20294;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#21518;&#32493;&#25361;&#25112;&#20960;&#20046;&#27809;&#26377;&#34987;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00433</link><description>&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25152;&#26377;&#20219;&#21153;&#12290;&#20197;&#20219;&#21153;&#31639;&#26415;&#20026;&#20363;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26082;&#26377;&#25928;&#21448;&#21487;&#25193;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23547;&#25214;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#38745;&#24577;&#26368;&#20248;&#35299;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#26159;&#20943;&#36731;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#21066;&#24369;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22810;&#25968;&#21442;&#25968;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;Transformer&#23618;&#30340;MLP&#25193;&#23637;&#20026;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#22320;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36866;&#24212;&#27599;&#20010;&#23454;&#20363;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#31163;&#20849;&#20139;&#30693;&#35782;&#21644;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#28982;&#21518;&#21160;&#24577;&#22320;&#25972;&#21512;&#23427;&#20204;&#65292;
&lt;/p&gt;
&lt;p&gt;
Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00421</link><description>&lt;p&gt;
&#20174;PARIS&#21040;LE-PARIS&#65306;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#21644;&#21327;&#20316;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#19987;&#21033;&#21709;&#24212;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#21033;&#23457;&#26597;&#20013;&#65292;&#23545;&#20110;&#21450;&#26102;&#21644;&#26377;&#25928;&#22320;&#22238;&#24212;&#23457;&#26597;&#24847;&#35265;&#65288;OAs&#65289;&#23545;&#20110;&#33719;&#24471;&#19987;&#21033;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#36825;&#19968;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#23457;&#26597;&#24847;&#35265;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;&#65288;PARIS&#65289;&#21450;&#20854;&#20808;&#36827;&#29256;&#26412;LE-PARIS&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22312;&#21327;&#20316;&#22788;&#29702;OA&#22238;&#24212;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#21253;&#25324;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#65292;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#39564;&#35777;&#28041;&#21450;&#20351;&#29992;USPTO Office Action&#25968;&#25454;&#24211;&#21644;&#24459;&#24072;&#19982;&#25105;&#20204;&#31995;&#32479;&#30340;&#38271;&#26399;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#30340;&#22810;&#33539;&#24335;&#20998;&#26512;&#65292;&#20026;&#26399;&#20845;&#24180;&#12290;&#36890;&#36807;&#20116;&#20010;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25552;&#20986;&#30340;Delphi&#36807;&#31243;&#26469;&#26816;&#39564;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#65288;&#30740;&#31350;1&#21644;2&#65289;&#65292;&#36824;&#26377;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#26469;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#65288;&#30740;&#31350;3&#21644;4&#65289;&#65292;&#20197;&#21450;&#32463;&#36807;&#35757;&#32451;&#30340;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65288;&#30740;&#31350;5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.00418</link><description>&lt;p&gt;
&#30701;&#25991;: &#22522;&#20934;&#27979;&#35797;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Short: Benchmarking transferable adversarial attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#12290;&#23427;&#31995;&#32479;&#22320;&#20998;&#31867;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#29983;&#25104;&#32467;&#26500;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#26799;&#24230;&#32534;&#36753;&#12289;&#30446;&#26631;&#20462;&#25913;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;"TAA-Bench"&#65292;&#38598;&#25104;&#20102;&#21313;&#31181;&#20027;&#35201;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#21644;&#31995;&#32479;&#21270;&#30340;&#27604;&#36739;&#20998;&#26512;&#24179;&#21488;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#25928;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#21644;&#23454;&#38469;&#25928;&#29992;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#27604;&#20998;&#26512;&#30340;&#26631;&#20934;&#21270;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.00397</link><description>&lt;p&gt;
&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00397
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#23545;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#24110;&#21161;&#39640;&#25928;&#20998;&#37197;&#36164;&#28304;&#21644;&#26377;&#25928;&#25511;&#21046;&#20132;&#36890;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#35768;&#22810;&#22478;&#24066;&#30001;&#20110;&#35774;&#22791;&#25903;&#25345;&#26377;&#38480;&#32780;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35266;&#23519;&#65306;&#20132;&#36890;&#27169;&#24335;&#22312;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#65288;MTPB&#65289;&#12290;&#20027;&#35201;&#19978;&#65292;MTPB&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#21551;&#21160;&#20854;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#31867;&#25216;&#26415;&#20174;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20013;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20934;&#30830;&#30340;&#20132;&#36890;&#27169;&#24335;&#26816;&#32034;&#26426;&#21046;&#36827;&#34892;&#36328;&#22478;&#24066;&#30340;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#26426;&#26800;&#25163;&#33218;&#27515;&#21306;&#20869;&#30340;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#21487;&#29992;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00393</link><description>&lt;p&gt;
&#32771;&#34385;&#27515;&#21306;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Loss Function Considering Dead Zone for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#26426;&#26800;&#25163;&#33218;&#27515;&#21306;&#20869;&#30340;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#21487;&#29992;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#26426;&#26800;&#25163;&#33218;&#30340;&#36870;&#21160;&#21147;&#23398;&#26159;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#25511;&#21046;&#30340;&#25511;&#21046;&#24615;&#33021;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#34920;&#31034;&#22797;&#26434;&#36870;&#21160;&#21147;&#23398;&#30340;&#26377;&#26395;&#25216;&#26415;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#36816;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#25191;&#34892;&#26426;&#26500;&#30340;&#27515;&#21306;&#20013;&#30340;&#36816;&#21160;&#25968;&#25454;&#19981;&#36866;&#21512;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#38477;&#20302;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26377;&#29992;&#25968;&#25454;&#37327;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#26800;&#25163;&#33218;&#20851;&#33410;&#22312;&#27515;&#21306;&#20869;&#19981;&#24037;&#20316;&#30340;&#20107;&#23454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21482;&#32771;&#34385;&#19981;&#22312;&#27515;&#21306;&#30340;&#20851;&#33410;&#30340;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#24182;&#25552;&#39640;&#20102;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#33258;&#30001;&#24230;&#26426;&#26800;&#25163;&#33218;&#30340;&#23454;&#38469;&#35774;&#22791;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#24182;&#35752;&#35770;&#20102;&#22312;&#27515;&#21306;&#20013;&#20351;&#29992;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00388</link><description>&lt;p&gt;
&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Cumulative Distribution Function based General Temporal Point Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#22312;&#24314;&#27169;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20107;&#20214;&#24207;&#21015;&#65288;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#65289;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#25512;&#33616;&#31995;&#32479;&#21644;&#20449;&#24687;&#26816;&#32034;&#31574;&#30053;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#20132;&#20114;&#21644;&#20132;&#26131;&#31561;&#20107;&#20214;&#65292;&#26102;&#38388;&#28857;&#36807;&#31243;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#34892;&#20026;&#27169;&#24335;&#27934;&#23519;&#65292;&#26377;&#21161;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#26102;&#38388;&#28857;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24314;&#27169;&#24378;&#24230;&#20989;&#25968;&#12289;&#22797;&#26434;&#31215;&#20998;&#35745;&#31639;&#21644;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuFun&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#30340;&#19978;&#19979;&#25991;&#32858;&#31867;GAN&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20302;&#21058;&#37327;PET&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28857;&#30340;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#32858;&#31867;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#32467;&#26500;&#30340;&#26126;&#30830;&#34920;&#36798;&#65292;&#24182;&#20943;&#36731;&#20102;&#37325;&#24314;&#22270;&#20687;&#20013;&#23567;&#32467;&#26500;&#30340;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00376</link><description>&lt;p&gt;
Image2Points&#65306;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#30340;&#19978;&#19979;&#25991;&#32858;&#31867;GAN&#29992;&#20110;&#39640;&#36136;&#37327;PET&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00376
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#30340;&#19978;&#19979;&#25991;&#32858;&#31867;GAN&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20302;&#21058;&#37327;PET&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28857;&#30340;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#32858;&#31867;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#32467;&#26500;&#30340;&#26126;&#30830;&#34920;&#36798;&#65292;&#24182;&#20943;&#36731;&#20102;&#37325;&#24314;&#22270;&#20687;&#20013;&#23567;&#32467;&#26500;&#30340;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#22270;&#20687;&#21516;&#26102;&#26368;&#23567;&#21270;&#36752;&#23556;&#26292;&#38706;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20174;&#30456;&#24212;&#30340;&#20302;&#21058;&#37327;PET&#65288;LPET&#65289;&#22270;&#20687;&#37325;&#24314;&#26631;&#20934;&#21058;&#37327;PET&#65288;SPET&#65289;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#65292;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#31934;&#30830;&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#37325;&#24314;&#32467;&#26524;&#21463;&#25439;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#30340;&#19978;&#19979;&#25991;&#32858;&#31867;GAN&#65292;&#21363;PCC-GAN&#65292;&#29992;&#20110;&#20174;LPET&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;SPET&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#21040;&#28857;&#30340;&#20960;&#20309;&#34920;&#31034;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#28857;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#20687;&#32467;&#26500;&#30340;&#26126;&#30830;&#34920;&#36798;&#65292;&#20174;&#32780;&#26041;&#20415;&#24102;&#26377;&#26356;&#32454;&#33410;&#30340;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19978;&#19979;&#25991;&#32858;&#31867;&#31574;&#30053;&#26469;&#25506;&#32034;&#28857;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#20174;&#32780;&#20943;&#36731;&#37325;&#24314;&#22270;&#20687;&#20013;&#23567;&#32467;&#26500;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images. However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction. In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET. Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details. Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images. Experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#33258;&#36866;&#24212;&#23398;&#20064;&#36895;&#29575;&#20197;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00355</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Primal-Dual Method for Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#33258;&#36866;&#24212;&#23398;&#20064;&#36895;&#29575;&#20197;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#33258;&#28982;&#24212;&#29992;&#65292;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23558;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#27599;&#27425;&#35299;&#20915;&#23884;&#20837;&#30340;&#26080;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#36895;&#29575;&#65288;LR&#65289;&#21644;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#23545;&#20598;&#21464;&#37327;&#65289;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#20381;&#36182;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#12289;&#20998;&#26512;&#21644;&#35780;&#20272;&#20102;&#36866;&#24212;&#24615;&#21407;&#22987;-&#23545;&#20598;&#65288;APD&#65289;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#35843;&#25972;&#20004;&#20010;&#33258;&#36866;&#24212;LR&#20197;&#20351;&#20043;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;APD&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Bullet-Safey-Gym&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#29615;&#22659;&#65292;&#21033;&#29992;&#20004;&#20010;&#20808;&#36827;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO-Lagrangian&#21644;DDPG-Lagrangian&#65289;&#23545;&#23454;&#38469;APD&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;&#25152;&#26377;&#23454;&#39564;&#34920;&#26126;&#65292;&#23454;&#38469;APD&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#65288;&#25110;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#19988;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.00351</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Image-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#19988;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#27169;&#22411;&#20013;&#26377;&#24847;&#22320;&#36951;&#24536;&#25968;&#25454;&#26679;&#26412;&#20197;&#31526;&#21512;&#20005;&#26684;&#30340;&#35268;&#23450;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36951;&#24536;&#39046;&#22495;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20316;&#20026;&#19968;&#24231;&#26725;&#26753;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25506;&#35752;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#19979;&#38477;&#21487;&#24573;&#30053;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#20174;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;ImageNet-1K&#21644;Places-365&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#36827;&#19968;&#27493;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#36827;&#34892;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36951;&#24536;&#30740;&#31350;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first tha
&lt;/p&gt;</description></item><item><title>ODICE&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#37325;&#35201;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#21069;&#21521;&#26799;&#24230;&#21644;&#21518;&#21521;&#26799;&#24230;&#20004;&#20010;&#26799;&#24230;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00348</link><description>&lt;p&gt;
ODICE: &#36890;&#36807;&#27491;&#20132;&#26799;&#24230;&#26356;&#26032;&#25581;&#31034;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00348
&lt;/p&gt;
&lt;p&gt;
ODICE&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#37325;&#35201;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#21069;&#21521;&#26799;&#24230;&#21644;&#21518;&#21521;&#26799;&#24230;&#20004;&#20010;&#26799;&#24230;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#20013;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22522;&#20110;DICE&#30340;&#26041;&#27861;&#23545;&#29366;&#24577;&#34892;&#20026;&#32423;&#21035;&#30340;&#34892;&#20026;&#32422;&#26463;&#26045;&#21152;&#20102;&#65292;&#36825;&#23545;&#20110;&#31163;&#32447;&#23398;&#20064;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#20165;&#20351;&#29992;&#21160;&#20316;&#32423;&#21035;&#34892;&#20026;&#32422;&#26463;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22312;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;DICE&#30340;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#20004;&#20010;&#26799;&#24230;&#39033;&#65306;&#21069;&#21521;&#26799;&#24230;&#65288;&#22312;&#24403;&#21069;&#29366;&#24577;&#19978;&#65289;&#21644;&#21518;&#21521;&#26799;&#24230;&#65288;&#22312;&#19979;&#19968;&#20010;&#29366;&#24577;&#19978;&#65289;&#12290;&#20351;&#29992;&#21069;&#21521;&#26799;&#24230;&#19982;&#35768;&#22810;&#31163;&#32447;RL&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#35270;&#20026;&#24212;&#29992;&#21160;&#20316;&#32423;&#21035;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20004;&#20010;&#26799;&#24230;&#26377;&#30456;&#20114;&#20914;&#31361;&#30340;&#26041;&#21521;&#65292;&#30452;&#25509;&#21152;&#19978;&#21518;&#21521;&#26799;&#24230;&#21487;&#33021;&#20250;&#36864;&#21270;&#25110;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#20174;&#19968;&#32452;&#21516;&#26679;&#22909;&#30340;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#22791;&#39044;&#26399;&#35299;&#37322;&#30340;&#20934;&#30830;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29289;&#29702;&#23450;&#24459;&#24182;&#28385;&#36275;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35201;&#27714;&#65292;&#24182;&#20026;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#31185;&#23398;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.00347</link><description>&lt;p&gt;
&#26469;&#33258;&#25968;&#25454;&#39537;&#21160;&#21644;&#39046;&#22495;&#39537;&#21160;&#35270;&#35282;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22810;&#26679;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Diverse Explanations from Data-driven and Domain-driven Perspectives for Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#20174;&#19968;&#32452;&#21516;&#26679;&#22909;&#30340;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#22791;&#39044;&#26399;&#35299;&#37322;&#30340;&#20934;&#30830;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29289;&#29702;&#23450;&#24459;&#24182;&#28385;&#36275;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35201;&#27714;&#65292;&#24182;&#20026;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#31185;&#23398;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26159;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21644;&#29289;&#29702;&#31561;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#25351;&#23548;&#26410;&#26469;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#21644;&#36164;&#28304;&#38656;&#27714;&#12290;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#20174;&#35757;&#32451;&#33391;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#25968;&#25454;&#39537;&#21160;&#35270;&#35282;&#65289;&#25110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#65288;&#39046;&#22495;&#39537;&#21160;&#35270;&#35282;&#65289;&#20013;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20855;&#26377;&#29305;&#23450;&#38656;&#27714;&#12289;&#24895;&#26395;&#25110;&#30446;&#26631;&#30340;&#21508;&#26041;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#24615;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#21516;&#26679;&#22909;&#30340;&#27169;&#22411;&#20013;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39044;&#26399;&#35299;&#37322;&#30340;&#20934;&#30830;&#27169;&#22411;&#65292;&#20197;&#21152;&#24378;&#29289;&#29702;&#23450;&#24459;&#24182;&#28385;&#36275;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35201;&#27714;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#34987;&#31216;&#20026;&#25289;&#35799;&#23391;&#65288;Rashomon&#65289;&#27169;&#22411;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#24615;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#26368;&#32456;&#20026;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25972;&#21512;&#21040;&#31185;&#23398;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements. These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective). However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims. This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets. Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#65292;&#24182;&#23545;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#20856;&#22411;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.00342</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Privacy Threats and Countermeasures in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#65292;&#24182;&#23545;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#20856;&#22411;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#27809;&#26377;&#30452;&#25509;&#20132;&#25442;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#19988;&#24050;&#32463;&#23545;&#38544;&#31169;&#23545;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#24120;&#35265;&#21644;&#29420;&#29305;&#30340;&#38544;&#31169;&#23041;&#32961;&#22312;&#20856;&#22411;&#31867;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20197;&#20840;&#38754;&#21644;&#20855;&#20307;&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#22312;&#35889;&#20559;&#24046;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00332</link><description>&lt;p&gt;
&#27604;&#36739;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#65306;SGD&#19982;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#20043;&#38388;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Spectral Bias and Robustness For Two-Layer Neural Networks: SGD vs Adaptive Random Fourier Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#22312;&#35889;&#20559;&#24046;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#36873;&#25321;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#20851;&#38190;&#24046;&#24322;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#32780;&#35889;&#20559;&#24046;&#19982;&#35757;&#32451;&#31639;&#27861;&#30340;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#65292;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#20135;&#29983;&#26356;&#25509;&#36817;&#38646;&#30340;&#35889;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SGD&#21644;ARFF&#35757;&#32451;&#20102;&#20004;&#20010;&#23436;&#20840;&#30456;&#21516;&#32467;&#26500;&#30340;&#20998;&#31867;&#22120;&#65292;&#23558;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;&#30456;&#21516;&#27700;&#24179;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25239;&#22122;&#22768;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present experimental results highlighting two key differences resulting from the choice of training algorithm for two-layer neural networks. The spectral bias of neural networks is well known, while the spectral bias dependence on the choice of training algorithm is less studied. Our experiments demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (SGD). Additionally, we train two identically structured classifiers, employing SGD and ARFF, to the same accuracy levels and empirically assess their robustness against adversarial noise attacks.
&lt;/p&gt;</description></item><item><title>PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00326</link><description>&lt;p&gt;
PirateNets&#65306;&#37319;&#29992;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00326
&lt;/p&gt;
&lt;p&gt;
PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;(PINNs)&#24050;&#25104;&#20026;&#35299;&#20915;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25511;&#21046;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#27969;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20294;&#22312;&#37319;&#29992;&#26356;&#22823;&#21644;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#21453;&#30452;&#35273;&#34892;&#20026;&#30340;&#26681;&#28304;&#22312;&#20110;&#20351;&#29992;&#19981;&#36866;&#21512;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#32593;&#32476;&#23548;&#25968;&#30340;&#21487;&#35757;&#32451;&#24615;&#36739;&#24046;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;PDE&#27531;&#24046;&#25439;&#22833;&#30340;&#19981;&#31283;&#23450;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;(PirateNets)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#20419;&#36827;&#28145;&#24230;PINN&#27169;&#22411;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#35757;&#32451;&#12290;PirateNets&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#65292;&#20801;&#35768;&#32593;&#32476;&#20316;&#20026;&#27973;&#23618;&#32593;&#32476;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#21152;&#28145;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;PINN&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#21202;&#36125;&#26684;&#27979;&#24230;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#22120;(CLML)&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;CLML&#21487;&#20197;&#19968;&#33268;&#22320;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20854;&#20027;&#35201;&#24615;&#33021;&#22240;&#32032;&#26159;&#21202;&#36125;&#26684;&#27979;&#24230;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.00324</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19968;&#33268;&#21202;&#36125;&#26684;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Consistent Lebesgue Measure for Multi-label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00324
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#21202;&#36125;&#26684;&#27979;&#24230;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#22120;(CLML)&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;CLML&#21487;&#20197;&#19968;&#33268;&#22320;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20854;&#20027;&#35201;&#24615;&#33021;&#22240;&#32032;&#26159;&#21202;&#36125;&#26684;&#27979;&#24230;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#21487;&#24494;&#30340;&#65292;&#38656;&#35201;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#26799;&#24230;&#20248;&#21270;&#12290; &#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#23578;&#26410;&#35777;&#26126;&#65292;&#24182;&#19988;&#30001;&#20110;&#22810;&#26631;&#31614;&#25439;&#22833;&#20989;&#25968;&#30340;&#20914;&#31361;&#24615;&#36136;&#32780;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290; &#20026;&#20102;&#30452;&#25509;&#20174;&#22810;&#20010;&#30456;&#20851;&#20294;&#28508;&#22312;&#20914;&#31361;&#30340;&#22810;&#26631;&#31614;&#25439;&#22833;&#20989;&#25968;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#21202;&#36125;&#26684;&#27979;&#24230;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#22120;&#65288;CLML&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36125;&#21494;&#26031;&#39118;&#38505;&#26694;&#26550;&#19979;CLML&#21487;&#20197;&#23454;&#29616;&#29702;&#35770;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290; &#32463;&#39564;&#35777;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#36890;&#36807;&#23637;&#31034;&#65306;&#65288;1&#65289;CLML&#21487;&#20197;&#19968;&#36143;&#22320;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#65288;2&#65289;&#20027;&#35201;&#30340;&#24615;&#33021;&#22240;&#32032;&#26159;&#21202;&#36125;&#26684;&#27979;&#24230;&#35774;&#35745;&#65292;&#22240;&#20026;CLML&#20248;&#21270;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#21069;&#39304;&#27169;&#22411;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#26631;&#31614;&#22270;&#12289;&#22522;&#20110;&#25200;&#21160;&#30340;&#26465;&#20214;&#25110;&#35821;&#20041;&#23884;&#20837;&#65307;&#20197;&#21450;&#65288;3&#65289;&#32467;&#26524;&#30340;&#20998;&#26512;&#19981;&#20165;&#21487;&#20197;&#21306;&#20998;CLML&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#20984;&#26174;&#20102;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#19982;&#26399;&#26395;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation. The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions. To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework. Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#26799;&#24230;&#25110;&#32773;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#37327;&#21270;&#26799;&#24230;&#30340;&#26041;&#24335;&#35843;&#24230;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#21463;&#38480;&#20110;&#20449;&#22122;&#27604;&#26368;&#24046;&#35774;&#22791;&#38382;&#39064;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#38477;&#20302;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.00318</link><description>&lt;p&gt;
&#27169;&#25311;&#25968;&#23383;&#35843;&#24230;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#26799;&#24230;&#25110;&#32773;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#37327;&#21270;&#26799;&#24230;&#30340;&#26041;&#24335;&#35843;&#24230;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#21463;&#38480;&#20110;&#20449;&#22122;&#27604;&#26368;&#24046;&#35774;&#22791;&#38382;&#39064;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#38477;&#20302;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;OTA&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20449;&#22122;&#27604;&#26368;&#24046;&#30340;&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#26356;&#26032;&#36895;&#24230;&#24555;&#20294;&#22122;&#22768;&#36739;&#22810;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25968;&#23383;&#36890;&#36947;&#23558;&#27491;&#20132;&#36164;&#28304;&#22359;&#65288;RB&#65289;&#20998;&#37197;&#32473;&#27599;&#20010;&#35774;&#22791;&#21487;&#20197;&#20943;&#36731;&#22122;&#22768;&#38382;&#39064;&#65292;&#20294;&#20250;&#22686;&#21152;&#36890;&#20449;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;ADFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65306;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#23558;&#27599;&#20010;&#35774;&#22791;&#35843;&#24230;&#20026;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#20854;&#26799;&#24230;&#65292;&#25110;&#32773;&#20351;&#29992;&#8220;&#25968;&#23383;&#8221;&#26041;&#26696;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#20854;&#37327;&#21270;&#26799;&#24230;&#12290;&#38024;&#23545;&#21333;&#20010;FL&#36718;&#27425;&#65292;&#25105;&#20204;&#23558;&#26368;&#20248;&#35843;&#24230;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#23567;&#21270;PS&#20272;&#35745;&#30340;&#20840;&#23616;&#26799;&#24230;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#24471;&#21040;&#26368;&#20248;&#35774;&#22791;&#35843;&#24230;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks. However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates. On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency. In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital" scheme. Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling conf
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#20855;&#26377;&#26080;&#30028;&#26631;&#31614;&#38598;&#30340;&#22312;&#32447;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$(\epsilon,0)$-&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;KL&#39118;&#38505;&#38543;&#30528;&#26102;&#38388;&#30340;&#22686;&#38271;&#36895;&#24230;&#20026;$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$&#65292;&#20854;&#20013;$K=|\mathcal{F}|$&#65292;&#36825;&#19982;&#26377;&#30028;&#26631;&#31614;&#38598;&#30340;&#24773;&#20917;&#24418;&#25104;&#26126;&#26174;&#30340;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2402.00315</link><description>&lt;p&gt;
&#22312;&#23616;&#37096;&#31169;&#26377;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Distribution Learning with Local Private Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00315
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#20855;&#26377;&#26080;&#30028;&#26631;&#31614;&#38598;&#30340;&#22312;&#32447;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$(\epsilon,0)$-&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;KL&#39118;&#38505;&#38543;&#30528;&#26102;&#38388;&#30340;&#22686;&#38271;&#36895;&#24230;&#20026;$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$&#65292;&#20854;&#20013;$K=|\mathcal{F}|$&#65292;&#36825;&#19982;&#26377;&#30028;&#26631;&#31614;&#38598;&#30340;&#24773;&#20917;&#24418;&#25104;&#26126;&#26174;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#20855;&#26377;&#26080;&#30028;&#26631;&#31614;&#38598;&#30340;&#22312;&#32447;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30446;&#26631;&#26159;&#20197;&#22312;&#32447;&#26041;&#24335;&#20272;&#35745;&#19968;&#20010;&#26410;&#30693;&#30340;&#20989;&#25968;$f\in \mathcal{F}$&#65292;&#22312;&#26102;&#38388;$t$&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;$\boldsymbol{x}_t$&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#21482;&#30693;&#36947;&#20174;$f(\boldsymbol{x}_t)$&#20013;&#21462;&#26679;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#31169;&#26377;&#21270;&#29256;&#26412;&#65292;&#29983;&#25104;&#19968;&#20010;$f(\boldsymbol{x}_t)$&#30340;KL&#25955;&#24230;&#20272;&#35745;&#12290;&#26368;&#32456;&#30340;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#26102;&#38388;$T$&#20869;&#26368;&#23567;&#21270;&#32047;&#31215;&#30340;KL&#39118;&#38505;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#31169;&#26377;&#21270;&#26631;&#31614;&#30340;$(\epsilon,0)$-&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#65292;KL&#39118;&#38505;&#22686;&#38271;&#30340;&#36895;&#24230;&#20026;$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$&#65292;&#20854;&#20013;$K=|\mathcal{F}|$&#65292;&#19982;Wu&#31561;&#20154;(2023a)&#23545;&#20110;&#26377;&#30028;&#26631;&#31614;&#38598;&#30340;$\tilde{\Theta}(\sqrt{T\log K})$&#30028;&#38480;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#24674;&#22797;&#20986;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of online conditional distribution estimation with \emph{unbounded} label sets under local differential privacy. Let $\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \emph{unknown} function $f\in \mathcal{F}$ in an online fashion so that at time $t$ when the context $\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$ upto poly-logarithmic factors where $K=|\mathcal{F}|$. This is in stark contrast to the $\tilde{\Theta}(\sqrt{T\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#24102;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#38543;&#26426;&#35268;&#21010;&#65292;&#33021;&#22815;&#23884;&#20837;&#39118;&#38505;&#20559;&#22909;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#36716;&#25442;&#38382;&#39064;&#20013;&#24674;&#22797;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.00313</link><description>&lt;p&gt;
&#25511;&#21046;&#38543;&#26426;&#29615;&#22659;&#20013;&#24102;&#24310;&#36831;&#30340;&#38382;&#39064;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Control in Stochastic Environment with Delays: A Model-based Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00313
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#24102;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#38543;&#26426;&#35268;&#21010;&#65292;&#33021;&#22815;&#23884;&#20837;&#39118;&#38505;&#20559;&#22909;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#36716;&#25442;&#38382;&#39064;&#20013;&#24674;&#22797;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#38543;&#26426;&#35268;&#21010;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#20351;&#29992;&#30830;&#23450;&#24615;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#20013;&#23884;&#20837;&#39118;&#38505;&#20559;&#22909;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#20844;&#24335;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#30830;&#23450;&#24615;&#36716;&#25442;&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31574;&#30053;&#19982;&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#20808;&#21069;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#20854;&#29305;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#25511;&#21046;&#22810;&#20010;Atari&#28216;&#25103;&#20013;&#30340;&#26041;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we are introducing a new reinforcement learning method for control problems in environments with delayed feedback. Specifically, our method employs stochastic planning, versus previous methods that used deterministic planning. This allows us to embed risk preference in the policy optimization problem. We show that this formulation can recover the optimal policy for problems with deterministic transitions. We contrast our policy with two prior methods from literature. We apply the methodology to simple tasks to understand its features. Then, we compare the performance of the methods in controlling multiple Atari games.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00310</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Seismic Traveltime Tomography with Label-free Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#20013;&#34987;&#24212;&#29992;&#20110;&#26500;&#24314;&#36895;&#24230;&#27169;&#22411;&#65288;VMs&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65288;&#21363;&#36755;&#20837;&#21644;&#26631;&#31614;&#30340;&#23545;&#24212;&#65289;&#65292;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35757;&#32451;&#65292;&#32780;&#29616;&#23454;&#25968;&#25454;&#21453;&#28436;&#30340;&#30495;&#23454;&#26631;&#31614;&#36890;&#24120;&#32570;&#22833;&#25110;&#38750;&#24120;&#26114;&#36149;&#12290;&#19968;&#20123;&#20256;&#32479;&#23618;&#26512;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#23454;&#26045;&#65292;&#20294;&#20854;&#25928;&#26524;&#36890;&#24120;&#21463;&#21040;&#20808;&#39564;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;LSQR&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;VMs&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#27973;&#23618;&#31616;&#21333;&#30340;NN&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#31574;&#30053;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;VMs&#65306;&#65288;1&#65289;&#39044;&#28909;&#38454;&#27573;&#12290;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#65292;&#20174;LSQR&#20272;&#35745;&#20013;&#35757;&#32451;&#20986;&#21021;&#22987;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have been used to build velocity models (VMs) for seismic traveltime tomography and have shown encouraging performance in recent years. However, they need to generate labeled samples (i.e., pairs of input and label) to train the deep neural network (NN) with end-to-end learning, and the real labels for field data inversion are usually missing or very expensive. Some traditional tomographic methods can be implemented quickly, but their effectiveness is often limited by prior assumptions. To avoid generating labeled samples, we propose a novel method by integrating deep learning and dictionary learning to enhance the VMs with low resolution by using the traditional tomography-least square method (LSQR). We first design a type of shallow and simple NN to reduce computational cost followed by proposing a two-step strategy to enhance the VMs with low resolution: (1) Warming up. An initial dictionary is trained from the estimation by LSQR through dictionary learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.00306</link><description>&lt;p&gt;
&#19968;&#20010;&#20934;&#30830;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#26159;&#19968;&#38376;&#28041;&#21450;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#23398;&#31185;&#12290;&#20854;&#24212;&#29992;&#21253;&#25324;&#36164;&#28304;&#20998;&#37197;&#12289;&#26381;&#21153;&#36136;&#37327;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#12289;&#23567;&#22411;&#21644;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#65292;&#21487;&#37096;&#32626;&#22312;&#26222;&#36890;&#22522;&#31449;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25972;&#20010;&#22478;&#24066;&#30340;&#23436;&#25972;&#20154;&#21592;&#27969;&#21160;&#27169;&#24335;&#36827;&#34892;&#20102;&#19968;&#30334;&#20010;&#36229;&#21442;&#25968;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#31934;&#30830;&#30340;ML&#26550;&#26500;&#65292;&#20854;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#26368;&#23569;&#25968;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#21488;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#24050;&#21457;&#34920;&#30340;ML&#26550;&#26500;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#12290;&#36825;&#23558;&#27169;&#22411;&#21442;&#25968;&#30340;&#24635;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#20869;&#23384;&#37327;&#20063;&#20943;&#23569;&#20102;&#19968;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#21644;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#65292;&#24182;&#32771;&#34385;&#20102;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.00299</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#21644;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#65292;&#24182;&#32771;&#34385;&#20102;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20449;&#29992;&#35780;&#20998;&#20542;&#21521;&#20110;&#20165;&#20351;&#29992;&#20010;&#20307;&#20511;&#27454;&#20154;&#25110;&#36151;&#27454;&#32423;&#21035;&#30340;&#39044;&#27979;&#22240;&#32032;&#65292;&#28982;&#32780;&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#23548;&#33268;&#39118;&#38505;&#22312;&#32593;&#32476;&#19978;&#30340;&#20256;&#25773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#21160;&#24577;&#22810;&#23618;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#21453;&#26144;&#20102;&#19981;&#21516;&#26469;&#28304;&#30340;&#32593;&#32476;&#36830;&#25509;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#25269;&#25276;&#36151;&#27454;&#20844;&#21496;Freddie Mac&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#34892;&#20026;&#20449;&#29992;&#35780;&#20998;&#30340;&#32972;&#26223;&#19979;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#36830;&#25509;&#28304;&#33258;&#20511;&#27454;&#20154;&#30340;&#22320;&#29702;&#20301;&#32622;&#21644;&#20182;&#20204;&#36873;&#25321;&#30340;&#25269;&#25276;&#36151;&#27454;&#25552;&#20379;&#21830;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#36830;&#25509;&#20197;&#21450;&#36825;&#20123;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#37325;&#35201;&#24615;&#23545;&#19981;&#21516;&#30340;&#26102;&#38388;&#24555;&#29031;&#36827;&#34892;&#21152;&#26435;&#12290;&#32463;&#36807;&#22810;&#27425;&#37197;&#32622;&#27979;&#35797;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PAP-REC&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#25552;&#31034;&#26631;&#35760;&#26469;&#20943;&#36731;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#25152;&#24102;&#26469;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00284</link><description>&lt;p&gt;
PAP-REC: &#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAP-REC: Personalized Automatic Prompt for Recommendation Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PAP-REC&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#25552;&#31034;&#26631;&#35760;&#26469;&#20943;&#36731;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#25152;&#24102;&#26469;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#21487;&#20197;&#32479;&#19968;&#35299;&#20915;&#22810;&#20010;&#25512;&#33616;&#20219;&#21153;&#12290;&#36825;&#20123;RLM&#20805;&#20998;&#21033;&#29992;&#20102;&#20174;&#20016;&#23500;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#36951;&#20256;&#30693;&#35782;&#65292;&#36890;&#36807;&#25552;&#31034;&#26469;&#35299;&#20915;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#32593;&#32476;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#38656;&#35201;&#26174;&#33879;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#20154;&#21147;&#25237;&#20837;&#65292;&#31245;&#24494;&#25913;&#20889;&#25552;&#31034;&#23601;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAP-REC&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20302;&#25928;&#29575;&#21644;&#20302;&#25928;&#26524;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#20801;&#35768;&#19981;&#21516;&#30340;&#29992;&#25143;&#22312;&#30456;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#36825;&#20123;&#26631;&#35760;&#26159;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large sear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00261</link><description>&lt;p&gt;
&#20351;&#29992;&#21521;&#37327;&#31354;&#38388;&#21644;&#36870;&#26144;&#23556;&#20102;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#25968;&#23398;&#26041;&#27861;&#26469;&#29702;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20449;&#21495;&#31354;&#38388;&#26469;&#21487;&#35270;&#21270;&#26435;&#37325;&#31354;&#38388;&#21644;&#21367;&#31215;&#23618;&#21367;&#31215;&#26680;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21487;&#36870;&#32593;&#32476;&#21644;ResNet18&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00258</link><description>&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#30340;&#23618;&#27425;&#32452;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-group Learning for Hierarchical Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#27169;&#22411;&#23558;&#23398;&#20064;&#22330;&#26223;&#35268;&#33539;&#21270;&#20026;&#21333;&#19968;&#39044;&#27979;&#22120;&#22312;&#22810;&#20010;&#21487;&#33021;&#37325;&#21472;&#30340;&#20852;&#36259;&#23376;&#32452;&#19978;&#24517;&#39035;&#24191;&#20041;&#21270;&#12290;&#25105;&#20204;&#23558;&#22810;&#32452;&#23398;&#20064;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#33258;&#28982;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20855;&#26377;&#23618;&#27425;&#32452;&#32467;&#26500;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#28041;&#21450;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.00254</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Vertical Symbolic Regression via Deep Policy Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00254
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#28041;&#21450;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR&#65289;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#24555;&#36895;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#12290;VSR&#36890;&#36807;&#26500;&#24314;&#30001;&#28041;&#21450;&#19968;&#37096;&#20998;&#29420;&#31435;&#21464;&#37327;&#30340;&#31616;&#21270;&#24418;&#24335;&#26041;&#31243;&#21040;&#23436;&#25972;&#26041;&#31243;&#30340;&#22402;&#30452;&#21457;&#29616;&#36335;&#24452;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35768;&#22810;&#31526;&#21495;&#22238;&#24402;&#22120;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#65292;&#39044;&#35745;&#33021;&#36827;&#19968;&#27493;&#25193;&#22823;VSR&#30340;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;VSR&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#23558;&#23548;&#33268;&#26799;&#24230;&#20256;&#36882;&#22256;&#38590;&#21644;&#20854;&#20182;&#24037;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;VSR-DPG&#21487;&#20197;&#24674;&#22797;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#30495;&#23454;&#26041;&#31243;&#65292;&#26174;&#33879;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#20808;&#21069;&#30340;VSR&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;VSR-DPG&#23558;&#31526;&#21495;&#22238;&#24402;&#24314;&#27169;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#26041;&#31243;&#26159;&#36890;&#36807;&#22810;&#27425;&#24212;&#29992;&#26469;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of 
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.00253</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23454;&#38469;&#30340;&#23454;&#26045;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#8220;&#24187;&#35273;&#8221;&#65292;&#25110;&#32773;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21363;&#35270;&#35273;&#20869;&#23481;&#19982;&#30456;&#24212;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#22312;&#21033;&#29992;LVLMs&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;LVLM&#30456;&#20851;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#27010;&#35272;&#24182;&#20419;&#36827;&#26410;&#26469;&#30340;&#32531;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#28548;&#28165;&#20102;LVLMs&#20013;&#24187;&#35273;&#27010;&#24565;&#65292;&#21576;&#29616;&#20102;&#21508;&#31181;&#24187;&#35273;&#30151;&#29366;&#65292;&#24182;&#24378;&#35843;&#20102;LVLM&#24187;&#35273;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LVLM&#29420;&#29305;&#24187;&#35273;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35843;&#26597;&#20102;&#36825;&#20123;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#21253;&#25324;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#32452;&#20214;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#23545;&#29616;&#26377;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.00251</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36880;&#27493;&#20915;&#31574;&#35268;&#21010;&#22312;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21457;&#23637;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20915;&#31574;&#35268;&#21010;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#26159;&#30333;&#30418;&#26041;&#27861;&#65292;&#35201;&#20040;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38480;&#21046;&#20102;&#40657;&#30418;&#19987;&#26377;LLMs&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;LLMs&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#19968;&#25512;&#29702;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;logits&#12290;&#35813;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#22914;&#8220;&#25171;&#24320;&#28020;&#23460;&#28783;&#8221;&#65292;&#29983;&#25104;&#21160;&#20316;&#12290;&#24403;&#26377;&#22810;&#20010;&#21160;&#20316;&#30340;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#37117;&#24456;&#39640;&#26102;&#65292;&#29992;&#25143;&#23558;&#34987;&#35201;&#27714;&#25552;&#20379;&#20559;&#22909;&#12290;&#24635;&#32467;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26410;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20915;&#31574;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Gram&#36845;&#20195;&#35745;&#31639;&#35889;&#33539;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#24490;&#29615;&#21367;&#31215;&#23618;&#21644;&#38646;&#22635;&#20805;&#21367;&#31215;&#23618;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#35889;&#37325;&#32553;&#25918;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.00240</link><description>&lt;p&gt;
&#24490;&#29615;&#21367;&#31215;&#23618;&#21644;&#38646;&#22635;&#20805;&#21367;&#31215;&#23618;&#30340;&#35889;&#33539;&#25968;
&lt;/p&gt;
&lt;p&gt;
Spectral Norm of Convolutional Layers with Circular and Zero Paddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Gram&#36845;&#20195;&#35745;&#31639;&#35889;&#33539;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#24490;&#29615;&#21367;&#31215;&#23618;&#21644;&#38646;&#22635;&#20805;&#21367;&#31215;&#23618;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#35889;&#37325;&#32553;&#25918;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#19988;&#21487;&#24494;&#30340;&#26041;&#27861;&#8212;&#8212;Gram&#36845;&#20195;&#26469;&#35745;&#31639;&#35889;&#33539;&#25968;&#65292;&#24182;&#20445;&#35777;&#20102;&#19978;&#30028;&#12290;&#38024;&#23545;&#24490;&#29615;&#21367;&#31215;&#23618;&#65292;&#25105;&#20204;&#23558;Gram&#36845;&#20195;&#30340;&#20351;&#29992;&#25512;&#24191;&#21040;&#38646;&#22635;&#20805;&#21367;&#31215;&#23618;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20108;&#27425;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#23450;&#29702;&#26469;&#24357;&#21512;&#24490;&#29615;&#21367;&#31215;&#21644;&#38646;&#22635;&#20805;&#21367;&#31215;&#30340;&#35889;&#33539;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35889;&#37325;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#31454;&#20105;&#24615;&#30340;1-Lipschitz&#23618;&#65292;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;https://github.com/blaisedelattre/lip4conv&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper leverages the use of \emph{Gram iteration} an efficient, deterministic, and differentiable method for computing spectral norm with an upper bound guarantee. Designed for circular convolutional layers, we generalize the use of the Gram iteration to zero padding convolutional layers and prove its quadratic convergence. We also provide theorems for bridging the gap between circular and zero padding convolution's spectral norm. We design a \emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer that enhances network robustness. Demonstrated through experiments, our method outperforms state-of-the-art techniques in precision, computational cost, and scalability. The code of experiments is available at https://github.com/blaisedelattre/lip4conv.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20114;&#32852;&#32593;&#29983;&#29289;&#32435;&#31859;&#29289;&#32852;&#32593;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#19982;&#32852;&#37030;&#23398;&#20064;(FL)&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#24494;&#35266;&#21644;&#32435;&#31859;&#23610;&#24230;&#19978;&#25968;&#23383;&#23402;&#29983;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#29983;&#29289;&#25216;&#26415;&#34892;&#19994;&#36827;&#34892;&#36171;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00238</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#29983;&#29289;&#32435;&#31859;&#29289;&#32852;&#32593;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#36171;&#33021;&#29983;&#29289;&#25216;&#26415;&#34892;&#19994;&#30340;CNN-FL
&lt;/p&gt;
&lt;p&gt;
CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano Things and Digital Twins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20114;&#32852;&#32593;&#29983;&#29289;&#32435;&#31859;&#29289;&#32852;&#32593;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#19982;&#32852;&#37030;&#23398;&#20064;(FL)&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#24494;&#35266;&#21644;&#32435;&#31859;&#23610;&#24230;&#19978;&#25968;&#23383;&#23402;&#29983;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#29983;&#29289;&#25216;&#26415;&#34892;&#19994;&#36827;&#34892;&#36171;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;(DTs)&#36890;&#36807;&#23454;&#29616;&#29983;&#29289;&#36164;&#20135;&#12289;&#24494;&#29983;&#29289;&#12289;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#21644;&#25968;&#23383;&#20581;&#24247;&#24212;&#29992;&#30340;&#22797;&#26434;&#25968;&#23383;&#21270;&#34920;&#31034;&#65292;&#27491;&#22312;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#29983;&#29289;&#25216;&#26415;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#24494;&#35266;&#21644;&#32435;&#31859;&#23610;&#24230;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#65292;&#29305;&#21035;&#26159;&#23545;&#32454;&#33740;&#31561;&#22797;&#26434;&#23454;&#20307;&#36827;&#34892;&#24314;&#27169;&#65292;&#38754;&#20020;&#30528;&#22312;&#38656;&#35201;&#20808;&#36827;&#30340;&#29289;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;&#21644;&#35745;&#31639;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20114;&#32852;&#32593;&#29983;&#29289;&#32435;&#31859;&#29289;&#32852;&#32593;(IoBNT)&#19982;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#32852;&#37030;&#23398;&#20064;(FL)&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#24212;&#23545;&#25152;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;IoBNT&#35774;&#22791;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25910;&#38598;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#29289;&#25968;&#25454;&#65292;&#20805;&#20998;&#21033;&#29992;CNN&#22312;&#26426;&#22120;&#35270;&#35273;&#21644;&#27169;&#24335;&#35782;&#21035;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications. However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability. In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges. Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition. Subsequently,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#34701;&#20837;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#23545;&#27604;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.00232</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Label Hierarchy with Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#34701;&#20837;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#23545;&#27604;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#26694;&#26550;&#23558;&#27599;&#20010;&#31867;&#21035;&#35270;&#20026;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#35748;&#20026;&#25152;&#26377;&#31867;&#21035;&#21516;&#31561;&#37325;&#35201;&#12290;&#36825;&#24573;&#30053;&#20102;&#26631;&#31614;&#23618;&#27425;&#23384;&#22312;&#30340;&#19968;&#33324;&#24773;&#20917;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#19979;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#20043;&#38388;&#27604;&#38750;&#24120;&#19981;&#21516;&#30340;&#31867;&#21035;&#26356;&#30456;&#20284;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26631;&#31614;&#24863;&#30693;&#30340;SCL&#26041;&#27861;&#65288;LASCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23558;&#23618;&#27425;&#20449;&#24687;&#34701;&#20837;SCL&#20013;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#36825;&#26159;&#36890;&#36807;&#20808;&#26681;&#25454;&#31867;&#21035;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#30340;&#65292;&#36890;&#36807;&#32553;&#25918;&#30340;&#23454;&#20363;-&#23454;&#20363;&#23545;&#27604;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23454;&#20363;-&#20013;&#24515;&#23545;&#27604;&#65292;&#23558;&#21516;&#31867;&#21035;&#30340;&#31034;&#20363;&#31227;&#21160;&#21040;&#23427;&#20204;&#30340;&#20013;&#24515;&#38468;&#36817;&#65292;&#20013;&#24515;&#30001;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#26631;&#31614;&#21442;&#25968;&#34920;&#31034;&#12290;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#21442;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LASCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#26126;&#23612;&#33487;&#36798;&#24030;&#21452;&#22478;&#22320;&#21306;&#30340;&#31227;&#21160;&#30005;&#35805;&#23450;&#20301;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#31038;&#21306;&#20013;&#37325;&#21472;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#25910;&#20837;&#21644;&#31181;&#26063;&#25351;&#26631;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#35299;&#24320;&#20102;&#32654;&#22269;&#22478;&#24066;&#20013;&#22797;&#26434;&#30340;&#38548;&#31163;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.00222</link><description>&lt;p&gt;
&#25581;&#31034;&#22478;&#24066;&#20013;&#37325;&#21472;&#31038;&#21306;&#30340;&#26412;&#36136;
&lt;/p&gt;
&lt;p&gt;
Uncover the nature of overlapping community in cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#26126;&#23612;&#33487;&#36798;&#24030;&#21452;&#22478;&#22320;&#21306;&#30340;&#31227;&#21160;&#30005;&#35805;&#23450;&#20301;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#31038;&#21306;&#20013;&#37325;&#21472;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#25910;&#20837;&#21644;&#31181;&#26063;&#25351;&#26631;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#35299;&#24320;&#20102;&#32654;&#22269;&#22478;&#24066;&#20013;&#22797;&#26434;&#30340;&#38548;&#31163;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#38388;&#34429;&#28982;&#24120;&#34987;&#35270;&#20026;&#31163;&#25955;&#31038;&#21306;&#65292;&#20294;&#23454;&#38469;&#19978;&#34987;&#19981;&#21516;&#21151;&#33021;&#21644;&#31038;&#20250;&#32676;&#20307;&#20849;&#20139;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#29289;&#29702;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#31038;&#21306;&#20013;&#38169;&#32508;&#22797;&#26434;&#30340;&#37325;&#21472;&#26412;&#36136;&#12290;&#36890;&#36807;&#23545;&#32654;&#22269;&#26126;&#23612;&#33487;&#36798;&#24030;&#21452;&#22478;&#22823;&#37117;&#24066;&#21306;&#65288;TCMA&#65289;&#30340;&#20010;&#20154;&#31227;&#21160;&#30005;&#35805;&#23450;&#20301;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;95.7&#65285;&#30340;&#22478;&#24066;&#21151;&#33021;&#22797;&#26434;&#24615;&#28304;&#20110;&#24037;&#20316;&#26085;&#31038;&#21306;&#30340;&#37325;&#21472;&#32467;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#37327;&#21270;&#20102;&#36825;&#20123;&#37325;&#21472;&#65292;&#36824;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#25910;&#20837;&#21644;&#31181;&#26063;&#25351;&#26631;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#35299;&#24320;&#20102;&#32654;&#22269;&#22478;&#24066;&#20013;&#22797;&#26434;&#30340;&#38548;&#31163;&#27169;&#24335;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#38416;&#26126;&#22478;&#24066;&#31038;&#21306;&#37325;&#21472;&#26412;&#36136;&#30340;&#30740;&#31350;&#65292;&#26412;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#22320;&#29702;&#31354;&#38388;&#35270;&#35282;&#35266;&#23519;&#22478;&#24066;&#32467;&#26500;&#65292;&#31361;&#26174;&#20102;&#22478;&#24066;&#20869;&#31038;&#20250;&#32463;&#27982;&#21160;&#21147;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban spaces, though often perceived as discrete communities, are shared by various functional and social groups. Our study introduces a graph-based physics-aware deep learning framework, illuminating the intricate overlapping nature inherent in urban communities. Through analysis of individual mobile phone positioning data at Twin Cities metro area (TCMA) in Minnesota, USA, our findings reveal that 95.7 % of urban functional complexity stems from the overlapping structure of communities during weekdays. Significantly, our research not only quantifies these overlaps but also reveals their compelling correlations with income and racial indicators, unraveling the complex segregation patterns in U.S. cities. As the first to elucidate the overlapping nature of urban communities, this work offers a unique geospatial perspective on looking at urban structures, highlighting the nuanced interplay of socioeconomic dynamics within cities.
&lt;/p&gt;</description></item><item><title>FedCore&#26159;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#23458;&#25143;&#31471;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.00219</link><description>&lt;p&gt;
FedCore: &#20351;&#29992;&#20998;&#24067;&#24335;&#26680;&#24515;&#38598;&#35299;&#20915;&#26080;&#25302;&#36710;&#29616;&#35937;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedCore: Straggler-Free Federated Learning with Distributed Coresets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00219
&lt;/p&gt;
&lt;p&gt;
FedCore&#26159;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#23458;&#25143;&#31471;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#30041;&#33258;&#24049;&#30340;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24930;&#36895;&#23458;&#25143;&#31471;&#65292;&#25302;&#36710;&#29616;&#35937;&#32463;&#24120;&#24433;&#21709;FL&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedCore&#65292;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#65288;&#25968;&#25454;&#38598;&#30340;&#20195;&#34920;&#23376;&#38598;&#65289;&#21019;&#26032;&#22320;&#35299;&#20915;&#25302;&#36710;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#26680;&#24515;&#38598;&#26041;&#27861;&#19981;&#21516;&#65292;FedCore&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#30452;&#25509;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#26680;&#24515;&#38598;&#65292;&#30830;&#20445;&#22312;FL&#20013;&#20445;&#25252;&#38544;&#31169;&#12290;FedCore&#23558;&#26680;&#24515;&#38598;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;k-medoids&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#25805;&#20316;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#20102;FedCore&#30340;&#25910;&#25947;&#24615;&#65292;&#23454;&#38469;&#35780;&#20272;&#26174;&#31034;FL&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;8&#20493;&#65292;&#32780;&#27169;&#22411;&#20934;&#30830;&#24615;&#27809;&#26377;&#38477;&#20302;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#36824;&#34920;&#26126;&#65292;FedCore&#23545;&#29616;&#26377;&#30340;FL&#26694;&#26550;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#36339;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#65288;MP-SL&#65289;&#26469;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#22788;&#29702;&#24322;&#26500;&#35774;&#22791;&#21644;&#22823;&#37327;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#31215;&#26497;&#21442;&#19982;&#21327;&#20316;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#33410;&#28857;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.00208</link><description>&lt;p&gt;
MP-SL&#65306;&#22810;&#36339;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MP-SL: Multihop Parallel Split Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00208
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#36339;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#65288;MP-SL&#65289;&#26469;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#22788;&#29702;&#24322;&#26500;&#35774;&#22791;&#21644;&#22823;&#37327;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#31215;&#26497;&#21442;&#19982;&#21327;&#20316;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#33410;&#28857;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20998;&#25955;&#21270;&#25968;&#25454;&#30340;&#21516;&#26102;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#19981;&#21516;&#21442;&#19982;&#35774;&#22791;&#30340;&#24322;&#26500;&#38598;&#26102;&#65292;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#24310;&#36831;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#20013;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#37327;&#21442;&#25968;&#36827;&#34892;ML&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#36229;&#20986;&#23567;&#35774;&#22791;&#65288;&#20363;&#22914;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#65289;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#22914;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#65292;&#20801;&#35768;&#22810;&#20010;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#22312;&#26377;&#36164;&#28304;&#30340;&#35745;&#31639;&#33410;&#28857;&#30340;&#24110;&#21161;&#19979;&#31215;&#26497;&#21442;&#19982;&#21327;&#20316;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24182;&#34892;SL&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#22312;&#35745;&#31639;&#33410;&#28857;&#19978;&#20998;&#37197;&#22823;&#37327;&#20869;&#23384;&#65292;&#20363;&#22914;&#20351;&#29992;100&#20010;&#21442;&#19982;&#32773;&#35757;&#32451;VGG-19&#38656;&#35201;80 GB&#30340;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#36339;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#65288;MP-SL&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#33410;&#28857;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data. However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources. Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices. To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce Multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#65292;&#23427;&#21487;&#20197;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38480;&#21046;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.00205</link><description>&lt;p&gt;
&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#21307;&#38498;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#65292;&#23427;&#21487;&#20197;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38480;&#21046;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26469;&#33258;&#19981;&#21516;&#28304;&#22836;&#21644;&#29615;&#22659;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#22797;&#26434;&#19988;&#22810;&#21464;&#30340;&#38544;&#31169;&#21644;&#30417;&#31649;&#35201;&#27714;&#65292;&#36328;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#20849;&#20139;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#20801;&#35768;&#22810;&#20010;&#26041;&#21442;&#19982;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#38598;&#25110;&#36890;&#36807;&#21512;&#20316;&#25439;&#23475;&#25968;&#25454;&#38598;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#21508;&#26041;&#29616;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20855;&#26377;&#22256;&#38590;&#20294;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20855;&#26377;&#20197;&#19979;&#20851;&#38190;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20256;&#36755;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#65288;2&#65289;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;L1&#21644;L2&#27491;&#21017;&#21270;&#30340;&#32467;&#26524;&#65292;&#23545;CIC-IDS2018&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27809;&#26377;&#21457;&#29616;&#26174;&#33879;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00201</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#22238;&#24402;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment on Feature Selection using Logistic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;L1&#21644;L2&#27491;&#21017;&#21270;&#30340;&#32467;&#26524;&#65292;&#23545;CIC-IDS2018&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27809;&#26377;&#21457;&#29616;&#26174;&#33879;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29305;&#24449;&#36873;&#25321;&#22312;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#20197;&#35745;&#31639;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30456;&#20851;&#24230;&#37327;&#20026;&#34913;&#37327;&#26631;&#20934;&#26041;&#38754;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#20013;&#24191;&#20026;&#20154;&#30693;&#30340;L1&#21644;L2&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#23398;&#21040;&#30340;&#31995;&#25968;&#21487;&#20197;&#29992;&#20316;&#26435;&#37325;&#26469;&#23545;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32508;&#21512;L1&#21644;L2&#27491;&#21017;&#21270;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CIC-IDS2018&#25968;&#25454;&#38598;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#35268;&#27169;&#65292;&#21516;&#26102;&#22240;&#20026;&#23384;&#22312;&#20004;&#20010;&#38590;&#20197;&#21306;&#20998;&#30340;&#38382;&#39064;&#31867;&#21035;&#12290;&#25105;&#20204;&#39318;&#20808;&#25490;&#38500;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#31867;&#21035;&#65292;&#28982;&#21518;&#20877;&#21253;&#21547;&#23427;&#12290;&#25105;&#20204;&#20808;&#36890;&#36807;L1&#36827;&#34892;&#29305;&#24449;&#25490;&#24207;&#65292;&#28982;&#21518;&#20877;&#36890;&#36807;L2&#36827;&#34892;&#29305;&#24449;&#25490;&#24207;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20004;&#20010;&#25490;&#24207;&#20013;&#29305;&#24449;&#38598;&#30340;&#22823;&#23567;&#26469;&#27604;&#36739;&#24102;&#26377;L1&#30340;&#36923;&#36753;&#22238;&#24402;&#65288;LR+L1&#65289;&#21644;&#24102;&#26377;L2&#30340;&#36923;&#36753;&#22238;&#24402;&#65288;LR+L2&#65289;&#12290;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised machine learning, feature selection plays a very important role by potentially enhancing explainability and performance as measured by computing time and accuracy-related metrics. In this paper, we investigate a method for feature selection based on the well-known L1 and L2 regularization strategies associated with logistic regression (LR). It is well known that the learned coefficients, which serve as weights, can be used to rank the features. Our approach is to synthesize the findings of L1 and L2 regularization. For our experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also to the existence of two problematic classes that are hard to separate. We report first with the exclusion of one of them and then with its inclusion. We ranked features first with L1 and then with L2, and then compared logistic regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of the feature sets for each of the two rankings. We found no significant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#65288;SERS&#65289;&#20174;&#26410;&#32463;&#22788;&#29702;&#30340;&#25968;&#25454;&#20013;&#39044;&#27979;&#24494;&#37327;&#26377;&#26426;&#27745;&#26579;&#29289;&#30340;&#27987;&#24230;&#12290;&#20351;&#29992;&#39057;&#22495;&#21464;&#25442;&#26041;&#27861;&#23545;&#19977;&#31181;&#27169;&#25311;&#24494;&#27745;&#26579;&#29289;&#30340;&#25289;&#26364;&#20809;&#35889;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20998;&#31867;&#22120;&#65292;&#20197;&#35299;&#20915;SERS&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.00197</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20998;&#31867;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#30830;&#23450;&#24494;&#37327;&#26377;&#26426;&#27745;&#26579;&#29289;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Determination of Trace Organic Contaminant Concentration via Machine Classification of Surface-Enhanced Raman Spectra
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#65288;SERS&#65289;&#20174;&#26410;&#32463;&#22788;&#29702;&#30340;&#25968;&#25454;&#20013;&#39044;&#27979;&#24494;&#37327;&#26377;&#26426;&#27745;&#26579;&#29289;&#30340;&#27987;&#24230;&#12290;&#20351;&#29992;&#39057;&#22495;&#21464;&#25442;&#26041;&#27861;&#23545;&#19977;&#31181;&#27169;&#25311;&#24494;&#27745;&#26579;&#29289;&#30340;&#25289;&#26364;&#20809;&#35889;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20998;&#31867;&#22120;&#65292;&#20197;&#35299;&#20915;SERS&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#26512;&#27700;&#20013;&#24494;&#37327;&#25345;&#20037;&#24615;&#26377;&#26426;&#27745;&#26579;&#29289;&#23545;&#20110;&#29615;&#22659;&#30417;&#27979;&#21644;&#39135;&#21697;&#36136;&#37327;&#25511;&#21046;&#31561;&#35768;&#22810;&#39046;&#22495;&#37117;&#26159;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36739;&#38271;&#30340;&#29615;&#22659;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#29983;&#29289;&#31215;&#32047;&#24615;&#12290;&#20256;&#32479;&#30340;&#26377;&#26426;&#27745;&#26579;&#29289;&#20998;&#26512;&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#65292;&#32780;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#65288;SERS&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#20934;&#30830;&#26816;&#27979;&#36825;&#20123;&#27745;&#26579;&#29289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;SERS&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#65292;&#22914;&#20809;&#35889;&#39044;&#22788;&#29702;&#12289;&#21435;&#22122;&#21644;&#22522;&#36136;&#24341;&#36215;&#30340;&#20809;&#35889;&#21464;&#21270;&#65292;&#38459;&#30861;&#20102;&#35813;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#20174;&#20940;&#20081;&#30340;&#12289;&#26410;&#32463;&#22788;&#29702;&#30340;&#25289;&#26364;&#25968;&#25454;&#20013;&#39044;&#27979;&#26679;&#21697;&#27745;&#26579;&#29289;&#27987;&#24230;&#30340;&#26041;&#27861;&#12290;&#39057;&#22495;&#21464;&#25442;&#26041;&#27861;&#65292;&#21253;&#25324;&#20613;&#31435;&#21494;&#21464;&#25442;&#21644;Walsh-Hadamard&#21464;&#25442;&#65292;&#34987;&#24212;&#29992;&#20110;&#19977;&#20010;&#27169;&#25311;&#24494;&#27745;&#26579;&#29289;&#65288;&#32599;&#20025;&#26126;6G&#12289;&#27695;&#27696;&#33738;&#37231;&#21644;&#19977;&#27695;&#29983;&#65289;&#22312;&#27700;&#20013;&#30340;&#25289;&#26364;&#20809;&#35889;&#38598;&#21512;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#27745;&#26579;&#29289;&#30340;&#27987;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection and analysis of traces of persistent organic pollutants in water is important in many areas, including environmental monitoring and food quality control, due to their long environmental stability and potential bioaccumulation. While conventional analysis of organic pollutants requires expensive equipment, surface enhanced Raman spectroscopy (SERS) has demonstrated great potential for accurate detection of these contaminants. However, SERS analytical difficulties, such as spectral preprocessing, denoising, and substrate-based spectral variation, have hindered widespread use of the technique. Here, we demonstrate an approach for predicting the concentration of sample pollutants from messy, unprocessed Raman data using machine learning. Frequency domain transform methods, including the Fourier and Walsh Hadamard transforms, are applied to sets of Raman spectra of three model micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are then used to tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#25345;&#20037;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#20102;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00195</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#39537;&#21160;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation Driven Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#25345;&#20037;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#20102;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30417;&#31649;&#35201;&#27714;&#21644;&#27880;&#37325;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#24403;&#21069;&#30340;&#36235;&#21183;&#24378;&#35843;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#24536;&#35760;&#26679;&#26412;&#30340;&#34917;&#38598;&#26469;&#36951;&#24536;&#35757;&#32451;&#25968;&#25454;&#30340;&#26420;&#32032;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#35745;&#31639;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25361;&#25112;&#24050;&#32463;&#36890;&#36807;&#19968;&#31995;&#21015;&#23646;&#20110;&#26426;&#22120;&#36951;&#24536;&#33539;&#30068;&#30340;&#25216;&#26415;&#24471;&#21040;&#20102;&#26377;&#25928;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#25345;&#20037;&#24615;&#35745;&#31639;&#25361;&#25112;&#19982;&#26410;&#36951;&#24536;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#24615;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35282;&#24230;&#25913;&#36827;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#31934;&#31616;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#36951;&#24536;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning. The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges. These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning. However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model. We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset. In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification. To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning schem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.00176</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65306;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#32463;&#20856;&#20998;&#31867;&#22120;&#65292;&#37327;&#23376;&#20998;&#31867;&#22120;&#20063;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#20854;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#23545;&#31574;&#26159;&#37319;&#29992;&#19968;&#20010;&#25915;&#20987;&#24863;&#30693;&#25110;&#23545;&#25239;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#37327;&#23376;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#26377;&#30028;&#33539;&#25968;&#30333;&#30418;&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37327;&#23376;&#23545;&#25163;&#36890;&#36807;&#23558;&#36755;&#20837;&#29366;&#24577;&#961;(x)&#36716;&#21270;&#20026;&#19982;&#21407;&#22987;&#29366;&#24577;&#961;(x)&#22312;p-Schatten&#36317;&#31163;&#19978;&#949;&#25509;&#36817;&#30340;&#29366;&#24577;&#955;&#26469;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#12290;&#22312;&#37327;&#23376;&#23884;&#20837;&#961;(x)&#30340;&#36866;&#24403;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#22312;p = 1&#21644;p = &#8734;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;&#23548;&#20986;&#30340;&#19978;&#30028;&#21253;&#21547;&#20004;&#20010;&#39033;&#65306;&#31532;&#19968;&#20010;&#26159;&#32463;&#20856;&#25968;&#25454;&#21644;&#37327;&#23376;&#23884;&#20837;&#20043;&#38388;&#30340;2-R&#233;nyi&#30456;&#20114;&#20449;&#24687;&#30340;&#25351;&#25968;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
In a manner analogous to their classical counterparts, quantum classifiers are vulnerable to adversarial attacks that perturb their inputs. A promising countermeasure is to train the quantum classifier by adopting an attack-aware, or adversarial, loss function. This paper studies the generalization properties of quantum classifiers that are adversarially trained against bounded-norm white-box attacks. Specifically, a quantum adversary maximizes the classifier's loss by transforming an input state $\rho(x)$ into a state $\lambda$ that is $\epsilon$-close to the original state $\rho(x)$ in $p$-Schatten distance. Under suitable assumptions on the quantum embedding $\rho(x)$, we derive novel information-theoretic upper bounds on the generalization error of adversarially trained quantum classifiers for $p = 1$ and $p = \infty$. The derived upper bounds consist of two terms: the first is an exponential function of the 2-R\'enyi mutual information between classical data and quantum embedding,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00168</link><description>&lt;p&gt;
&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#36827;&#34892;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Effects with Surrogate Outcomes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22240;&#26524;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#24120;&#24120;&#26159;&#37096;&#20998;&#32570;&#22833;&#30340;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23427;&#20204;&#24456;&#26114;&#36149;&#25110;&#24456;&#38590;&#25910;&#38598;&#12290;&#22914;&#26524;&#32570;&#22833;&#20381;&#36182;&#20110;&#21327;&#21464;&#37327;&#65288;&#21363;&#32570;&#22833;&#19981;&#23436;&#20840;&#38543;&#26426;&#65289;&#65292;&#20165;&#22522;&#20110;&#23436;&#20840;&#35266;&#27979;&#26679;&#26412;&#30340;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20559;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32467;&#21512;&#19982;&#20027;&#35201;&#32467;&#26524;&#30456;&#20851;&#30340;&#23436;&#20840;&#35266;&#27979;&#30340;&#27835;&#30103;&#21518;&#21464;&#37327;&#65288;&#26367;&#20195;&#32467;&#26524;&#65289;&#21487;&#20197;&#25913;&#36827;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26367;&#20195;&#32467;&#26524;&#22312;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#23558;&#26367;&#20195;&#32467;&#26524;&#32435;&#20837;&#20998;&#26512;&#20013;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#19978;&#36848;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26041;&#24046;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00162</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#25506;&#32034;&#32972;&#21518;&#30340;&#31070;&#35805;
&lt;/p&gt;
&lt;p&gt;
Behind the Myth of Exploration in Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#26159;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#22312;&#23454;&#36341;&#20013;&#24517;&#39035;&#22312;&#23398;&#20064;&#30446;&#26631;&#20013;&#21253;&#21547;&#25506;&#32034;&#39033;&#12290;&#23613;&#31649;&#36825;&#20123;&#39033;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25506;&#32034;&#29615;&#22659;&#30340;&#20869;&#22312;&#38656;&#27714;&#36827;&#34892;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#20004;&#31181;&#19981;&#21516;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20351;&#24471;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#20445;&#25345;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#20272;&#35745;&#65292;&#22686;&#21152;&#20102;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26368;&#32456;&#25552;&#20379;&#26368;&#20248;&#31574;&#30053;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#25928;&#24212;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#31361;&#20986;&#20102;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#20123;&#31574;&#30053;&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#29992;&#20102;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGANs&#65289;&#26469;&#25552;&#39640;&#22320;&#38663;&#24037;&#31243;&#20013;&#20256;&#24863;&#22120;&#32593;&#32476;&#33719;&#21462;&#30340;&#26102;&#38388;&#21382;&#21490;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#65292;&#25552;&#39640;&#20102;&#22320;&#38663;&#24037;&#31243;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00153</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#21152;&#22320;&#38663;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#30340;&#20840;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fully Data-Driven Model for Increasing Sampling Rate Frequency of Seismic Data using Super-Resolution Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#29992;&#20102;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGANs&#65289;&#26469;&#25552;&#39640;&#22320;&#38663;&#24037;&#31243;&#20013;&#20256;&#24863;&#22120;&#32593;&#32476;&#33719;&#21462;&#30340;&#26102;&#38388;&#21382;&#21490;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#65292;&#25552;&#39640;&#20102;&#22320;&#38663;&#24037;&#31243;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26159;&#20219;&#20309;&#24037;&#31243;&#24212;&#29992;&#30340;&#20851;&#38190;&#35201;&#27714;&#20043;&#19968;&#12290;&#22312;&#22320;&#38663;&#24037;&#31243;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;&#32467;&#26500;&#21709;&#24212;&#25110;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#24212;&#29992;&#20013;&#36827;&#34892;&#25439;&#20260;&#26816;&#27979;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#26114;&#36149;&#30340;&#25104;&#26412;&#12289;&#24222;&#22823;&#30340;&#25968;&#25454;&#36890;&#36947;&#21644;&#24040;&#22823;&#30340;&#23384;&#20648;&#38656;&#27714;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGANs&#65289;&#25913;&#21892;&#26102;&#38388;&#21382;&#21490;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#65292;&#20363;&#22914;SHM&#24212;&#29992;&#20013;&#20256;&#24863;&#22120;&#32593;&#32476;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#22320;&#38663;&#24037;&#31243;&#39046;&#22495;&#24212;&#29992;SRGANs&#12290;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#36716;&#25442;&#20026;RGB&#20540;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#12290;&#28982;&#21518;&#20351;&#29992;SRGANs&#26469;&#25552;&#21319;&#36825;&#20123;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#28508;&#21147;&#20943;&#23569;&#25968;&#25454;&#36890;&#36947;&#25968;&#37327;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#22320;&#38663;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#22320;&#38663;&#24037;&#31243;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality data is one of the key requirements for any engineering application. In earthquake engineering practice, accurate data is pivotal in predicting the response of structure or damage detection process in an Structural Health Monitoring (SHM) application with less uncertainty. However, obtaining high-resolution data is fraught with challenges, such as significant costs, extensive data channels, and substantial storage requirements. To address these challenges, this study employs super-resolution generative adversarial networks (SRGANs) to improve the resolution of time-history data such as the data obtained by a sensor network in an SHM application, marking the first application of SRGANs in earthquake engineering domain. The time-series data are transformed into RGB values, converting raw data into images. SRGANs are then utilized to upscale these low-resolution images, thereby enhancing the overall sensor resolution. This methodology not only offers potential reductions in d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00152</link><description>&lt;p&gt;
&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;: &#20174;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36861;&#27714;&#65292;&#21040;&#24213;&#26159;&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DeNNs&#65289;&#21644;&#20855;&#26377;&#26377;&#38480;&#38544;&#34255;&#23618;&#30340;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;WeNNs&#65289;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#26412;&#28857;&#30340;&#25968;&#37327;&#65292;&#31070;&#32463;&#32593;&#32476;&#20869;&#30340;&#21442;&#25968;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26356;&#22810;&#30340;&#21442;&#25968;&#20542;&#21521;&#20110;&#36873;&#25321;WeNNs&#65292;&#32780;&#26356;&#22810;&#30340;&#26679;&#26412;&#28857;&#21644;&#26356;&#39640;&#30340;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#20542;&#21521;&#20110;&#36873;&#25321;DeNNs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;Ritz&#21644;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00138</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Decomposable Submodular Maximization in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31119;&#21033;&#26368;&#22823;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#20989;&#25968;&#20197;&#21450;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#21450;&#20854;&#20248;&#21270;&#38382;&#39064;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#32452;&#20998;&#20989;&#25968;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#32452;&#20998;&#20989;&#25968;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#65288;&#20363;&#22914;&#21487;&#33021;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#20989;&#25968;&#65289;&#65292;&#19981;&#33021;&#24191;&#27867;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#30340;&#8220;&#32852;&#37030;&#20248;&#21270;&#8221;&#35774;&#32622;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#20989;&#25968;&#65292;&#38656;&#35201;&#26368;&#22823;&#21270;&#36825;&#20123;&#20559;&#22909;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#22312;&#35813;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#27969;&#34892;&#30340;&#8220;&#36830;&#32493;&#36138;&#23146;&#8221;&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#30340;&#26041;&#24335;&#26397;&#30528;&#23616;&#37096;&#35299;&#21521;&#21069;&#36808;&#20986;&#23567;&#30340;&#23616;&#37096;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#23616;&#37096;&#21464;&#21270;&#32858;&#21512;&#21040;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#20687;&#12289;&#36951;&#20256;&#23398;&#21644;&#20020;&#24202;&#35780;&#20272;&#31561;&#26089;&#26399;&#25351;&#26631;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#22312;&#26089;&#26399;&#38454;&#27573;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#20122;&#22411;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22411;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.00137</link><description>&lt;p&gt;
&#36890;&#36807;ChatGPT&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#20687;&#12289;&#36951;&#20256;&#23398;&#21644;&#20020;&#24202;&#35780;&#20272;&#31561;&#26089;&#26399;&#25351;&#26631;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#22312;&#26089;&#26399;&#38454;&#27573;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#20122;&#22411;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22411;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20294;&#30446;&#21069;&#21487;&#29992;&#30340;&#27835;&#30103;&#26041;&#27861;&#20165;&#38480;&#20110;&#20572;&#27490;&#30142;&#30149;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#30142;&#30149;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#20123;&#27835;&#30103;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#19981;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#22312;&#26089;&#26399;&#38454;&#27573;&#30830;&#23450;&#30142;&#30149;&#20122;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;AD&#25110;&#30456;&#20851;&#30142;&#30149;&#21518;&#26399;&#23545;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#39044;&#27979;&#26080;&#30151;&#29366;&#25110;&#21069;&#39537;&#26399;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20998;&#31867;&#32972;&#21518;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35201;&#20040;&#21482;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#36827;&#34892;&#35780;&#20272;&#65292;&#38480;&#21046;&#20102;&#20854;&#20998;&#26512;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#25104;&#20687;&#12289;&#36951;&#20256;&#23398;&#21644;&#20020;&#24202;&#35780;&#20272;&#31561;&#26089;&#26399;&#25351;&#26631;&#23558;AD&#24739;&#32773;&#20998;&#31867;&#20026;&#26089;&#26399;&#20122;&#22411;&#12290;&#31867;&#20284;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#25552;&#31034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#25105;&#20204;&#27169;&#22411;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is the most prevalent neurodegenerative disease; yet its currently available treatments are limited to stopping disease progression. Moreover, effectiveness of these treatments is not guaranteed due to the heterogenetiy of the disease. Therefore, it is essential to be able to identify the disease subtypes at a very early stage. Current data driven approaches are able to classify the subtypes at later stages of AD or related disorders, but struggle when predicting at the asymptomatic or prodromal stage. Moreover, most existing models either lack explainability behind the classification or only use a single modality for the assessment, limiting scope of its analysis. Thus, we propose a multimodal framework that uses early-stage indicators such as imaging, genetics and clinical assessments to classify AD patients into subtypes at early stages. Similarly, we build prompts and use large language models, such as ChatGPT, to interpret the findings of our model. In our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.00123</link><description>&lt;p&gt;
&#27604;&#36739;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Template-based and Template-free Language Model Probing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#19987;&#23478;&#21046;&#20316;&#30340;&#27169;&#26495;&#21644;&#33258;&#28982;&#21457;&#29983;&#30340;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#26041;&#27861;&#30340;&#24046;&#24322;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;16&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;10&#20010;&#33521;&#25991;&#25506;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;4&#20010;&#22522;&#20110;&#27169;&#26495;&#30340;&#21644;6&#20010;&#38750;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#38024;&#23545;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#20102;&#22238;&#31572;&#65306;&#65288;RQ1&#65289;&#27169;&#22411;&#25490;&#21517;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ2&#65289;&#27169;&#22411;&#30340;&#32477;&#23545;&#24471;&#20998;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ3&#65289;RQ1&#21644;RQ2&#30340;&#31572;&#26696;&#22312;&#19968;&#33324;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#26159;&#21542;&#19981;&#21516;&#65311;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65306;1&#65289;&#38500;&#20102;&#39030;&#32423;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#22806;&#65292;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#26041;&#27861;&#36890;&#24120;&#25490;&#21517;&#19981;&#21516;&#12290;2&#65289;&#19982;&#24179;&#34892;&#30340;&#38750;&#27169;&#26495;&#21644;&#27169;&#26495;&#25552;&#31034;&#30456;&#27604;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;&#26368;&#22810;42%&#12290;3&#65289;&#22312;&#38750;&#27169;&#26495;&#26041;&#27861;&#20013;&#65292;&#22256;&#24785;&#24230;&#19982;&#20934;&#30830;&#24230;&#21576;&#36127;&#30456;&#20851;&#65292;&#20294;&#26159;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25506;&#27979;&#20013;&#65292;&#23427;&#20204;&#21576;&#27491;&#30456;&#20851;&#65292;&#36825;&#19982;&#30452;&#35273;&#30456;&#21453;&#12290;4&#65289;&#27169;&#22411;&#20542;&#21521;&#20110;&#39044;&#27979;&#30456;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#65288;SymPrompt&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#24182;&#20197;&#39537;&#21160;&#31574;&#30053;&#25512;&#21160;&#27599;&#20010;&#38454;&#27573;&#65292;&#25913;&#21892;&#20102;&#27979;&#35797;&#29983;&#25104;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00097</link><description>&lt;p&gt;
&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#65306;&#22522;&#20110;LLM&#30340;&#22238;&#24402;&#35774;&#32622;&#19979;&#35206;&#30422;&#29575;&#23548;&#21521;&#30340;&#27979;&#35797;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#65288;SymPrompt&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#24182;&#20197;&#39537;&#21160;&#31574;&#30053;&#25512;&#21160;&#27599;&#20010;&#38454;&#27573;&#65292;&#25913;&#21892;&#20102;&#27979;&#35797;&#29983;&#25104;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#22312;&#30830;&#20445;&#36719;&#20214;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#32463;&#24120;&#22312;&#22797;&#26434;&#30340;&#36719;&#20214;&#21333;&#20803;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#36798;&#19981;&#21040;&#26368;&#20339;&#30340;&#27979;&#35797;&#35206;&#30422;&#29575;&#12290;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27979;&#35797;&#29983;&#25104;&#30340;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#20248;&#21270;&#27979;&#35797;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#32416;&#27491;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#26469;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21363;&#25552;&#31034;&#27169;&#22411;&#22312;&#27809;&#26377;&#39069;&#22806;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;LLM&#29983;&#25104;&#30340;&#27979;&#35797;&#22871;&#20214;&#20173;&#28982;&#23384;&#22312;&#20302;&#35206;&#30422;&#29575;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymPrompt&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#26469;&#36827;&#34892;&#27979;&#35797;&#29983;&#25104;&#12290;SymPrompt&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;LLMs&#22312;&#20197;&#22810;&#27493;&#26041;&#24335;&#24605;&#32771;&#38382;&#39064;&#26102;&#21487;&#20197;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#36923;&#36753;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#29983;&#25104;&#65292;&#23558;&#27979;&#35797;&#22871;&#20214;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#30001;&#19968;&#31181;&#39537;&#21160;&#31574;&#30053;&#26469;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31867;&#21035;&#65292;&#20854;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#24182;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;DNNs&#26159;&#31283;&#20581;&#30340;&#23545;&#23454;&#20540;&#20989;&#25968;&#21644;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.00094</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;: &#38750;&#38463;&#22522;&#31859;&#24503;&#20998;&#26512;&#30340;&#34920;&#36848;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks: A Formulation Via Non-Archimedean Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31867;&#21035;&#65292;&#20854;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#24182;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;DNNs&#26159;&#31283;&#20581;&#30340;&#23545;&#23454;&#20540;&#20989;&#25968;&#21644;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#26550;&#26500;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#20013;&#30340;&#25968;&#23383;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;&#29615;&#20855;&#26377;&#33258;&#28982;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#31867;&#20284;&#26080;&#38480;&#26681;&#26641;&#12290;&#36825;&#20123;&#29615;&#19978;&#30340;&#33258;&#28982;&#24577;&#23556;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#26377;&#38480;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#26032;&#30340;DNNs&#26159;&#23545;&#22312;&#25152;&#25552;&#21040;&#30340;&#29615;&#19978;&#23450;&#20041;&#30340;&#23454;&#20540;&#20989;&#25968;&#30340;&#31283;&#20581;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;DNNs&#20063;&#26159;&#23545;&#22312;&#21333;&#20301;&#21306;&#38388;&#19978;&#23450;&#20041;&#30340;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#31283;&#20581;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of deep neural networks (DNNs) with multilayered tree-like architectures. The architectures are codified using numbers from the ring of integers of non-Archimdean local fields. These rings have a natural hierarchical organization as infinite rooted trees. Natural morphisms on these rings allow us to construct finite multilayered architectures. The new DNNs are robust universal approximators of real-valued functions defined on the mentioned rings. We also show that the DNNs are robust universal approximators of real-valued square-integrable functions defined in the unit interval.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#26029;&#35328;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26029;&#35328;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00093</link><description>&lt;p&gt;
ChIRAAG: &#36890;&#36807;ChatGPT&#29983;&#25104;&#24555;&#36895;&#21644;&#33258;&#21160;&#26029;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#26029;&#35328;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26029;&#35328;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
System Verilog Assertion (SVA)&#30340;&#24418;&#24335;&#21270;&#26159;Formal Property Verification (FPV)&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;SVA&#30340;&#24418;&#24335;&#21270;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#19987;&#23478;&#35299;&#37322;&#35268;&#26684;&#12290;&#36825;&#26159;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#26029;&#35328;&#29983;&#25104;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#20013;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#30340;&#26029;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;OpenAI GPT4&#30340;&#33258;&#23450;&#20041;LLM&#29992;&#20110;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#27979;&#35797;&#24179;&#21488;&#26469;&#39564;&#35777;LLM&#29983;&#25104;&#30340;&#26029;&#35328;&#12290;&#21482;&#26377;43%&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22987;&#26029;&#35328;&#23384;&#22312;&#38169;&#35823;&#65292;&#21253;&#25324;&#35821;&#27861;&#21644;&#36923;&#36753;&#38169;&#35823;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#27979;&#35797;&#26696;&#20363;&#22833;&#36133;&#20013;&#24471;&#20986;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#20419;&#20351;LLM&#65292;&#35813;&#27969;&#27700;&#32447;&#22312;&#26368;&#22810;&#20061;&#27425;&#25552;&#31034;&#36845;&#20195;&#21518;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#30340;SVA&#12290;
&lt;/p&gt;
&lt;p&gt;
System Verilog Assertion (SVA) formulation, a critical yet complex task, is a pre-requisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications. This is time consuming and prone to human error. However, recent advances in Large Language Models (LLM), LLM-informed automatic assertion generation is gaining interest. We designed a novel LLM-based pipeline to generate assertions in English Language, Linear Temporal Logic, and SVA from natural language specifications. We developed a custom LLM-based on OpenAI GPT4 for our experiments. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors, including syntax and logical errors. By iteratively prompting the LLMs using carefully crafted prompts derived from test case failures, the pipeline could generate correct SVAs after a maximum of nine iterations of prompting. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#20803;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20146;&#21644;&#24615;&#26631;&#20934;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00092</link><description>&lt;p&gt;
&#26080;&#20998;&#38598;&#20219;&#21153;&#36873;&#25321;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Episodic-free Task Selection for Few-shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#20803;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20146;&#21644;&#24615;&#26631;&#20934;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#38598;&#35757;&#32451;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20027;&#27969;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#22330;&#26223;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#24448;&#24448;&#21155;&#20110;&#19968;&#20123;&#38750;&#20998;&#38598;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#37051;&#22495;&#20998;&#37327;&#20998;&#26512;&#65288;NCA&#65289;&#65292;&#36825;&#25361;&#25112;&#20102;&#35757;&#32451;&#26465;&#20214;&#24517;&#39035;&#19982;&#27979;&#35797;&#26465;&#20214;&#21305;&#37197;&#30340;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#25628;&#32034;&#26080;&#20998;&#38598;&#20219;&#21153;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25928;&#26524;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20998;&#38598;&#20219;&#21153;&#19981;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#65292;&#32780;&#26159;&#29992;&#20110;&#35780;&#20272;&#20174;&#20219;&#21153;&#38598;&#20013;&#36873;&#25321;&#30340;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;&#36873;&#25321;&#26631;&#20934;&#26159;&#36890;&#36807;&#20146;&#21644;&#24615;&#35774;&#35745;&#30340;&#65292;&#20146;&#21644;&#24615;&#34913;&#37327;&#20102;&#22312;&#20351;&#29992;&#36873;&#23450;&#20219;&#21153;&#35757;&#32451;&#21518;&#65292;&#25191;&#34892;&#30446;&#26631;&#20219;&#21153;&#26102;&#25439;&#22833;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35757;&#32451;&#20219;&#21153;&#38598;&#21253;&#21547;&#20102;&#19968;&#20123;&#26377;&#21069;&#26223;&#30340;&#31867;&#22411;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Episodic training is a mainstream training strategy for few-shot learning. In few-shot scenarios, however, this strategy is often inferior to some non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA), which challenges the principle that training conditions must match testing conditions. Thus, a question is naturally asked: How to search for episodic-free tasks for better few-shot learning? In this work, we propose a novel meta-training framework beyond episodic training. In this framework, episodic tasks are not used directly for training, but for evaluating the effectiveness of some selected episodic-free tasks from a task set that are performed for training the meta-learners. The selection criterion is designed with the affinity, which measures the degree to which loss decreases when executing the target tasks after training with the selected tasks. In experiments, the training task set contains some promising types, e. g., contrastive learning and classifica
&lt;/p&gt;</description></item><item><title>RetroWISE &#26159;&#19968;&#20010;&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#29983;&#25104;&#37197;&#23545;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00086</link><description>&lt;p&gt;
&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis prediction enhanced by in-silico reaction data augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00086
&lt;/p&gt;
&lt;p&gt;
RetroWISE &#26159;&#19968;&#20010;&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#29983;&#25104;&#37197;&#23545;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312; retrosynthesis &#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#24110;&#21161;&#21270;&#23398;&#23478;&#26356;&#39640;&#25928;&#22320;&#35774;&#35745;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22823;&#37327;&#25104;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#21270;&#23398;&#21453;&#24212;&#65306;&#20135;&#29289;-&#21453;&#24212;&#29289;&#23545;&#65289;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20844;&#21496;&#23558;&#21453;&#24212;&#25968;&#25454;&#35270;&#20026;&#26377;&#20215;&#20540;&#30340;&#36164;&#20135;&#65292;&#24182;&#38480;&#21046;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#26356;&#24378;&#22823;&#30340; retrosynthesis &#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#65288;&#21363;&#20135;&#29289;-&#21453;&#24212;&#29289;&#23545;&#20013;&#30340;&#19968;&#20010;&#32452;&#20998;&#65289;&#29983;&#25104;&#35745;&#31639;&#27169;&#25311;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#24110;&#21161;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RetroWISE&#65292;&#19968;&#20010;&#33258;&#25105;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20174;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25512;&#26029;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#38750;&#37197;&#23545;&#25968;&#25454;&#25191;&#34892;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#29983;&#25104;&#21644;&#22686;&#24378;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19968;&#20010;&#21331;&#36234;&#30340;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RetroWISE &#23454;&#29616;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning (ML) have expedited retrosynthesis research by assisting chemists to design experiments more efficiently. However, all ML-based methods consume substantial amounts of paired training data (i.e., chemical reaction: product-reactant(s) pair), which is costly to obtain. Moreover, companies view reaction data as a valuable asset and restrict the accessibility to researchers. These issues prevent the creation of more powerful retrosynthesis models due to their data-driven nature. As a response, we exploit easy-to-access unpaired data (i.e., one component of product-reactant(s) pair) for generating in-silico paired data to facilitate model training. Specifically, we present RetroWISE, a self-boosting framework that employs a base model inferred from real paired data to perform in-silico reaction generation and augmentation using unpaired data, ultimately leading to a superior model. On three benchmark datasets, RetroWISE achieves the best overall performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00085</link><description>&lt;p&gt;
&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q&#65306;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#22521;&#35757;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#38656;&#35201;&#19982;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#23545;&#35805;&#32463;&#39564;&#20013;&#25484;&#25569;&#23545;&#35805;&#31574;&#30053;&#20173;&#28982;&#26159;&#20351;&#20195;&#29702;&#22521;&#35757;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26694;&#26550;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#22521;&#35757;&#26679;&#26412;&#24320;&#22987;&#22521;&#35757;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25439;&#23475;&#20102;&#22521;&#35757;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#35805;&#27169;&#22411;Deep Dyna-Q(DDQ)&#30340;&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q (SC-DDQ)&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;SC-DDQ&#21644;DDQ&#35774;&#35745;&#20102;&#23398;&#20064;&#35745;&#21010;&#65292;&#36981;&#24490;&#20004;&#31181;&#30456;&#21453;&#30340;&#22521;&#35757;&#31574;&#30053;&#65306;&#32463;&#20856;&#35838;&#31243;&#23398;&#20064;&#21450;&#20854;&#36870;&#21521;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26032;&#26694;&#26550;&#22312;DDQ&#21644;Dee&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2402.00084</link><description>&lt;p&gt;
EPSD: &#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
EPSD: Early Pruning with Self-Distillation for Efficient Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00084
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#32593;&#32476;&#20462;&#21098;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#33976;&#39311;&#8221;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#36866;&#21512;&#23398;&#29983;&#30340;&#25945;&#24072;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25945;&#24072;-&#23398;&#29983;&#27969;&#31243;&#65292;&#28041;&#21450;&#32321;&#29712;&#30340;&#25945;&#24072;&#39044;&#35757;&#32451;&#21644;&#22797;&#26434;&#30340;&#21387;&#32553;&#27493;&#39588;&#65292;&#20351;&#24471;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#20462;&#21098;&#21464;&#24471;&#19981;&#22815;&#39640;&#25928;&#12290;&#38500;&#20102;&#21387;&#32553;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#21387;&#32553;&#25216;&#26415;&#20063;&#24378;&#35843;&#25928;&#29575;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;&#26089;&#26399;&#20462;&#21098;&#19982;&#20256;&#32479;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35201;&#27714;&#30340;&#35745;&#31639;&#25104;&#26412;&#35201;&#23567;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#21516;&#26679;&#65292;&#33258;&#33976;&#39311;&#20316;&#20026;&#30693;&#35782;&#33976;&#39311;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#25110;&#23398;&#29983;-&#25945;&#24072;&#23545;&#30340;&#36873;&#25321;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23558;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21517;&#20026;&#8220;&#26089;&#26399;&#20462;&#21098;&#8221;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Bridge&#30340;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#21033;&#29992;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#12289;&#27979;&#24207;&#25216;&#26415;&#30340;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#31232;&#30095;&#24615;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.00077</link><description>&lt;p&gt;
&#22810;&#26426;&#26500;&#25968;&#25454;&#30340;&#37322;&#25918;&#21147;&#37327;&#65306;&#25972;&#21512;&#21644;&#21327;&#35843;&#36328;&#26426;&#26500;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Bridge&#30340;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#21033;&#29992;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#12289;&#27979;&#24207;&#25216;&#26415;&#30340;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#31232;&#30095;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#30001;&#22522;&#22240;&#31361;&#21464;&#39537;&#21160;&#30340;&#22797;&#26434;&#30142;&#30149;&#65292;&#32959;&#30244;&#27979;&#24207;&#24050;&#25104;&#20026;&#30284;&#30151;&#24739;&#32773;&#20020;&#24202;&#25252;&#29702;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#20986;&#29616;&#30340;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#20026;&#23398;&#20064;&#30495;&#23454;&#19990;&#30028;&#30340;&#35777;&#25454;&#20197;&#22686;&#24378;&#31934;&#20934;&#32959;&#30244;&#21307;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#36164;&#28304;&#12290;&#30001;&#32654;&#22269;&#30284;&#30151;&#30740;&#31350;&#21327;&#20250;&#39046;&#23548;&#30340;GENIE BPC&#24314;&#31435;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#24211;&#65292;&#23558;&#22522;&#22240;&#32452;&#25968;&#25454;&#19982;&#22810;&#20010;&#30284;&#30151;&#20013;&#24515;&#30340;&#20020;&#24202;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#23548;&#33268;&#22312;&#20351;&#29992;&#24120;&#35265;&#22522;&#22240;&#38598;&#36827;&#34892;&#20998;&#26512;&#26102;&#20449;&#24687;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#27979;&#24207;&#25216;&#26415;&#21644;&#26426;&#26500;&#20043;&#38388;&#30340;&#24739;&#32773;&#24322;&#36136;&#24615;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#39640;&#32500;&#25968;&#25454;&#12289;&#31232;&#30095;&#22522;&#22240;&#31361;&#21464;&#27169;&#24335;&#20197;&#21450;&#20010;&#20307;&#22522;&#22240;&#27700;&#24179;&#19978;&#30340;&#24369;&#20449;&#21495;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bridge&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by the American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging such multi-institutional sequencing data presents significant challenges. Variations in gene panels result in loss of information when the analysis is conducted on common gene sets. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It use
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#26041;&#38754;&#20135;&#29983;&#30340;&#35299;&#37322;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00072</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#65306;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for survival analysis: a median-SHAP approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#26041;&#38754;&#20135;&#29983;&#30340;&#35299;&#37322;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#65292;&#38656;&#35201;&#38024;&#23545;&#24615;&#30340;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;Shapley&#20540;&#22312;&#23616;&#37096;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#30340;&#35299;&#37322;&#24615;&#24378;&#28872;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#21644;&#20272;&#35745;&#37327;&#65292;&#36825;&#20123;&#32479;&#35745;&#37327;&#21644;&#20272;&#35745;&#37327;&#23450;&#20041;&#20102;&#25105;&#20204;&#25152;&#35748;&#20026;&#30340;&#8220;&#38170;&#28857;&#8221;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#22343;&#20540;&#38170;&#28857;&#30340;&#24815;&#20363;&#21487;&#33021;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#20013;&#20301;&#25968;-SHAP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the adoption of machine learning into routine clinical practice comes the need for Explainable AI methods tailored to medical applications. Shapley values have sparked wide interest for locally explaining models. Here, we demonstrate their interpretation strongly depends on both the summary statistic and the estimator for it, which in turn define what we identify as an 'anchor point'. We show that the convention of using a mean anchor point may generate misleading interpretations for survival analysis and introduce median-SHAP, a method for explaining black-box models predicting individual survival times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.00071</link><description>&lt;p&gt;
&#25581;&#31034;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Impact of Initial Choices and In-Loop Interventions on Learning Dynamics in Autonomous Scanning Probe Microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23454;&#39564;&#65288;AE&#65289;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#26377;&#25928;&#36827;&#34892;AE&#30340;&#40065;&#26834;&#24037;&#20316;&#27969;&#12290;&#36825;&#38656;&#35201;&#26126;&#30830;&#23450;&#20041;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;AE&#36807;&#31243;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#21644;&#24037;&#20316;&#27969;&#24490;&#29615;&#20013;&#30340;&#39640;&#32423;&#20154;&#21592;&#24178;&#39044;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#20840;&#38754;&#38416;&#36848;&#20102;&#21021;&#22987;&#23454;&#39564;&#26465;&#20214;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#21021;&#22987;&#23454;&#39564;&#35774;&#32622;&#23545;&#21518;&#32493;&#23398;&#20064;&#36712;&#36857;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AE&#20013;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#26041;&#27861;&#65292;&#20351;&#25805;&#20316;&#32773;&#33021;&#22815;&#24433;&#21709;&#25506;&#32034;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;PbTiO3&#34180;&#33180;&#19978;&#30340;Piezoresponse&#21147;&#26174;&#24494;&#38236;&#65288;PFM&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#24490;&#29615;&#31181;&#23376;&#24178;&#39044;&#23545;DKL&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current focus in Autonomous Experimentation (AE) is on developing robust workflows to conduct the AE effectively. This entails the need for well-defined approaches to guide the AE process, including strategies for hyperparameter tuning and high-level human interventions within the workflow loop. This paper presents a comprehensive analysis of the influence of initial experimental conditions and in-loop interventions on the learning dynamics of Deep Kernel Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore the concept of 'seed effect', where the initial experiment setup has a substantial impact on the subsequent learning trajectory. Additionally, we introduce an approach of the seed point interventions in AE allowing the operator to influence the exploration process. Using a dataset from Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the impact of the 'seed effect' and in-loop seed interventions on the effectiveness of DKL in pre
&lt;/p&gt;</description></item><item><title>EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.00070</link><description>&lt;p&gt;
EvoMerge:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EvoMerge: Neuroevolution for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00070
&lt;/p&gt;
&lt;p&gt;
EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24494;&#35843;&#20013;&#65292;&#24182;&#19981;&#24635;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#24448;&#24448;&#27169;&#22411;&#26356;&#25797;&#38271;&#27169;&#20223;&#19968;&#31181;&#25968;&#25454;&#24418;&#24335;&#32780;&#19981;&#20855;&#22791;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#33021;&#22833;&#21435;&#19968;&#20123;&#26234;&#33021;&#12290;&#36825;&#37324;&#25105;&#20171;&#32461;&#20102;EvoMerge&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;EvoMerge&#24314;&#31435;&#20102;&#19968;&#20010;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#32447;&#23545;&#38271;&#26102;&#38388;&#20250;&#35758;&#24405;&#38899;&#36827;&#34892;&#35828;&#35805;&#20154;&#36776;&#35782;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#38899;&#20998;&#31163;&#26469;&#24341;&#23548;&#36776;&#35782;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#35828;&#35805;&#20154;&#25968;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.00067</link><description>&lt;p&gt;
&#20250;&#35758;&#26102;&#38271;&#22312;&#32447;&#35828;&#35805;&#20154;&#36776;&#35782;&#24341;&#23548;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Online speaker diarization of meetings guided by speech separation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#32447;&#23545;&#38271;&#26102;&#38388;&#20250;&#35758;&#24405;&#38899;&#36827;&#34892;&#35828;&#35805;&#20154;&#36776;&#35782;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#38899;&#20998;&#31163;&#26469;&#24341;&#23548;&#36776;&#35782;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#35828;&#35805;&#20154;&#25968;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#35821;&#38899;&#23545;&#35828;&#35805;&#20154;&#36776;&#35782;&#31995;&#32479;&#19968;&#30452;&#26159;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#38899;&#20998;&#31163;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20855;&#26377;&#22266;&#23450;&#20154;&#25968;&#30340;&#27169;&#25311;&#28151;&#21512;&#29289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#22312;&#29616;&#23454;&#25968;&#25454;&#19978;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#32447;&#23545;&#38271;&#26102;&#38388;&#20250;&#35758;&#24405;&#38899;&#36827;&#34892;&#35828;&#35805;&#20154;&#36776;&#35782;&#30340;&#35821;&#38899;&#20998;&#31163;&#24341;&#23548;&#26041;&#26696;&#65292;&#36825;&#20123;&#24405;&#38899;&#20855;&#26377;&#21487;&#21464;&#30340;&#35828;&#35805;&#20154;&#25968;&#37327;&#65292;&#23601;&#20687;&#22312;AMI&#35821;&#26009;&#24211;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;ConvTasNet&#21644;DPRNN&#35270;&#20026;&#20998;&#31163;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36755;&#20986;&#20004;&#20010;&#25110;&#19977;&#20010;&#28304;&#12290;&#20026;&#20102;&#24471;&#21040;&#35828;&#35805;&#20154;&#36776;&#35782;&#32467;&#26524;&#65292;&#23545;&#27599;&#20010;&#20272;&#35745;&#30340;&#28304;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#12290;&#22312;&#39318;&#20808;&#20351;&#29992;AMI&#23558;&#20998;&#31163;&#27169;&#22411;&#36866;&#24212;&#20110;&#30495;&#23454;&#25968;&#25454;&#21518;&#65292;&#26368;&#32456;&#27169;&#22411;&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#30340;&#24494;&#35843;&#12290;&#31995;&#32479;&#22312;&#30701;&#27573;&#19978;&#25805;&#20316;&#65292;&#20351;&#29992;&#35828;&#35805;&#20154;&#23884;&#20837;&#21644;&#22686;&#37327;&#24335; stitching &#20197;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overlapped speech is notoriously problematic for speaker diarization systems. Consequently, the use of speech separation has recently been proposed to improve their performance. Although promising, speech separation models struggle with realistic data because they are trained on simulated mixtures with a fixed number of speakers. In this work, we introduce a new speech separation-guided diarization scheme suitable for the online speaker diarization of long meeting recordings with a variable number of speakers, as present in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for the separation networks, with two or three output sources. To obtain the speaker diarization result, voice activity detection is applied on each estimated source. The final model is fine-tuned end-to-end, after first adapting the separation to real data using AMI. The system operates on short segments, and inference is performed by stitching the local predictions using speaker embeddings and increm
&lt;/p&gt;</description></item><item><title>TrackGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21253;&#25324;&#38271;&#26399;&#39044;&#27979;&#21644;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00066</link><description>&lt;p&gt;
TrackGPT -- &#29992;&#20110;&#36328;&#39046;&#22495;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00066
&lt;/p&gt;
&lt;p&gt;
TrackGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21253;&#25324;&#38271;&#26399;&#39044;&#27979;&#21644;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21830;&#19994;&#21644;&#22269;&#38450;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#65292;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#23454;&#20307;&#36712;&#36857;&#36827;&#34892;&#39044;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#33021;&#21147;&#32570;&#21475;&#12290;&#36817;&#24180;&#26469;&#65292;&#21464;&#25442;&#22120;&#21644;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#32593;&#32476;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#20960;&#20010;&#39046;&#22495;&#38761;&#21629;&#21270;&#65292;&#20854;&#20013;&#26368;&#20026;&#33879;&#21517;&#30340;&#26159;&#20351;&#29992;&#24320;&#25918;AI&#30340;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TrackGPT&#65292;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#28023;&#19978;&#21644;&#33322;&#31354;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#39044;&#35745;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#33021;&#34920;&#29616;&#20986;&#33394;&#12290;TrackGPT&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;GPT&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23454;&#20307;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#38271;&#26399;&#39044;&#27979;&#20855;&#26377;&#25345;&#32493;&#20934;&#30830;&#24615;&#21644;&#30701;&#26399;&#39044;&#27979;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;TrackGPT&#30340;&#39044;&#27979;&#33021;&#21147;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capa
&lt;/p&gt;</description></item><item><title>FengWu-GHR&#26159;&#20840;&#29699;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#30340;&#20844;&#37324;&#32423;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.00059</link><description>&lt;p&gt;
FengWu-GHR: &#23398;&#20064;&#20844;&#37324;&#32423;&#20013;&#31243;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00059
&lt;/p&gt;
&lt;p&gt;
FengWu-GHR&#26159;&#20840;&#29699;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#30340;&#20844;&#37324;&#32423;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#37324;&#32423;&#21035;&#30340;&#20840;&#29699;&#22823;&#27668;&#21160;&#21147;&#23398;&#24314;&#27169;&#21487;&#20197;&#23454;&#29616;&#31934;&#32454;&#21270;&#30340;&#22825;&#27668;&#39044;&#25253;&#65292;&#38477;&#20302;&#28798;&#23475;&#24615;&#22825;&#27668;&#21644;&#27668;&#20505;&#27963;&#21160;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#20844;&#37324;&#32423;&#20840;&#29699;&#39044;&#25253;&#27169;&#22411;&#26159;&#27668;&#35937;&#39046;&#22495;&#19968;&#30452;&#20197;&#26469;&#30340;&#36861;&#27714;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#22269;&#38469;&#31038;&#20250;&#31215;&#26497;&#21162;&#21147;&#25913;&#21892;&#25968;&#20540;&#22825;&#27668;&#27169;&#22411;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36164;&#28304;&#30340;&#28040;&#32791;&#24040;&#22823;&#65292;&#21457;&#23637;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#20540;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21033;&#29992;&#20877;&#20998;&#26512;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#21463;&#38480;&#20110;&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#29983;&#25104;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#39044;&#25253;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FengWu-GHR&#65292;&#36825;&#26159;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#12289;0.09$^{\circ}$&#27700;&#24179;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kilometer-scale modeling of global atmosphere dynamics enables fine-grained weather forecasting and decreases the risk of disastrous weather and climate activity. Therefore, building a kilometer-scale global forecast model is a persistent pursuit in the meteorology domain. Active international efforts have been made in past decades to improve the spatial resolution of numerical weather models. Nonetheless, developing the higher resolution numerical model remains a long-standing challenge due to the substantial consumption of computational resources. Recent advances in data-driven global weather forecasting models utilize reanalysis data for model training and have demonstrated comparable or even higher forecasting skills than numerical models. However, they are all limited by the resolution of reanalysis data and incapable of generating higher-resolution forecasts. This work presents FengWu-GHR, the first data-driven global weather forecasting model running at the 0.09$^{\circ}$ horizo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22522;&#22240;&#31361;&#21464;&#30340;&#23646;&#24615;&#39044;&#27979;&#20102;&#21151;&#33021;&#20007;&#22833;&#24433;&#21709;&#65292;&#20026;&#35782;&#21035;&#26377;&#23475;&#31361;&#21464;&#25552;&#20379;&#20102;&#26032;&#30340;&#25163;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.00054</link><description>&lt;p&gt;
&#39044;&#27979;&#22522;&#22240;&#31361;&#21464;&#30340;&#21151;&#33021;&#20007;&#22833;&#24433;&#21709;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting loss-of-function impact of genetic mutations: a machine learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22522;&#22240;&#31361;&#21464;&#30340;&#23646;&#24615;&#39044;&#27979;&#20102;&#21151;&#33021;&#20007;&#22833;&#24433;&#21709;&#65292;&#20026;&#35782;&#21035;&#26377;&#23475;&#31361;&#21464;&#25552;&#20379;&#20102;&#26032;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#27979;&#24207;(NGS)&#25216;&#26415;&#30340;&#21019;&#26032;&#22823;&#22823;&#38477;&#20302;&#20102;&#22522;&#22240;&#32452;&#27979;&#24207;&#30340;&#20215;&#26684;&#65292;&#38477;&#20302;&#20102;&#26410;&#26469;&#21307;&#23398;&#30740;&#31350;&#30340;&#38376;&#27099;&#65307;&#29616;&#22312;&#21487;&#20197;&#23558;&#22522;&#22240;&#32452;&#27979;&#24207;&#24212;&#29992;&#20110;&#20197;&#21069;&#25104;&#26412;&#25928;&#30410;&#20302;&#30340;&#30740;&#31350;&#20013;&#12290;&#22312;&#22823;&#37327;&#22797;&#26434;&#30340;&#39640;&#32500;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#20013;&#35782;&#21035;&#26377;&#23475;&#25110;&#33268;&#30149;&#31361;&#21464;&#23545;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#29305;&#21035;&#24863;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#22240;&#31361;&#21464;&#30340;&#23646;&#24615;&#26469;&#39044;&#27979;LoFtool&#35780;&#20998;&#65288;&#34913;&#37327;&#22522;&#22240;&#23545;&#21151;&#33021;&#20007;&#22833;&#31361;&#21464;&#30340;&#19981;&#32784;&#21463;&#24615;&#65289;&#12290;&#36825;&#20123;&#23646;&#24615;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#31361;&#21464;&#22312;&#26579;&#33394;&#20307;&#19978;&#30340;&#20301;&#32622;&#12289;&#27688;&#22522;&#37240;&#30340;&#21464;&#21270;&#20197;&#21450;&#31361;&#21464;&#24341;&#36215;&#30340;&#23494;&#30721;&#23376;&#30340;&#21464;&#21270;&#12290;&#20351;&#29992;&#21333;&#21464;&#37327;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;f-regression&#32467;&#21512;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#38543;&#26426;&#25277;&#26679;&#20849;&#35782;&#65288;RANSAC&#65289;&#12289;&#20915;&#31574;&#26641;&#12289;&#36923;&#36753;&#22238;&#24402;&#31561;&#26500;&#24314;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The innovation of next-generation sequencing (NGS) techniques has significantly reduced the price of genome sequencing, lowering barriers to future medical research; it is now feasible to apply genome sequencing to studies where it would have previously been cost-inefficient. Identifying damaging or pathogenic mutations in vast amounts of complex, high-dimensional genome sequencing data may be of particular interest to researchers. Thus, this paper's aims were to train machine learning models on the attributes of a genetic mutation to predict LoFtool scores (which measure a gene's intolerance to loss-of-function mutations). These attributes included, but were not limited to, the position of a mutation on a chromosome, changes in amino acids, and changes in codons caused by the mutation. Models were built using the univariate feature selection technique f-regression combined with K-nearest neighbors (KNN), Support Vector Machine (SVM), Random Sample Consensus (RANSAC), Decision Trees, R
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38543;&#26426;&#25277;&#26679;&#24102;&#26469;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00053</link><description>&lt;p&gt;
&#25105;&#20204;&#22312;&#28010;&#36153;&#26102;&#38388;&#21527;&#65311;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38543;&#26426;&#25277;&#26679;&#24102;&#26469;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#34913;&#37327;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#36136;&#37327;&#30340;&#26631;&#20934;&#35780;&#20272;&#21327;&#35758;&#36890;&#24120;&#21253;&#25324;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#27599;&#20010;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#20316;&#20026;&#20505;&#36873;&#38142;&#25509;&#22836;&#37096;&#25110;&#23614;&#37096;&#30340;&#36866;&#21512;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35268;&#27169;&#36739;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#36825;&#20010;&#20219;&#21153;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#23454;&#20307;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#26469;&#35780;&#20272;&#39044;&#27979;&#25110;&#24314;&#35758;&#26041;&#27861;&#30340;&#38142;&#25509;&#36136;&#37327;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#20135;&#29983;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#27491;&#30830;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#25928;&#24212;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#35770;&#35777;&#25214;&#20986;&#20102;&#20026;&#20160;&#20040;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26497;&#22823;&#22320;&#39640;&#20272;&#20102;&#26041;&#27861;&#30340;&#25490;&#21517;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#26131;/&#38590;&#24230;&#36127;&#20505;&#36873;&#32773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S
&lt;/p&gt;</description></item><item><title>PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00046</link><description>&lt;p&gt;
&#24341;&#20837;PetriRL&#65306;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;JSSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00046
&lt;/p&gt;
&lt;p&gt;
PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#36710;&#38388;&#20013;&#65292;&#20248;&#36136;&#35843;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20854;&#22312;&#24037;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Petri&#32593;&#23545;&#20316;&#19994;&#36710;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#33021;&#30452;&#25509;&#23558;&#21407;&#22987;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#65292;&#26080;&#38656;&#23545;JSSP&#23454;&#20363;&#36827;&#34892;&#39044;&#22788;&#29702;&#25104;&#38750;&#20132;&#26367;&#22270;&#12290;Petri&#32593;&#30340;&#25511;&#21046;&#33021;&#21147;&#36824;&#21487;&#20197;&#31649;&#29702;&#36807;&#31243;&#20013;&#30340;&#33258;&#21160;&#21270;&#32452;&#20214;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#19987;&#27880;&#20110;&#20851;&#38190;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#22312;&#20844;&#20849;&#27979;&#35797;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36328;&#19968;&#31995;&#21015;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65288;&#21253;&#25324;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#23398;&#20064;&#31639;&#27861;&#65289;&#36827;&#34892;&#30340;&#27604;&#36739;&#20998;&#26512;&#31361;&#20986;&#20102;&#20854;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(GPT-4)&#35757;&#32451;&#20004;&#20010;&#24494;&#22411;&#26426;&#22120;&#20154;&#28216;&#27891;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#27905;&#32479;&#19968;&#25552;&#31034;&#65292;&#25104;&#21151;&#24341;&#23548;&#36825;&#20004;&#31181;&#24494;&#22411;&#26426;&#22120;&#20154;&#25484;&#25569;&#20854;&#29305;&#33394;&#21010;&#27700;&#21160;&#20316;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00044</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24494;&#22411;&#26426;&#22120;&#20154;&#28216;&#27891;
&lt;/p&gt;
&lt;p&gt;
Training microrobots to swim by a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(GPT-4)&#35757;&#32451;&#20004;&#20010;&#24494;&#22411;&#26426;&#22120;&#20154;&#28216;&#27891;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#27905;&#32479;&#19968;&#25552;&#31034;&#65292;&#25104;&#21151;&#24341;&#23548;&#36825;&#20004;&#31181;&#24494;&#22411;&#26426;&#22120;&#20154;&#25484;&#25569;&#20854;&#29305;&#33394;&#21010;&#27700;&#21160;&#20316;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#25104;&#20026;&#35774;&#35745;&#21644;&#20248;&#21270;&#21508;&#31181;&#23610;&#24230;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#25511;&#21046;[1]&#21644;&#25351;&#23548;&#33151;&#24335;&#34892;&#36208;&#26426;&#22120;&#20154;[2]&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;LLM&#65292;GPT-4&#65292;&#26469;&#35757;&#32451;&#20004;&#20010;&#21407;&#22411;&#24494;&#22411;&#26426;&#22120;&#20154;&#22312;&#31896;&#24615;&#27969;&#20307;&#20013;&#28216;&#27891;&#12290;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20165;&#30001;&#20116;&#20010;&#21477;&#23376;&#32452;&#25104;&#30340;&#31616;&#27905;&#32479;&#19968;&#25552;&#31034;&#12290;&#30456;&#21516;&#30340;&#31616;&#27905;&#25552;&#31034;&#25104;&#21151;&#22320;&#24341;&#23548;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20851;&#33410;&#24335;&#24494;&#22411;&#26426;&#22120;&#20154;&#8212;&#8212;&#19977;&#36830;&#26438;&#28216;&#27891;&#26426;&#22120;&#20154;&#21644;&#19977;&#29699;&#28216;&#27891;&#26426;&#22120;&#20154;&#8212;&#8212;&#25484;&#25569;&#23427;&#20204;&#30340;&#29305;&#33394;&#21010;&#27700;&#21160;&#20316;&#12290;&#36825;&#20123;&#21160;&#20316;&#26368;&#21021;&#30001;&#29289;&#29702;&#23398;&#23478;&#26500;&#24605;&#65292;&#29616;&#22312;&#36890;&#36807;LLM&#24471;&#21040;&#26377;&#25928;&#30340;&#35299;&#37322;&#21644;&#24212;&#29992;&#65292;&#20351;&#24471;&#24494;&#22411;&#26426;&#22120;&#20154;&#33021;&#22815;&#20811;&#26381;&#24494;&#23567;&#36816;&#21160;&#20013;&#22266;&#26377;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#20915;&#31574;&#31574;&#30053;&#26126;&#26174;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and artificial intelligence have recently represented a popular paradigm for designing and optimizing robotic systems across various scales. Recent studies have showcased the innovative application of large language models (LLMs) in industrial control [1] and in directing legged walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two prototypical microrobots for swimming in viscous fluids. Adopting a few-shot learning approach, we develop a minimal, unified prompt composed of only five sentences. The same concise prompt successfully guides two distinct articulated microrobots -- the three-link swimmer and the three-sphere swimmer -- in mastering their signature strokes. These strokes, initially conceptualized by physicists, are now effectively interpreted and applied by the LLM, enabling the microrobots to circumvent the physical constraints inherent to micro-locomotion. Remarkably, our LLM-based decision-making strategy substantially surpasses a trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#24182;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00043</link><description>&lt;p&gt;
&#21033;&#29992;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20114;&#21160;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#24182;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#20043;&#20013;&#65292;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#26159;&#35782;&#21035;&#25925;&#38556;&#21407;&#22240;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#19978;&#65292;RCA&#26159;&#20381;&#38752;&#36807;&#31243;&#19987;&#23478;&#30693;&#35782;&#25163;&#24037;&#36827;&#34892;&#30340;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;RCA&#33021;&#22815;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;(&#22914;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;)&#22312;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#21046;&#36896;&#36807;&#31243;&#20013;&#20855;&#26377;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#37327;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;(CERs)&#12290;&#27492;&#22806;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#24573;&#30053;&#24050;&#30693;&#30340;CERs&#25110;&#23398;&#20064;&#21040;&#38169;&#35823;&#30340;CERs&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;RCA&#24037;&#20855;&#65292;&#23558;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#21046;&#36896;&#36807;&#31243;&#30340;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis (RCA) in the manufacturing of electric vehicles is the process of identifying fault causes. Traditionally, the RCA is conducted manually, relying on process expert knowledge. Meanwhile, sensor networks collect significant amounts of data in the manufacturing process. Using this data for RCA makes it more efficient. However, purely data-driven methods like Causal Bayesian Networks have problems scaling to large-scale, real-world manufacturing processes due to the vast amount of potential cause-effect relationships (CERs). Furthermore, purely data-driven methods have the potential to leave out already known CERs or to learn spurious CERs. The paper contributes by proposing an interactive and intelligent RCA tool that combines expert knowledge of an electric vehicle manufacturing process and a data-driven machine learning method. It uses reasoning over a large-scale Knowledge Graph of the manufacturing process while learning a Causal Bayesian Network. In addition, an I
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35299;&#20915;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#38656;&#27714;&#25968;&#25454;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#36335;&#24452;&#38382;&#39064;&#65292;&#28982;&#21518;&#20998;&#21035;&#35299;&#20915;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00041</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;-&#38656;&#27714;&#32858;&#31867;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35299;&#20915;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#38656;&#27714;&#25968;&#25454;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#36335;&#24452;&#38382;&#39064;&#65292;&#28982;&#21518;&#20998;&#21035;&#35299;&#20915;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#20351;&#29992;&#20998;&#35299;&#21644;&#20462;&#21098;&#31574;&#30053;&#26469;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#22823;&#35268;&#27169;&#23454;&#20363;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#20943;&#23569;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#12289;&#38382;&#39064;&#29305;&#23450;&#30340;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#21644;&#35745;&#31639;&#26426;&#30828;&#20214;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;-&#36335;&#24452;&#25913;&#36827;&#65288;DRI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#32858;&#31867;&#23558;&#23458;&#25143;&#20998;&#32452;&#12290;&#20854;&#30456;&#20284;&#24230;&#24230;&#37327;&#25351;&#26631;&#21253;&#25324;&#23458;&#25143;&#30340;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#38656;&#27714;&#25968;&#25454;&#65292;&#24182;&#19988;&#34987;&#21046;&#23450;&#25104;&#21453;&#26144;&#38382;&#39064;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#30340;&#24418;&#24335;&#12290;&#23548;&#33268;&#30340;&#23376;&#36335;&#24452;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#21512;&#36866;&#30340;&#31639;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#12290;&#25105;&#20204;&#22312;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#20043;&#38388;&#24212;&#29992;&#20462;&#21098;&#30340;&#23616;&#37096;&#25628;&#32034;&#65288;LS&#65289;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#12290;&#20462;&#21098;&#22522;&#20110;&#22312;&#20998;&#35299;&#38454;&#27573;&#33719;&#24471;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#20449;&#24687;&#12290;&#22312;&#35745;&#31639;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21442;&#25968;&#21270;&#24182;&#27604;&#36739;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several metaheuristics use decomposition and pruning strategies to solve large-scale instances of the vehicle routing problem (VRP). Those complexity reduction techniques often rely on simple, problem-specific rules. However, the growth in available data and advances in computer hardware enable data-based approaches that use machine learning (ML) to improve scalability of solution algorithms. We propose a decompose-route-improve (DRI) framework that groups customers using clustering. Its similarity metric incorporates customers' spatial, temporal, and demand data and is formulated to reflect the problem's objective function and constraints. The resulting sub-routing problems can independently be solved using any suitable algorithm. We apply pruned local search (LS) between solved subproblems to improve the overall solution. Pruning is based on customers' similarity information obtained in the decomposition phase. In a computational study, we parameterize and compare existing clustering
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;&#65292;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;98%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00038</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
Detecting Brain Tumors through Multimodal Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00038
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;&#65292;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;98%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32959;&#30244;&#21487;&#20197;&#20197;&#21508;&#31181;&#24418;&#24335;&#20986;&#29616;&#22312;&#20154;&#20307;&#30340;&#19981;&#21516;&#37096;&#20301;&#12290;&#30001;&#20110;&#33041;&#32452;&#32455;&#30340;&#22797;&#26434;&#24615;&#65292;&#33041;&#32959;&#30244;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#29305;&#21035;&#22256;&#38590;&#12290;&#21450;&#26102;&#26816;&#27979;&#32959;&#30244;&#21487;&#20197;&#38477;&#20302;&#27515;&#20129;&#39118;&#38505;&#65292;&#24182;&#20026;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#20415;&#21033;&#12290;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36890;&#36807;&#25104;&#20687;&#25216;&#26415;&#33719;&#21462;&#22270;&#20687;&#26469;&#21457;&#29616;&#21644;&#35782;&#21035;&#32959;&#30244;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#23558;&#22788;&#29702;&#25104;&#28784;&#24230;&#22270;&#20687;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#29992;&#20110;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#24182;&#19982;&#31867;&#20284;&#30740;&#31350;&#19968;&#33268;&#65292;&#27169;&#22411;&#20934;&#30830;&#29575;&#32422;&#20026;98&#65285;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#20154;&#31867;&#25511;&#21046;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tumors can manifest in various forms and in different areas of the human body. Brain tumors are specifically hard to diagnose and treat because of the complexity of the organ in which they develop. Detecting them in time can lower the chances of death and facilitate the therapy process for patients. The use of Artificial Intelligence (AI) and, more specifically, deep learning, has the potential to significantly reduce costs in terms of time and resources for the discovery and identification of tumors from images obtained through imaging techniques. This research work aims to assess the performance of a multimodal model for the classification of Magnetic Resonance Imaging (MRI) scans processed as grayscale images. The results are promising, and in line with similar works, as the model reaches an accuracy of around 98\%. We also highlight the need for explainability and transparency to ensure human control and safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#34701;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;Kronecker&#31215;(KPFF)&#23558;&#21152;&#27861;&#21644;&#36830;&#25509;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00036</link><description>&lt;p&gt;
Kronecker&#31215;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Kronecker Product Feature Fusion for Convolutional Neural Network in Remote Sensing Scene Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#34701;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;Kronecker&#31215;(KPFF)&#23558;&#21152;&#27861;&#21644;&#36830;&#25509;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#22312;&#20854;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;CNN&#21487;&#20197;&#20174;&#36965;&#24863;&#22270;&#20687;&#20013;&#25552;&#21462;&#23618;&#27425;&#30340;&#21367;&#31215;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#23618;&#30340;&#29305;&#24449;&#34701;&#21512;&#26469;&#22686;&#24378;CNN&#30340;&#24615;&#33021;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;CNN&#31639;&#27861;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#25104;&#21151;&#30340;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#21363;&#21152;&#27861;&#21644;&#36830;&#25509;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#34701;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;Kronecker&#31215;(KPFF)&#23558;&#19978;&#36848;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#35813;&#31639;&#27861;&#30456;&#20851;&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;CNN&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote Sensing Scene Classification is a challenging and valuable research topic, in which Convolutional Neural Network (CNN) has played a crucial role. CNN can extract hierarchical convolutional features from remote sensing imagery, and Feature Fusion of different layers can enhance CNN's performance. Two successful Feature Fusion methods, Add and Concat, are employed in certain state-of-the-art CNN algorithms. In this paper, we propose a novel Feature Fusion algorithm, which unifies the aforementioned methods using the Kronecker Product (KPFF), and we discuss the Backpropagation procedure associated with this algorithm. To validate the efficacy of the proposed method, a series of experiments are designed and conducted. The results demonstrate its effectiveness of enhancing CNN's accuracy in Remote sensing scene classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35768;&#22810;&#35745;&#31639;&#38382;&#39064;&#19978;&#25104;&#20026;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#33322;&#31354;&#19994;&#24076;&#26395;&#25506;&#32034;&#23427;&#20204;&#22312;&#20943;&#36731;&#39134;&#34892;&#21592;&#36127;&#25285;&#21644;&#25913;&#21892;&#36816;&#33829;&#23433;&#20840;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;DNNs&#38656;&#35201;&#36827;&#34892;&#24443;&#24213;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#36825;&#19968;&#38656;&#27714;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26469;&#35299;&#20915;&#65292;&#24418;&#24335;&#39564;&#35777;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#35777;&#26126;&#26576;&#20123;&#35823;&#21028;&#30340;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Airbus&#24403;&#21069;&#27491;&#22312;&#24320;&#21457;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;DNN&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20010;DNN&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#22122;&#22768;&#12289;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#37096;&#20998;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22810;&#27425;&#35843;&#29992;&#24213;&#23618;&#39564;&#35777;&#22120;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22242;&#38431;&#24418;&#25104;&#20043;&#21069;&#30340;&#38431;&#21592;&#25216;&#33021;&#25968;&#25454;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#22635;&#34917;&#20102;FIRST Robotics Competition&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.00031</link><description>&lt;p&gt;
FIRST Robotics Competition&#20013;&#22242;&#38431;&#32452;&#24314;&#21644;&#20896;&#20891;&#39044;&#27979;&#30340;&#32508;&#21512;&#26694;&#26550;&#65306;&#27169;&#22411;&#65292;&#31639;&#27861;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22242;&#38431;&#24418;&#25104;&#20043;&#21069;&#30340;&#38431;&#21592;&#25216;&#33021;&#25968;&#25454;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#22635;&#34917;&#20102;FIRST Robotics Competition&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#22242;&#38431;&#25104;&#21592;&#25216;&#33021;&#30340;&#25968;&#25454;&#65292;&#20248;&#21270;&#22242;&#38431;&#32452;&#24314;&#21644;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#20197;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#26368;&#22823;&#21270;&#22242;&#38431;&#25928;&#33021;&#12290;&#30446;&#21069;&#65292;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#20010;&#20307;&#25104;&#21592;&#30340;&#32454;&#31890;&#24230;&#25216;&#33021;&#32479;&#35745;&#25110;&#22242;&#38431;&#25104;&#21592;&#32452;&#21512;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#21040;&#39640;&#24230;&#32422;&#26463;&#30340;FIRST Robotics Competition&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#20801;&#35768;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#21033;&#29992;&#20808;&#21069;&#22242;&#38431;&#34920;&#29616;&#30340;&#25351;&#26631;&#26469;&#20248;&#21270;&#21644;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;FIRST Robotics&#31454;&#36187;&#30340;&#36873;&#31168;&#36807;&#31243;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#25216;&#33021;&#27599;&#24180;&#25913;&#21464;&#65292;&#22242;&#38431;&#25104;&#21592;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research work aims to develop an analytical approach for optimizing team formation and predicting team performance in a competitive environment based on data on the competitors' skills prior to the team formation. There are several approaches in scientific literature to optimize and predict a team's performance. However, most studies employ fine-grained skill statistics of the individual members or constraints such as teams with a set group of members. Currently, no research tackles the highly constrained domain of the FIRST Robotics Competition. This research effort aims to fill this gap by providing an analytical method for optimizing and predicting team performance in a competitive environment while allowing these constraints and only using metrics on previous team performance, not on each individual member's performance. We apply our method to the drafting process of the FIRST Robotics competition, a domain in which the skills change year-over-year, team members change through
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25972;&#21512;Sentinel-1&#21644;Sentinel-2&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#28304;&#21644;&#22810;&#26102;&#30456;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#27700;&#36164;&#28304;&#30417;&#27979;&#12290;&#24212;&#29992;&#22303;&#22756;&#21547;&#27700;&#25351;&#25968;&#21644;&#24402;&#19968;&#21270;&#24046;&#24322;&#27700;&#20307;&#25351;&#25968;&#31561;&#25351;&#26631;&#20197;&#21450;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00023</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26102;&#30456;Sentinel-1&#21644;Sentinel-2&#25968;&#25454;&#36827;&#34892;&#27700;&#20307;&#27979;&#32472;
&lt;/p&gt;
&lt;p&gt;
Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25972;&#21512;Sentinel-1&#21644;Sentinel-2&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#28304;&#21644;&#22810;&#26102;&#30456;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#27700;&#36164;&#28304;&#30417;&#27979;&#12290;&#24212;&#29992;&#22303;&#22756;&#21547;&#27700;&#25351;&#25968;&#21644;&#24402;&#19968;&#21270;&#24046;&#24322;&#27700;&#20307;&#25351;&#25968;&#31561;&#25351;&#26631;&#20197;&#21450;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#21152;&#21095;&#20102;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#65292;&#23548;&#33268;&#27700;&#36164;&#28304;&#21294;&#20047;&#21644;&#38477;&#38632;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#23545;&#21487;&#25345;&#32493;&#21457;&#23637;&#12289;&#29983;&#29289;&#22810;&#26679;&#24615;&#20197;&#21450;&#27700;&#21644;&#21355;&#29983;&#35774;&#26045;&#30340;&#21487;&#21450;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#22312;&#21508;&#31181;&#27668;&#35937;&#26465;&#20214;&#19979;&#36827;&#34892;&#20840;&#38754;&#27700;&#36164;&#28304;&#30417;&#27979;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25552;&#20986;&#20102;&#25193;&#23637;SEN2DWATER&#25968;&#25454;&#38598;&#20197;&#22686;&#24378;&#20854;&#23545;&#27700;&#30406;&#22320;&#20998;&#21106;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;Sentinel-1&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#23545;&#40784;&#30340;&#38647;&#36798;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#22810;&#20809;&#35889;Sentinel-2&#25968;&#25454;&#38598;&#38598;&#25104;&#65292;&#29983;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#28304;&#21644;&#22810;&#26102;&#30456;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24212;&#29992;&#22303;&#22756;&#21547;&#27700;&#25351;&#25968;&#65288;SWI&#65289;&#21644;&#24402;&#19968;&#21270;&#24046;&#24322;&#27700;&#20307;&#25351;&#25968;&#65288;NDWI&#65289;&#20197;&#21450;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;K&#22343;&#20540;&#32858;&#31867;&#65289;&#12290;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26410;&#26469;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is intensifying extreme weather events, causing both water scarcity and severe rainfall unpredictability, and posing threats to sustainable development, biodiversity, and access to water and sanitation. This paper aims to provide valuable insights for comprehensive water resource monitoring under diverse meteorological conditions. An extension of the SEN2DWATER dataset is proposed to enhance its capabilities for water basin segmentation. Through the integration of temporally and spatially aligned radar information from Sentinel-1 data with the existing multispectral Sentinel-2 data, a novel multisource and multitemporal dataset is generated. Benchmarking the enhanced dataset involves the application of indices such as the Soil Water Index (SWI) and Normalized Difference Water Index (NDWI), along with an unsupervised Machine Learning (ML) classifier (k-means clustering). Promising results are obtained and potential future developments and applications arising from this re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#30340;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#20248;&#21270;&#20102;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#33647;&#29289;&#26679;&#24615;&#20272;&#35745;&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;30%&#12290;</title><link>https://arxiv.org/abs/2402.00014</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#23567;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum cycle generative adversarial network for small molecule generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#30340;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#20248;&#21270;&#20102;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#33647;&#29289;&#26679;&#24615;&#20272;&#35745;&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#26469;&#24320;&#21457;&#39318;&#27425;&#36827;&#20837;&#24066;&#22330;&#30340;&#27599;&#20010;&#26032;&#21270;&#21512;&#29289;&#12290;&#29983;&#25104;&#23567;&#20998;&#23376;&#26159;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23545;&#20110;&#24320;&#21457;&#21019;&#26032;&#30340;&#21046;&#33647;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;&#29420;&#29305;&#24615;&#12289;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33647;&#29289;&#26679;&#24615;&#12289;&#21487;&#21512;&#25104;&#24615;&#21644;&#28342;&#35299;&#24230;&#20998;&#23376;&#33647;&#20195;&#21160;&#21147;&#23398;&#24615;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20960;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#24037;&#31243;&#25972;&#21512;&#21040;&#24050;&#30693;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#12290;&#24341;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20102;&#20197;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#33647;&#29289;&#35774;&#35745;&#25968;&#25454;&#38598;QM9&#21644;PC9&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#24341;&#20837;&#30340;&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#35780;&#20998;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26032;&#30340;&#35780;&#20998;&#34920;&#26126;&#33647;&#29289;&#26679;&#24615;&#23450;&#37327;&#20272;&#35745;&#22686;&#21152;&#20102;&#26368;&#22810;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
The contemporary drug design process demands considerable time and resources to develop each new compound entering the market. Generating small molecules is a pivotal aspect of drug discovery, essential for developing innovative pharmaceuticals. Uniqueness, validity, diversity, druglikeliness, synthesizability, and solubility molecular pharmacokinetic properties, however, are yet to be maximized. This work introduces several new generative adversarial network models based on engineering integration of parametrized quantum circuits into known molecular generative adversarial networks. The introduced machine learning models incorporate a new multi-parameter reward function grounded in reinforcement learning principles. Through extensive experimentation on benchmark drug design datasets, QM9 and PC9, the introduced models are shown to outperform scores achieved previously. Most prominently, the new scores indicate an increase of up to 30% in the druglikeness quantitative estimation. The n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;EPC&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#36716;&#21521;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20182;&#20204;&#26681;&#25454;&#33258;&#36523;&#19994;&#21153;&#25112;&#30053;&#21644;&#36164;&#28304;&#29366;&#20917;&#65292;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#33719;&#21462;AI&#25216;&#26415;&#12290;&#36825;&#26159;&#22522;&#20110;&#20840;&#29699;&#26368;&#22823;&#30340;EPC&#25215;&#21253;&#21830;&#20043;&#19968;&#22312;&#22522;&#20110;AI&#30340;&#20135;&#21697;&#24320;&#21457;&#39033;&#30446;&#25191;&#34892;&#20013;&#30340;&#32463;&#39564;&#21644;&#24050;&#23558;AI&#25972;&#21512;&#21040;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;EPC&#20379;&#24212;&#21830;&#20844;&#21496;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.00011</link><description>&lt;p&gt;
&#22312;&#24037;&#31243;&#20844;&#21496;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;AI&#25972;&#21512;&#36335;&#24452;&#65306;&#25112;&#30053;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Choosing the Right Path for AI Integration in Engineering Companies: A Strategic Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;EPC&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#36716;&#21521;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20182;&#20204;&#26681;&#25454;&#33258;&#36523;&#19994;&#21153;&#25112;&#30053;&#21644;&#36164;&#28304;&#29366;&#20917;&#65292;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#33719;&#21462;AI&#25216;&#26415;&#12290;&#36825;&#26159;&#22522;&#20110;&#20840;&#29699;&#26368;&#22823;&#30340;EPC&#25215;&#21253;&#21830;&#20043;&#19968;&#22312;&#22522;&#20110;AI&#30340;&#20135;&#21697;&#24320;&#21457;&#39033;&#30446;&#25191;&#34892;&#20013;&#30340;&#32463;&#39564;&#21644;&#24050;&#23558;AI&#25972;&#21512;&#21040;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;EPC&#20379;&#24212;&#21830;&#20844;&#21496;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#28304;&#39046;&#22495;&#30340;&#24037;&#31243;&#37319;&#36141;&#24314;&#35774;&#65288;EPC&#65289;&#19994;&#21153;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#34987;&#35748;&#21516;&#12290;&#35768;&#22810;EPC&#20844;&#21496;&#21450;&#20854;&#23458;&#25143;&#24050;&#24847;&#35782;&#21040;&#24212;&#29992;AI&#23545;&#19994;&#21153;&#36827;&#34892;&#33258;&#21160;&#21270;&#12289;&#25552;&#39640;&#29983;&#20135;&#21147;&#24182;&#31616;&#21270;&#26410;&#26469;&#25805;&#20316;&#30340;&#22909;&#22788;&#65292;&#20197;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#34892;&#19994;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#24403;&#21069;&#30340;AI&#24066;&#22330;&#25552;&#20379;&#20102;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#21644;&#26381;&#21153;&#26469;&#25903;&#25345;&#36825;&#20010;&#34892;&#19994;&#65292;&#20294;&#32452;&#32455;&#24517;&#39035;&#26681;&#25454;&#20854;&#19994;&#21153;&#25112;&#30053;&#21644;&#21487;&#29992;&#36164;&#28304;&#30340;&#24773;&#20917;&#65292;&#20102;&#35299;&#22914;&#20309;&#20197;&#26368;&#26377;&#30410;&#30340;&#26041;&#24335;&#33719;&#21462;AI&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;EPC&#20844;&#21496;&#30340;&#36716;&#22411;AI&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#20840;&#29699;&#26368;&#22823;&#30340;EPC&#25215;&#21253;&#21830;&#20043;&#19968;&#30340;&#22522;&#20110;AI&#30340;&#20135;&#21697;&#24320;&#21457;&#39033;&#30446;&#25191;&#34892;&#30340;&#26696;&#20363;&#65292;&#24182;&#32467;&#21512;&#24050;&#32463;&#23558;AI&#25972;&#21512;&#21040;&#20854;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;EPC&#20379;&#24212;&#21830;&#20844;&#21496;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#28085;&#30422;&#20102;&#25972;&#20010;li&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Engineering, Procurement and Construction (EPC) businesses operating within the energy sector are recognizing the increasing importance of Artificial Intelligence (AI). Many EPC companies and their clients have realized the benefits of applying AI to their businesses in order to reduce manual work, drive productivity, and streamline future operations of engineered installations in a highly competitive industry. The current AI market offers various solutions and services to support this industry, but organizations must understand how to acquire AI technology in the most beneficial way based on their business strategy and available resources. This paper presents a framework for EPC companies in their transformation towards AI. Our work is based on examples of project execution of AI-based products development at one of the biggest EPC contractors worldwide and on insights from EPC vendor companies already integrating AI into their engineering solutions. The paper covers the entire li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07359</link><description>&lt;p&gt;
&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reliability and Interpretability in Science and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#26085;&#30410;&#37325;&#35201;&#65292;&#24182;&#19988;&#19982;&#27492;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#24050;&#32463;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#20165;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#23545;DNN&#27169;&#22411;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#30340;&#21487;&#33021;&#24046;&#24322;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#26356;&#28145;&#23618;&#27425;&#30340;&#35748;&#35782;&#35770;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#20551;&#35774;&#65288;&#22312;ML&#21644;&#20256;&#32479;&#31185;&#23398;&#20013;&#22343;&#23384;&#22312;&#65289;&#22312;&#26080;&#29702;&#35770;&#31185;&#23398;&#30340;&#38169;&#35273;&#19979;&#30340;&#26222;&#36941;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#20174;&#65288;&#35748;&#35782;&#35770;&#30340;&#65289;&#22797;&#26434;&#24615;&#35282;&#24230;&#20998;&#26512;&#20102;&#27169;&#22411;&#20551;&#35774;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#20551;&#35774;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.17429</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense for Zero-Shot Natural Language Video Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35270;&#39057;&#29255;&#27573;&#21644;&#20266;&#26597;&#35810;&#27880;&#37322;&#65292;&#22312;&#20165;&#29992;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#35757;&#32451;NLVL&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26597;&#35810;&#32463;&#24120;&#32570;&#20047;&#23545;&#28304;&#35270;&#39057;&#30340;&#25166;&#23454;&#22522;&#30784;&#65292;&#23548;&#33268;&#20869;&#23481;&#19981;&#32467;&#26500;&#21270;&#21644;&#19981;&#36830;&#36143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;NLVL&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORONET&#65292;&#19968;&#20010;&#38646;&#26679;&#26412;NLVL&#26694;&#26550;&#65292;&#36890;&#36807;&#24120;&#35782;&#22686;&#24378;&#27169;&#22359;&#26725;&#25509;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;CORONET&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#32534;&#30721;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30340;&#24120;&#35782;&#20449;&#24687;&#65292;&#26465;&#20214;&#26159;&#35270;&#39057;&#65292;&#20197;&#21450;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#32534;&#30721;&#35270;&#39057;&#21644;&#20266;&#26597;&#35810;&#34920;&#31034;&#20197;&#36827;&#34892;&#23450;&#20301;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2312.12839</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20132;&#24182;&#30340;&#27867;&#28145;&#24230;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Machine Learning Algorithms by Union-Free Generic Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#27010;&#24565;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#32447;&#24615;&#31354;&#38388;&#21644;&#24230;&#37327;&#31354;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#28145;&#20837;&#65292;&#20294;&#20851;&#20110;&#20559;&#24207;&#38598;&#21512;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#20960;&#20046;&#27809;&#26377;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#30340;&#33879;&#21517;&#31616;&#21333;&#28145;&#24230;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;ufg&#28145;&#24230;&#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#25351;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#23545;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#22120;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#19981;&#21516;&#20998;&#26512;&#26041;&#27861;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28909;&#28872;&#35752;&#35770;&#22686;&#28155;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.
&lt;/p&gt;</description></item><item><title>EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;</title><link>https://arxiv.org/abs/2312.04916</link><description>&lt;p&gt;
EE-LLM: &#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#30340;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04916
&lt;/p&gt;
&lt;p&gt;
EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EE-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21021;&#27493;&#35777;&#26126;&#20102;&#26089;&#36864;&#20986;&#22312;&#21152;&#36895;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;EE-LLM&#36890;&#36807;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#24182;&#34892;&#24615;&#26469;&#25512;&#21160;&#26089;&#36864;&#20986;LLM&#30340;&#35268;&#27169;&#21270;&#12290;&#22522;&#20110;Megatron-LM&#26500;&#24314;&#30340;EE-LLM&#23454;&#29616;&#20102;&#21508;&#31181;&#31639;&#27861;&#21019;&#26032;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#26089;&#36864;&#20986;&#65292;&#21253;&#25324;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#27700;&#32447;&#24182;&#34892;&#24615;&#20419;&#36827;&#26089;&#36864;&#20986;&#35757;&#32451;&#30446;&#26631;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21033;&#29992;&#21407;&#22987;&#27969;&#27700;&#32447;&#35843;&#24230;&#20013;&#30340;&#31354;&#38386;&#36164;&#28304;&#36827;&#34892;&#19982;&#26089;&#36864;&#20986;&#23618;&#30456;&#20851;&#30340;&#35745;&#31639;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#20004;&#31181;&#19982;KV&#32531;&#23384;&#20860;&#23481;&#30340;&#26089;&#36864;&#20986;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#24573;&#30053;&#30340;&#35745;&#31639;&#24320;&#38144;&#30456;&#27604;&#65292;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
&lt;/p&gt;</description></item><item><title>RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2312.01057</link><description>&lt;p&gt;
RLHF&#21644;IIA&#65306;&#20498;&#32622;&#28608;&#21169;
&lt;/p&gt;
&lt;p&gt;
RLHF and IIA: Perverse Incentives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01057
&lt;/p&gt;
&lt;p&gt;
RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RLHF&#65289;&#21487;&#20197;&#28608;&#21169;&#19982;&#20559;&#22909;&#19981;&#31526;&#30340;&#22238;&#24212;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#20551;&#35774;&#26080;&#20851;&#27010;&#25324;&#30340;&#27169;&#22411;&#65288;IIA&#65289;&#12290;IIA&#24341;&#21457;&#30340;&#20498;&#32622;&#28608;&#21169;&#38459;&#30861;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#21644;&#21442;&#25968;&#23376;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.18826</link><description>&lt;p&gt;
&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#22312;&#26368;&#20248;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#21644;&#21442;&#25968;&#23376;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#19982;&#21442;&#25968;&#23376;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#23545;&#20960;&#20309;&#25935;&#24863;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;TMLE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;CNFs&#25913;&#36827;TMLE&#65292;&#20248;&#21270;Cram\'er-Rao&#30028;&#38480;&#65292;&#24182;&#20174;&#39044;&#23450;&#20041;&#20998;&#24067;$p_0$&#36807;&#28193;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#24067;$p_1$&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#26032;&#22320;&#23558;Wasserstein&#26799;&#24230;&#27969;&#23884;&#20837;&#21040;Fokker-Planck&#26041;&#31243;&#20013;&#65292;&#20174;&#32780;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#28155;&#21152;&#20102;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;CNFs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a groundbreaking approach to causal inference by integrating continuous normalizing flows (CNFs) with parametric submodels, enhancing their geometric sensitivity and improving upon traditional Targeted Maximum Likelihood Estimation (TMLE). Our method employs CNFs to refine TMLE, optimizing the Cram\'er-Rao bound and transitioning from a predefined distribution $p_0$ to a data-driven distribution $p_1$. We innovate further by embedding Wasserstein gradient flows within Fokker-Planck equations, thus imposing geometric structures that boost the robustness of CNFs, particularly in optimal transport theory.   Our approach addresses the disparity between sample and population distributions, a critical factor in parameter estimation bias. We leverage optimal transport and Wasserstein gradient flows to develop causal inference methodologies with minimal variance in finite-sample settings, outperforming traditional methods like TMLE and AIPW. This novel framework, centered o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;&#23454;&#29616;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20559;&#32622;&#20027;&#35201;&#26469;&#33258;&#20110;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#21644;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2311.02546</link><description>&lt;p&gt;
&#20851;&#20110;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Second-Order Convergence of Biased Policy Gradient Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;&#23454;&#29616;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20559;&#32622;&#20027;&#35201;&#26469;&#33258;&#20110;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#21644;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#26159;&#39640;&#24230;&#38750;&#20984;&#30340;&#65292;&#22240;&#27492;&#24076;&#26395;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#33021;&#22815;&#33073;&#31163;&#38797;&#28857;&#24182;&#36798;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#21482;&#32771;&#34385;&#20102;&#24102;&#26377;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26222;&#36890;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20294;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#22238;&#25253;&#35774;&#32622;&#19979;&#65292;&#23454;&#38469;&#23454;&#29616;&#26159;&#26377;&#20559;&#30340;&#65292;&#22240;&#20026;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35780;&#35770;&#23478;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#35780;&#35770;&#23478;-&#28436;&#21592;&#26041;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#20063;&#26410;&#34987;&#35777;&#23454;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26377;&#20559;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#26032;&#39062;&#30340;&#20108;&#38454;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#35745;&#31639;&#24471;&#21040;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#22312;&#20869;&#24490;&#29615;&#20013;&#65292;&#35780;&#35770;&#23478;&#36890;&#36807;TD(0)&#23398;&#20064;&#25913;&#36827;&#20102;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;TD(0)&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)
&lt;/p&gt;</description></item><item><title>AMAGO&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#20102;&#27867;&#21270;&#12289;&#38271;&#26399;&#23384;&#20648;&#21644;&#20803;&#23398;&#20064;&#31561;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20102;&#38271;&#24207;&#21015;Transformer&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.09971</link><description>&lt;p&gt;
AMAGO&#65306;&#21487;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#36866;&#24212;&#24615;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09971
&lt;/p&gt;
&lt;p&gt;
AMAGO&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#20102;&#27867;&#21270;&#12289;&#38271;&#26399;&#23384;&#20648;&#21644;&#20803;&#23398;&#20064;&#31561;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20102;&#38271;&#24207;&#21015;Transformer&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AMAGO&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#65292;&#23427;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#26469;&#35299;&#20915;&#27867;&#21270;&#12289;&#38271;&#26399;&#23384;&#20648;&#21644;&#20803;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#23398;&#20064;&#21487;&#20197;&#20351;&#20855;&#26377;&#24490;&#29615;&#31574;&#30053;&#30340;&#19978;&#19979;&#25991;RL&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35843;&#25972;&#65292;&#24182;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#20869;&#23384;&#23481;&#37327;&#12289;&#35268;&#21010;&#33539;&#22260;&#21644;&#27169;&#22411;&#22823;&#23567;&#19978;&#21019;&#24314;&#20851;&#38190;&#29942;&#39048;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;AMAGO&#37325;&#26032;&#23457;&#35270;&#21644;&#37325;&#26032;&#35774;&#35745;&#20102;&#31163;&#32447;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#22312;RL&#30340;&#25972;&#20010;&#23637;&#24320;&#36807;&#31243;&#20013;&#24182;&#34892;&#22320;&#20351;&#29992;&#38271;&#24207;&#21015;Transformer&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20803;-RL&#21644;&#38271;&#26399;&#23384;&#20648;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;AMAGO&#23545;&#31232;&#30095;&#22870;&#21169;&#21644;&#31163;&#32447;&#25968;&#25454;&#30340;&#20851;&#27880;&#36824;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#25193;&#23637;&#21040;&#20855;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25506;&#32034;&#30446;&#26631;&#38382;&#39064;&#12290;&#24403;&#19982;&#22810;&#30446;&#26631;&#24310;&#36831;&#37325;&#26032;&#26631;&#35760;&#26041;&#26696;&#30456;&#32467;&#21512;&#26102;&#65292;AMAGO&#21487;&#20197;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can sol
&lt;/p&gt;</description></item><item><title>InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.07713</link><description>&lt;p&gt;
InstructRetro: &#26816;&#32034;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07713
&lt;/p&gt;
&lt;p&gt;
InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#23545;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22256;&#24785;&#24230;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35268;&#27169;&#20173;&#28982;&#26377;&#38480;&#65288;&#22914;Retro&#20855;&#26377;75&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#25351;&#20196;&#35843;&#20248;&#21644;&#38646;&#26679;&#20363;&#27867;&#21270;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Retro 48B&#65292;&#36825;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#25216;&#26415;&#20174;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#20013;&#32487;&#32493;&#39044;&#35757;&#32451;&#19968;&#20010;43B&#30340;GPT&#27169;&#22411;&#65292;&#24182;&#20511;&#21161;Retro&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;4800&#20159;&#20010;&#21442;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#22522;&#30784;&#27169;&#22411;Retro 48B&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20165;&#20351;&#29992;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#30340;43B GPT&#27169;&#22411;&#65292;&#19988;&#21482;&#22686;&#21152;&#20102;2.58%&#30340;GPU&#20351;&#29992;&#26102;&#38388;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25193;&#23637;&#28508;&#21147;&#12290;&#22312;&#23545;Retro&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#21518;&#65292;InstructRetro&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.05866</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative quantum machine learning via denoising diffusion probabilistic models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#32467;&#26500;&#28789;&#27963;&#12289;&#35757;&#32451;&#31616;&#21333;&#30340;&#29305;&#28857;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#32416;&#32544;&#21644;&#21472;&#21152;&#30340;&#33021;&#21147;&#20026;&#23398;&#20064;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#21463;&#32463;&#20856;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;QuDDPM&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#12290;QuDDPM&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#26469;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#22122;&#22768;&#20043;&#38388;&#30340;&#25554;&#20540;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#35823;&#24046;&#30340;&#19978;&#30028;&#21644;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#21152;&#36895;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#26500;&#35937;&#12290;</title><link>https://arxiv.org/abs/2310.04915</link><description>&lt;p&gt;
&#20851;&#20110;&#21152;&#36895;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#21152;&#36895;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#20855;&#26377;&#25968;&#21315;&#20010;&#26356;&#26032;&#27493;&#39588;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22914;&#20309;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#26126;&#30830;&#26377;&#25928;&#22320;&#21152;&#36895;&#36825;&#20010;&#36807;&#31243;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#36825;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#24341;&#36215;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#31995;&#32479;&#30740;&#31350;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#25237;&#24433;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#21457;&#23637;&#20102;&#26356;&#31934;&#30830;&#30340;SE(3)&#20869;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#23558;&#36229;&#21442;&#25968;&#19982;&#36825;&#20123;&#35823;&#24046;&#32852;&#31995;&#36215;&#26469;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#29983;&#25104;&#20998;&#23376;&#26500;&#35937;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world. In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space. Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#30740;&#31350;&#38754;&#21521;&#32593;&#32476;&#25968;&#25454;&#25366;&#25496;&#30340;&#36328;&#34920;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#26174;&#31034;&#20986;&#23545;&#25366;&#25496;&#32593;&#32476;&#34920;&#26684;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2307.04308</link><description>&lt;p&gt;
&#38754;&#21521;&#32593;&#32476;&#25968;&#25454;&#25366;&#25496;&#30340;&#36328;&#34920;&#25513;&#30721;&#39044;&#35757;&#32451;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Cross-Table Masked Pretraining for Web Data Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#30740;&#31350;&#38754;&#21521;&#32593;&#32476;&#25968;&#25454;&#25366;&#25496;&#30340;&#36328;&#34920;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#26174;&#31034;&#20986;&#23545;&#25366;&#25496;&#32593;&#32476;&#34920;&#26684;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#22312;&#20840;&#29699;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#22312;&#25903;&#25745;&#22312;&#32447;&#20449;&#24687;&#30340;&#25968;&#23383;&#26550;&#26500;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#37492;&#20110;ChatGPT&#21644;SAM&#31561;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24433;&#21709;&#21147;&#65292;&#25506;&#32034;&#23558;&#39044;&#35757;&#32451;&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#34920;&#26684;&#25968;&#25454;&#25366;&#25496;&#24050;&#25104;&#20026;&#19968;&#20010;&#26497;&#20855;&#28508;&#21147;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#37117;&#23616;&#38480;&#20110;&#22266;&#23450;&#27169;&#24335;/&#21333;&#34920;&#30340;&#33539;&#22260;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#20808;&#21069;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36824;&#27809;&#26377;&#36798;&#21040;&#8220;BERT&#26102;&#21051;&#8221;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#26126;&#26174;&#28382;&#21518;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#30456;&#24212;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#32972;&#21518;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#20811;&#26381;&#36328;&#34920;&#38556;&#30861;&#12290;&#20316;&#20026;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#21162;&#21147;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#36328;&#34920;&#39044;&#35757;&#32451;&#26469;&#25366;&#25496;&#32593;&#32476;&#34920;&#26684;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data pervades the landscape of the World Wide Web, playing a foundational role in the digital architecture that underpins online information. Given the recent influence of large-scale pretrained models like ChatGPT and SAM across various domains, exploring the application of pretraining techniques for mining tabular data on the web has emerged as a highly promising research direction. Indeed, there have been some recent works around this topic where most (if not all) of them are limited in the scope of a fixed-schema/single table. Due to the scale of the dataset and the parameter size of the prior models, we believe that we have not reached the ''BERT moment'' for the ubiquitous tabular data. The development on this line significantly lags behind the counterpart research domains such as natural language processing. In this work, we first identify the crucial challenges behind tabular data pretraining, particularly overcoming the cross-table hurdle. As a pioneering endeavor, thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#20013;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#20107;&#20214;&#35302;&#21457;&#22120;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#21319;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#24615;&#33021;&#12290;&#22312;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#32500;&#22522;&#30334;&#31185;&#21040;&#26032;&#38395;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#25928;&#26524;&#26174;&#33879;&#12290;&#21516;&#26102;&#32467;&#21512;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#33021;&#36827;&#19968;&#27493;&#22686;&#24378;&#36716;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2305.14163</link><description>&lt;p&gt;
&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#26469;&#22686;&#24378;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#20013;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#20107;&#20214;&#35302;&#21457;&#22120;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#21319;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#24615;&#33021;&#12290;&#22312;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#32500;&#22522;&#30334;&#31185;&#21040;&#26032;&#38395;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#25928;&#26524;&#26174;&#33879;&#12290;&#21516;&#26102;&#32467;&#21512;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#33021;&#36827;&#19968;&#27493;&#22686;&#24378;&#36716;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#20851;&#38190;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#32500;&#22522;&#30334;&#31185;&#25110;&#26032;&#38395;&#12290;&#35813;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#20110;&#35302;&#21457;&#35782;&#21035;&#65288;TD&#65289;&#8212;&#35782;&#21035;&#25991;&#26412;&#20013;&#24341;&#36215;&#29305;&#23450;&#20107;&#20214;&#30340;&#26631;&#35760;&#33539;&#22260;&#12290;&#23613;&#31649;&#35302;&#21457;&#22120;&#30340;&#27010;&#24565;&#24212;&#29702;&#24819;&#22320;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65292;&#20294;&#20174;&#39640;&#36164;&#28304;&#39046;&#22495;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;TD&#39046;&#22495;&#36716;&#31227;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#31995;&#32479;&#33719;&#21462;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#35302;&#21457;&#22120;&#22312;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32806;&#21512;&#26469;&#35299;&#20915;TD&#20013;&#30340;&#36127;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#27880;&#20837;&#30340;OIE&#20851;&#31995;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#22686;&#24378;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;TD&#39046;&#22495;&#36716;&#31227;&#65292;&#24182;&#20943;&#23569;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#20174;&#39640;&#36164;&#28304;&#28304;&#39046;&#22495;&#65288;&#32500;&#22522;&#30334;&#31185;&#65289;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#30446;&#26631;&#39046;&#22495;&#65288;&#26032;&#38395;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25913;&#36827;&#30340;&#36716;&#31227;&#19982;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) -- identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#21464;&#37327;&#38598;&#21512;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#36755;&#20986;&#22810;&#21464;&#37327;&#39044;&#27979;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#20004;&#27493;&#26041;&#27861;&#38590;&#20197;&#21253;&#21547;&#38468;&#21152;&#39044;&#27979;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2211.01345</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22810;&#21464;&#37327;&#38598;&#21512;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generative machine learning methods for multivariate ensemble post-processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#21464;&#37327;&#38598;&#21512;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#36755;&#20986;&#22810;&#21464;&#37327;&#39044;&#27979;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#20004;&#27493;&#26041;&#27861;&#38590;&#20197;&#21253;&#21547;&#38468;&#21152;&#39044;&#27979;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#27425;&#36816;&#34892;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#38598;&#21512;&#22825;&#27668;&#39044;&#25253;&#36890;&#24120;&#20250;&#26174;&#31034;&#31995;&#32479;&#35823;&#24046;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#21518;&#22788;&#29702;&#25165;&#33021;&#33719;&#24471;&#21487;&#38752;&#30340;&#39044;&#25253;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#20934;&#30830;&#24314;&#27169;&#22810;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#21508;&#31181;&#22810;&#21464;&#37327;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#38598;&#21512;&#39044;&#27979;&#39318;&#20808;&#22312;&#27599;&#20010;&#36793;&#38469;&#19978;&#20998;&#21035;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;copulas&#24674;&#22797;&#22810;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20123;&#20004;&#27493;&#26041;&#27861;&#23384;&#22312;&#20849;&#21516;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#20381;&#36182;&#20851;&#31995;&#26102;&#38590;&#20197;&#21253;&#21547;&#38468;&#21152;&#30340;&#39044;&#27979;&#21464;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#21019;&#26032;&#22810;&#21464;&#37327;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#24067;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#22810;&#21464;&#37327;&#39044;&#27979;&#20998;&#24067;&#30340;&#26679;&#26412;&#30452;&#25509;&#20316;&#20026;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#24471;&#21040;&#12290;&#36890;&#36807;&#20248;&#21270;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require post-processing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate post-processing have been proposed where ensemble predictions are first post-processed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate post-processing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25193;&#23637;&#21387;&#32553;&#26080;&#39321;&#20811;&#29305;&#21464;&#25442;&#30340;&#29366;&#24577;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#26102;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2209.12644</link><description>&lt;p&gt;
FORESEE:&#20351;&#29992;&#25193;&#23637;&#21387;&#32553;&#26080;&#39321;&#20811;&#29305;&#21464;&#25442;&#36827;&#34892;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FORESEE: Prediction with Expansion-Compression Unscented Transform for Online Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.12644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25193;&#23637;&#21387;&#32553;&#26080;&#39321;&#20811;&#29305;&#21464;&#25442;&#30340;&#29366;&#24577;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#26102;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19981;&#30830;&#23450;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#20256;&#25773;&#29366;&#24577;&#20998;&#24067;&#34987;&#35748;&#20026;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#36890;&#24120;&#20250;&#24341;&#21457;&#25968;&#20540;&#25110;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#25193;&#23637;&#21387;&#32553;&#26080;&#39321;&#20811;&#29305;&#21464;&#25442;&#30340;&#29366;&#24577;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#19968;&#20010;&#29366;&#24577;&#30456;&#20851;&#30340;&#20998;&#24067;&#20256;&#25773;&#26377;&#38480;&#25968;&#37327;&#30340;&#35199;&#26684;&#29595;&#28857;&#65292;&#36825;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#37117;&#38656;&#35201;&#22686;&#21152;&#35199;&#26684;&#29595;&#28857;&#30340;&#25968;&#37327;&#26469;&#34920;&#31034;&#29983;&#25104;&#30340;&#20998;&#24067;&#65307;&#36825;&#23601;&#26159;&#25105;&#20204;&#25152;&#35859;&#30340;&#25193;&#23637;&#25805;&#20316;&#12290;&#20026;&#20102;&#20445;&#25345;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#23558;&#25193;&#23637;&#25805;&#20316;&#19982;&#22522;&#20110;&#30697;&#21305;&#37197;&#30340;&#21387;&#32553;&#25805;&#20316;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20445;&#25345;&#35199;&#26684;&#29595;&#28857;&#30340;&#25968;&#37327;&#24658;&#23450;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#24403;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29366;&#24577;&#21644;&#25511;&#21046;&#36755;&#20837;&#32422;&#26463;&#19979;&#65292;&#29366;&#24577;&#39044;&#27979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propagating state distributions through a generic, uncertain nonlinear dynamical model is known to be intractable and usually begets numerical or analytical approximations. We introduce a method for state prediction, called the Expansion-Compression Unscented Transform, and use it to solve a class of online policy optimization problems. Our proposed algorithm propagates a finite number of sigma points through a state-dependent distribution, which dictates an increase in the number of sigma points at each time step to represent the resulting distribution; this is what we call the expansion operation. To keep the algorithm scalable, we augment the expansion operation with a compression operation based on moment matching, thereby keeping the number of sigma points constant across predictions over multiple time steps. Its performance is empirically shown to be comparable to Monte Carlo but at a much lower computational cost. Under state and control input constraints, the state prediction i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#23454;&#29616;&#21160;&#24577;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2207.07624</link><description>&lt;p&gt;
&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Latent Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.07624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#23454;&#29616;&#21160;&#24577;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#24230;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#36866;&#24212;&#20854;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#35748;&#35782;&#21040;&#35774;&#22791;&#30340;&#25968;&#25454;&#24456;&#21487;&#33021;&#26469;&#33258;&#21253;&#21547;&#28151;&#21512;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#30456;&#20851;&#21644;&#39046;&#22495;&#19981;&#30456;&#20851;&#31034;&#20363;&#30340;&#22810;&#20010;&#28508;&#22312;&#39046;&#22495;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#36793;&#32536;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30446;&#26631;&#26159;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197;&#21069;&#39304;&#26041;&#24335;&#36827;&#34892;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#12290;&#24314;&#27169;&#36825;&#20123;&#29616;&#23454;&#32422;&#26463;&#23558;&#25105;&#20204;&#24102;&#21040;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#39062;&#21644;&#23454;&#29992;&#37325;&#35201;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20803;&#23398;&#20064;&#19968;&#20010;&#32593;&#32476;&#65292;&#33021;&#22815;&#23558;&#28151;&#21512;&#30456;&#20851;&#30446;&#26631;&#25968;&#25454;&#38598;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#21160;&#24577;&#22320;&#36866;&#24212;&#30446;&#26631;&#31034;&#20363;&#30340;&#25512;&#29702;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#30456;&#23545;&#20110;&#24378;ERM&#22522;&#32447;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. Recognizing that device's data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. Considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. Modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. Our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. The resulting framework leads to consistent improvements over strong ERM baselines. We also 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2206.14397</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Learning in Healthcare: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.14397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#30340;&#25968;&#23383;&#21270;&#19982;&#35745;&#31639;&#33021;&#21147;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#21152;&#21095;&#25110;&#29978;&#33267;&#21152;&#37325;&#29616;&#26377;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#36164;&#28304;&#19981;&#22343;&#21644;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#31561;&#20844;&#24179;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#20844;&#24179;&#38382;&#39064;&#23545;&#20110;&#38450;&#27490;&#31038;&#20250;&#19981;&#20844;&#27491;&#30340;&#36827;&#19968;&#27493;&#24041;&#22266;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#21307;&#30103;&#20445;&#20581;&#19981;&#20844;&#24179;&#30340;&#20132;&#21449;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#65292;&#24182;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problem is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in machine learning and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a machine learning standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#38388;&#30340;&#20284;&#28982;&#27604;&#65292;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.14461</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Collaborative likelihood-ratio estimation over graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#38388;&#30340;&#20284;&#28982;&#27604;&#65292;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#26377;&#26469;&#33258;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968; (pdfs) p &#21644; q &#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35266;&#27979;&#20540;&#65292;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;LRE&#65289;&#26159;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#21482;&#20381;&#38752;&#29616;&#26377;&#25968;&#25454;&#26469;&#27604;&#36739;&#36825;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30446;&#21069;&#20026;&#27490;&#39318;&#20010;&#22522;&#20110;&#22270;&#30340;&#25193;&#23637;&#38382;&#39064;&#65292;&#20854;&#20551;&#35774;&#22266;&#23450;&#22270;&#30340;&#27599;&#20010;&#33410;&#28857; v &#37117;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#20004;&#20010;&#26410;&#30693;&#33410;&#28857;&#29305;&#23450;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968; p_v &#21644; q_v &#30340;&#35266;&#27979;&#20540;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22270;&#32467;&#26500;&#25552;&#20379;&#30340;&#20449;&#24687;&#26469;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20284;&#28982;&#27604;&#12290;&#33410;&#28857;&#32423;&#21035;&#30340;&#20272;&#35745;&#20219;&#21153;&#24212;&#35813;&#23637;&#29616;&#20986;&#22270;&#20256;&#36882;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#26263;&#31034;&#30528;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20855;&#20307;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861; GRULSIF &#26469;&#24320;&#21457;&#36825;&#20010;&#24819;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assuming we have iid observations from two unknown probability density functions (pdfs), $p$ and $q$, the likelihood-ratio estimation (LRE) is an elegant approach to compare the two pdfs only by relying on the available data. In this paper, we introduce the first -to the best of our knowledge-graph-based extension of this problem, which reads as follows: Suppose each node $v$ of a fixed graph has access to observations coming from two unknown node-specific pdfs, $p_v$ and $q_v$, and the goal is to estimate for each node the likelihood-ratio between both pdfs by also taking into account the information provided by the graph structure. The node-level estimation tasks are supposed to exhibit similarities conveyed by the graph, which suggests that the nodes could collaborate to solve them more efficiently. We develop this idea in a concrete non-parametric method that we call Graph-based Relative Unconstrained Least-squares Importance Fitting (GRULSIF). We derive convergence rates for our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#20998;&#26512;&#20102;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#23545;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#65292;&#26174;&#31034;&#20102;&#40065;&#26834;&#27867;&#21270;&#30340;&#24615;&#33021;&#26126;&#26174;&#24046;&#20110;&#26631;&#20934;&#27867;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2201.05149</link><description>&lt;p&gt;
&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35781;&#21650;&#65306;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#30340;&#40065;&#26834;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#20998;&#26512;&#20102;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#23545;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#65292;&#26174;&#31034;&#20102;&#40065;&#26834;&#27867;&#21270;&#30340;&#24615;&#33021;&#26126;&#26174;&#24046;&#20110;&#26631;&#20934;&#27867;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#28041;&#21450;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20248;&#28857;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#65288;&#36890;&#36807;&#21452;&#19979;&#38477;&#29616;&#35937;&#65289;&#21644;&#35745;&#31639;&#35282;&#24230;&#65288;&#36890;&#36807;&#20248;&#21270;&#26223;&#35266;&#30340;&#32467;&#26500;&#29305;&#24615;&#65289;&#24050;&#32463;&#24471;&#21040;&#24314;&#31435;&#12290;&#23613;&#31649;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#12290;&#21363;&#20351;&#22312;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#22312;&#34987;&#25200;&#21160;&#30340;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#65288;&#40065;&#26834;&#27867;&#21270;&#65289;&#20063;&#26126;&#26174;&#27604;&#22312;&#33391;&#24615;&#36755;&#20837;&#19978;&#30340;&#26368;&#20339;&#24615;&#33021;&#65288;&#26631;&#20934;&#27867;&#21270;&#65289;&#35201;&#24046;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36807;&#24230;&#21442;&#25968;&#21270;&#22914;&#20309;&#20174;&#26681;&#26412;&#19978;&#24433;&#21709;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape.   Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness.   In this paper, we will provide a preci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#20013;&#24212;&#29992;&#30340;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#65292;&#25193;&#23637;&#20102;RBF&#26680;&#24182;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2112.00365</link><description>&lt;p&gt;
&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#22312;&#29699;&#38754;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probability-Generating Function Kernels for Spherical Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.00365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#20013;&#24212;&#29992;&#30340;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#65292;&#25193;&#23637;&#20102;RBF&#26680;&#24182;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#65288;PGF&#65289;&#26680;&#65292;&#26500;&#25104;&#20102;&#19968;&#31867;&#22312;&#21333;&#20301;&#36229;&#29699;&#19978;&#25903;&#25345;&#30340;&#26680;&#65292;&#29992;&#20110;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#12290;PGF&#26680;&#22312;&#29699;&#38754;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#25512;&#24191;&#20102;RBF&#26680;&#12290;&#30740;&#31350;&#20102;PGF&#26680;&#30340;&#29305;&#24615;&#12290;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#29699;&#38754;&#25968;&#25454;&#20013;&#20351;&#29992;PGF&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability-generating function (PGF) kernels are introduced, which constitute a class of kernels supported on the unit hypersphere, for the purposes of spherical data analysis. PGF kernels generalize RBF kernels in the context of spherical data. The properties of PGF kernels are studied. A semi-parametric learning algorithm is introduced to enable the use of PGF kernels with spherical data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#19982;AMSGrad&#31867;&#22411;&#21160;&#37327;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2110.08531</link><description>&lt;p&gt;
&#23545;&#20110;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24102;&#26377;&#38468;&#21152;&#21160;&#37327;&#27493;&#39588;&#21644;&#24179;&#31227;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#19982;AMSGrad&#31867;&#22411;&#21160;&#37327;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#33258;&#36866;&#24212;&#20248;&#21270;&#31639;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#34028;&#21187;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#37117;&#33268;&#21147;&#20110;&#21152;&#36895;&#26799;&#24230;&#27861;&#65292;&#20294;&#21482;&#26377;&#26368;&#36817;&#25165;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#33258;&#36866;&#24212;&#36845;&#20195;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#21160;&#37327;&#39033;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#23637;&#31034;&#21152;&#36895;&#26041;&#27861;&#19982;AMSGrad&#31867;&#22411;&#21160;&#37327;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#21644;&#21487;&#33021;&#38750;&#20984;&#30446;&#26631;&#26144;&#23556;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#22312;&#33258;&#36866;&#24212;&#31639;&#27861;&#30740;&#31350;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#19968;&#20123;&#20551;&#35774;&#12290;&#38500;&#20102;&#35752;&#35770;&#26377;&#38480;&#26102;&#38388;&#20869;&#19982;&#29305;&#23450;&#26368;&#32456;&#36845;&#20195;&#30456;&#20851;&#30340;&#20998;&#26512;&#20197;&#21450;&#21040;&#36798;&#31283;&#23450;&#28857;&#30340;&#20960;&#20046;&#30830;&#23450;&#25910;&#25947;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that adaptive optimization algorithms represent the key pillar behind the rise of the Machine Learning field. In the Optimization literature numerous studies have been devoted to accelerated gradient methods but only recently adaptive iterative techniques were analyzed from a theoretical point of view. In the present paper we introduce new adaptive algorithms endowed with momentum terms for stochastic non-convex optimization problems. Our purpose is to show a deep connection between accelerated methods endowed with different inertial steps and AMSGrad-type momentum methods. Our methodology is based on the framework of stochastic and possibly non-convex objective mappings, along with some assumptions that are often used in the investigation of adaptive algorithms. In addition to discussing the finite-time horizon analysis in relation to a certain final iteration and the almost sure convergence to stationary points, we shall also look at the worst-case iteration complexity. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2107.08020</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Graph Topology Learning from Matrix-valued Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.08020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#22312;&#19968;&#20010;&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#65288;&#36890;&#24120;&#26159;&#19968;&#32452;&#31354;&#38388;&#20301;&#32622;&#65289;&#65292;&#35266;&#27979;&#21040;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;&#30001;&#19968;&#20010;&#21521;&#37327;&#26102;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#25105;&#20204;&#24076;&#26395;&#35782;&#21035;&#36825;&#20123;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#24182;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#23427;&#12290;&#24403;&#27599;&#20010;&#20256;&#24863;&#22120;&#21482;&#26377;&#19968;&#20010;&#29305;&#24449;&#26102;&#65292;&#30690;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#26029;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#32467;&#26500;&#12290;&#25152;&#24471;&#21040;&#30340;&#22270;&#34987;&#31216;&#20026;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#65292;&#22312;&#26032;&#26679;&#26412;&#21040;&#36798;&#26102;&#21487;&#20197;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the statistical analysis of matrix-valued time series. These are data collected over a network of sensors (typically a set of spatial locations) along time, where a vector of features is observed per time instant per sensor. Thus each sensor is characterized by a vectorial time series. We would like to identify the dependency structure among these sensors and represent it by a graph. When there is only one feature per sensor, the vector auto-regressive models have been widely adapted to infer the structure of Granger causality. The resulting graph is referred to as causal graph. Our first contribution is then extending VAR models to matrix-variate models to serve the purpose of graph learning. Secondly, we propose two online procedures respectively in low and high dimensions, which can update quickly the estimates of coefficients when new samples arrive. In particular in high dimensional regime, a novel Lasso-type is introduced and we develop its homotopy a
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.16719</link><description>&lt;p&gt;
OptiState&#65306;&#22522;&#20110;&#38376;&#25511;&#32593;&#32476;&#12289;Transformer&#35270;&#35273;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#39640;&#21160;&#24577;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#31934;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#20511;&#21161;&#20851;&#33410;&#32534;&#30721;&#22120;&#21644;IMU&#27979;&#37327;&#65292;&#25105;&#20204;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#21333;&#21018;&#20307;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#35813;&#27169;&#22411;&#36824;&#32467;&#21512;&#20102;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#30340;&#25509;&#22320;&#21453;&#21147;&#25511;&#21046;&#36755;&#20986;&#12290;&#36890;&#36807;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#36827;&#19968;&#27493;&#25913;&#36827;&#20272;&#35745;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#20174;&#28145;&#24230;&#22270;&#20687;&#19978;&#24212;&#29992;&#35270;&#35273;Transformer&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#35821;&#20041;&#27934;&#23519;&#21644;&#26426;&#22120;&#20154;&#39640;&#24230;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16661</link><description>&lt;p&gt;
&#20801;&#35768;&#28151;&#28102;&#30340;LiNGAM&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiNGAM&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#37327;&#39034;&#24207;&#65292;&#20294;&#22312;&#28151;&#28102;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;LiNGAM&#30340;&#22522;&#26412;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#35797;&#22270;&#35782;&#21035;&#21644;&#22788;&#29702;&#21463;&#28151;&#28102;&#24433;&#21709;&#30340;&#21464;&#37327;&#12290;&#32467;&#26524;&#26159;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#19988;&#19981;&#33021;&#30830;&#20445;&#26816;&#27979;&#21040;&#25152;&#26377;&#30340;&#28151;&#28102;&#31867;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LiNGAM-MMI&#23545;LiNGAM&#36827;&#34892;&#20102;&#22686;&#24378;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#23433;&#25490;&#21464;&#37327;&#20197;&#26368;&#23567;&#21270;&#20854;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#24418;&#24335;&#39640;&#25928;&#22320;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;&#22312;&#26080;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#65292;LiNGAM-MMI&#30340;&#22788;&#29702;&#25968;&#25454;&#25928;&#29575;&#19982;&#20256;&#32479;LiNGAM&#30456;&#24403;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#28151;&#28102;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LiNGAM-MMI&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;...
&lt;/p&gt;
&lt;p&gt;
LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.13744</link><description>&lt;p&gt;
&#12298;&#35268;&#33539;&#39044;&#27979;&#38598;&#25552;&#21319;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23545;&#26085;&#24120;&#26597;&#35810;&#30340;&#22238;&#24212;&#65292;&#20154;&#31867;&#26126;&#30830;&#22320;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26367;&#20195;&#31572;&#26696;&#12290;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#36755;&#20986;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#36825;&#31181;&#34892;&#20026;&#65307;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#34920;&#31034;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#39044;&#27880;&#20876;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#24182;&#32473;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#35268;&#33539;&#39044;&#27979;&#38598;&#65292;&#30740;&#31350;&#20102;&#35268;&#33539;&#39044;&#27979;&#38598;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20154;&#31867;&#33719;&#24471;&#35268;&#33539;&#39044;&#27979;&#38598;&#26102;&#65292;&#20182;&#20204;&#22312;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#20351;&#29992;&#30456;&#21516;&#35206;&#30422;&#20445;&#35777;&#30340;&#22266;&#23450;&#23610;&#23544;&#39044;&#27979;&#38598;&#26102;&#26377;&#25152;&#25552;&#39640;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;&#30340;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#35780;&#20998;&#21644;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#30340;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#24322;&#26500;&#35774;&#22791;&#21644;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2401.13366</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems. (arXiv:2401.13366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;&#30340;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#35780;&#20998;&#21644;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#30340;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#24322;&#26500;&#35774;&#22791;&#21644;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#24322;&#26500;&#35774;&#22791;&#21644;&#23458;&#25143;&#31471;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#26102;&#38754;&#20020;&#24615;&#33021;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#23545;&#20854;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#35780;&#20998;&#21644;&#35843;&#25972;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#35774;&#22791;&#33021;&#21147;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23458;&#25143;&#31471;&#19978;&#20256;&#26412;&#22320;&#27169;&#22411;&#21518;&#31435;&#21363;&#21521;&#20854;&#25552;&#20379;&#26356;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#31354;&#38386;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;10&#20010;&#27169;&#25311;&#23458;&#25143;&#31471;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#37096;&#32626;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#20855;&#26377;&#24322;&#26500;&#35745;&#31639;&#32422;&#26463;&#21644;&#38750;IID&#25968;&#25454;&#12290;&#20351;&#29992;FashionMNIST&#25968;&#25454;&#38598;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;PAPAYA&#21644;FedAsync&#30456;&#27604;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#21892;&#20102;10%&#21644;19%&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.11261</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#36127;&#39640;&#26031;&#28151;&#21512;&#26799;&#24230;&#30340;&#25193;&#25955;&#27169;&#22411;&#26465;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#19968;&#31181;&#23545;&#22270;&#20687;&#21512;&#25104;&#21644;&#20854;&#20182;&#39046;&#22495;&#26377;&#24040;&#22823;&#24433;&#21709;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#22810;&#26679;&#30340;&#26465;&#20214;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#36793;&#30028;&#26694;&#65292;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#12290;&#22522;&#20110;&#38598;&#21512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#22522;&#20110;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#22240;&#27492;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#27604;&#22522;&#20110;&#31867;&#21035;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;&#20998;&#21035;&#35757;&#32451;&#20102;&#20004;&#20010;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20989;&#25968;&#65292;&#31216;&#20026;&#36127;&#39640;&#26031;&#28151;&#21512;&#26799;&#24230;&#65288;NGMG&#65289;&#65292;&#24182;&#24212;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr
&lt;/p&gt;</description></item><item><title>Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10371</link><description>&lt;p&gt;
Langevin&#36951;&#24536;&#65306;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#26426;&#22120;&#36951;&#24536;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10371
&lt;/p&gt;
&lt;p&gt;
Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37319;&#29992;&#30830;&#20445;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27861;&#24459;&#65292;&#26426;&#22120;&#36951;&#24536;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#36817;&#20284;&#36951;&#24536;&#23450;&#20041;&#65292;&#31867;&#20284;&#20110;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#23450;&#20041;&#65292;&#20854;&#20013;&#38544;&#31169;&#34987;&#23450;&#20041;&#20026;&#23545;&#37325;&#26032;&#35757;&#32451;&#30340;&#32479;&#35745;&#19981;&#21487;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Langevin&#36951;&#24536;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#36951;&#24536;&#26694;&#26550;&#12290;Langevin&#36951;&#24536;&#22312;&#31639;&#27861;&#19978;&#32479;&#19968;&#20102;DP&#23398;&#20064;&#36807;&#31243;&#21644;&#38544;&#31169;&#35748;&#35777;&#30340;&#36951;&#24536;&#36807;&#31243;&#12290;&#20854;&#20013;&#21253;&#25324;&#38750;&#20984;&#38382;&#39064;&#30340;&#36817;&#20284;&#35748;&#35777;&#36951;&#24536;&#65292;&#30456;&#23545;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#20010;&#36951;&#24536;&#35831;&#27714;&#30340;&#39034;&#24207;&#21644;&#25209;&#37327;&#36951;&#24536;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Langevin&#36951;&#24536;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10306</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#21453;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#31243;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#37117;&#26159;&#38750;&#32447;&#24615;&#19988;&#21464;&#21270;&#30340;&#12290;&#22312;&#31532;&#19968;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21463;&#31354;&#38388;&#21464;&#21270;&#30340;&#31995;&#32479;&#35823;&#24046;&#65288;&#21363;&#20559;&#24046;&#65292;&#20063;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65289;&#20559;&#31227;&#30340;&#25968;&#25454;&#12290;&#20219;&#21153;&#26159;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#30495;&#23454;&#29366;&#24577;&#65292;&#21363;PDE&#30340;&#35299;&#12290;&#22312;&#31532;&#20108;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;PDE&#35299;&#30340;&#31232;&#30095;&#20449;&#24687;&#12290;&#20219;&#21153;&#26159;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#31354;&#38388;&#20013;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PC-CNN&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#30340;&#26102;&#38388;&#31383;&#21475;&#26041;&#26696;&#32422;&#26463;PDE&#26469;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PC-CNN&#22312;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#35299;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#20197;&#21450;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#21518;&#32773;&#25551;&#36848;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#26102;&#31354;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.08224</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#19979;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;CATE&#30340;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#22330;&#26223;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#12290;&#34429;&#28982;&#23454;&#39564;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20272;&#35745;&#31934;&#24230;&#65292;&#20294;&#30001;&#20110;&#31038;&#20250;&#31119;&#21033;&#30340;&#35201;&#27714;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#20855;&#26377;&#20248;&#36234;&#32467;&#26524;&#30340;&#27835;&#30103;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29615;&#22659;&#25209;&#27425;&#26694;&#26550;&#20013;&#30340;&#36951;&#25022;&#26469;&#34913;&#37327;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#32463;&#24120;&#23548;&#33268;&#23545;&#27604;&#20248;&#21270;&#20998;&#37197;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#22312;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#65288;&#22914;&#24739;&#32773;&#20581;&#24247;&#35760;&#24405;&#65289;&#30340;&#20020;&#24202;&#22330;&#26223;&#20013;&#20986;&#29616;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#24517;&#39035;&#32435;&#20837;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29615;&#22659;&#25209;&#27425;&#23454;&#39564;&#20013;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#37319;&#29992;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#30340;&#27010;&#24565;&#26469;&#25968;&#23398;&#22320;&#21051;&#30011;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.06203</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;
&lt;/p&gt;
&lt;p&gt;
Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators. (arXiv:2401.06203v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31934;&#35843;&#30340;&#28304;&#20998;&#31163;&#22120;&#21512;&#22863;&#28151;&#38899;&#38899;&#20048;&#20197;&#25913;&#21892;&#21161;&#21548;&#22120;&#38899;&#36136;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;Cadenza ICASSP 2024&#22823;&#25361;&#25112;&#20013;&#30340;&#31995;&#32479;&#25552;&#20132;&#65292;&#35813;&#22823;&#25361;&#25112;&#25552;&#20986;&#20102;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#28151;&#38899;&#21644;&#22686;&#24378;&#38899;&#20048;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21161;&#21548;&#22120;&#38899;&#36136;&#25351;&#25968;&#65288;HAAQI&#65289;&#24179;&#22343;&#20998;&#25968;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#32452;&#32463;&#36807;&#32454;&#35843;&#30340;&#28145;&#24230;&#23398;&#20064;&#38899;&#20048;&#28304;&#20998;&#31163;&#22120;&#65292;&#36825;&#20123;&#20998;&#31163;&#22120;&#22312;&#25361;&#25112;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#32454;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25361;&#25112;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31995;&#32479;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies.
&lt;/p&gt;</description></item><item><title>SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05821</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05821
&lt;/p&gt;
&lt;p&gt;
SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#38590;&#20197;&#24402;&#22240;&#30340;&#38382;&#39064;&#20197;&#21450;&#19981;&#23545;&#40784;&#31561;&#31561;&#37117;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#36825;&#20123;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24182;&#32416;&#27491;&#38169;&#35823;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65288;SCoBots&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65292;&#20351;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#36879;&#26126;&#21270;&#12290;SCoBots&#19981;&#20165;&#21033;&#29992;&#30456;&#20851;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#36824;&#21033;&#29992;&#20851;&#31995;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;SCoBots&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#21644;&#35268;&#33539;&#20182;&#20204;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SCoBots&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#26368;&#31616;&#21333;&#19988;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;Pong&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#21152;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05580</link><description>&lt;p&gt;
&#21152;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65306;&#19968;&#31181;&#24102;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#32418;&#22806;&#30456;&#24178;&#28857;&#20809;&#28304;&#29031;&#23556;&#26469;&#26816;&#27979;&#20809;&#35889;&#21464;&#21270;&#26469;&#27979;&#37327;&#32452;&#32455;&#34880;&#27969;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#27979;&#37327;&#34880;&#27969;&#25351;&#25968;&#65288;BFi&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#19968;&#20010;&#26377;&#20851;&#35813;&#26041;&#27861;&#25104;&#21151;&#19982;&#21542;&#30340;&#38382;&#39064;&#26159;&#20854;&#22312;&#28041;&#21450;&#19981;&#21516;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21644;&#21508;&#31181;&#19981;&#21516;&#20020;&#24202;&#24212;&#29992;&#21644;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20559;&#24046;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;SNR&#23545;&#23398;&#20064;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#19981;&#21516;&#28155;&#21152;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;SNR&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#20197;1x64&#30340;&#33258;&#30456;&#20851;&#26354;&#32447;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;BFi&#21644;&#30456;&#20851;&#21442;&#25968;beta&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05578</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#20998;&#26512;&#33041;&#34880;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#24555;&#36895;&#31934;&#30830;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#26469;&#20998;&#26512;&#33041;&#34880;&#27969;&#65288;CBF&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ELM&#21644;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#32508;&#21512;&#25351;&#26631;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#21322;&#26080;&#31351;&#21644;&#22810;&#23618;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#65292;ELM&#22987;&#32456;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#36845;&#20195;&#25311;&#21512;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;ELM&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;ELM&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27809;&#26377;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#23548;&#33268;&#35757;&#32451;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26356;&#24555;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#32593;&#32476;&#39640;&#25928;&#24615;&#21644;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.05739</link><description>&lt;p&gt;
&#36793;&#32536;&#32593;&#32476;&#25928;&#29575;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#32593;&#32476;&#39640;&#25928;&#24615;&#21644;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#23548;&#33268;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#29616;&#26377;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65307;&#30001;&#20110;&#20854;&#22312;&#35774;&#22791;&#33021;&#32791;&#12289;&#24310;&#36831;&#12289;&#32593;&#32476;&#21033;&#29992;&#21644;&#38544;&#31169;&#25913;&#36827;&#31561;&#26041;&#38754;&#30340;&#22810;&#37325;&#22909;&#22788;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20998;&#31163;&#24182;&#35745;&#31639;&#30340;&#20998;&#31163;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#32467;&#21512;&#23545;&#36890;&#20449;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#22909;&#22788;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65289;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65288;'deprune'&#65289;&#65292;&#20197;&#25913;&#21892;&#21644;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#21152;&#32593;&#32476;&#39640;&#25928;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;&#32593;&#32476;&#36164;&#28304;&#21644;&#26356;&#24555;&#65289;&#65292;&#36825;&#23558;&#20351;&#23427;&#20204;&#25104;&#20026;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing number of AI-driven applications in the mobile devices has led to solutions that integrate deep learning models with the available edge-cloud resources; due to multiple benefits such as reduction in on-device energy consumption, improved latency, improved network usage, and certain privacy improvements, split learning, where deep learning models are split away from the mobile device and computed in a distributed manner, has become an extensively explored topic. Combined with compression-aware methods where learning adapts to compression of communicated data, the benefits of this approach have further improved and could serve as an alternative to established approaches like federated learning methods. In this work, we develop an adaptive compression-aware split learning method ('deprune') to improve and train deep learning models so that they are much more network-efficient (use less network resources and are faster), which would make them ideal to deploy in weaker devices w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#25512;&#26029;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.19390</link><description>&lt;p&gt;
&#38544;&#24335;&#27969;&#24418;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Implicit Manifold Gaussian Process Regression. (arXiv:2310.19390v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#25512;&#26029;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#22788;&#29702;&#23567;&#22411;&#25110;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#23427;&#23384;&#22312;&#19968;&#23450;&#22256;&#38590;&#12290;&#19968;&#31181;&#23558;&#36825;&#31181;&#25216;&#26415;&#25193;&#23637;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#21487;&#33021;&#36884;&#24452;&#26159;&#21033;&#29992;&#25968;&#25454;&#23454;&#38469;&#25152;&#22788;&#30340;&#38544;&#24335;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#26159;&#27969;&#24418;&#20551;&#35774;&#25152;&#20551;&#23450;&#30340;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#35201;&#27714;&#26174;&#24335;&#25552;&#20379;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#30001;&#32593;&#26684;&#25110;&#24050;&#30693;&#20026;&#20247;&#25152;&#21608;&#30693;&#30340;&#27969;&#24418;&#20043;&#19968;&#65288;&#22914;&#29699;&#20307;&#65289;&#32473;&#20986;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20197;&#23436;&#20840;&#21487;&#24494;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#65288;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#65289;&#20013;&#25512;&#26029;&#20986;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#12290;&#23545;&#20110;&#24471;&#21040;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#22312;&#20551;&#35774;&#27969;&#24418;&#19978;&#25910;&#25947;&#20110;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#28857;&#65292;&#24182;&#19988;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18001</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-SGD with weight clipping. (arXiv:2310.18001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#20381;&#36182;&#20110;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#30340;&#39640;&#24230;&#27969;&#34892;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#22312;&#25552;&#20379;&#26368;&#23567;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#38480;&#21046;&#21442;&#19982;&#32773;&#23558;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#30340;&#28789;&#25935;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#26799;&#24230;&#21098;&#35009;&#20135;&#29983;&#30340;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#20851;&#20110;&#24403;&#21069;&#20840;&#23616;&#27169;&#22411;&#21450;&#20854;&#22312;&#25628;&#32034;&#39046;&#22495;&#20013;&#20301;&#32622;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#26799;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30830;&#23450;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#22122;&#22768;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17496</link><description>&lt;p&gt;
&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#65306;&#19968;&#31181;&#21152;&#26435;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26631;&#20934;&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#29992;&#25143;&#34892;&#20026;&#24182;&#25345;&#32493;&#25913;&#36827;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#21487;&#33021;&#22312;A/B&#27979;&#35797;&#20013;&#24341;&#20837;&#24178;&#25200;&#65292;&#20854;&#20013;&#25511;&#21046;&#32452;&#21644;&#23454;&#39564;&#32452;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#26435;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#20986;&#29616;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#25968;&#25454;&#20013;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#20272;&#35745;&#37327;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#26041;&#24046;&#65292;&#19988;&#19981;&#20250;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10899</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#20960;&#20046;&#27809;&#26377;&#30693;&#35782;&#25110;&#25511;&#21046;&#33021;&#21147;&#12290;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;--&#23545;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#20559;&#22909;--&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#34892;&#20026;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#26469;&#30740;&#31350;&#27169;&#22411;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#32467;&#26500;&#25110;&#31934;&#24515;&#31574;&#21010;&#30340;&#35757;&#32451;&#26041;&#24335;&#27880;&#20837;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#26426;&#26800;&#30340;&#26041;&#27861;&#65306;&#23376;&#20219;&#21153;&#24402;&#32435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#21151;&#33021;&#23376;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#27880;&#20837;&#23545;&#21033;&#29992;&#35813;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#23376;&#20219;&#21153;&#24402;&#32435;&#28789;&#27963;&#39640;&#25928;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00692</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22122;&#22768;&#20960;&#20309;&#65306;&#23450;&#37327;&#21644;&#20998;&#26512;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#21644;&#23450;&#37327;&#35299;&#37322;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#36848;&#8220;&#22122;&#22768;&#20960;&#20309;&#8221;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#32454;&#33268;&#22320;&#30740;&#31350;&#20102;&#24179;&#22343;&#21644;&#26041;&#21521;&#30340;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#26679;&#26412;&#22823;&#23567;&#21644;&#36755;&#20837;&#25968;&#25454;&#36864;&#21270;&#23545;&#19968;&#33268;&#24615;&#24378;&#24230;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#22122;&#22768;&#20960;&#20309;&#29305;&#24449;&#30740;&#31350;&#20102;SGD&#22914;&#20309;&#20174;&#23574;&#38160;&#26497;&#23567;&#20540;&#20013;&#36867;&#33073;&#65292;&#21457;&#29616;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#65292;&#36825;&#19982;&#21482;&#22312;&#26368;&#23574;&#38160;&#26041;&#21521;&#36867;&#33073;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13223</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#65306;&#20026;&#19979;&#19968;&#20195;AI&#26412;&#22320;&#21270;&#26080;&#32447;&#32593;&#32476;&#24320;&#36767;&#38761;&#21629;&#24615;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#26412;&#21069;&#25552;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#65288;&#20363;&#22914;6G&#65289;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26412;&#22320;&#21270;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#20173;&#28982;&#35201;&#20040;&#26159;&#23450;&#24615;&#30340;&#65292;&#35201;&#20040;&#26159;&#23545;&#29616;&#26377;&#8220;AI&#29992;&#20110;&#26080;&#32447;&#8221;&#33539;&#24335;&#30340;&#22686;&#37327;&#25193;&#23637;&#12290;&#23454;&#38469;&#19978;&#65292;&#21019;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;&#30340;AI&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#23427;&#20204;&#30340;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#65288;&#36825;&#21487;&#33021;&#38480;&#21046;&#23427;&#20204;&#30340;&#25512;&#29702;&#21644;&#36866;&#24212;&#33021;&#21147;&#65289;&#12289;&#23427;&#20204;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#20855;&#26377;&#21069;&#30651;&#24615;&#30340;&#24895;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#22240;&#26524;&#21457;&#29616;&#12289;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05915</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#35843;&#33410;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#22686;&#24378;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer (DT) &#21033;&#29992;&#34920;&#36798;&#20016;&#23500;&#30340;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#26469;&#25191;&#34892;&#21160;&#20316;&#29983;&#25104;&#65292;&#24050;&#25104;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;DT &#29983;&#25104;&#30340;&#21160;&#20316;&#26159;&#22522;&#20110;&#26399;&#26395;&#26410;&#26469;&#22238;&#25253;&#30340;&#26465;&#20214;&#65292;&#24050;&#30693;&#20855;&#26377;&#26576;&#20123;&#24369;&#28857;&#65292;&#27604;&#22914;&#26131;&#21463;&#29615;&#22659;&#38543;&#26426;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;DT&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DT&#20013;&#22686;&#21152;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#26469;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#36825;&#28041;&#21450;&#21040;MDP&#32467;&#26500;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#26469;&#35780;&#20272;&#21160;&#20316;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21183;&#20272;&#35745;&#22120;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20197;&#20272;&#35745;&#30340;&#20248;&#21183;&#20026;&#26465;&#20214;&#29983;&#25104;&#21160;&#20316;&#30340;&#20248;&#21183;&#26465;&#20214;Transformer (ACT)&#12290;&#26368;&#21518;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;ACT&#26681;&#25454;&#25152;&#38656;&#30340;&#20248;&#21183;&#29983;&#25104;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
&lt;/p&gt;</description></item><item><title>FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12264</link><description>&lt;p&gt;
FECoM: &#26397;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12264
&lt;/p&gt;
&lt;p&gt;
FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#12289;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#33021;&#28304;&#28040;&#32791;&#36805;&#36895;&#22686;&#38271;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20419;&#36827;&#32511;&#33394;&#21457;&#23637;&#21644;&#19981;&#21516;&#31890;&#24230;&#30340;&#33021;&#28304;&#24847;&#35782;&#65292;&#20197;&#38480;&#21046;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#30899;&#25490;&#25918;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20934;&#30830;&#27979;&#37327;&#21644;&#20248;&#21270;&#32454;&#31890;&#24230;&#65288;&#20363;&#22914;&#26041;&#27861;&#32423;&#21035;&#65289;&#33021;&#32791;&#30340;&#26631;&#20934;&#21644;&#21487;&#37325;&#22797;&#24037;&#20855;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FECoM&#65288;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#20202;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FECoM&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;FECoM&#36890;&#36807;&#20351;&#29992;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#21508;&#31181;&#22240;&#32032;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;FECoM&#22312;&#26368;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#19978;&#27979;&#37327;&#32454;&#31890;&#24230;&#33021;&#32791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.00206</link><description>&lt;p&gt;
SkullGAN: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00206
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28041;&#21450;&#20154;&#31867;&#39045;&#39592;&#30340;&#21508;&#31181;&#21307;&#30103;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#32463;&#36807;&#31574;&#21010;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#39045;&#39592;CT&#20999;&#29255;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#24182;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;38&#20010;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;&#39045;&#39592;CT&#20999;&#29255;&#36755;&#20837;SkullGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2&#20159;&#20010;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#39045;&#39592;&#22270;&#20687;&#26681;&#25454;&#19977;&#20010;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#65306;&#39045;&#39592;&#23494;&#24230;&#27604;&#65288;SDR&#65289;&#12289;&#24179;&#22343;&#21402;&#24230;&#21644;&#24179;&#22343;&#24378;&#24230;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;SkullGAN&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#31867;&#22120;&#36827;&#34892;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SkullGAN&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#30495;&#23454;&#39045;&#39592;&#20855;&#26377;&#31867;&#20284;&#30340;&#20851;&#38190;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#30340;&#30830;&#23450;&#24615;&#20998;&#26512;&#26159;&#36890;&#36807;&#36827;&#34892;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2307.13903</link><description>&lt;p&gt;
&#33104;&#36133;&#40065;&#26834;&#30340;Lipschitz&#19978;&#19979;&#25991;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#30001;&#23545;&#25163;&#36873;&#25321;&#30340;Lipschitz&#20989;&#25968;$f$&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#23545;&#25163;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#19978;&#19979;&#25991;&#21521;&#37327;$x_t$&#65292;&#23398;&#20064;&#32773;&#23545;&#30495;&#23454;&#20989;&#25968;&#20540;$f(x_t)$&#36827;&#34892;&#29468;&#27979;&#65292;&#24182;&#25509;&#25910;&#19968;&#20010;&#25351;&#31034;&#29468;&#27979;&#26159;&#39640;&#36824;&#26159;&#20302;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;&#22312;&#24635;&#20849;$C$&#36718;&#20013;&#65292;&#20449;&#21495;&#21487;&#33021;&#34987;&#31713;&#25913;&#65292;&#20294;&#23398;&#20064;&#32773;&#19981;&#30693;&#36947;$C$&#30340;&#20540;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#36896;&#25104;&#23567;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#32780;&#24378;&#22823;&#30340;&#25216;&#26415;&#39564;&#35777;&#65292;&#23545;&#35774;&#35745;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#35774;&#35745;&#20102;&#19968;&#20123;&#31639;&#27861;&#65288;&#23558;Lipschitz&#21442;&#25968;$L$&#35270;&#20026;&#24120;&#25968;&#65289;&#65306;&#23545;&#20110;&#23545;&#31216;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d=1$&#26102;&#36798;&#21040;&#21518;&#24724;$O(C\log T)$&#65292;&#22312;$d&gt;1$&#26102;&#36798;&#21040;&#21518;&#24724;$O_d(C\log T + T^{(d-1)/d})$&#65307;&#23545;&#20110;&#35745;&#20215;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d/(d+1)$&#26102;&#36798;&#21040;&#21518;&#24724;$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$&#12290;
&lt;/p&gt;
&lt;p&gt;
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.13831</link><description>&lt;p&gt;
&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#19982;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#34429;&#28982;SGD&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#29575;&#65292;&#22914;&#24120;&#25968;&#25110;&#36882;&#20943;&#30340;&#23398;&#20064;&#29575;&#65292;&#20294;&#20043;&#21069;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;SGD&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#32473;&#20986;&#30340;&#23398;&#20064;&#29575;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#32473;&#20986;&#23398;&#20064;&#29575;&#30340;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;SGD&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#27493;&#25968;&#21644;&#25209;&#22823;&#23567;&#37117;&#24456;&#22823;&#26102;&#65292;&#20840;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#30340;&#26399;&#26395;&#19978;&#30028;&#21464;&#23567;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#23398;&#20064;&#29575;&#30340;SGD&#26469;&#35828;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#26159;&#25209;&#22823;&#23567;&#30340;&#21333;&#35843;&#36882;&#20943;&#20984;&#20989;&#25968;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38543;&#26426;&#28779;&#28798;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13149</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#27493;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19987;&#23478;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20013;&#23624;&#26381;&#26354;&#38754;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#19968;&#32452;&#21333;&#21464;&#37327;&#29305;&#24449;&#26144;&#23556;&#26469;&#34920;&#31034;&#30340;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#36825;&#32452;&#21333;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#20989;&#25968;&#37325;&#26032;&#35299;&#37322;&#20026;&#25968;&#23398;&#24418;&#24335;&#12290;&#36825;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#25552;&#39640;&#20102;&#29992;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26448;&#26009;&#30340;&#23646;&#24615;&#65288;&#22914;&#20984;&#24615;&#21644;&#23545;&#31216;&#24615;&#65289;&#26377;&#19968;&#20010;&#20855;&#20307;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08670</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#22312;&#19968;&#20010;&#30001;$n$&#20010;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#33410;&#28857;&#32452;&#25104;&#30340;&#31181;&#32676;&#20013;&#65292;&#37319;&#29992;&#20102;&#35875;&#35328;&#27169;&#22411;&#65306;&#27599;&#36718;&#65292;&#27599;&#20010;&#33410;&#28857;&#26412;&#22320;&#37319;&#29992;$m$&#20010;&#33218;&#20043;&#19968;&#65292;&#35266;&#23519;&#20174;&#33218;&#30340;&#65288;&#23545;&#25239;&#36873;&#25321;&#30340;&#65289;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22870;&#21169;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#25277;&#21462;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#36718;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#65306;&#27599;&#20010;&#33410;&#28857;&#30340;&#20915;&#31574;&#23436;&#20840;&#26159;&#23616;&#37096;&#30340;&#65292;&#21482;&#20381;&#36182;&#20110;&#20854;&#26368;&#26032;&#33719;&#24471;&#30340;&#22870;&#21169;&#20197;&#21450;&#23427;&#25277;&#26679;&#30340;&#37051;&#23621;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#28436;&#21270;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#36825;&#20123;&#33258;&#28982;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#65288;&#21363;&#65292;&#31181;&#32676;&#30340;&#22823;&#23567;&#21644;nu&#30340;&#22823;&#23567;&#65289;&#19979;&#25512;&#23548;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05587</link><description>&lt;p&gt;
MC-NN&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#23545;&#20844;&#20849;&#21355;&#29983;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#23545;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#21644;&#24739;&#26377;&#28508;&#22312;&#30142;&#30149;&#30340;&#20154;&#26469;&#35828;&#26356;&#20026;&#20005;&#37325;&#12290;&#20005;&#37325;&#30149;&#20917;&#30340;&#21457;&#29983;&#65292;&#22914;&#32954;&#28814;&#65292;&#20984;&#26174;&#20102;&#39044;&#38450;&#27969;&#24863;&#20256;&#25773;&#30340;&#37325;&#35201;&#24615;&#12290;&#20934;&#30830;&#32780;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#23545;&#20110;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#34880;&#20957;&#32032;&#21644;&#31070;&#32463;&#27688;&#37240;&#37238;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#23436;&#25972;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#21508;&#31181;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#24207;&#21015;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26469;&#33258;&#23436;&#25972;&#21644;&#37096;&#20998;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#20855;&#26377;&#28508;&#21147;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2306.04072</link><description>&lt;p&gt;
L2&#24402;&#19968;&#21270;&#25216;&#26415;&#22312;&#31616;&#21333;&#39640;&#36136;&#37327;OoD&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;--&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;L2&#24402;&#19968;&#21270;--&#33021;&#22815;&#20135;&#29983;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;&#24403;&#22312;&#27979;&#35797;&#26102;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#30340;L2&#33539;&#25968;&#25104;&#20026;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24778;&#20154;&#30340;&#26367;&#20195;&#32773;&#65292;&#32780;&#24403;&#27809;&#26377;L2&#24402;&#19968;&#21270;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#34892;&#20026;&#21364;&#27809;&#26377;&#37027;&#20040;&#26377;&#25928;&#12290;&#30452;&#35266;&#19978;&#65292;&#29087;&#24713;&#30340;&#22270;&#20687;&#20250;&#20135;&#29983;&#22823;&#30340;&#21521;&#37327;&#65292;&#32780;&#38476;&#29983;&#30340;&#22270;&#20687;&#21017;&#20250;&#20135;&#29983;&#23567;&#30340;&#21521;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#26102;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22312;&#27979;&#35797;&#26102;&#20063;&#27809;&#26377;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
&lt;/p&gt;</description></item><item><title>MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.01697</link><description>&lt;p&gt;
MutateNN&#65306;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#31361;&#21464;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01697
&lt;/p&gt;
&lt;p&gt;
MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24182;&#25512;&#21160;&#25216;&#26415;&#21457;&#23637;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#29305;&#21035;&#26159;&#34987;&#20998;&#37197;&#20102;&#24863;&#30693;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24182;&#23548;&#33268;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#38656;&#27714;&#20063;&#26377;&#25152;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20248;&#21270;&#21644;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#27010;&#24565;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35753;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#22312;&#19981;&#21516;&#30828;&#20214;&#21152;&#36895;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MutateNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27492;&#30446;&#30340;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#21151;&#33021;&#65292;&#25105;&#20204;&#23545;7&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;21&#31181;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;4&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#21464;&#24322;&#20307;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;&#20102;MutateNN&#22312;&#26816;&#27979;&#20986;&#19981;&#27491;&#30830;&#25110;&#19981;&#31934;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18460</link><description>&lt;p&gt;
Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#22343;&#21248;&#36890;&#29992;&#36924;&#36817;&#20013;&#30340;&#26368;&#23567;&#23485;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18460
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65288;UAP&#65289;&#30340;&#30740;&#31350;&#21382;&#21490;&#24736;&#20037;&#12290;&#24403;&#32593;&#32476;&#23485;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#38544;&#34255;&#23618;&#21363;&#21487;&#36827;&#34892;UAP&#12290;&#30456;&#21453;&#65292;&#24403;&#28145;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;UAP&#30340;&#23485;&#24230;&#38656;&#35201;&#19981;&#23567;&#20110;&#20020;&#30028;&#23485;&#24230;$w^*_{\min}=\max(d_x,d_y)$, &#20854;&#20013;$d_x$&#21644;$d_y$&#20998;&#21035;&#26159;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#12290;&#26368;&#36817;&#65292;\cite{cai2022achieve}&#34920;&#26126;&#65292;&#20855;&#26377;&#36825;&#31181;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;$K$&#19978;&#23454;&#29616;$L^p$&#20989;&#25968;&#30340;UAP&#65292;&#21363;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#30340;&#22343;&#21248;UAP&#65292;&#24182;&#32473;&#20986;&#20102;Leaky-ReLU NN&#30340;&#30830;&#20999;&#26368;&#23567;&#23485;&#24230;&#65292;&#20026;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#65292;&#20854;&#20013;&#28041;&#21450;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;lift-flow-discretization&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22343;&#21248;UAP&#19982;&#25299;&#25169;&#29702;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \emph{i.e.,} the UAP for $L^p(K,\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#21644;&#20854;Polyak-averaging&#29256;&#26412;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.17665</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22343;&#21152;&#36895;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#26377;&#38480;&#26679;&#26412;&#36895;&#29575;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;
&lt;/p&gt;
&lt;p&gt;
Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality. (arXiv:2305.17665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17665
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#21644;&#20854;Polyak-averaging&#29256;&#26412;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;SGDM&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20855;&#26377;&#35266;&#23519;&#21040;&#30340;&#32463;&#39564;&#20248;&#21183;&#65292;&#20294;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21160;&#37327;&#23545;&#19981;&#21516;&#23398;&#20064;&#29575;&#30340;&#20316;&#29992;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#12290;&#25105;&#20204;&#22312;&#24378;&#20984;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;SGDM&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGDM&#20272;&#35745;&#37327;&#30340;Polyak&#24179;&#22343;&#29256;&#26412;&#65292;&#24314;&#31435;&#20102;&#23427;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#19982;&#24179;&#22343;SGD&#30340;&#28176;&#36817;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent with momentum (SGDM) has been widely used in many machine learning and statistical applications. Despite the observed empirical benefits of SGDM over traditional SGD, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. We analyze the finite-sample convergence rate of SGDM under the strongly convex settings and show that, with a large batch size, the mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value. Furthermore, we analyze the Polyak-averaging version of the SGDM estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged SGD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13514</link><description>&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37325;&#20889;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#24040;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#20307;&#31215;&#21644;&#36890;&#36807;API&#30340;&#21463;&#38480;&#35775;&#38382;&#20351;&#24471;&#38024;&#23545;&#20219;&#21153;&#30340;&#24494;&#35843;&#19981;&#20999;&#23454;&#38469;&#12290;&#32780;&#19988;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#28436;&#31034;&#30340;&#36873;&#25321;&#21644;&#39034;&#24207;&#65289;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20854;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;LLM&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#20505;&#36873;&#27744;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;LM-corrector&#65288;LMCor&#65289;&#26469;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;LMCor&#34987;&#35757;&#32451;&#29992;&#20110;&#23545;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#21517;&#12289;&#32452;&#21512;&#21644;&#37325;&#20889;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#30446;&#26631;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#23567;&#30340;LMCor&#27169;&#22411;&#65288;250M&#65289;&#65292;&#20063;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;LLMs&#65288;62B&#65289;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;LMCor&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25913;&#36827;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#21892;LLMs&#23454;&#38469;&#21487;&#29992;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.02930</link><description>&lt;p&gt;
&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02930
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#22522;&#30784;&#20998;&#24067;&#36827;&#34892;&#21487;&#36870;&#36716;&#25442;&#26469;&#23545;&#22797;&#26434;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#24314;&#27169;&#30340;&#25104;&#29087;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20998;&#24067;&#33021;&#21542;&#31934;&#30830;&#22320;&#34987;&#24402;&#19968;&#21270;&#27969;&#25152;&#25429;&#25417;&#65292;&#24378;&#28872;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#30446;&#26631;&#21644;&#22522;&#30784;&#20998;&#24067;&#20043;&#38388;&#30340;&#25299;&#25169;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#24046;&#65292;&#22914;&#23545;&#20110;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#19968;&#20123;&#19981;&#21516;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411; [Izmailov et al., 2020&#12289;Ardizzone et al., 2020&#12289;Hagemann and Neumayer, 2021] &#25110;&#23398;&#20064;&#25509;&#21463;/&#25298;&#32477;&#37319;&#26679; [Stimper et al., 2022] &#26469;&#20462;&#25913;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#35757;&#32451;&#19968;&#31995;&#21015;&#27969;&#26469;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
&lt;/p&gt;</description></item><item><title>FedIN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#65292;&#26080;&#38656;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22312;FedIN&#20013;&#65292;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;IN&#35757;&#32451;&#21487;&#29992;&#20110;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00759</link><description>&lt;p&gt;
FedIN&#65306;&#29992;&#20110;&#27169;&#22411;&#24322;&#26500;&#30340;&#32852;&#37030;&#20013;&#38388;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00759
&lt;/p&gt;
&lt;p&gt;
FedIN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#65292;&#26080;&#38656;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22312;FedIN&#20013;&#65292;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;IN&#35757;&#32451;&#21487;&#29992;&#20110;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#21512;&#20316;&#35757;&#32451;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#21644;&#31169;&#23494;&#22320;&#20445;&#30041;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#20013;&#19968;&#20010;&#26222;&#36941;&#20294;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#26159;&#21442;&#19982;&#36793;&#32536;&#35774;&#22791;&#25317;&#26377;&#30456;&#21516;&#30340;&#24517;&#38656;&#36164;&#28304;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Intermediate Layers Learning&#65288;FedIN&#65289;&#30340;&#26032;&#22411;FL&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;FedIN&#20013;&#30340;&#35757;&#32451;&#27169;&#22411;&#20998;&#20026;&#19977;&#37096;&#20998;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#12290;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#20197;&#20445;&#25345;&#20013;&#38388;&#23618;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;&#20026;&#20102;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IN&#35757;&#32451;&#65292;&#20197;IN&#26631;&#20934;&#21270;&#20026;&#22522;&#30784;&#35757;&#32451;&#20013;&#38388;&#23618;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06021</link><description>&lt;p&gt;
&#20307;&#32946;&#21338;&#24425;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#39044;&#27979;&#27169;&#22411;&#24212;&#20248;&#21270;&#20934;&#30830;&#24615;&#36824;&#26159;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#26368;&#36817;&#23545;&#20307;&#32946;&#21338;&#24425;&#36827;&#34892;&#20102;&#32852;&#37030;&#21512;&#27861;&#21270;&#65292;&#36825;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#30456;&#36935;&#12290;&#22914;&#26524;&#21338;&#24425;&#32773;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20182;&#20204;&#21487;&#20197;&#35748;&#35782;&#21040;&#20309;&#26102;&#20070;maker&#30340;&#36180;&#29575;&#23545;&#20182;&#20204;&#26377;&#21033;&#12290;&#30001;&#20110;&#20307;&#32946;&#21338;&#24425;&#20165;&#22312;&#32654;&#22269;&#30340;&#24066;&#22330;&#19978;&#23601;&#26159;&#19968;&#20010;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#65292;&#22240;&#27492;&#25214;&#21040;&#36825;&#26679;&#30340;&#26426;&#20250;&#21487;&#33021;&#20250;&#38750;&#24120;&#26377;&#21033;&#21487;&#22270;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20307;&#32946;&#36187;&#26524;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#20934;&#30830;&#24230;&#26469;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20551;&#35774;&#65292;&#23545;&#20110;&#20307;&#32946;&#21338;&#24425;&#38382;&#39064;&#65292;&#27169;&#22411;&#26657;&#20934;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#36187;&#23395;&#30340;NBA&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#20010;&#36187;&#23395;&#19978;&#20351;&#29992;&#24050;&#21457;&#24067;&#30340;&#36180;&#29575;&#36827;&#34892;&#21338;&#24425;&#23454;&#39564;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#21338;&#24425;&#31995;&#32479;&#65292;&#25105;&#20204;&#34920;&#26126;&#20248;&#21270;&#26657;&#20934;&#30340;&#39044;&#27979;&#27169;&#22411;&#27604;&#20248;&#21270;&#20934;&#30830;&#24230;&#24179;&#22343;&#24102;&#26469;&#26356;&#39640;&#30340;&#22238;&#25253;&#29575;&#65288;&#25237;&#36164;&#22238;&#25253;&#29575;&#20026;$110.42&#65285;$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13144</link><description>&lt;p&gt;
&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#12290;&#32467;&#21512;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#22312;&#949;-&#33539;&#25968;&#24847;&#20041;&#19979;&#25509;&#36817;LQR&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#26368;&#36817;&#23558;RHPG&#24212;&#29992;&#20110;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#20013;&#36827;&#34892;&#25299;&#23637;&#20998;&#26512;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#38544;&#31169;&#25918;&#22823;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09624</link><description>&lt;p&gt;
&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#25171;&#30772;&#36890;&#20449;-&#38544;&#31169;-&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy. (arXiv:2302.09624v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#38544;&#31169;&#25918;&#22823;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#32852;&#37030;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#21327;&#35843;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#24120;&#37319;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#25913;&#21892;&#36890;&#20449;&#25928;&#29575;&#30340;&#21516;&#26102;&#24341;&#20837;&#20102;&#23616;&#37096;&#25968;&#25454;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#20123;&#31163;&#25955;&#20540;&#26426;&#21046;&#26159;&#21542;&#25552;&#20379;&#20102;&#20219;&#20309;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#36755;&#20986;&#31354;&#38388;&#30340;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#21508;&#31181;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#21253;&#25324;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20108;&#39033;&#22122;&#22768;&#21644;&#20108;&#39033;&#26426;&#21046;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31232;&#30095;&#21270;&#23545;&#38544;&#31169;&#30340;&#25918;&#22823;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2211.04125</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;MRI&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#39033;&#36328;36&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#20010;&#32593;&#31449;&#27719;&#38598;&#20844;&#24320;&#21487;&#29992;&#30340;MRI&#25968;&#25454;&#21487;&#20197;&#32452;&#35013;&#22823;&#37327;&#21463;&#35797;&#23545;&#35937;&#65292;&#22686;&#21152;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20419;&#36827;&#25968;&#25454;&#37325;&#29992;&#12290;&#22810;&#20013;&#24515;&#25968;&#25454;&#30340;&#21327;&#35843;&#26159;&#20943;&#23569;&#25968;&#25454;&#20013;&#19982;&#38750;&#29983;&#29289;&#26469;&#28304;&#30340;&#21464;&#24322;&#24230;&#37327;&#30340;&#28151;&#26434;&#25928;&#24212;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#23558;&#21327;&#35843;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#30340;&#25972;&#20010;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#65292;&#22240;&#20026;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#26500;&#24314;&#24182;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36807;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;1&#65289;&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#27979;&#37327;&#26041;&#27861;&#65307;2&#65289;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#65292;&#21363;ComBat&#21327;&#35843;&#26041;&#27861;&#30340;&#19968;&#20010;&#23454;&#29616;&#65292;&#23427;&#20801;&#35768;&#23558;&#20854;&#23553;&#35013;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;36&#20010;&#32593;&#31449;&#30340;1740&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;&#22823;&#33041;T1&#21152;&#26435;MRI&#25968;&#25454;&#26469;&#27979;&#35797;&#36825;&#20123;&#24037;&#20855;&#12290;&#32463;&#36807;&#21327;&#35843;&#21518;&#65292;&#32593;&#31449;&#25928;&#24212;&#34987;&#21024;&#38500;&#25110;&#20943;&#23569;&#20102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
&lt;/p&gt;</description></item></channel></rss>