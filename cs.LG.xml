<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02310</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#20154;&#20687;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#25293;&#25668;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#38754;&#37096;&#32918;&#20687;&#65289;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#23454;&#26102;&#21512;&#25104;&#12290; &#32473;&#23450;&#21333;&#20010;RGB&#36755;&#20837;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#30452;&#25509;&#39044;&#27979;&#30001;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#35268;&#33539;&#19977;&#38754;&#22270;&#34920;&#31034;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#36827;&#34892;&#19977;&#32500;&#24863;&#30693;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#24555;&#36895;&#65288;24fps&#65289;&#65292;&#19988;&#20135;&#29983;&#30340;&#36136;&#37327;&#39640;&#20110;&#38656;&#35201;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#30340;&#24378;GAN&#21453;&#28436;&#22522;&#32447;&#12290;&#20026;&#20102;&#35757;&#32451;&#19977;&#38754;&#22270;&#32534;&#30721;&#22120;&#31649;&#36947;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#39044;&#35757;&#32451;&#30340;3D GAN&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#33976;&#39311;&#25104;&#21069;&#39304;&#32534;&#30721;&#22120;&#12290;&#25216;&#26415;&#36129;&#29486;&#21253;&#25324;&#22522;&#20110;Vision Transformer&#30340;&#19977;&#38754;&#22270;&#32534;&#30721;&#22120;&#12289;&#30456;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#33391;&#22909;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#40065;&#26834;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#32534;&#31243;&#21512;&#25104;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#26550;&#26500;&#12289;&#23398;&#20064;&#26041;&#27861;&#12289;&#22635;&#20805;&#37319;&#26679;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02309</link><description>&lt;p&gt;
CodeGen2&#65306;&#32534;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;LLM&#35757;&#32451;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#32534;&#31243;&#21512;&#25104;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#26550;&#26500;&#12289;&#23398;&#20064;&#26041;&#27861;&#12289;&#22635;&#20805;&#37319;&#26679;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#21512;&#25104;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36136;&#37327;&#20284;&#20046;&#30001;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#21644;&#35266;&#23519;&#20540;&#30340;&#20989;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#36890;&#36807;&#21487;&#29992;&#25968;&#25454;&#21644;&#35745;&#31639;&#37327;&#30340;&#25968;&#37327;&#38480;&#21046;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#32479;&#19968;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#20351;LLMs&#30340;&#31243;&#24207;&#21512;&#25104;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65306;&#65288;1&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#65288;2&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#65288;3&#65289;&#22635;&#20805;&#37319;&#26679;&#21644;&#65288;4&#65289;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#28789;&#27963;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35777;&#26126;&#22312;&#20219;&#24847;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#29305;&#24449;&#30340;SVP&#26465;&#20214;</title><link>http://arxiv.org/abs/2305.02304</link><description>&lt;p&gt;
&#25581;&#31034;&#25554;&#20540;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#20043;&#38388;&#30340;&#26032;&#31561;&#20215;&#24615;&#65306;&#26680;&#20989;&#25968;&#21644;&#32467;&#26500;&#21270;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
New Equivalences Between Interpolation and SVMs: Kernels and Structured Features. (arXiv:2305.02304v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#28789;&#27963;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35777;&#26126;&#22312;&#20219;&#24847;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#29305;&#24449;&#30340;SVP&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26680;&#25216;&#24039;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#25214;&#21040;&#26368;&#22823;&#38388;&#38548;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;SVM&#30340;&#20915;&#31574;&#20989;&#25968;&#19982;&#26368;&#23567;&#33539;&#25968;&#26631;&#31614;&#25554;&#20540;&#23436;&#20840;&#37325;&#21512;&#12290;&#36825;&#31181;&#25903;&#25345;&#21521;&#37327;&#22686;&#27542;&#65288;SVP&#65289;&#29616;&#35937;&#29305;&#21035;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#21644;&#26680;&#27169;&#22411;&#20013;&#26080;&#23475;&#25554;&#20540;&#30340;&#26368;&#36817;&#20998;&#26512;&#26469;&#29702;&#35299;SVM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20851;&#20110;SVP&#30340;&#24037;&#20316;&#23545;&#25968;&#25454;/&#29305;&#24449;&#20998;&#24067;&#21644;&#39057;&#35889;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#28789;&#27963;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20219;&#24847;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#23545;&#26631;&#31614;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#31867;&#28789;&#27963;&#24615;&#29305;&#24449;&#36827;&#34892;SVP&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#38480;&#20110;&#19968;&#33324;&#26377;&#30028;&#27491;&#20132;&#31995;&#32479;&#26063;&#65288;&#20363;&#22914;Fourier&#20989;&#25968;&#26063;&#65289;&#29305;&#24449;&#30340;SVP&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g. Fourier f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.02299</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#22312;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21305;&#37197;&#20102;&#23494;&#38598;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#65292;&#21516;&#26102;&#20351;&#24471;&#31232;&#30095;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#24471;&#21040;&#30340;&#27169;&#22411;&#39640;&#24230;&#31232;&#30095;&#65292;&#29702;&#35770;&#19978;&#35757;&#32451;&#26356;&#20415;&#23452;&#65292;&#20294;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#21152;&#36895;&#20381;&#28982;&#20855;&#26377;&#20154;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181; DST &#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450; N:M &#31232;&#30095;&#26041;&#27861;&#65288;&#24120;&#25968;&#25159;&#20837;&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#32463;&#36807;&#23545; PyTorch CPU &#23454;&#29616;&#30340;&#31616;&#21333;&#34920;&#31034;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/calgaryml/condensed-sparsity &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20234;&#26391;&#36710;&#29260;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#36710;&#29260;&#35782;&#21035;&#20998;&#20026;&#20004;&#27493;&#65292;&#31532;&#19968;&#27493;&#26816;&#27979;&#36710;&#29260;&#21306;&#22495;&#65292;&#31532;&#20108;&#27493;&#35782;&#21035;&#36710;&#29260;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2305.02292</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20234;&#26391;&#36710;&#29260;&#35782;&#21035;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Iranian License Plate Recognition Using a Reliable Deep Learning Approach. (arXiv:2305.02292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20234;&#26391;&#36710;&#29260;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#36710;&#29260;&#35782;&#21035;&#20998;&#20026;&#20004;&#27493;&#65292;&#31532;&#19968;&#27493;&#26816;&#27979;&#36710;&#29260;&#21306;&#22495;&#65292;&#31532;&#20108;&#27493;&#35782;&#21035;&#36710;&#29260;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#36710;&#29260;&#35782;&#21035;&#65288;ALPR&#65289;&#26159;&#26368;&#36817;&#20960;&#24180;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22825;&#27668;&#26465;&#20214;&#12289;&#35270;&#35282;&#12289;&#20809;&#32447;&#29366;&#20917;&#12289;&#19981;&#21516;&#30340;&#36710;&#29260;&#23383;&#31526;&#31561;&#31561;&#22240;&#32032;&#37117;&#26159;ALPR&#30340;&#25361;&#25112;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20123;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#23427;&#20204;&#30340;&#27169;&#22411;&#26469;&#25191;&#34892;&#20234;&#26391;&#36710;&#29260;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#36710;&#29260;&#35782;&#21035;&#20998;&#20026;&#20004;&#27493;&#12290;&#31532;&#19968;&#27493;&#26159;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#26816;&#27979;&#20986;&#36710;&#29260;&#30340;&#30697;&#24418;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#36825;&#20123;&#36710;&#29260;&#20174;&#22270;&#20687;&#20013;&#35009;&#21098;&#20986;&#26469;&#36827;&#34892;&#23383;&#31526;&#35782;&#21035;&#12290;&#20026;&#31532;&#19968;&#27493;&#20934;&#22791;&#20102;3065&#24352;&#36710;&#29260;&#29031;&#29255;&#25968;&#25454;&#38598;&#65292;&#31532;&#20108;&#27493;&#20934;&#22791;&#20102;3364&#24352;&#21253;&#25324;&#36710;&#29260;&#23383;&#31526;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The issue of Automatic License Plate Recognition (ALPR) has been one of the most challenging issues in recent years. Weather conditions, camera angle of view, lighting conditions, different characters written on license plates, and many other factors are among the challenges for the issue of ALPR. Given the advances that have been made in recent years in the field of deep neural networks, some types of neural networks and models based on them can be used to perform the task of Iranian license plate recognition. In the proposed method presented in this paper, the license plate recognition is done in two steps. The first step is to detect the rectangles of the license plates from the input image. In the second step, these license plates are cropped from the image and their characters are recognized. For the first step, 3065 images including license plates and for the second step, 3364 images including characters of license plates have been prepared and considered as the desired datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;10$-$90 GHz&#33539;&#22260;&#30340;&#26292;&#38706;&#24773;&#26223;&#65292;&#29983;&#25104;&#20102;&#19968;&#20221;&#32508;&#21512;&#30340;&#12289;&#21487;&#24320;&#28304;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.02260</link><description>&lt;p&gt;
10$-$90 GHz&#33539;&#22260;&#20869;&#30495;&#23454;&#28304;&#23450;&#20301;&#26292;&#38706;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Standardized Benchmark Dataset for Localized Exposure to a Realistic Source at 10$-$90 GHz. (arXiv:2305.02260v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;10$-$90 GHz&#33539;&#22260;&#30340;&#26292;&#38706;&#24773;&#26223;&#65292;&#29983;&#25104;&#20102;&#19968;&#20221;&#32508;&#21512;&#30340;&#12289;&#21487;&#24320;&#28304;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#20813;&#36153;&#20844;&#24320;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#26159;&#35780;&#20272;&#21644;&#21058;&#37327;&#30740;&#31350;&#20013;&#26032;&#25216;&#26415;&#24320;&#21457;&#21644;&#27979;&#35797;&#30340;&#21152;&#37325;&#22240;&#32032;&#12290;&#26412;&#25991;&#38024;&#23545;10$-$90 GHz&#33539;&#22260;&#20869;&#30340;&#31283;&#24577;&#26292;&#38706;&#24773;&#26223;&#65292;&#23545;&#20837;&#23556;&#21151;&#29575;&#23494;&#24230;&#21644;&#30382;&#32932;&#34920;&#38754;&#26368;&#22823;&#28201;&#21319;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#24314;&#27169;&#24182;&#29983;&#25104;&#32508;&#21512;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23545;&#35745;&#31639;&#25216;&#26415;&#30340;&#27979;&#35797;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of freely available standardized datasets represents an aggravating factor during the development and testing the performance of novel computational techniques in exposure assessment and dosimetry research. This hinders progress as researchers are required to generate numerical data (field, power and temperature distribution) anew using simulation software for each exposure scenario. Other than being time consuming, this approach is highly susceptible to errors that occur during the configuration of the electromagnetic model. To address this issue, in this paper, the limited available data on the incident power density and resultant maximum temperature rise on the skin surface considering various steady-state exposure scenarios at 10$-$90 GHz have been statistically modeled. The synthetic data have been sampled from the fitted statistical multivariate distribution with respect to predetermined dosimetric constraints. We thus present a comprehensive and open-source dataset comp
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02252</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Algorithm for Learning with Unknown Distribution Drift. (arXiv:2305.02252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02252
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#23398;&#20064;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#36890;&#29992;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#20174;&#28418;&#31227;&#20998;&#24067;&#30340;&#26368;&#21518;$T$&#27493;&#20013;&#29420;&#31435;&#35266;&#27979;&#21040;&#30340;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;$T$&#26102;&#21051;&#19981;&#21152;&#21306;&#20998;&#22320;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#38656;&#35201;&#20851;&#20110;&#28418;&#31227;&#22823;&#23567;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#26679;&#26412;&#25968;&#25454;&#12290;&#22312;&#19981;&#26126;&#30830;&#20272;&#35745;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#30340;&#20989;&#25968;&#26063;&#30340;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;</title><link>http://arxiv.org/abs/2305.02251</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65306;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. (arXiv:2305.02251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21644;&#31526;&#21495;&#22238;&#24402;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#21644;&#20195;&#29702;&#12290;&#20174;&#8220;&#23439;&#35266;&#8221;&#21644;&#19978;&#19979;&#25991;&#35282;&#24230;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#20063;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35282;&#33394;&#65292;&#24110;&#21161;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#65292;&#20174;Adam&#31995;&#32479;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#21040;&#24403;&#21069;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#30340;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#33258;&#20027;&#24615;&#65292;&#24182;&#20197;&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#20027;&#32423;&#21035;&#20026;&#31867;&#27604;&#12290;&#26368;&#22823;&#32423;&#21035;&#65292;&#31532;&#20116;&#32423;&#65292;&#23450;&#20041;&#20026;&#22312;&#29983;&#20135;&#31185;&#23398;&#30693;&#35782;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#23454;&#29616;&#36825;&#19968;&#28857;&#26159;&#36808;&#21521;&#35299;&#20915;Nobel Turing Grand Challenge&#30340;&#19968;&#27493;&#65306;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#31185;&#23398;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478; - &#33021;&#21147;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#25968;&#25454;&#29420;&#31435;&#30340;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20960;&#20046;&#25152;&#26377;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#37117;&#33021;&#22815;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#25152;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#26696;&#21644;&#38543;&#26426;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.02247</link><description>&lt;p&gt;
&#27627;&#19981;&#30031;&#24807;&#22320;&#36873;&#25321;&#65306;&#20960;&#20046;&#25152;&#26377;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#26696;&#37117;&#33021;&#22815;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#25968;&#25454;&#29420;&#31435;&#30340;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20960;&#20046;&#25152;&#26377;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#37117;&#33021;&#22815;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#25152;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#26696;&#21644;&#38543;&#26426;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#12289;&#25968;&#25454;&#29420;&#31435;&#30340;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#21305;&#37197;&#19978;&#19979;&#19968;&#33324;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#20294;&#25209;&#37327;&#36873;&#25321;&#35268;&#21017;&#26159;&#20219;&#24847;&#30340;&#12290;&#25105;&#20204;&#32771;&#34385;&#20809;&#28369;&#30340;Lipschitz-&#20984;&#24615;/&#38750;&#20984;&#24615;/&#24378;&#20984;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#32463;&#20856;&#19978;&#38480;&#30028;&#38480;&#20063;&#36866;&#29992;&#20110;&#36825;&#26679;&#20219;&#24847;&#30340;&#38750;&#33258;&#36866;&#24212;&#25209;&#37327;&#35843;&#24230;&#65292;&#21253;&#25324;&#25152;&#26377;&#30830;&#23450;&#24615;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#23545;&#20110;&#20984;&#21644;&#24378;&#20984;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30452;&#25509;&#35777;&#26126;&#20102;&#22312;&#19978;&#36848;&#25209;&#37327;&#35843;&#24230;&#31867;&#19978;&#19968;&#33268;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#30340;&#21305;&#37197;&#19979;&#38480;&#30028;&#38480;&#65292;&#34920;&#26126;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#20809;&#28369;&#30340;&#65288;&#38750;Lipschitz&#65289;&#38750;&#20984;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25152;&#32771;&#34385;&#30340;&#31867;&#21035;&#20869;&#65292;&#21253;&#25324;&#25152;&#26377;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20840;&#25209;&#37327;&#65288;&#30830;&#23450;&#24615;&#65289;&#26799;&#24230;&#19979;&#38477;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02231</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#12289;&#20262;&#29702;&#21644;&#20027;&#35201;&#38656;&#27714;&#21040;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation. (arXiv:2305.02231v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#65292;&#20998;&#21035;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#21442;&#19982;&#27969;&#31243;&#21644;&#21442;&#19982;&#32773;&#21487;&#20449;&#24615;&#30340;&#32771;&#37327;&#12290;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#24895;&#26223;&#23558;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#19979;&#35201;&#20214;&#30340;&#25903;&#25345;&#24230;&#20197;&#21450;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;&#35780;&#20272;&#19971;&#20010;&#38656;&#27714;&#20043;&#25216;&#26415;&#26041;&#38754;&#12289;&#20262;&#29702;&#26041;&#38754;&#21644;&#30417;&#31649;&#25361;&#25112;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;LESS-VFL&#26041;&#27861;&#65292;&#29992;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#39640;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#30701;&#26242;&#30340;&#39044;&#35757;&#32451;&#21644;&#26412;&#22320;&#29305;&#24449;&#36873;&#25321;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#27169;&#22411;&#35757;&#32451;&#20013;&#21024;&#38500;&#34394;&#20551;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.02219</link><description>&lt;p&gt;
LESS-VFL&#65306;&#29992;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#39640;&#25928;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning. (arXiv:2305.02219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02219
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LESS-VFL&#26041;&#27861;&#65292;&#29992;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#39640;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#30701;&#26242;&#30340;&#39044;&#35757;&#32451;&#21644;&#26412;&#22320;&#29305;&#24449;&#36873;&#25321;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#27169;&#22411;&#35757;&#32451;&#20013;&#21024;&#38500;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LESS-VFL&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#31446;&#30452;&#20998;&#21106;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#36890;&#20449;&#39640;&#25928;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#30001;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#24102;&#26377;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#21442;&#19982;&#32773;&#32452;&#25104;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20849;&#20139;&#26679;&#26412;ID&#31354;&#38388;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#12290;&#21442;&#19982;&#32773;&#24076;&#26395;&#21327;&#20316;&#22320;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#35757;&#32451;&#30340;&#19968;&#37096;&#20998;&#65292;&#21442;&#19982;&#32773;&#24076;&#26395;&#22312;&#31995;&#32479;&#20013;&#21024;&#38500;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;LESS-VFL&#20013;&#65292;&#32463;&#36807;&#30701;&#26242;&#30340;&#39044;&#35757;&#32451;&#21518;&#65292;&#26381;&#21153;&#22120;&#20248;&#21270;&#20854;&#20840;&#23616;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#30830;&#23450;&#26469;&#33258;&#21442;&#19982;&#32773;&#27169;&#22411;&#30340;&#30456;&#20851;&#36755;&#20986;&#12290;&#36825;&#20123;&#20449;&#24687;&#19982;&#21442;&#19982;&#32773;&#20849;&#20139;&#65292;&#28982;&#21518;&#20801;&#35768;&#26412;&#22320;&#29305;&#24449;&#36873;&#25321;&#32780;&#26080;&#38656;&#36890;&#20449;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;LESS-VFL&#21487;&#20197;&#20174;&#27169;&#22411;&#35757;&#32451;&#20013;&#21024;&#38500;&#34394;&#20551;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;LESS-VFL&#21487;&#20197;&#20197;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30340;&#36890;&#20449;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#33719;&#24471;&#39640;&#31934;&#24230;&#24182;&#21024;&#38500;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.02217</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#24448;&#24448;&#38543;&#30528;&#26102;&#38388;&#30340;&#31215;&#32047;&#20197;&#27969;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#20174;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19981;&#21516;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#33021;&#24573;&#35270;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#26159;&#26080;&#20241;&#27490;&#30340;&#12289;&#35268;&#27169;&#24040;&#22823;&#12289;&#21464;&#21270;&#26410;&#30693;&#65292;&#24182;&#19988;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;/&#23384;&#20648;&#36164;&#28304;&#21487;&#20197;&#21450;&#26102;&#22788;&#29702;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#34987;&#21450;&#26102;&#22320;&#26377;&#25928;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#65292;&#20877;&#21152;&#19978;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#27969;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26680;&#31639;&#23376;&#22312;&#20165;&#21462;$\pm 1$&#20540;&#30340;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#31616;&#21270;&#31283;&#23450;&#24615;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02213</link><description>&lt;p&gt;
&#20851;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
On the stability test for reproducing kernel Hilbert spaces. (arXiv:2305.02213v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02213
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26680;&#31639;&#23376;&#22312;&#20165;&#21462;$\pm 1$&#20540;&#30340;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#31616;&#21270;&#31283;&#23450;&#24615;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHSs)&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#20854;&#25152;&#26377;&#35780;&#20272;&#27867;&#20989;&#37117;&#26159;&#32447;&#24615;&#19988;&#26377;&#30028;&#30340;&#12290;&#23427;&#20204;&#19982;&#31216;&#20026;&#26680;&#30340;&#27491;&#23450;&#26144;&#23556;&#19968;&#19968;&#23545;&#24212;&#12290;&#31283;&#23450;&#30340;RKHSs&#20855;&#26377;&#20165;&#21253;&#21547;&#20989;&#25968;&#21644;&#32477;&#23545;&#21487;&#31215;&#30340;&#38468;&#21152;&#23646;&#24615;&#12290;&#24050;&#30693;&#25991;&#29486;&#20013;RKHS&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65306;&#30001;&#26680;&#24341;&#36215;&#30340;&#31215;&#20998;&#31639;&#23376;&#24517;&#39035;&#20316;&#20026;&#20174;&#26412;&#36136;&#26377;&#30028;&#65288;&#27979;&#35797;&#65289;&#20989;&#25968;&#30340;&#31354;&#38388;$\mathcal{L}_{\infty}$&#21040;&#32477;&#23545;&#21487;&#31215;&#20989;&#25968;&#30340;&#31354;&#38388;$\mathcal{L}_1$&#30340;&#26144;&#23556;&#34987;&#38480;&#21046;&#12290;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#30340;Mercer(&#36830;&#32493;)&#26680;&#21644;&#25972;&#20010;&#31163;&#25955;&#26102;&#38388;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31283;&#23450;&#24615;&#27979;&#35797;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26680;&#31639;&#23376;&#22312;&#20165;&#21462;$\pm 1$&#20540;(&#20960;&#20046;&#22788;&#22788;)&#30340;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#30740;&#31350;&#12290;&#23427;&#20204;&#20195;&#34920;&#20102;&#30740;&#31350;RKHS&#20013;&#20219;&#20309;&#21333;&#20010;&#20803;&#32032;&#30340;&#31283;&#23450;&#24615;&#25152;&#38656;&#30340;&#30456;&#21516;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;RKHS&#30340;&#31283;&#23450;&#24615;&#27979;&#35797;&#34987;&#31616;&#21270;&#20026;&#28041;&#21450;&#26377;&#38480;&#25968;&#37327;&#30340;&#27979;&#35797;&#20989;&#25968;&#30340;&#20004;&#32423;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert spaces (RKHSs) are special Hilbert spaces where all the evaluation functionals are linear and bounded. They are in one-to-one correspondence with positive definite maps called kernels. Stable RKHSs enjoy the additional property of containing only functions and absolutely integrable. Necessary and sufficient conditions for RKHS stability are known in the literature: the integral operator induced by the kernel must be bounded as map between $\mathcal{L}_{\infty}$, the space of essentially bounded (test) functions, and $\mathcal{L}_1$, the space of absolutely integrable functions. Considering Mercer (continuous) kernels in continuous-time and the entire discrete-time class, we show that the stability test can be reduced to the study of the kernel operator over test functions which assume (almost everywhere) only the values $\pm 1$. They represent the same functions needed to investigate stability of any single element in the RKHS. In this way, the RKHS stability
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DeepIM&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02200</link><description>&lt;p&gt;
&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#30340;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Graph Representation Learning and Optimization for Influence Maximization. (arXiv:2305.02200v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DeepIM&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#34987;&#23450;&#20041;&#20026;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#36873;&#25321;&#19968;&#32452;&#21021;&#22987;&#29992;&#25143;&#20197;&#26368;&#22823;&#21270;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20854;&#29702;&#35770;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#21319;&#25509;&#36817;&#20110;&#26497;&#38480;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;IM&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#26410;&#30693;&#22270;&#24418;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;IM&#26041;&#27861;&#30340;&#21457;&#23637;&#20173;&#28982;&#21463;&#21040;&#22522;&#26412;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#65306;1&#65289;&#26377;&#25928;&#35299;&#20915;&#30446;&#26631;&#20989;&#25968;&#30340;&#22256;&#38590;&#24615;&#65307;2&#65289;&#34920;&#24449;&#22810;&#26679;&#21270;&#30340;&#22522;&#30784;&#25193;&#25955;&#27169;&#24335;&#30340;&#22256;&#38590;&#24615;&#65307;&#20197;&#21450;3&#65289;&#22312;&#21508;&#31181;&#33410;&#28857;&#20013;&#24515;&#24615;&#32422;&#26463;&#30340;IM&#21464;&#20307;&#19979;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DeepIM&#26469;&#29983;&#25104;&#22320;&#34920;&#24449;&#31181;&#23376;&#38598;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#22810;&#26679;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#22823;&#33041;&#36830;&#25509;&#32452;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#21367;&#31215;&#22836;&#37096;&#65292;&#21253;&#25324;&#36793;&#32536;&#21644;&#33410;&#28857;&#65292;&#20840;&#38754;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#33021;&#20174;&#36830;&#25509;&#32452;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.02199</link><description>&lt;p&gt;
&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32467;&#26500;&#36830;&#25509;&#32452;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Graph Convolutional Network for Structural Connectome Classification. (arXiv:2305.02199v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#22823;&#33041;&#36830;&#25509;&#32452;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#21367;&#31215;&#22836;&#37096;&#65292;&#21253;&#25324;&#36793;&#32536;&#21644;&#33410;&#28857;&#65292;&#20840;&#38754;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#33021;&#20174;&#36830;&#25509;&#32452;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#22522;&#20110;&#25193;&#25955;&#30913;&#20849;&#25391;&#22270;&#20687;&#25552;&#21462;&#30340;&#22823;&#33041;&#36830;&#25509;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#22810;&#22836;&#24182;&#34892;GCN&#26426;&#21046;&#65292;&#20998;&#21035;&#23545;&#22823;&#33041;&#36830;&#25509;&#36755;&#20837;&#22270;&#36827;&#34892;&#22788;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#22836;&#37096;&#65292;&#28041;&#21450;&#36793;&#32536;&#21644;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#65292;&#20805;&#20998;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#22823;&#33041;&#36830;&#25509;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#34920;&#24449;&#20102;&#36830;&#25509;&#32452;&#22312;&#24615;&#21035;&#26041;&#38754;&#30340;&#21464;&#21270;&#31243;&#24230;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#25105;&#20204;&#23545;&#20004;&#24615;&#20581;&#24247;&#21644;&#30142;&#30149;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;PREVENT-AD&#65288;347&#20010;&#21463;&#35797;&#32773;&#65289;&#21644;OASIS3&#65288;771&#20010;&#21463;&#35797;&#32773;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle classification based on brain connectivity derived from diffusion magnetic resonance images. We propose a machine-learning model inspired by graph convolutional networks (GCNs), which takes a brain connectivity input graph and processes the data separately through a parallel GCN mechanism with multiple heads. The proposed network is a simple design that employs different heads involving graph convolutions focused on edges and nodes, capturing representations from the input data thoroughly. To test the ability of our model to extract complementary and representative features from brain connectivity data, we chose the task of sex classification. This quantifies the degree to which the connectome varies depending on the sex, which is important for improving our understanding of health and disease in both sexes. We show experiments on two publicly available datasets: PREVENT-AD (347 subjects) and OASIS3 (771 subjects). The proposed model demonstrates the highest performance compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#24425;&#31080;&#38382;&#39064;&#20013;&#22270;&#30340;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#33410;&#28857;&#36755;&#20837;&#36755;&#20986;&#29305;&#24449;&#30340;&#23545;&#31216;&#20462;&#21098;&#25216;&#26415;&#21644;&#20445;&#30041;&#29305;&#24449;&#22270;&#31354;&#38388;&#20301;&#32622;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02190</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#24425;&#31080;: &#22270;&#30340;&#31232;&#30095;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#24425;&#31080;&#38382;&#39064;&#20013;&#22270;&#30340;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#33410;&#28857;&#36755;&#20837;&#36755;&#20986;&#29305;&#24449;&#30340;&#23545;&#31216;&#20462;&#21098;&#25216;&#26415;&#21644;&#20445;&#30041;&#29305;&#24449;&#22270;&#31354;&#38388;&#20301;&#32622;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#29468;&#24819; (LTH) &#22768;&#31216;&#23384;&#22312;&#19968;&#31181;&#33719;&#32988;&#30340;&#24425;&#31080; (&#21363;&#65292;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#20462;&#21098;&#30340;&#23376;&#32593;&#32476;&#20197;&#21450;&#21407;&#22987;&#26435;&#37325;&#21021;&#22987;&#21270;)&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#31216;&#20026; UGS&#65292;&#25193;&#23637;&#20102; LTH &#20197;&#20462;&#21098;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN)&#65292;&#20197;&#26377;&#25928;&#21152;&#36895; GNN &#25512;&#29702;&#12290;UGS &#21516;&#26102;&#20351;&#29992;&#30456;&#21516;&#30340;&#23631;&#34109;&#26426;&#21046;&#20462;&#21098;&#22270;&#37051;&#25509;&#30697;&#38453;&#21644;&#27169;&#22411;&#26435;&#37325;&#65292;&#20294;&#30001;&#20110;&#22270;&#37051;&#25509;&#30697;&#38453;&#21644;&#26435;&#37325;&#30697;&#38453;&#30340;&#35282;&#33394;&#38750;&#24120;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#31232;&#30095;&#21270;&#23548;&#33268;&#19981;&#21516;&#30340;&#24615;&#33021;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22270;&#30340;&#31232;&#30095;&#31243;&#24230;&#36229;&#36807;&#19968;&#23450;&#31243;&#24230;&#26102;&#65292;&#31232;&#30095; GNN &#30340;&#24615;&#33021;&#20250;&#26174;&#30528;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#24403;&#22270;&#30340;&#31232;&#30095;&#31243;&#24230;&#36739;&#39640;&#26102;&#30340; GNN &#24615;&#33021;&#12290;&#39318;&#20808;&#65292;UGS &#20351;&#29992;&#20002;&#22833;&#20844;&#24335;&#20462;&#21098;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#32780;&#65292;&#35813;&#25216;&#26415;&#24182;&#26410;&#36866;&#24403;&#28041;&#21450;&#37051;&#25509;&#30697;&#38453;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31216;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20462;&#21098;&#37051;&#25509;&#30697;&#38453;&#30340;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#33410;&#28857;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#29305;&#24449;&#34987;&#20445;&#30041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#23427;&#36991;&#20813;&#20102;&#30452;&#25509;&#20462;&#21098;&#22270;&#32467;&#26500;&#65292;&#32780;&#26159;&#22312;&#20445;&#30041;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20301;&#32622;&#30340;&#21516;&#26102;&#20462;&#21098;&#20102; GNN &#23618;&#30340;&#29305;&#24449;&#36890;&#36947;&#65292;&#31867;&#20284;&#20110;&#22270;&#20687;&#21367;&#31215;&#32593;&#32476;&#37319;&#29992;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102; 7.0% &#21644; 5.0x&#12290;
&lt;/p&gt;
&lt;p&gt;
Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02171</link><description>&lt;p&gt;
&#25345;&#32493;&#25512;&#29702;&#65306;&#22312;&#31070;&#32463;&#31526;&#21495; AI &#20013;&#20351;&#29992;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#38750;&#21333;&#35843;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning. (arXiv:2305.02171v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#22312;&#30456;&#20284;&#24615;&#25512;&#29702;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#25237;&#36164;&#21644;&#20196;&#20154;&#30633;&#30446;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#22312;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#24418;&#24335;&#65292;&#22914;&#38750;&#21333;&#35843;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#38750;&#21333;&#35843;&#26159;&#38750;&#32463;&#20856;&#25512;&#29702;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#36890;&#24120;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#30475;&#21040;&#65292;&#25512;&#29702;&#31995;&#32479;&#20801;&#35768;&#65288;&#19982;&#21476;&#20856;&#36923;&#36753;&#19981;&#21516;&#65289;&#20316;&#20986;&#21487;&#33021;&#31245;&#21518;&#34987;&#25764;&#22238;&#30340;&#32467;&#35770;&#65292;&#24403;&#26377;&#26032;&#20449;&#24687;&#21487;&#29992;&#26102;&#12290;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65288;&#22914;&#36923;&#36753;&#24352;&#37327;&#32593;&#32476;&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;LTN&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#20174;&#30693;&#35782;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22238;&#24518;&#30340;&#23398;&#20064;&#35838;&#31243;&#23558;&#25345;&#32493;&#23398;&#20064;&#21152;&#20837;&#21040;LTN&#20013;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#36807;&#31243;&#20026;&#8220;&#25345;&#32493;&#25512;&#29702;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the extensive investment and impressive recent progress at reasoning by similarity, deep learning continues to struggle with more complex forms of reasoning such as non-monotonic and commonsense reasoning. Non-monotonicity is a property of non-classical reasoning typically seen in commonsense reasoning, whereby a reasoning system is allowed (differently from classical logic) to jump to conclusions which may be retracted later, when new information becomes available. Neural-symbolic systems such as Logic Tensor Networks (LTN) have been shown to be effective at enabling deep neural networks to achieve reasoning capabilities. In this paper, we show that by combining a neural-symbolic system with methods from continual learning, LTN can obtain a higher level of accuracy when addressing non-monotonic reasoning tasks. Continual learning is added to LTNs by adopting a curriculum of learning from knowledge and data with recall. We call this process Continual Reasoning, a new methodolog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02164</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#26465;&#20214;&#21644;&#23616;&#37096;&#36830;&#25509;&#20999;&#29255;Wasserstein&#27969;&#37327;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#27969;&#65288;SWF&#65289;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#29983;&#25104;&#24314;&#27169;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#21457;&#29983;&#29983;&#25104;&#36136;&#37327;&#30340;&#20122;&#20248;&#24615;&#21644;&#32570;&#20047;&#26465;&#20214;&#24314;&#27169;&#33021;&#21147;&#32780;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#19968;&#20010;&#24841;&#24742;&#30340;&#35266;&#23519;&#65288;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65289;&#65292;&#32852;&#21512;&#20998;&#24067;&#30340;SWF&#19982;&#26465;&#20214;&#20998;&#24067;&#30340;SWF&#30456;&#31526;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#36825;&#26159;SWF&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#24403;&#30340;&#22270;&#20687;&#24402;&#32435;&#20559;&#32622;&#21040;SWF&#20013;&#65292;&#29992;&#20004;&#20010;&#25216;&#26415;&#21463;&#21040;&#35270;&#35273;&#30740;&#31350;&#20013;&#30340;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#36890;&#36807;&#20840;&#37096;&#25913;&#36827;&#65292;&#22312;&#36827;&#34892;&#32431;&#38750;&#21442;&#25968;&#24314;&#27169;&#30340;&#21516;&#26102;&#65292;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#35768;&#22810;&#28145;&#24230;&#21442;&#25968;&#21270;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#12289;&#39046;&#22495;&#36866;&#37197;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#22312;&#32454;&#32990;&#27700;&#24179;&#19978;&#23545;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#21462;&#24471;&#20102;&#20808;&#36827;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02148</link><description>&lt;p&gt;
&#32454;&#32990;&#27700;&#24179;&#30340;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level. (arXiv:2305.02148v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#12289;&#39046;&#22495;&#36866;&#37197;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#22312;&#32454;&#32990;&#27700;&#24179;&#19978;&#23545;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#21462;&#24471;&#20102;&#20808;&#36827;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#32454;&#32990;&#27700;&#24179;&#30340;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#26368;&#23567;&#21270;HPA&#21644;HubMAP&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#25429;&#33719;&#35774;&#32622;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#22312;&#32454;&#32990;&#27700;&#24179;&#30340;&#20998;&#21106;&#26041;&#38754;&#65292;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#25311;&#30340;&#32467;&#26524;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/VSydorskyy/hubmap_2022_htt_solution &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for functional tissue unit segmentation at the cellular level, which utilizes the latest deep learning semantic segmentation approaches together with domain adaptation and semi-supervised learning techniques. This approach allows for minimizing the domain gap, class imbalance, and captures settings influence between HPA and HubMAP datasets. The presented approach achieves comparable with state-of-the-art-result in functional tissue unit segmentation at the cellular level. The source code is available at https://github.com/VSydorskyy/hubmap_2022_htt_solution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;&#65292;&#20197;&#26356;&#30452;&#35266;&#30340;&#26041;&#24335;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65292;&#25351;&#20986;&#27424;&#25311;&#21512;&#30340;&#21407;&#22240;&#26159;&#30001;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#32780;&#24341;&#36215;&#30340;&#65292;&#23545;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#20248;&#21270;&#21017;&#26159;&#36890;&#36807;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#26356;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#26469;&#23454;&#29616;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#35838;&#31243;&#20462;&#27491;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#23545;&#20110;&#40065;&#26834;&#24615;&#20063;&#26377;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02139</link><description>&lt;p&gt;
&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;&#65292;&#20197;&#26356;&#30452;&#35266;&#30340;&#26041;&#24335;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65292;&#25351;&#20986;&#27424;&#25311;&#21512;&#30340;&#21407;&#22240;&#26159;&#30001;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#32780;&#24341;&#36215;&#30340;&#65292;&#23545;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#20248;&#21270;&#21017;&#26159;&#36890;&#36807;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#26356;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#26469;&#23454;&#29616;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#35838;&#31243;&#20462;&#27491;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#23545;&#20110;&#40065;&#26834;&#24615;&#20063;&#26377;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#20854;&#40065;&#26834;&#24615;&#36890;&#24120;&#30001;&#19982;&#35757;&#32451;&#21160;&#24577;&#26080;&#20851;&#30340;&#29702;&#35770;&#30028;&#38480;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#33021;&#26080;&#27861;&#34920;&#24449;&#23454;&#35777;&#24615;&#33021;&#65292;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#20250;&#27424;&#25311;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#37325;&#20889;&#25104;&#20855;&#26377;&#30456;&#21516;&#31867;-&#20998;&#25968;&#38388;&#38548;&#21644;&#19981;&#21516;&#26679;&#26412;&#21152;&#26435;&#20989;&#25968;&#24418;&#24335;&#30340;&#24418;&#24335;&#12290;&#25152;&#24471;&#21040;&#30340;&#35838;&#31243;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30452;&#35266;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#23558;&#27424;&#25311;&#21512;&#24402;&#22240;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#21644;&#23558;&#22122;&#22768;&#40065;&#26834;&#24615;&#24402;&#22240;&#20110;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#36739;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#35838;&#31243;&#35270;&#35282;&#36827;&#34892;&#31616;&#21333;&#30340;&#20462;&#27491;&#21487;&#20197;&#20351;&#27424;&#25311;&#21512;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#37319;&#29992;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#20195;&#30721;&#21487;&#22312;\url{github}&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust loss functions are designed to combat the adverse impacts of label noise, whose robustness is typically supported by theoretical bounds agnostic to the training dynamics. However, these bounds may fail to characterize the empirical performance as it remains unclear why robust loss functions can underfit. We show that most loss functions can be rewritten into a form with the same class-score margin and different sample-weighting functions. The resulting curriculum view provides a straightforward analysis of the training dynamics, which helps attribute underfitting to diminished average sample weights and noise robustness to larger weights for clean samples. We show that simple fixes to the curriculums can make underfitting robust loss functions competitive with the state-of-the-art, and training schedules can substantially affect the noise robustness even with robust loss functions. Code is available at \url{github}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02128</link><description>&lt;p&gt;
&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65306;&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#24230;&#37327;&#34892;&#20026;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31185;&#23398;&#25552;&#20379;&#20102;&#22810;&#26679;&#24615;&#20855;&#26377;&#38887;&#24615;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#24378;&#21046;&#35201;&#27714;&#21516;&#36136;&#24615;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25928;&#29575;&#12290;&#24403;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#19981;&#21463;&#21516;&#36136;&#31574;&#30053;&#30340;&#38480;&#21046;&#26102;&#65292;&#20010;&#20307;&#20195;&#29702;&#21487;&#33021;&#20250;&#21457;&#23637;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20135;&#29983;&#26377;&#21033;&#20110;&#31995;&#32479;&#30340;&#26032;&#20852;&#20114;&#34917;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32570;&#20047;&#34913;&#37327;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#20013;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#24037;&#20855;&#24847;&#21619;&#30528;&#25105;&#20204;&#26080;&#27861;&#28145;&#20837;&#20102;&#35299;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65288;SND&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#25506;&#35752;&#24182;&#35777;&#26126;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#34892;&#20026;&#22810;&#26679;&#24615;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
&lt;/p&gt;</description></item><item><title>Bicubic++&#26159;&#19968;&#31181;&#24037;&#19994;&#32423;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#30340;&#31354;&#38388;&#32500;&#24230;&#24182;&#23398;&#20064;&#24555;&#36895;&#21487;&#36870;&#38477;&#32423;&#21644;&#36739;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20840;&#23616;&#32467;&#26500;&#21098;&#26525;&#21644;&#21435;&#20559;&#24046;&#22788;&#29702;&#20248;&#21270;&#35009;&#21098;&#32593;&#32476;&#30340;PSNR&#65292;&#35813;&#26041;&#27861;&#33021;&#22312;&#25152;&#26377;SR&#25968;&#25454;&#38598;&#19978;&#20026;Bicubic&#19978;&#37319;&#26679;PSNR&#22686;&#21152;&#32422;1dB&#12290;</title><link>http://arxiv.org/abs/2305.02126</link><description>&lt;p&gt;
Bicubic++&#65306;&#35774;&#35745;&#19968;&#31181;&#24037;&#19994;&#32423;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#8212;&#8212;&#26356;&#23567;&#65292;&#26356;&#36731;&#65292;&#26356;&#34180;
&lt;/p&gt;
&lt;p&gt;
Bicubic++: Slim, Slimmer, Slimmest -- Designing an Industry-Grade Super-Resolution Network. (arXiv:2305.02126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02126
&lt;/p&gt;
&lt;p&gt;
Bicubic++&#26159;&#19968;&#31181;&#24037;&#19994;&#32423;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#30340;&#31354;&#38388;&#32500;&#24230;&#24182;&#23398;&#20064;&#24555;&#36895;&#21487;&#36870;&#38477;&#32423;&#21644;&#36739;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20840;&#23616;&#32467;&#26500;&#21098;&#26525;&#21644;&#21435;&#20559;&#24046;&#22788;&#29702;&#20248;&#21270;&#35009;&#21098;&#32593;&#32476;&#30340;PSNR&#65292;&#35813;&#26041;&#27861;&#33021;&#22312;&#25152;&#26377;SR&#25968;&#25454;&#38598;&#19978;&#20026;Bicubic&#19978;&#37319;&#26679;PSNR&#22686;&#21152;&#32422;1dB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Bicubic++&#30340;&#23454;&#26102;&#36731;&#37327;&#32423;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#32593;&#32476;&#12290;&#34429;&#28982;&#25972;&#20010;&#32593;&#32476;&#37117;&#20351;&#29992;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#31354;&#38388;&#32500;&#24230;&#65292;&#20294;Bicubic++&#39318;&#20808;&#23398;&#20064;&#20102;&#22270;&#20687;&#30340;&#24555;&#36895;&#21487;&#36870;&#38477;&#32423;&#21644;&#36739;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#35757;&#32451;&#31649;&#36947;&#65292;&#20854;&#20013;&#24212;&#29992;&#20102;&#21367;&#31215;&#23618;&#30340;&#31471;&#21040;&#31471;&#20840;&#23616;&#32467;&#26500;&#21098;&#26525;&#65292;&#32780;&#19981;&#20351;&#29992;&#20687;&#24133;&#24230;&#21644;&#26799;&#24230;&#33539;&#25968;&#36825;&#26679;&#30340;&#24230;&#37327;&#65292;&#24182;&#19987;&#27880;&#20110;&#20248;&#21270;&#35009;&#21098;&#32593;&#32476;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;PSNR&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20559;&#24046;&#39033;&#22312;&#22686;&#21152;PSNR&#26102;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#23545;&#21367;&#31215;&#23618;&#24212;&#29992;&#20102;&#21435;&#20559;&#24046;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;SR&#25968;&#25454;&#38598;&#19978;&#20026;Bicubic&#19978;&#37319;&#26679;PSNR&#22686;&#21152;&#20102;&#32422;1dB&#65292;&#24182;&#22312;720p&#36755;&#20837;&#21644;4K&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#20197;FP16&#31934;&#24230;&#22312;RTX3090&#19978;&#36816;&#34892;1.17ms&#65292;&#22312;RTX3070&#19978;&#36816;&#34892;2.9ms&#12290;Bicubic++&#36824;&#33719;&#24471;&#20102;NTIRE 2023 RTSR Track 2 x3&#30340;&#32988;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a real-time and lightweight single-image super-resolution (SR) network named Bicubic++. Despite using spatial dimensions of the input image across the whole network, Bicubic++ first learns quick reversible downgraded and lower resolution features of the image in order to decrease the number of computations. We also construct a training pipeline, where we apply an end-to-end global structured pruning of convolutional layers without using metrics like magnitude and gradient norms, and focus on optimizing the pruned network's PSNR on the validation set. Furthermore, we have experimentally shown that the bias terms take considerable amount of the runtime while increasing PSNR marginally, hence we have also applied bias removal to the convolutional layers. Our method adds ~1dB on Bicubic upscaling PSNR for all tested SR datasets and runs with ~1.17ms on RTX3090 and ~2.9ms on RTX3070, for 720p inputs and 4K outputs, both in FP16 precision. Bicubic++ won NTIRE 2023 RTSR Track 2 x3 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20915;&#31574;&#26641;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#37319;&#38598;&#29305;&#24449;&#20540;&#26469;&#38477;&#20302;&#25104;&#26412;&#65292;&#21516;&#26102;&#20351;&#29992;&#21518;&#39564;&#25277;&#26679;&#26041;&#26696;&#26469;&#20445;&#25345;&#22312;&#32447;&#39044;&#27979;&#30340;&#20302;&#36951;&#25022;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02093</link><description>&lt;p&gt;
&#24102;&#26377;&#20027;&#21160;&#29305;&#24449;&#33719;&#21462;&#30340;&#39640;&#25928;&#22312;&#32447;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Decision Tree Learning with Active Feature Acquisition. (arXiv:2305.02093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02093
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20915;&#31574;&#26641;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#37319;&#38598;&#29305;&#24449;&#20540;&#26469;&#38477;&#20302;&#25104;&#26412;&#65292;&#21516;&#26102;&#20351;&#29992;&#21518;&#39564;&#25277;&#26679;&#26041;&#26696;&#26469;&#20445;&#25345;&#22312;&#32447;&#39044;&#27979;&#30340;&#20302;&#36951;&#25022;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20915;&#31574;&#26641;&#22312;&#32447;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#36827;&#20837;&#30340;&#25968;&#25454;&#28857;&#30340;&#29305;&#24449;&#24050;&#32463;&#20934;&#22791;&#22909;&#20102;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29305;&#24449;&#20540;&#21644;&#26631;&#31614;&#37117;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#33021;&#20197;&#19968;&#23450;&#30340;&#25104;&#26412;&#33719;&#21462;&#12290;&#20363;&#22914;&#65292;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#65292;&#21307;&#29983;&#24517;&#39035;&#36873;&#25321;&#23545;&#30149;&#20154;&#36827;&#34892;&#21738;&#20123;&#27979;&#35797;&#65288;&#21363;&#36827;&#34892;&#26114;&#36149;&#30340;&#29305;&#24449;&#26597;&#35810;&#65289;&#65292;&#20197;&#20415;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65288;&#21363;&#39044;&#27979;&#26631;&#31614;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#23884;&#20837;&#22312;&#32447;&#23398;&#20064;&#26041;&#26696;&#30340;&#20027;&#21160;&#35745;&#21010;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#20449;&#24687;&#25910;&#38598;&#21151;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#23376;&#27169;&#24615;&#30340;&#20195;&#29702;&#20449;&#24687;&#33719;&#21462;&#20989;&#25968;&#65292;&#20197;&#26368;&#23567;&#30340;&#25104;&#26412;&#20027;&#21160;&#26597;&#35810;&#29305;&#24449;&#20540;&#65292;&#21516;&#26102;&#20351;&#29992;&#21518;&#39564;&#25277;&#26679;&#26041;&#26696;&#26469;&#20445;&#25345;&#22312;&#32447;&#39044;&#27979;&#30340;&#20302;&#36951;&#25022;&#24230;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing decision trees online is a classical machine learning problem. Existing works often assume that features are readily available for each incoming data point. However, in many real world applications, both feature values and the labels are unknown a priori and can only be obtained at a cost. For example, in medical diagnosis, doctors have to choose which tests to perform (i.e., making costly feature queries) on a patient in order to make a diagnosis decision (i.e., predicting labels). We provide a fresh perspective to tackle this practical challenge. Our framework consists of an active planning oracle embedded in an online learning scheme for which we investigate several information acquisition functions. Specifically, we employ a surrogate information acquisition function based on adaptive submodularity to actively query feature values with a minimal cost, while using a posterior sampling scheme to maintain a low regret for online prediction. We demonstrate the efficiency a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30740;&#31350;&#20102;&#21367;&#31215;&#20113;&#30340;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#12290;&#21151;&#33021;&#23646;&#24615;&#26041;&#27861;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#20851;&#31995;&#65292;&#20102;&#35299;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#21367;&#31215;&#20113;&#20013;&#30340;&#20912;&#26230;&#25968;&#27987;&#24230;&#21644;&#20912;&#27700;&#21547;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02090</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29702;&#35299;&#21367;&#31215;&#20113;
&lt;/p&gt;
&lt;p&gt;
Understanding cirrus clouds using explainable machine learning. (arXiv:2305.02090v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30740;&#31350;&#20102;&#21367;&#31215;&#20113;&#30340;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#12290;&#21151;&#33021;&#23646;&#24615;&#26041;&#27861;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#20851;&#31995;&#65292;&#20102;&#35299;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#21367;&#31215;&#20113;&#20013;&#30340;&#20912;&#26230;&#25968;&#27987;&#24230;&#21644;&#20912;&#27700;&#21547;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#20113;&#26159;&#22320;&#29699;&#27668;&#20505;&#30340;&#20851;&#38190;&#35843;&#33410;&#22240;&#32032;&#65292;&#20294;&#23427;&#20204;&#19982;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#30340;&#20851;&#31995;&#26159;&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#20013;&#26368;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#24180;&#30340;&#21355;&#26143;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#65292;&#30740;&#31350;&#21367;&#31215;&#20113;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#23618;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#39044;&#27979;&#20912;&#27700;&#21547;&#37327;&#21644;&#20912;&#26230;&#25968;&#27987;&#24230;&#12290;&#27169;&#22411;&#34920;&#26126;&#65292;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#65292;$R^2=0.49$&#12290;&#20351;&#29992;SHapley Additive exPlanations (SHAP)&#35745;&#31639;&#21151;&#33021;&#23646;&#24615;&#65292;&#20197;&#37327;&#21270;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#19982;&#21367;&#31215;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20363;&#22914;&#65292;&#24341;&#36215;&#20912;&#26230;&#25968;&#27987;&#24230;&#39044;&#27979;&#19979;&#38477;&#25152;&#38656;&#30340;&#36229;&#24494;&#31859;&#23576;&#22467;&#31890;&#23376;&#30340;&#26368;&#23567;&#27987;&#24230;&#20026;$2 \times 10^{-4}$ mg m\textsuperscript{-3}&#12290;&#35266;&#23519;&#21069;15&#20010;&#23567;&#26102;
&lt;/p&gt;
&lt;p&gt;
Cirrus clouds are key modulators of Earth's climate. Their dependencies on meteorological and aerosol conditions are among the largest uncertainties in global climate models. This work uses three years of satellite and reanalysis data to study the link between cirrus drivers and cloud properties. We use a gradient-boosted machine learning model and a Long Short-Term Memory (LSTM) network with an attention layer to predict the ice water content and ice crystal number concentration. The models show that meteorological and aerosol conditions can predict cirrus properties with $R^2 = 0.49$. Feature attributions are calculated with SHapley Additive exPlanations (SHAP) to quantify the link between meteorological and aerosol conditions and cirrus properties. For instance, the minimum concentration of supermicron-sized dust particles required to cause a decrease in ice crystal number concentration predictions is $2 \times 10^{-4}$ mg m\textsuperscript{-3}. The last 15 hours before the observat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02054</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20869;&#23384;&#33410;&#32422;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#24120;&#24120;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#36951;&#24536;&#20808;&#21069;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22238;&#25918;&#35760;&#24518;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#23427;&#20250;&#23545;&#26087;&#21644;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21435;&#20851;&#32852;&#21644;&#28151;&#27927;&#12290;&#20182;&#20204;&#22825;&#30495;&#22320;&#25353;&#29031;&#29366;&#24577;&#36807;&#28193;&#30340;&#39034;&#24207;&#23384;&#20648;&#29366;&#24577;&#36716;&#21464;&#65292;&#32780;&#19981;&#32771;&#34385;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Grow-When-Required&#65288;GWR&#65289;&#33258;&#32452;&#32455;&#32593;&#32476;&#30340;&#26032;&#22411;&#35748;&#30693;&#21551;&#21457;&#24335;&#22238;&#25918;&#20869;&#23384;&#26041;&#27861;&#65292;&#23427;&#31867;&#20284;&#20110;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#19990;&#30028;&#35748;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#20010;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#23558;&#30456;&#20284;&#30340;&#26679;&#26412;&#21512;&#24182;&#20197;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#22686;&#21152;&#26679;&#26412;&#20043;&#38388;&#30340;&#20004;&#20004;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#20801;&#35768;&#26174;&#30528;&#20943;&#23569;&#20869;&#23384;&#65292;&#21482;&#20250;&#20135;&#29983;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02041</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#20302;&#22797;&#26434;&#24230;&#30340;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#23545;&#20989;&#25968;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#21464;&#20307;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992; carefully chosen &#30340;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#26356;&#26032;&#21487;&#20197;&#20889;&#25104;&#36845;&#20195;&#30340; Cholesky &#22240;&#23376;&#21644;&#19968;&#20010;&#31232;&#30095;&#30697;&#38453;&#30340;&#20056;&#31215;&#24418;&#24335;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26356;&#26032;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#30697;&#38453;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#25351;&#25968;&#21644;&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#65292;&#36825;&#20123;&#25805;&#20316;&#36890;&#24120;&#22312;&#20960;&#20046;&#25152;&#26377;&#20854;&#20182; Riemannian &#20248;&#21270;&#31639;&#27861;&#20013;&#37117;&#26159;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21709;&#24212;&#26465;&#20214;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#24819;&#35201;&#34920;&#36798;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#65292;&#24182;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#21644;&#21709;&#24212;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02036</link><description>&lt;p&gt;
&#21709;&#24212;&#26465;&#20214;&#30340;&#20132;&#26367;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Response-conditioned Turn-taking Prediction. (arXiv:2305.02036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21709;&#24212;&#26465;&#20214;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#24819;&#35201;&#34920;&#36798;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#65292;&#24182;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#21644;&#21709;&#24212;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#20013;&#36716;&#25442;&#21450;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#39318;&#20808;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#26469;&#26816;&#27979;&#19968;&#36718;&#23545;&#35805;&#26159;&#21542;&#24050;&#32467;&#26463;&#65292;&#28982;&#21518;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#19981;&#20165;&#20165;&#22240;&#20026;&#24456;&#21487;&#33021;&#36718;&#21040;&#33258;&#24049;&#20102;&#23601;&#21457;&#35328;&#65292;&#36824;&#35201;&#32771;&#34385;&#33258;&#24049;&#24819;&#35828;&#30340;&#35805;&#26159;&#21542;&#36866;&#21512;&#24403;&#21069;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65288;TurnGPT&#30340;&#25193;&#23637;&#65289;&#65292;&#23427;&#22522;&#20110;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#35201;&#35828;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25351;&#26631;&#26041;&#38754;&#19968;&#30452;&#34920;&#29616;&#20248;&#24322;&#12290;&#22312;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#23588;&#20026;&#26174;&#33879;&#65292;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#36718;&#25442;&#39044;&#27979;&#20165;&#30001;&#23545;&#35805;&#21382;&#21490;&#21487;&#33021;&#34920;&#29616;&#20026;&#27169;&#26865;&#20004;&#21487;&#30340;&#65306;1&#65289;&#24403;&#21069;&#35805;&#35821;&#20013;&#21253;&#21547;&#38472;&#36848;&#21477;&#21644;&#32039;&#25509;&#30528;&#30340;&#30097;&#38382;&#21477;&#65307;2&#65289;&#24403;&#21069;&#35805;&#35821;&#30340;&#32467;&#23614;&#21644;&#25152;&#38656;&#21709;&#24212;&#22312;&#35821;&#20041;&#19978;&#21305;&#37197;&#12290;&#23558;&#20132;&#26367;&#39044;&#27979;&#21644;&#21709;&#24212;&#25490;&#24207;&#35270;&#20026;&#19968;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Gym-preCICE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#21333;&#19968;&#21644;&#22810;&#29289;&#29702;&#23398;AFC&#24212;&#29992;&#30340;RL&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;preCICE&#26469;&#22788;&#29702;&#25511;&#21046;&#22120;&#21644;AFC&#27169;&#25311;&#29615;&#22659;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#22522;&#20110;&#29616;&#23454;&#29289;&#29702;&#30340;&#27169;&#25311;&#24037;&#20855;&#31665;&#19982;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02033</link><description>&lt;p&gt;
Gym-preCICE&#65306;&#29992;&#20110;&#20027;&#21160;&#27969;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Gym-preCICE: Reinforcement Learning Environments for Active Flow Control. (arXiv:2305.02033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gym-preCICE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#21333;&#19968;&#21644;&#22810;&#29289;&#29702;&#23398;AFC&#24212;&#29992;&#30340;RL&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;preCICE&#26469;&#22788;&#29702;&#25511;&#21046;&#22120;&#21644;AFC&#27169;&#25311;&#29615;&#22659;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#22522;&#20110;&#29616;&#23454;&#29289;&#29702;&#30340;&#27169;&#25311;&#24037;&#20855;&#31665;&#19982;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#27969;&#25511;&#21046;(AFC)&#26159;&#25351;&#36890;&#36807;&#26102;&#38388;&#19978;&#30340;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#24615;&#33021;&#25110;&#25928;&#29575;&#12290;AFC&#20316;&#20026;&#19968;&#20010;&#39034;&#24207;&#20248;&#21270;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;(RL)&#26469;&#36827;&#34892;&#21160;&#24577;&#20248;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Gym-preCICE&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#31526;&#21512;Gymnasium (&#20197;&#21069;&#31216;&#20026;OpenAI Gym) API&#30340;Python&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#21333;&#19968;&#21644;&#22810;&#29289;&#29702;&#23398;AFC&#24212;&#29992;&#30340;RL&#29615;&#22659;&#12290;&#22312;&#28436;&#21592;-&#29615;&#22659;&#35774;&#32622;&#20013;&#65292;Gym-preCICE&#21033;&#29992;preCICE&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#21306;&#22810;&#29289;&#29702;&#23398;&#27169;&#25311;&#30340;&#24320;&#28304;&#32806;&#21512;&#24211;&#65292;&#22788;&#29702;&#25511;&#21046;&#22120;(&#28436;&#21592;)&#21644;AFC&#27169;&#25311;&#29615;&#22659;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#24320;&#21457;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#23558;&#22522;&#20110;&#29616;&#23454;&#29289;&#29702;&#30340;&#27169;&#25311;&#24037;&#20855;&#31665;&#19982;RL&#31639;&#27861;&#26080;&#32541;&#38750;&#20405;&#20837;&#24335;&#22320;&#38598;&#25104;&#12290;Gym-preCICE&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;RL&#29615;&#22659;&#26469;&#27169;&#25311;AFC&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#19968;&#20010;&#24212;&#29992;RL&#31639;&#27861;&#20110;&#21508;&#31181;AFC&#30456;&#20851;&#22330;&#26223;&#30340;&#28216;&#20048;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active flow control (AFC) involves manipulating fluid flow over time to achieve a desired performance or efficiency. AFC, as a sequential optimisation task, can benefit from utilising Reinforcement Learning (RL) for dynamic optimisation. In this work, we introduce Gym-preCICE, a Python adapter fully compliant with Gymnasium (formerly known as OpenAI Gym) API to facilitate designing and developing RL environments for single- and multi-physics AFC applications. In an actor-environment setting, Gym-preCICE takes advantage of preCICE, an open-source coupling library for partitioned multi-physics simulations, to handle information exchange between a controller (actor) and an AFC simulation environment. The developed framework results in a seamless non-invasive integration of realistic physics-based simulation toolboxes with RL algorithms. Gym-preCICE provides a framework for designing RL environments to model AFC tasks, as well as a playground for applying RL algorithms in various AFC-relat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#20114;&#30456;&#36716;&#25442;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#21513;&#26684;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#65292;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02032</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20114;&#30456;&#36716;&#25442;&#23398;&#20064;&#29992;&#20110;&#22810;&#21513;&#26684;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification. (arXiv:2305.02032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#20114;&#30456;&#36716;&#25442;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#21513;&#26684;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#65292;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#21513;&#26684;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#30340;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312; WSI &#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#26524;&#24613;&#21095;&#22686;&#21152;&#65292;&#20854;&#20013;&#21253;&#25324;&#32959;&#30244;&#26816;&#27979;&#25110;&#20174; WSI &#39044;&#27979;&#20998;&#23376;&#31361;&#21464;&#31561;&#20020;&#24202;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#30001;&#19987;&#23478;&#30149;&#29702;&#23398;&#23478;&#36827;&#34892;&#26114;&#36149;&#21644;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#25163;&#21160;&#27880;&#37322;&#12290;&#24369;&#30417;&#30563;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#26368;&#36817;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#38656;&#35201;&#19987;&#19994;&#30149;&#29702;&#23398;&#23478;&#23545;&#27599;&#20010;&#20999;&#29255;&#36827;&#34892;&#20180;&#32454;&#26816;&#26597;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#20114;&#30456;&#36716;&#25442;&#23398;&#20064;&#30340; WSI &#20998;&#31867;&#31639;&#27861;&#12290;&#26469;&#33258;&#21513;&#26684;&#20687;&#32032; WSI&#65288;&#21363;&#22270;&#20687;&#34917;&#19969;&#65289;&#30340;&#23454;&#20363;&#34987;&#36716;&#25442;&#25104;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#34987;&#21453;&#21521;&#36716;&#25442;&#21040;&#21407;&#22987;&#31354;&#38388;&#12290;&#20351;&#29992;&#36716;&#25442;&#25439;&#22833;&#65292;&#20135;&#29983;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#28165;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#21513;&#26684;&#20687;&#32032; WSI &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of gigapixel Whole Slide Images (WSIs) is an important prediction task in the emerging area of computational pathology. There has been a surge of research in deep learning models for WSI classification with clinical applications such as cancer detection or prediction of molecular mutations from WSIs. Most methods require expensive and labor-intensive manual annotations by expert pathologists. Weakly supervised Multiple Instance Learning (MIL) methods have recently demonstrated excellent performance; however, they still require large slide-level labeled training datasets that need a careful inspection of each slide by an expert pathologist. In this work, we propose a fully unsupervised WSI classification algorithm based on mutual transformer learning. Instances from gigapixel WSI (i.e., image patches) are transformed into a latent space and then inverse-transformed to the original space. Using the transformation loss, pseudo-labels are generated and cleaned using a transf
&lt;/p&gt;</description></item><item><title>LearnDefend&#26159;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#65292;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#34987;&#27745;&#26579;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02022</link><description>&lt;p&gt;
LearnDefend&#65306;&#23398;&#20064;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning. (arXiv:2305.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02022
&lt;/p&gt;
&lt;p&gt;
LearnDefend&#26159;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#65292;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#34987;&#27745;&#26579;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#26500;&#25104;&#20102;&#24040;&#22823;&#30340;&#23041;&#32961;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#30446;&#26631;&#36793;&#32536;&#26696;&#20363;&#22411;&#25915;&#20987;&#65288;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#38024;&#23545;&#24615;&#25915;&#20987;&#65289;&#20960;&#20046;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#21453;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#27492;&#31867;&#25915;&#20987;&#12290;&#38450;&#24481;&#25968;&#25454;&#38598;&#21487;&#20197;&#30001;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#20013;&#22830;&#31649;&#29702;&#26426;&#26500;&#25910;&#38598;&#65292;&#20854;&#20013;&#24212;&#21253;&#21547;&#19968;&#20123;&#34987;&#27745;&#26579;&#30340;&#21644;&#27809;&#26377;&#34987;&#27745;&#26579;&#30340;&#31034;&#20363;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;LearnDefend&#20250;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#20855;&#26377;&#24694;&#24847;&#30340;&#27010;&#29575;&#12290;&#38450;&#24481;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#19981;&#38656;&#35201;&#20107;&#20808;&#26631;&#35760;&#20026;&#34987;&#27745;&#26579;&#25110;&#26410;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#36824;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#29992;&#20110;&#26631;&#35760;&#38450;&#24481;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31034;&#20363;&#20026;&#24178;&#20928;&#25110;&#27745;&#26579;&#30340;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LearnDefend&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Targeted model poisoning attacks pose a significant threat to federated learning systems. Recent studies show that edge-case targeted attacks, which target a small fraction of the input space are nearly impossible to counter using existing fixed defense strategies. In this paper, we strive to design a learned-defense strategy against such attacks, using a small defense dataset. The defense dataset can be collected by the central authority of the federated learning task, and should contain a mix of poisoned and clean examples. The proposed framework, LearnDefend, estimates the probability of a client update being malicious. The examples in defense dataset need not be pre-marked as poisoned or clean. We also learn a poisoned data detector model which can be used to mark each example in the defense dataset as clean or poisoned. We estimate the poisoned data detector and the client importance models in a coupled optimization approach. Our experiments demonstrate that LearnDefend is capable
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#26041;&#27861;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#21253;fairml&#65292;&#35813;&#21253;&#23454;&#29616;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#26680;&#20272;&#35745;&#31561;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#38556;&#20844;&#24179;&#21644;&#38382;&#36131;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.02009</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#30340;&#32479;&#35745;&#23398;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
fairml: A Statistician's Take on Fair Machine Learning Modelling. (arXiv:2305.02009v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#26041;&#27861;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#21253;fairml&#65292;&#35813;&#21253;&#23454;&#29616;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#26680;&#20272;&#35745;&#31561;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#38556;&#20844;&#24179;&#21644;&#38382;&#36131;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20851;&#38190;&#20445;&#38556;&#20844;&#24179;&#21644;&#38382;&#36131;&#30340;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#65292;&#23548;&#33268;&#25991;&#29486;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#27169;&#22411;&#24314;&#35758;&#65292;&#20027;&#35201;&#20197;&#32422;&#26463;&#26465;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#24418;&#24335;&#20943;&#23569;&#25110;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#23545;&#21709;&#24212;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#20174;&#29702;&#35770;&#19978;&#26469;&#30475;&#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#25152;&#24471;&#30340;&#27169;&#22411;&#26377;&#20123;&#40657;&#30418;&#24615;&#36136;&#65306;&#24456;&#23569;&#26377;&#20851;&#20110;&#20854;&#32479;&#35745;&#23646;&#24615;&#30340;&#34920;&#36848;&#65292;&#20851;&#20110;&#20854;&#24212;&#29992;&#26368;&#20339;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#20854;&#20182;&#38382;&#39064;&#32780;&#19981;&#26159;&#20854;&#26368;&#21021;&#35774;&#35745;&#29992;&#36884;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#29305;&#23450;&#30340;&#23454;&#29616;&#65292;&#28041;&#21450;&#36866;&#24403;&#30340;&#27714;&#35299;&#22120;&#65292;&#20174;&#36719;&#20214;&#24037;&#31243;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#22826;&#29702;&#24819;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;fairml R&#36719;&#20214;&#21253;&#65292;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Scutari&#65292;Panero&#21644;Proissl 2022&#65289;&#20197;&#21450;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#27169;&#22411;&#12290;fairml&#26159;&#22260;&#32469;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#26680;&#20272;&#35745;&#30340;&#32479;&#35745;&#26041;&#27861;&#35774;&#35745;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of machine learning in applications where it is crucial to ensure fairness and accountability has led to a large number of model proposals in the literature, largely formulated as optimisation problems with constraints reducing or eliminating the effect of sensitive attributes on the response. While this approach is very flexible from a theoretical perspective, the resulting models are somewhat black-box in nature: very little can be said about their statistical properties, what are the best practices in their applied use, and how they can be extended to problems other than those they were originally designed for. Furthermore, the estimation of each model requires a bespoke implementation involving an appropriate solver which is less than desirable from a software engineering perspective.  In this paper, we describe the fairml R package which implements our previous work (Scutari, Panero, and Proissl 2022) and related models in the literature. fairml is designed around cla
&lt;/p&gt;</description></item><item><title>Zenseact Open Dataset&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#35206;&#30422;&#33539;&#22260;&#24191;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#20197;&#21450;&#35814;&#32454;&#30340;&#20851;&#38190;&#24103;&#27880;&#37322;&#65292;&#19987;&#27880;&#20110;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.02008</link><description>&lt;p&gt;
Zenseact&#24320;&#25918;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving. (arXiv:2305.02008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02008
&lt;/p&gt;
&lt;p&gt;
Zenseact Open Dataset&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#35206;&#30422;&#33539;&#22260;&#24191;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#20197;&#21450;&#35814;&#32454;&#30340;&#20851;&#38190;&#24103;&#27880;&#37322;&#65292;&#19987;&#27880;&#20110;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#25968;&#25454;&#38598;&#36890;&#24120;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#38271;&#31243;&#33021;&#21147;&#65292;&#32780;&#26356;&#20851;&#27880;&#20110; 360&#24230;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Zenseact&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;ZOD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27431;&#27954;&#21508;&#22269;&#25910;&#38598;&#20004;&#24180;&#30340;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#38754;&#31215;&#26159;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;9&#20493;&#12290;&#19982;&#21487;&#27604;&#36739;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;ZOD&#25317;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#65292;&#21516;&#26102;&#37197;&#22791;&#20102;2D&#21644;3D&#23545;&#35937;&#65288;&#38271;&#36798;245m&#65289;&#12289;&#36947;&#36335;&#23454;&#20363;/&#35821;&#20041;&#20998;&#21106;&#12289;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#21644;&#36947;&#36335;&#20998;&#31867;&#30340;&#35814;&#32454;&#20851;&#38190;&#24103;&#27880;&#37322;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#29420;&#29305;&#32452;&#21512;&#23558;&#26377;&#21161;&#20110;&#31361;&#30772;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#38590;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;Frames&#12289; Sequences&#21644; Drives&#19977;&#37096;&#20998;&#32452;&#25104;&#65292;&#26088;&#22312;&#21253;&#21547;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#25903;&#25345;&#26102;&#31354;&#23398;&#20064;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#12289;&#23450;&#20301;&#21644;&#26144;&#23556;&#12290;Frames&#30001;10&#19975;&#20010;&#31579;&#36873;&#21518;&#30340;&#30456;&#26426;&#22270;&#20687;&#21644;&#20004;&#31186;&#38047;&#30340;&#20854;&#20182;&#25903;&#25345;&#25968;&#25454;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360{\deg} perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large-scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9x that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, coupled with detailed keyframe annotations for 2D and 3D objects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio-temporal learning, sensor fusion, localization, and mapping. Frames consist of 100k curated camera images with two seconds of other support
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01997</link><description>&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#20307;&#31215;&#25351;&#25968;&#30340;&#25552;&#21462;&#65306;&#21738;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20020;&#24202;&#65311;
&lt;/p&gt;
&lt;p&gt;
Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#33258;&#21160;&#20998;&#26512;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#21033;&#29992;&#22810;&#20010;&#30001;&#19987;&#23478;&#27880;&#37322;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;CAMUS&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#20043;&#19968;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#24191;&#33021;&#21147;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34987;&#20020;&#24202;&#21307;&#29983;&#35748;&#20026;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CARDINAL&#30340;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#25324;&#24515;&#23574;&#20004;&#33108;&#21644;&#24515;&#23574;&#22235;&#33108;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#23436;&#25972;&#24515;&#33039;&#21608;&#26399;&#30340;&#21442;&#32771;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;3D nnU-Net&#20248;&#20110;&#26367;&#20195;&#30340;2D&#21644;&#24490;&#29615;&#20998;&#21106;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#25253;&#21578;&#20102;&#22312;CARDINAL&#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.01975</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#30740;&#31350;&#32508;&#36848;: &#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01975
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#38598;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#28982;&#21518;&#31995;&#32479;&#22320;&#22238;&#39038;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPSeq&#30340;&#26032;&#39062;&#39640;&#25928;&#30340;&#25968;&#23383;&#30149;&#29702;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#24494;&#35843;&#19968;&#20010;&#38598;&#25104;&#20102;&#27700;&#24179;&#21644;&#31446;&#30452;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;(BiLSTM)&#32593;&#32476;&#30340;&#24207;&#21015;&#22120;&#26550;&#26500;&#65292;&#26469;&#39044;&#27979;&#30284;&#30151;&#29983;&#29289;&#26631;&#35760;&#29289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPSeq&#34920;&#29616;&#20986;&#20102;&#22312;&#39044;&#27979;CRC&#20013;&#20851;&#38190;&#29983;&#29289;&#26631;&#35760;&#29289;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#22312;&#21516;&#32452;&#24739;&#32773;&#32676;&#20869;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01968</link><description>&lt;p&gt;
DPSeq&#65306;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25968;&#23383;&#30149;&#29702;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#24207;&#21015;&#22120;&#26550;&#26500;&#39044;&#27979;&#30284;&#30151;&#29983;&#29289;&#26631;&#35760;&#29289;&#65288;arXiv:2305.01968v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
DPSeq: A Novel and Efficient Digital Pathology Classifier for Predicting Cancer Biomarkers using Sequencer Architecture. (arXiv:2305.01968v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPSeq&#30340;&#26032;&#39062;&#39640;&#25928;&#30340;&#25968;&#23383;&#30149;&#29702;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#24494;&#35843;&#19968;&#20010;&#38598;&#25104;&#20102;&#27700;&#24179;&#21644;&#31446;&#30452;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;(BiLSTM)&#32593;&#32476;&#30340;&#24207;&#21015;&#22120;&#26550;&#26500;&#65292;&#26469;&#39044;&#27979;&#30284;&#30151;&#29983;&#29289;&#26631;&#35760;&#29289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPSeq&#34920;&#29616;&#20986;&#20102;&#22312;&#39044;&#27979;CRC&#20013;&#20851;&#38190;&#29983;&#29289;&#26631;&#35760;&#29289;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#22312;&#21516;&#32452;&#24739;&#32773;&#32676;&#20869;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#20219;&#21153;&#20013;&#65292;&#21464;&#21387;&#22120;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#36890;&#24120;&#36807;&#20110;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DPSeq&#30340;&#26032;&#39062;&#39640;&#25928;&#30340;&#25968;&#23383;&#30149;&#29702;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#24494;&#35843;&#19968;&#20010;&#38598;&#25104;&#20102;&#27700;&#24179;&#21644;&#31446;&#30452;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;(BiLSTM)&#32593;&#32476;&#30340;&#24207;&#21015;&#22120;&#26550;&#26500;&#65292;&#26469;&#39044;&#27979;&#30284;&#30151;&#29983;&#29289;&#26631;&#35760;&#29289;&#12290;&#20351;&#29992;&#20004;&#20010;&#22269;&#38469;&#25968;&#25454;&#38598;&#65306;&#30284;&#30151;&#22522;&#22240;&#32452;&#22270;&#35889;(TCGA)&#21644;&#20998;&#23376;&#19982;&#32454;&#32990;&#32959;&#30244;&#23398;(MCO)&#20013;&#30340;&#32467;&#32928;&#30284;(CRC)&#30340;&#34880;&#32418;&#34507;&#30333;&#21644;&#21994;&#21833;&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#23545;DPSeq&#30340;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35780;&#20272;&#12290; DPSeq&#34920;&#29616;&#20986;&#20102;&#22312;&#39044;&#27979;CRC&#20013;&#20851;&#38190;&#29983;&#29289;&#26631;&#35760;&#29289;&#65288;MSI&#29366;&#24577;&#12289;&#39640;&#31361;&#21464;&#12289;CIMP&#29366;&#24577;&#12289;BRAF&#31361;&#21464;&#12289;TP53&#31361;&#21464;&#21644;&#26579;&#33394;&#20307;&#19981;&#31283;&#23450;&#24615;[CING]&#65289;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#22312;&#21516;&#32452;&#24739;&#32773;&#32676;&#20869;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital pathology tasks, transformers have achieved state-of-the-art results, surpassing convolutional neural networks (CNNs). However, transformers are usually complex and resource intensive. In this study, we developed a novel and efficient digital pathology classifier called DPSeq, to predict cancer biomarkers through fine-tuning a sequencer architecture integrating horizon and vertical bidirectional long short-term memory (BiLSTM) networks. Using hematoxylin and eosin (H&amp;E)-stained histopathological images of colorectal cancer (CRC) from two international datasets: The Cancer Genome Atlas (TCGA) and Molecular and Cellular Oncology (MCO), the predictive performance of DPSeq was evaluated in series of experiments. DPSeq demonstrated exceptional performance for predicting key biomarkers in CRC (MSI status, Hypermutation, CIMP status, BRAF mutation, TP53 mutation and chromosomal instability [CING]), outperforming most published state-of-the-art classifiers in a within-cohort interna
&lt;/p&gt;</description></item><item><title>SeqAug&#26159;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65292;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#65292;&#24182;&#19988;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#20860;&#23481;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01954</link><description>&lt;p&gt;
SeqAug: &#24207;&#21015;&#29305;&#24449;&#37325;&#37319;&#26679;&#25216;&#26415;&#8212;&#8212;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01954
&lt;/p&gt;
&lt;p&gt;
SeqAug&#26159;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65292;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#65292;&#24182;&#19988;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#20860;&#23481;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24615;&#33021;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SeqAug&#65292;&#19968;&#31181;&#38024;&#23545;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;SeqAug&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#12290;&#37325;&#26032;&#37319;&#26679;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#29305;&#24449;&#32500;&#24230;&#24182;&#27839;&#26102;&#38388;&#36724;&#23545;&#23427;&#20204;&#36827;&#34892;&#32622;&#25442;&#26469;&#23454;&#29616;&#12290;&#22312;CMU-MOSEI&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SeqAug&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#24615;&#65307;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#23427;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#32467;&#26524;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a prevalent technique for improving performance in various machine learning applications. We propose SeqAug, a modality-agnostic augmentation method that is tailored towards sequences of extracted features. The core idea of SeqAug is to augment the sequence by resampling from the underlying feature distribution. Resampling is performed by randomly selecting feature dimensions and permuting them along the temporal axis. Experiments on CMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully applied to a single modality or multiple modalities. We further verify its compatibility with both recurrent and transformer architectures, and also demonstrate comparable to state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;$p$-&#33539;&#25968;&#30446;&#26631;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#31639;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#19988;&#26159;&#24050;&#30693;&#26368;&#22909;&#30340;&#30028;&#30340;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.01942</link><description>&lt;p&gt;
&#20219;&#24847;$p$-&#33539;&#25968;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Any $p$-Norm. (arXiv:2305.01942v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01942
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;$p$-&#33539;&#25968;&#30446;&#26631;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#31639;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#19988;&#26159;&#24050;&#30693;&#26368;&#22909;&#30340;&#30028;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;$p$-&#33539;&#25968;&#30446;&#26631;&#65292;&#29992;&#20110;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#20123;&#32463;&#20856;&#30446;&#26631;&#65288;D/A/E-&#35774;&#35745;&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#36827;&#34892;&#20102;&#27010;&#25324;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#25152;&#26377;&#30340;$p$-&#33539;&#25968;&#38382;&#39064;&#12290;&#36825;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#26222;&#36941;$p$-&#33539;&#25968;&#30446;&#26631;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#26159;&#29305;&#27530;&#24773;&#20917;&#19979;&#24050;&#30693;&#26368;&#20248;&#30028;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a general $p$-norm objective for experimental design problems that captures some well-studied objectives (D/A/E-design) as special cases. We prove that a randomized local search approach provides a unified algorithm to solve this problem for all $p$. This provides the first approximation algorithm for the general $p$-norm objective, and a nice interpolation of the best known bounds of the special cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#27010;&#36848;&#20102;&#20351;&#29992;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#31181;&#26041;&#27861;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#26032;&#30340;&#20154;&#24037;&#34507;&#30333;&#36136;&#65292;&#20351;&#29992;&#38750;Transformer&#26550;&#26500;&#30340;&#24037;&#20316;&#21644;&#24212;&#29992;&#20110;&#23450;&#21521;&#36827;&#21270;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.01941</link><description>&lt;p&gt;
&#29992;&#20840;&#23616;&#29983;&#25104;&#27169;&#22411;&#25506;&#32034;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring the Protein Sequence Space with Global Generative Models. (arXiv:2305.01941v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#27010;&#36848;&#20102;&#20351;&#29992;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#31181;&#26041;&#27861;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#26032;&#30340;&#20154;&#24037;&#34507;&#30333;&#36136;&#65292;&#20351;&#29992;&#38750;Transformer&#26550;&#26500;&#30340;&#24037;&#20316;&#21644;&#24212;&#29992;&#20110;&#23450;&#21521;&#36827;&#21270;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35757;&#32451;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#19987;&#29992;&#22823;&#35268;&#27169;&#26550;&#26500;&#30340;&#36827;&#27493;&#28145;&#21051;&#22320;&#24433;&#21709;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;&#26368;&#36817;&#30340;ChatGPT&#21644;GPT4&#65292;&#22312;&#22788;&#29702;&#12289;&#32763;&#35793;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31361;&#30772;&#20063;&#21453;&#26144;&#22312;&#34507;&#30333;&#36136;&#30740;&#31350;&#20013;&#65292;&#23548;&#33268;&#20102;&#30701;&#26102;&#38388;&#20869;&#35768;&#22810;&#26032;&#26041;&#27861;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#29992;&#20110;&#23884;&#20837;&#34507;&#30333;&#36136;&#12289;&#29983;&#25104;&#26032;&#30340;&#34507;&#30333;&#36136;&#21644;&#39044;&#27979;&#19977;&#32423;&#32467;&#26500;&#12290;&#22312;&#26412;&#20070;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#22238;&#39038;&#20102;1&#65289;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#20154;&#24037;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;2&#65289;&#20351;&#29992;&#38750;Transformer&#26550;&#26500;&#30340;&#24037;&#20316;&#65292;&#20197;&#21450;3&#65289;&#24212;&#29992;&#20110;&#23450;&#21521;&#36827;&#21270;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in specialized large-scale architectures for training image and language have profoundly impacted the field of computer vision and natural language processing (NLP). Language models, such as the recent ChatGPT and GPT4 have demonstrated exceptional capabilities in processing, translating, and generating human languages. These breakthroughs have also been reflected in protein research, leading to the rapid development of numerous new methods in a short time, with unprecedented performance. Language models, in particular, have seen widespread use in protein research, as they have been utilized to embed proteins, generate novel ones, and predict tertiary structures. In this book chapter, we provide an overview of the use of protein generative models, reviewing 1) language models for the design of novel artificial proteins, 2) works that use non-Transformer architectures, and 3) applications in directed evolution approaches.
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01939</link><description>&lt;p&gt;
&#35777;&#26126;AI&#27169;&#22411;&#20013;&#31232;&#30095;&#31526;&#21495;&#27010;&#24565;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models. (arXiv:2305.01939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01939
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#20013;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#65288;1&#65289;&#27169;&#22411;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#39640;&#38454;&#23548;&#25968;&#22343;&#20026;&#38646;&#65292;&#65288;2&#65289;AI&#27169;&#22411;&#21487;&#29992;&#20110;&#36974;&#25377;&#26679;&#26412;&#19988;&#36755;&#20837;&#26679;&#26412;&#36739;&#23569;&#36974;&#25377;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#65292;&#65288;3&#65289;AI&#27169;&#22411;&#22312;&#36974;&#25377;&#26679;&#26412;&#19978;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#65292;&#21017;AI&#27169;&#22411;&#23558;&#32534;&#30721;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#12290;&#27599;&#20010;&#20132;&#20114;&#27010;&#24565;&#34920;&#31034;&#29305;&#23450;&#19968;&#32452;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#19968;&#23450;&#30340;&#25968;&#20540;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#20998;&#25968;&#24635;&#26159;&#21487;&#20197;&#34920;&#31034;&#20026;&#25152;&#26377;&#20132;&#20114;&#27010;&#24565;&#30340;&#20132;&#20114;&#25928;&#24212;&#20043;&#21644;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24076;&#26395;&#35777;&#26126;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#26465;&#20214;&#38750;&#24120;&#26222;&#36941;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#22823;&#22810;&#25968;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#20132;&#20114;&#27010;&#24565;&#26469;&#27169;&#25311;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19977;&#31181;&#26465;&#20214;&#22788;&#29702;&#26041;&#24335;&#65306;&#24369;&#26465;&#20214;&#22788;&#29702;&#12289;&#24378;&#26465;&#20214;&#22788;&#29702;&#21644;&#32431;&#26465;&#20214;&#22788;&#29702;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;GNNs&#65292;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26465;&#20214;&#22788;&#29702;&#26041;&#24335;&#23545;&#20110;GNN&#30340;&#24615;&#33021;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01933</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26465;&#20214;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Exploration of Conditioning Methods in Graph Neural Networks. (arXiv:2305.01933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19977;&#31181;&#26465;&#20214;&#22788;&#29702;&#26041;&#24335;&#65306;&#24369;&#26465;&#20214;&#22788;&#29702;&#12289;&#24378;&#26465;&#20214;&#22788;&#29702;&#21644;&#32431;&#26465;&#20214;&#22788;&#29702;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;GNNs&#65292;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26465;&#20214;&#22788;&#29702;&#26041;&#24335;&#23545;&#20110;GNN&#30340;&#24615;&#33021;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#28789;&#27963;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#36827;&#27493;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;GNN&#22522;&#20110;&#20854;&#37051;&#23621;&#36882;&#24402;&#22320;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#21521;&#37327;&#26469;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;&#29289;&#29702;&#21644;&#21270;&#23398;&#31561;&#35745;&#31639;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#36793;&#23646;&#24615;&#65288;&#22914;&#30456;&#23545;&#20301;&#32622;&#25110;&#36317;&#31163;&#65289;&#34987;&#35777;&#26126;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#19981;&#26159;&#35201;&#20351;&#29992;&#20160;&#20040;&#31867;&#22411;&#30340;&#23646;&#24615;&#65292;&#32780;&#26159;&#22914;&#20309;&#22312;&#27492;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#26465;&#20214;&#22788;&#29702;&#65306;&#24369;&#26465;&#20214;&#22788;&#29702;&#12289;&#24378;&#26465;&#20214;&#22788;&#29702;&#21644;&#32431;&#26465;&#20214;&#22788;&#29702;&#65292;&#20998;&#21035;&#19982;&#22522;&#20110;&#36830;&#25509;&#30340;&#26465;&#20214;&#22788;&#29702;&#12289;&#38376;&#25511;&#21644;&#22240;&#26524;&#20381;&#36182;&#20110;&#23646;&#24615;&#30340;&#21464;&#25442;&#30456;&#20851;&#12290;&#36825;&#31181;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35266;&#28857;&#26469;&#30475;&#24453;&#19981;&#21516;&#31867;&#21035;&#30340;GNN&#65292;&#20174;&#21487;&#20998;&#31163;&#21367;&#31215;&#21040;&#21508;&#31181;&#24418;&#24335;&#30340;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;GNNs&#30340;&#24615;&#33021;&#21463;&#21040;&#35813;&#26465;&#20214;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., in computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.01932</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#37096;&#32626;&#20043;&#21069;&#65292;&#24418;&#24335;&#39564;&#35777;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#36824;&#26080;&#27861;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#31070;&#32463;&#20803;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20445;&#23432;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#20986;&#21407;&#32593;&#32476;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26500;&#36896;&#31616;&#21270;&#32593;&#32476;&#65292;&#39564;&#35777;&#21407;&#22987;&#32593;&#32476;&#21450;&#20854;&#35268;&#33539;&#12290;&#31616;&#21270;&#23558;&#25152;&#26377;&#36755;&#20986;&#30456;&#20284;&#30340;&#38750;&#32447;&#24615;&#23618;&#31070;&#32463;&#20803;&#21512;&#24182;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#20309;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#65292;sigmoid&#21644;tanh&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#30340;5&#65285;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;&#39564;&#35777;&#26102;&#38388;&#30456;&#20284;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolKD&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#19982;&#20998;&#23376;&#38388;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;&#25552;&#21462;&#21644;&#36716;&#31227;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#36741;&#21161;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01912</link><description>&lt;p&gt;
MolKD: &#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#25552;&#21462;&#21270;&#23398;&#21453;&#24212;&#20013;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction. (arXiv:2305.01912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolKD&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#19982;&#20998;&#23376;&#38388;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;&#25552;&#21462;&#21644;&#36716;&#31227;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#36741;&#21161;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#31034;&#20998;&#23376;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#21270;&#23398;&#21453;&#24212;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#32435;&#20837;&#21040;&#23398;&#20064;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#20013;&#12290; &#28982;&#32780;&#65292;&#21270;&#23398;&#21453;&#24212;&#21644;&#20998;&#23376;&#20043;&#38388;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#29305;&#24615;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;MolKD&#65292;&#23427;&#22312;&#21270;&#23398;&#21453;&#24212;&#20013;&#25552;&#21462;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20197;&#36741;&#21161;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to effectively represent molecules is a long-standing challenge for molecular property prediction and drug discovery. This paper studies this problem and proposes to incorporate chemical domain knowledge, specifically related to chemical reactions, for learning effective molecular representations. However, the inherent cross-modality property between chemical reactions and molecules presents a significant challenge to address. To this end, we introduce a novel method, namely MolKD, which Distills cross-modal Knowledge in chemical reactions to assist Molecular property prediction. Specifically, the reaction-to-molecule distillation model within MolKD transfers cross-modal knowledge from a pre-trained teacher network learning with one modality (i.e., reactions) into a student network learning with another modality (i.e., molecules). Moreover, MolKD learns effective molecular representations by incorporating reaction yields to measure transformation efficiency of the reactant-product 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#23383;&#20856;&#34920;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#21035;&#36882;&#22686;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22522;&#30784;&#20250;&#35805;&#20013;&#21516;&#26102;&#20248;&#21270;&#23383;&#20856;&#21644;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#65292;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#20165;&#24494;&#35843;&#23383;&#20856;&#20197;&#36866;&#24212;&#26032;&#31867;&#21035;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01885</link><description>&lt;p&gt;
&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#32780;&#19981;&#36951;&#24536;&#26087;&#30340;&#31867;&#21035;&#65306;&#22522;&#20110;&#28436;&#21270;&#23383;&#20856;&#34920;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#21035;&#36882;&#22686;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolving Dictionary Representation for Few-shot Class-incremental Learning. (arXiv:2305.01885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#23383;&#20856;&#34920;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#21035;&#36882;&#22686;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22522;&#30784;&#20250;&#35805;&#20013;&#21516;&#26102;&#20248;&#21270;&#23383;&#20856;&#21644;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#65292;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#20165;&#24494;&#35843;&#23383;&#20856;&#20197;&#36866;&#24212;&#26032;&#31867;&#21035;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#26032;&#30340;&#29289;&#20307;&#19981;&#26029;&#20986;&#29616;&#65292;&#19968;&#20010;&#30495;&#27491;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#25345;&#32493;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#31867;&#21035;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#26087;&#26377;&#30340;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#32487;&#32493;&#23398;&#20064;&#22330;&#26223;&#8212;&#8212;&#23569;&#26679;&#26412;&#31867;&#21035;&#36882;&#22686;&#23398;&#20064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21363;&#22312;&#22522;&#30784;&#20250;&#35805;&#20013;&#20026;&#31867;&#21035;&#25552;&#20379;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#26032;&#30340;&#22686;&#37327;&#31867;&#21035;&#20165;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#27905;&#30340;&#26041;&#27861;&#65292;&#21363;&#24341;&#20837;&#28145;&#24230;&#23383;&#20856;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#23398;&#20064;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#23383;&#20856;&#23398;&#20064;&#21644;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#31354;&#38388;&#26469;&#34920;&#24449;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#21516;&#26102;&#20248;&#21270;&#22522;&#30784;&#20250;&#35805;&#20013;&#30340;&#23383;&#20856;&#21644;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#65292;&#24182;&#20165;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#24494;&#35843;&#23383;&#20856;&#20197;&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#28436;&#21270;&#23383;&#20856;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#31867;&#21035;&#36882;&#22686;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
New objects are continuously emerging in the dynamically changing world and a real-world artificial intelligence system should be capable of continual and effectual adaptation to new emerging classes without forgetting old ones. In view of this, in this paper we tackle a challenging and practical continual learning scenario named few-shot class-incremental learning (FSCIL), in which labeled data are given for classes in a base session but very limited labeled instances are available for new incremental classes. To address this problem, we propose a novel and succinct approach by introducing deep dictionary learning which is a hybrid learning architecture that combines dictionary learning and visual representation learning to provide a better space for characterizing different classes. We simultaneously optimize the dictionary and the feature extraction backbone in the base session, while only finetune the dictionary in the incremental session for adaptation to novel classes, which can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#65292;&#20351;&#29992;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#26356;&#22909;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#28040;&#38500;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.01883</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#65292;&#20351;&#29992;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#26356;&#22909;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#28040;&#38500;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSPs&#65289;&#20013;&#20063;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22522;&#20110;&#20840;&#36830;&#25509;&#30340;&#27880;&#24847;&#27169;&#22411;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#12290;&#19982;&#26631;&#20934;Transformer&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;CNN-Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#12290;&#23427;&#36824;&#20351;&#29992;&#25552;&#20986;&#30340;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#28040;&#38500;&#20102;&#20840;&#36830;&#25509;&#27880;&#24847;&#27169;&#22411;&#20013;&#30340;&#30456;&#24403;&#25968;&#37327;&#30340;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;GPU&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;GPU&#20869;&#23384;&#32422;&#23569;20&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;45&#65285;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65292;&#32593;&#22336;&#20026;https://g
&lt;/p&gt;
&lt;p&gt;
Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;SpinalNet&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26143;&#31995;&#36827;&#34892;&#24418;&#24577;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01873</link><description>&lt;p&gt;
&#20351;&#29992;SpinalNet&#23545;&#26143;&#31995;&#36827;&#34892;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Morphological Classification of Galaxies Using SpinalNet. (arXiv:2305.01873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;SpinalNet&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26143;&#31995;&#36827;&#34892;&#24418;&#24577;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Galaxy Zoo&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;SpinalNet&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#27169;&#20223;&#20154;&#20307;&#30340;&#30382;&#32932;&#24863;&#35273;&#31995;&#32479;&#36880;&#27493;&#24341;&#20837;&#36755;&#20837;&#12290;SpinalNet&#20013;&#30340;&#36755;&#20837;&#20998;&#21106;&#20351;&#24471;&#20013;&#38388;&#23618;&#33021;&#22815;&#33719;&#21462;&#20043;&#21069;&#23618;&#30340;&#37096;&#20998;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20013;&#38388;&#23618;&#30340;&#26435;&#37325;&#25910;&#38598;&#37327;&#12290;&#32467;&#26524;&#65292;SpinalNet&#30340;&#20316;&#32773;&#25253;&#21578;&#35828;&#65292;&#22312;&#22823;&#22810;&#25968;&#20182;&#20204;&#27979;&#35797;&#30340;DNN&#20013;&#65292;&#19981;&#20165;&#38169;&#35823;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#32780;&#19988;&#35745;&#31639;&#25104;&#26412;&#20063;&#22823;&#22823;&#38477;&#20302;&#12290;&#22312;&#23558;&#20854;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#20043;&#21518;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#26143;&#31995;&#30340;&#19981;&#21516;&#31867;&#21035;&#21644;/&#25110;&#23376;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;98.2&#65285;&#12289;95&#65285;&#21644;82&#65285;&#30340;&#39640;&#20998;&#31867;&#31934;&#24230;&#65292;&#20998;&#21035;&#22312;&#26925;&#22278;&#21644;&#34746;&#26059;&#26143;&#31995;&#20043;&#38388;&#12289;&#22312;&#36825;&#20004;&#32773;&#21644;&#19981;&#35268;&#21017;&#26143;&#31995;&#20043;&#38388;&#20197;&#21450;&#22312;10&#20010;&#26143;&#31995;&#23376;&#31867;&#21035;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) with a step-by-step introduction of inputs, which is constructed by imitating the somatosensory system in human body, known as SpinalNet have been implemented in this work on a Galaxy Zoo dataset. The input segmentation in SpinalNet has enabled the intermediate layers to take some of the inputs as well as output of preceding layers thereby reducing the amount of the collected weights in the intermediate layers. As a result of these, the authors of SpinalNet reported to have achieved in most of the DNNs they tested, not only a remarkable cut in the error but also in the large reduction of the computational costs. Having applied it to the Galaxy Zoo dataset, we are able to classify the different classes and/or sub-classes of the galaxies. Thus, we have obtained higher classification accuracies of 98.2, 95 and 82 percents between elliptical and spirals, between these two and irregulars, and between 10 sub-classes of galaxies, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#20998;&#29255;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#25628;&#32034;&#30830;&#23450;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23884;&#20837;&#34920;&#20998;&#29255;&#20219;&#21153;&#20013;&#34920;&#29616;&#24456;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01868</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#25628;&#32034;&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#23884;&#20837;&#34920;&#20998;&#29255;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models. (arXiv:2305.01868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#20998;&#29255;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#25628;&#32034;&#30830;&#23450;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23884;&#20837;&#34920;&#20998;&#29255;&#20219;&#21153;&#20013;&#34920;&#29616;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#23558;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#29255;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#20197;&#24179;&#34913;&#25104;&#26412;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#20998;&#21306;&#26159;NP&#38590;&#38382;&#39064;&#19988;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#31639;&#25104;&#26412;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#8220;&#39044;&#35757;&#32451;&#21644;&#25628;&#32034;&#8221;&#33539;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#29255;&#12290;&#35813;&#26041;&#27861;&#26159;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#27704;&#20037;&#23384;&#22312;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#20998;&#29255;&#30340;&#25104;&#26412;&#65292;&#36825;&#20010;&#32593;&#32476;&#23601;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20998;&#29255;&#27169;&#25311;&#22120;&#12290;&#22312;&#27492;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#22312;&#32447;&#25628;&#32034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#20219;&#20309;&#29305;&#23450;&#20998;&#29255;&#20219;&#21153;&#30340;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#65288;DLRMs&#65289;&#20013;&#65292;&#25105;&#20204;&#23558;&#27492;&#24605;&#24819;&#23454;&#20363;&#21270;&#65292;&#24182;&#25552;&#35758;&#20102;NeuroShard&#29992;&#20110;&#23884;&#20837;&#34920;&#20998;&#29255;&#12290;NeuroShard&#22312;&#25193;&#23637;&#34920;&#19978;&#39044;&#20808;&#35757;&#32451;&#31070;&#32463;&#25104;&#26412;&#27169;&#22411;&#65292;&#20197;&#28085;&#30422;&#21508;&#31181;&#20998;&#29255;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#24515;&#32593;&#26684;&#25628;&#32034;&#65292;&#20998;&#21035;&#30830;&#23450;&#26368;&#20339;&#30340;&#21015;&#21644;&#34920;&#20998;&#29255;&#35745;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Sharding a large machine learning model across multiple devices to balance the costs is important in distributed training. This is challenging because partitioning is NP-hard, and estimating the costs accurately and efficiently is difficult. In this work, we explore a "pre-train, and search" paradigm for efficient sharding. The idea is to pre-train a universal and once-for-all neural network to predict the costs of all the possible shards, which serves as an efficient sharding simulator. Built upon this pre-trained cost model, we then perform an online search to identify the best sharding plans given any specific sharding task. We instantiate this idea in deep learning recommendation models (DLRMs) and propose NeuroShard for embedding table sharding. NeuroShard pre-trains neural cost models on augmented tables to cover various sharding scenarios. Then it identifies the best column-wise and table-wise sharding plans with beam search and greedy grid search, respectively. Experiments show
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01864</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25913;&#36827;&#38899;&#39057;-&#25991;&#26412;&#36328;&#27169;&#24577;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#33719;&#24471;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#31038;&#21306;&#33021;&#22815;&#22312;&#38646;-shot&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#21542;&#21017;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#30340;&#34920;&#24449;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26410;&#37197;&#23545;&#25991;&#26412;&#21644;&#38899;&#39057;&#25913;&#36827;&#36825;&#20123;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#39046;&#22495;&#38750;&#29305;&#23450;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#26041;&#27861;&#65292;&#21019;&#24314;&#25105;&#20204;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#19982;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#32467;&#21512;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#31579;&#36873;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19979;&#28216;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#25110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#26029;&#30697;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01841</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#25512;&#26029;&#30697;
&lt;/p&gt;
&lt;p&gt;
Inferential Moments of Uncertain Multivariable Systems. (arXiv:2305.01841v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#26029;&#30697;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31216;&#20026;&#8220;&#25512;&#26029;&#30697;&#8221;&#30340;&#19968;&#32452;&#37327;&#26469;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#36793;&#32536;&#21270;&#26159;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36807;&#31243;&#65292;&#23427;&#36890;&#36807;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#37327;&#21270;&#25152;&#20851;&#27880;&#27010;&#29575;&#30340;&#26399;&#26395;&#20540;&#12290;&#25512;&#26029;&#30697;&#26159;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#30340;&#39640;&#38454;&#26465;&#20214;&#27010;&#29575;&#30697;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#26029;&#20559;&#24046;&#65292;&#23427;&#26159;&#26399;&#26395;&#30340;&#27010;&#29575;&#27874;&#21160;&#65292;&#38543;&#30528;&#25512;&#26029;&#26356;&#26032;&#21478;&#19968;&#20010;&#21464;&#37327;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#20197;&#25512;&#26029;&#30697;&#30340;&#24418;&#24335;&#25214;&#21040;&#20102;&#20114;&#20449;&#24687;&#30340;&#24130;&#32423;&#25968;&#23637;&#24320;&#24335;&#65292;&#36825;&#24847;&#21619;&#30528;&#25512;&#26029;&#30697;&#36923;&#36753;&#21487;&#33021;&#23545;&#36890;&#24120;&#20351;&#29992;&#20449;&#24687;&#35770;&#24037;&#20855;&#25191;&#34892;&#30340;&#20219;&#21153;&#26377;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24212;&#29992;&#20013;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article offers a new paradigm for analyzing the behavior of uncertain multivariable systems using a set of quantities we call \emph{inferential moments}. Marginalization is an uncertainty quantification process that averages conditional probabilities to quantify the \emph{expected value} of a probability of interest. Inferential moments are higher order conditional probability moments that describe how a distribution is expected to respond to new information. Of particular interest in this article is the \emph{inferential deviation}, which is the expected fluctuation of the probability of one variable in response to an inferential update of another. We find a power series expansion of the Mutual Information in terms of inferential moments, which implies that inferential moment logic may be useful for tasks typically performed with information theoretic tools. We explore this in two applications that analyze the inferential deviations of a Bayesian Network to improve situational aw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#35270;&#39057;&#23450;&#20301;&#19982;&#20998;&#21106;&#30340;&#26694;&#26550;AV-SAM&#12290;AV-SAM&#22522;&#20110;SAM&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Flickr-SoundNet&#21644;AVSBench&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#22312;&#38899;&#35270;&#39057;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.01836</link><description>&lt;p&gt;
AV-SAM:&#38899;&#35270;&#39057;&#23450;&#20301;&#19982;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#19975;&#33021;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation. (arXiv:2305.01836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#35270;&#39057;&#23450;&#20301;&#19982;&#20998;&#21106;&#30340;&#26694;&#26550;AV-SAM&#12290;AV-SAM&#22522;&#20110;SAM&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Flickr-SoundNet&#21644;AVSBench&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#22312;&#38899;&#35270;&#39057;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#33021;&#20998;&#21106;&#27169;&#22411;&#65288;SAM&#65289;&#36817;&#26469;&#22312;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;&#20294;&#26159;&#65292;&#22312;&#38899;&#35270;&#39057;&#20219;&#21153;&#20013;&#65292;&#27604;&#22914;&#35270;&#38899;&#39057;&#23450;&#20301;&#19982;&#20998;&#21106;&#65292;SAM&#30340;&#34920;&#29616;&#21364;&#40092;&#26377;&#23454;&#39564;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;SAM&#30340;&#38899;&#35270;&#39057;&#23450;&#20301;&#19982;&#20998;&#21106;&#26694;&#26550;AV-SAM&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#30456;&#24212;&#20110;&#38899;&#39057;&#30340;&#22768;&#38899;&#23545;&#35937;&#25513;&#30721;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;AV-SAM&#31616;&#21333;&#22320;&#21033;&#29992;&#26469;&#33258;SAM&#20013;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#38899;&#39057;&#29305;&#24449;&#30340;&#36880;&#20687;&#32032;&#38899;&#35270;&#39057;&#34701;&#21512;&#26469;&#32858;&#21512;&#36328;&#27169;&#24577;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#32858;&#21512;&#21518;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#39304;&#20837;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#20013;&#29983;&#25104;&#26368;&#32456;&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#25513;&#30721;&#12290;&#25105;&#20204;&#22312;Flickr-SoundNet&#21644;AVSBench&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AV-SAM&#22312;&#22768;&#38899;&#23545;&#35937;&#23450;&#20301;&#21644;&#20998;&#21106;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) has recently shown its powerful effectiveness in visual segmentation tasks. However, there is less exploration concerning how SAM works on audio-visual tasks, such as visual sound localization and segmentation. In this work, we propose a simple yet effective audio-visual localization and segmentation framework based on the Segment Anything Model, namely AV-SAM, that can generate sounding object masks corresponding to the audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion across audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Then, the aggregated cross-modal features are fed into the prompt encoder and mask decoder to generate the final audio-visual segmentation masks. We conduct extensive experiments on Flickr-SoundNet and AVSBench datasets. The results demonstrate that the proposed AV-SAM can achieve competitive performance on sounding object localization an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#20219;&#20309;&#20998;&#36776;&#29575;&#21644;&#33033;&#20914;&#24207;&#21015;&#30340;&#20020;&#24202;&#33041;&#37096;MRI&#25195;&#25551;&#36827;&#34892;&#30382;&#23618;&#37325;&#24314;&#12289;&#37197;&#20934;&#12289;&#20998;&#21106;&#21644;&#21402;&#24230;&#20272;&#35745;&#65292;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#12289;&#23588;&#20854;&#26159;&#38024;&#23545;&#23569;&#25968;&#26063;&#35028;&#21644;&#32597;&#35265;&#30142;&#30149;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01827</link><description>&lt;p&gt;
&#23545;&#20020;&#24202;&#33041;&#37096;MRI&#25195;&#25551;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#36827;&#34892;&#30340;&#30382;&#23618;&#20998;&#26512;&#65288;arXiv&#65306;2305.01827v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Cortical analysis of heterogeneous clinical brain MRI scans for large-scale neuroimaging studies. (arXiv:2305.01827v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#20219;&#20309;&#20998;&#36776;&#29575;&#21644;&#33033;&#20914;&#24207;&#21015;&#30340;&#20020;&#24202;&#33041;&#37096;MRI&#25195;&#25551;&#36827;&#34892;&#30382;&#23618;&#37325;&#24314;&#12289;&#37197;&#20934;&#12289;&#20998;&#21106;&#21644;&#21402;&#24230;&#20272;&#35745;&#65292;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#12289;&#23588;&#20854;&#26159;&#38024;&#23545;&#23569;&#25968;&#26063;&#35028;&#21644;&#32597;&#35265;&#30142;&#30149;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;MRI&#30340;&#30382;&#23618;&#34920;&#38754;&#20998;&#26512;&#26159;&#20154;&#31867;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20363;&#22914;&#29992;&#20110;&#30382;&#23618;&#37197;&#20934;&#12289;&#20998;&#21106;&#25110;&#21402;&#24230;&#20272;&#35745;&#12290;&#20985;&#20984;&#19981;&#24179;&#30340;&#30382;&#23618;&#20960;&#20309;&#24418;&#29366;&#38656;&#35201;&#31561;&#36317;&#25195;&#25551;&#65288;&#20363;&#22914;1mm MPRAGEs&#65289;&#21644;&#22909;&#30340;&#28784;&#30333;&#36136;&#23545;&#27604;&#24230;&#36827;&#34892;3D&#37325;&#24314;&#12290;&#36825;&#25490;&#38500;&#20102;&#23545;&#20110;&#22823;&#22810;&#25968;&#20986;&#20110;&#20020;&#24202;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#33041;&#37096;MRI&#25195;&#25551;&#30340;&#20998;&#26512;&#12290;&#20998;&#26512;&#36825;&#20123;&#25195;&#25551;&#23558;&#20351;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#20855;&#22791;&#24403;&#21069;&#30740;&#31350;&#25968;&#25454;&#38598;&#26080;&#27861;&#23454;&#29616;&#30340;&#26679;&#26412;&#37327;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23569;&#25968;&#26063;&#35028;&#21644;&#32597;&#35265;&#30142;&#30149;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;&#20219;&#20309;&#20998;&#36776;&#29575;&#21644;&#33033;&#20914;&#24207;&#21015;&#30340;&#20020;&#24202;&#33041;&#37096;MRI&#25195;&#25551;&#36827;&#34892;&#30382;&#23618;&#37325;&#24314;&#12289;&#37197;&#20934;&#12289;&#20998;&#21106;&#21644;&#21402;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#21547;&#23398;&#20064;&#32452;&#20214;&#21644;&#32463;&#20856;&#20248;&#21270;&#27169;&#22359;&#12290;&#21069;&#32773;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#26469;&#35757;&#32451;CNN&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#30333;&#36136;&#21644;&#33041;&#22806;&#33180;&#34920;&#38754;&#65288;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65289;&#30340;&#38544;&#24335;&#34920;&#31034;&#65292;&#36798;&#21040;&#31561;&#36317;&#30340;1mm&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface analysis of the cortex is ubiquitous in human neuroimaging with MRI, e.g., for cortical registration, parcellation, or thickness estimation. The convoluted cortical geometry requires isotropic scans (e.g., 1mm MPRAGEs) and good gray-white matter contrast for 3D reconstruction. This precludes the analysis of most brain MRI scans acquired for clinical purposes. Analyzing such scans would enable neuroimaging studies with sample sizes that cannot be achieved with current research datasets, particularly for underrepresented populations and rare diseases. Here we present the first method for cortical reconstruction, registration, parcellation, and thickness estimation for clinical brain MRI scans of any resolution and pulse sequence. The methods has a learning component and a classical optimization module. The former uses domain randomization to train a CNN that predicts an implicit representation of the white matter and pial surfaces (a signed distance function) at 1mm isotropic res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26118;&#34411;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;OOD&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#31867;&#21035;&#30340;&#25216;&#26415;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22312;&#23454;&#38469;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.01823</link><description>&lt;p&gt;
&#38024;&#23545;&#26118;&#34411;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;OOD&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection algorithms for robust insect classification. (arXiv:2305.01823v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26118;&#34411;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;OOD&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#31867;&#21035;&#30340;&#25216;&#26415;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22312;&#23454;&#38469;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#26118;&#34411;&#20998;&#31867;&#20934;&#30830;&#24615;&#65307;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#25968;&#36866;&#29992;&#20110;&#21463;&#25511;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#65292;&#32780;&#22312;&#29616;&#23454;&#30340;&#20892;&#19994;&#39046;&#22495;&#20013;&#24448;&#24448;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24178;&#25200;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#38382;&#39064;&#26159;&#36755;&#20837;&#30340;&#22270;&#20687;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;&#20363;&#22914;&#36710;&#36742;&#12289;&#21160;&#29289;&#12289;&#20154;&#31867;&#25110;&#27169;&#31946;&#30340;&#26118;&#34411;&#25110;&#26410;&#32463;&#35757;&#32451;&#30340;&#26118;&#34411;&#31867;&#30340;&#22270;&#20687;&#65289;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#20998;&#31867;&#12290;OOD&#26816;&#27979;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23427;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#22312;&#38750;&#26118;&#34411;&#25110;&#26410;&#35757;&#32451;&#30340;&#26118;&#34411;&#31867;&#22270;&#20687;&#19978;&#36827;&#34892;&#38169;&#35823;&#30340;&#20998;&#31867;&#39044;&#27979;&#12290;&#25105;&#20204;&#29983;&#25104;&#24182;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;OOD&#31639;&#27861;&#22312;&#26118;&#34411;&#26816;&#27979;&#20998;&#31867;&#22120;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#31639;&#27861;&#20195;&#34920;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;OOD&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#12289;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#21644;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#31867;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#26816;&#27979;OOD&#22270;&#20687;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;1.0&#30340;AUC&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approaches have produced models with good insect classification accuracy; Most of these models are conducive for application in controlled environmental conditions. One of the primary emphasis of researchers is to implement identification and classification models in the real agriculture fields, which is challenging because input images that are wildly out of the distribution (e.g., images like vehicles, animals, humans, or a blurred image of an insect or insect class that is not yet trained on) can produce an incorrect insect classification. Out-of-distribution (OOD) detection algorithms provide an exciting avenue to overcome these challenge as it ensures that a model abstains from making incorrect classification prediction of non-insect and/or untrained insect class images. We generate and evaluate the performance of state-of-the-art OOD algorithms on insect detection classifiers. These algorithms represent a diversity of methods for addressing an OOD problem. Spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27969;&#20307;&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#22320;&#22686;&#24378;&#20998;&#36776;&#29575;&#24182;&#26657;&#27491;&#19978;&#19979;&#25991;&#30456;&#20851;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.01822</link><description>&lt;p&gt;
&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#27969;&#20307;&#27969;&#21160;&#38477;&#23610;&#24230;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26725;
&lt;/p&gt;
&lt;p&gt;
Unpaired Downscaling of Fluid Flows with Diffusion Bridges. (arXiv:2305.01822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27969;&#20307;&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#22320;&#22686;&#24378;&#20998;&#36776;&#29575;&#24182;&#26657;&#27491;&#19978;&#19979;&#25991;&#30456;&#20851;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#29702;&#24819;&#21270;&#22320;&#29699;&#29289;&#29702;&#27969;&#20307;&#27169;&#25311;&#36827;&#34892;&#38477;&#23610;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20174;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#22270;&#20687;&#30340;&#20613;&#37324;&#21494;&#20809;&#35889;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20004;&#20010;&#29420;&#31435;&#30340;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#38142;&#25509;&#22312;&#19968;&#36215;&#20197;&#29992;&#20110;&#22495;&#36716;&#25442;&#12290;&#25152;&#24471;&#21040;&#30340;&#36716;&#25442;&#26159;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#25193;&#25955;&#26725;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#29305;&#23450;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26679;&#26412;&#12290;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#33021;&#21147;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#26657;&#20934;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#35774;&#32622;&#36824;&#26088;&#22312;&#22312;&#27809;&#26377;&#35775;&#38382;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#38477;&#23610;&#24230;&#22788;&#29702;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20801;&#35768;&#23558;&#22810;&#20010;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22320;&#29699;&#29289;&#29702;&#27969;&#20307;&#27169;&#25311;&#30340;&#20998;&#36776;&#29575;&#24182;&#32416;&#27491;&#19978;&#19979;&#25991;&#30456;&#20851;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to downscale idealized geophysical fluid simulations using generative models based on diffusion maps. By analyzing the Fourier spectra of images drawn from different data distributions, we show how one can chain together two independent conditional diffusion models for use in domain translation. The resulting transformation is a diffusion bridge between a low resolution and a high resolution dataset and allows for new sample generation of high-resolution images given specific low resolution features. The ability to generate new samples allows for the computation of any statistic of interest, without any additional calibration or training. Our unsupervised setup is also designed to downscale images without access to paired training data; this flexibility allows for the combination of multiple source and target domains without additional training. We demonstrate that the method enhances resolution and corrects context-dependent biases in geophysical fluid simulations,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01807</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#23398;&#20064;&#21644;&#24212;&#29992;&#20110;&#35299;&#37322;&#24615;&#33041;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21033;&#29992;&#22522;&#20110;&#25299;&#25169;&#22270;&#30340;&#21367;&#31215;&#25805;&#20316;&#26469;&#32452;&#21512;&#22270;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#22270;&#26469;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20256;&#32479;PCA&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#30340;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;VNN&#65289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#21487;&#36716;&#31227;&#24615;&#30340;&#27010;&#24565;&#26159;&#20174;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#8220;&#20860;&#23481;&#8221;&#30340;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#30340;&#30452;&#35266;&#26399;&#26395;&#20013;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VNN&#20174;GCN&#32487;&#25215;&#30340;&#26080;&#26631;&#24230;&#25968;&#25454;&#22788;&#29702;&#26550;&#26500;&#65292;&#24182;&#35777;&#26126;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCN) leverage topology-driven graph convolutional operations to combine information across the graph for inference tasks. In our recent work, we have studied GCNs with covariance matrices as graphs in the form of coVariance neural networks (VNNs) that draw similarities with traditional PCA-driven data analysis approaches while offering significant advantages over them. In this paper, we first focus on theoretically characterizing the transferability of VNNs. The notion of transferability is motivated from the intuitive expectation that learning models could generalize to "compatible" datasets (possibly of different dimensionalities) with minimal effort. VNNs inherit the scale-free data processing architecture from GCNs and here, we show that VNNs exhibit transferability of performance over datasets whose covariance matrices converge to a limit object. Multi-scale neuroimaging datasets enable the study of the brain at multiple scales and hence, can validate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01801</link><description>&lt;p&gt;
&#24403;&#26032;&#30340;&#19981;&#19968;&#23450;&#26159;&#26356;&#22909;&#30340;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#21463;&#30410;&#20110;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#30340;&#25512;&#33616;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?. (arXiv:2305.01801v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#31070;&#32463;&#27169;&#22411;&#34987;&#22810;&#27425;&#23459;&#20256;&#20026;&#25512;&#33616;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22810;&#20010;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#24182;&#19981;&#33021;&#21487;&#38752;&#22320;&#22797;&#29616;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#30340;&#35780;&#20272;&#26159;&#22312;&#19981;&#19968;&#33268;&#30340;&#21327;&#35758;&#19979;&#36827;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#20351;&#20154;&#20204;&#38590;&#20197;&#20102;&#35299;&#23454;&#38469;&#19978;&#21487;&#20197;&#20174;&#36825;&#20123;&#31070;&#32463;&#27169;&#22411;&#20013;&#33719;&#24471;&#22810;&#23569;&#30410;&#22788;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#20844;&#24179;&#32780;&#20840;&#38754;&#30340;&#32489;&#25928;&#27604;&#36739;&#26469;&#27604;&#36739;&#20256;&#32479;&#27169;&#22411;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#12289;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#38544;&#24335;&#25968;&#25454;&#30340;&#39030;&#37096;&#25512;&#33616;&#30340;&#26368;&#26032;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#21644;&#20256;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#34913;&#37327;&#25512;&#33616;&#27169;&#22411;&#30340;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed.  Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. 
&lt;/p&gt;</description></item><item><title>MISNN&#26159;&#19968;&#31181;&#22522;&#20110;&#21322;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#37325;&#25554;&#34917;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33391;&#22909;&#30340;&#25554;&#34917;&#31934;&#24230;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#21644;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01794</link><description>&lt;p&gt;
MISNN: &#22522;&#20110;&#21322;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#37325;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MISNN: Multiple Imputation via Semi-parametric Neural Networks. (arXiv:2305.01794v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01794
&lt;/p&gt;
&lt;p&gt;
MISNN&#26159;&#19968;&#31181;&#22522;&#20110;&#21322;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#37325;&#25554;&#34917;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33391;&#22909;&#30340;&#25554;&#34917;&#31934;&#24230;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#21644;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#25554;&#34917;&#26041;&#27861;&#65288;MI&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#12289;&#31038;&#20250;&#21644;&#35745;&#37327;&#30740;&#31350;&#20013;&#30340;&#32570;&#22833;&#20540;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#19979;&#28216;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19981;&#24403;&#25512;&#26029;&#12290;&#22312;&#23384;&#22312;&#39640;&#32500;&#25968;&#25454;&#26102;&#65292;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#30340;&#25554;&#34917;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;$\ell_1$&#35268;&#21017;&#21270;&#22238;&#24402;&#65288;&#22914;Lasso&#12289;&#33258;&#36866;&#24212;Lasso&#21644;&#24377;&#24615;&#32593;&#32476;&#65289;&#26159;&#38450;&#27490;&#27169;&#22411;&#27424;&#20915;&#23450;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#36827;&#34892;MI&#26159;&#22256;&#38590;&#30340;&#65306;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#19988;&#24615;&#33021;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;MISNN&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#33021;&#21147;&#65292;&#26159;&#19968;&#20010;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20860;&#23481;&#20219;&#20309;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12289;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12289;&#39640;/&#20302;&#32500;&#25968;&#25454;&#21644;&#19968;&#33324;&#32570;&#22833;&#27169;&#24335;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#65292;MISNN&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#25554;&#34917;&#31934;&#24230;&#26041;&#38754;&#23637;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple imputation (MI) has been widely applied to missing value problems in biomedical, social and econometric research, in order to avoid improper inference in the downstream data analysis. In the presence of high-dimensional data, imputation models that include feature selection, especially $\ell_1$ regularized regression (such as Lasso, adaptive Lasso, and Elastic Net), are common choices to prevent the model from underdetermination. However, conducting MI with feature selection is difficult: existing methods are often computationally inefficient and poor in performance. We propose MISNN, a novel and efficient algorithm that incorporates feature selection for MI. Leveraging the approximation power of neural networks, MISNN is a general and flexible framework, compatible with any feature selection method, any neural network architecture, high/low-dimensional data and general missing patterns. Through empirical experiments, MISNN has demonstrated great advantages over state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#20065;&#22522;&#20110;&#21355;&#26143;&#30340;&#36139;&#22256;&#26144;&#23556;&#20013;&#30340;&#20195;&#34920;&#24615;&#24046;&#24322;&#12289;&#39044;&#27979;&#35823;&#24046;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#29616;&#35937;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#39044;&#27979;&#22320;&#22270;&#30340;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01783</link><description>&lt;p&gt;
&#21355;&#26143;&#34893;&#29983;&#36139;&#22256;&#22320;&#22270;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20195;&#34920;&#24615;: &#22478;&#20065;&#24046;&#36317;&#21450;&#20854;&#23545;&#19979;&#28216;&#25919;&#31574;&#30340;&#24433;&#21709;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy. (arXiv:2305.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#20065;&#22522;&#20110;&#21355;&#26143;&#30340;&#36139;&#22256;&#26144;&#23556;&#20013;&#30340;&#20195;&#34920;&#24615;&#24046;&#24322;&#12289;&#39044;&#27979;&#35823;&#24046;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#29616;&#35937;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#39044;&#27979;&#22320;&#22270;&#30340;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#34893;&#29983;&#30340;&#36139;&#22256;&#22320;&#22270;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#20915;&#23450;&#20998;&#37197;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#21644;&#25919;&#24220;&#36164;&#28304;&#31561;&#39640;&#39118;&#38505;&#25919;&#31574;&#38382;&#39064;&#12290;&#36825;&#20123;&#36139;&#22256;&#22320;&#22270;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35843;&#26597;&#8220;&#22320;&#38754;&#30495;&#23454;&#24615;&#8221;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#28982;&#21518;&#39044;&#27979;&#22312;&#23384;&#22312;&#22270;&#20687;&#20294;&#26080;&#35843;&#26597;&#30340;&#21306;&#22495;&#30340;&#36139;&#22256;&#27700;&#24179;&#26469;&#26500;&#24314;&#30340;&#12290;&#26412;&#25991;&#21033;&#29992;&#21313;&#20010;&#22269;&#23478;&#30340;&#35843;&#26597;&#21644;&#21355;&#26143;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22478;&#20065;&#22522;&#20110;&#21355;&#26143;&#30340;&#36139;&#22256;&#26144;&#23556;&#20013;&#30340;&#20195;&#34920;&#24615;&#24046;&#24322;&#65292;&#39044;&#27979;&#35823;&#24046;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#29616;&#35937;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#39044;&#27979;&#22320;&#22270;&#30340;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#23558;&#21355;&#26143;&#34893;&#29983;&#30340;&#36139;&#22256;&#22320;&#22270;&#29992;&#20110;&#29616;&#23454;&#25919;&#31574;&#20915;&#31574;&#21069;&#36827;&#34892;&#20180;&#32454;&#30340;&#35823;&#24046;&#21644;&#20559;&#24046;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poverty maps derived from satellite imagery are increasingly used to inform high-stakes policy decisions, such as the allocation of humanitarian aid and the distribution of government resources. Such poverty maps are typically constructed by training machine learning algorithms on a relatively modest amount of ``ground truth" data from surveys, and then predicting poverty levels in areas where imagery exists but surveys do not. Using survey and satellite data from ten countries, this paper investigates disparities in representation, systematic biases in prediction errors, and fairness concerns in satellite-based poverty mapping across urban and rural lines, and shows how these phenomena affect the validity of policies based on predicted maps. Our findings highlight the importance of careful error and bias analysis before using satellite-based poverty maps in real-world policy decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01777</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#24418;&#23637;&#24179;&#21644;&#37325;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#27969;&#24418;&#30340;&#26377;&#38480;&#26679;&#26412;&#20013;&#26174;&#24335;&#26500;&#24314;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32447;&#24615;&#21270;&#21644;&#37325;&#26500;&#23884;&#20837;&#23376;&#27969;&#24418;&#12290;&#25105;&#20204;&#25152;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#31216;&#20026;&#23637;&#24179;&#32593;&#32476;&#65288;FlatNet&#65289;&#65292;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#35745;&#31639;&#19978;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#31181;&#24179;&#34913;&#36890;&#24120;&#22312;&#22522;&#20110;&#27969;&#24418;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#38590;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#30340;&#39640;&#32500;&#27969;&#24418;&#25968;&#25454;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#22810;&#23792;&#39044;&#27979;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#23454;&#29616;&#20102;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01773</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24265;&#20215;&#21644;&#30830;&#23450;&#24615;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems. (arXiv:2305.01773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#22810;&#23792;&#39044;&#27979;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#23454;&#29616;&#20102;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34987;&#29992;&#20110;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#20248;&#38597;&#22320;&#36866;&#24212;&#20110;&#20855;&#26377;&#21464;&#21270;&#21644;&#22823;&#37327;&#20195;&#29702;&#30340;&#31995;&#32479;&#12290;&#34429;&#28982;&#22312;&#30830;&#23450;&#24615;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#26377;&#20852;&#36259;&#33719;&#24471;&#26410;&#26469;&#36712;&#36857;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#38543;&#26426;&#31995;&#32479;&#65292;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#36895;&#24230;&#24930;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#65292;&#35201;&#20040;&#20570;&#20986;&#31616;&#21270;&#20551;&#35774;&#65292;&#20351;&#24471;&#39044;&#27979;&#20998;&#24067;&#26159;&#21333;&#23792;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#24213;&#23618;&#30340;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#12290;&#39044;&#27979;&#20998;&#24067;&#26159;&#22810;&#23792;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#20854;&#20013;&#39640;&#26031;&#20998;&#37327;&#30340;&#30697;&#21487;&#20197;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30697;&#21305;&#37197;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#21644;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#38543;&#26426;&#31995;&#32479;&#30340;&#36712;&#36857;&#65292;&#21363;&#20351;&#23384;&#22312;&#24040;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are often used to model interacting dynamical systems since they gracefully scale to systems with a varying and high number of agents. While there has been much progress made for deterministic interacting systems, modeling is much more challenging for stochastic systems in which one is interested in obtaining a predictive distribution over future trajectories. Existing methods are either computationally slow since they rely on Monte Carlo sampling or make simplifying assumptions such that the predictive distribution is unimodal. In this work, we present a deep state-space model which employs graph neural networks in order to model the underlying interacting dynamical system. The predictive distribution is multimodal and has the form of a Gaussian mixture model, where the moments of the Gaussian components can be computed via deterministic moment matching rules. Our moment matching scheme can be exploited for sample-free inference, leading to more efficient and sta
&lt;/p&gt;</description></item><item><title>DeCom&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32806;&#21512;&#22240;&#23376;&#20998;&#35299;&#26426;&#30340;RSV&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#27491;&#24120;&#30340;&#23395;&#33410;&#24615;RSV&#20256;&#25773;&#27169;&#24335;&#21644;COVID-19&#31526;&#21512;NPI&#25514;&#26045;&#19979;RSV&#20256;&#25773;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#39044;&#27979;&#25928;&#26524;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.01770</link><description>&lt;p&gt;
DeCom&#65306;&#22522;&#20110;&#28145;&#24230;&#32806;&#21512;&#22240;&#23376;&#20998;&#35299;&#26426;&#30340;&#38750;&#33647;&#29289;&#24178;&#39044;&#24863;&#30693;&#30340;COVID-19&#21518;RSV&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness. (arXiv:2305.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01770
&lt;/p&gt;
&lt;p&gt;
DeCom&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32806;&#21512;&#22240;&#23376;&#20998;&#35299;&#26426;&#30340;RSV&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#27491;&#24120;&#30340;&#23395;&#33410;&#24615;RSV&#20256;&#25773;&#27169;&#24335;&#21644;COVID-19&#31526;&#21512;NPI&#25514;&#26045;&#19979;RSV&#20256;&#25773;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#39044;&#27979;&#25928;&#26524;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#36947;&#21512;&#32990;&#30149;&#27602;&#65288;RSV&#65289;&#26159;&#23156;&#24188;&#20799;&#26368;&#21361;&#38505;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#12290;&#30001;&#20110;COVID-19&#30340;&#38750;&#33647;&#29289;&#24178;&#39044;&#65288;NPI&#65289;&#25514;&#26045;&#65292;RSV&#30340;&#23395;&#33410;&#24615;&#20256;&#25773;&#27169;&#24335;&#22312;2020&#24180;&#24050;&#32463;&#20013;&#26029;&#65292;&#24182;&#22312;2021&#24180;&#21271;&#21322;&#29699;&#25552;&#21069;&#25968;&#26376;&#20986;&#29616;&#36716;&#21464;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;COVID-19&#22914;&#20309;&#24433;&#21709;RSV&#24182;&#26500;&#24314;&#39044;&#27979;&#31639;&#27861;&#20197;&#39044;&#27979;COVID-19&#21518;RSV&#20877;&#20986;&#29616;&#30340;&#26102;&#38388;&#21644;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCom&#30340;&#28145;&#24230;&#32806;&#21512;&#24352;&#37327;&#20998;&#35299;&#26426;&#65292;&#29992;&#20110;COVID-19&#21518;RSV&#39044;&#27979;&#12290;DeCom&#21033;&#29992;&#24352;&#37327;&#20998;&#35299;&#21644;&#27531;&#24046;&#24314;&#27169;&#65292;&#33021;&#21487;&#38752;&#22320;&#23398;&#20064;&#21463;COVID-19&#24433;&#21709;&#19979;&#30340;RSV&#20256;&#25773;&#65292;&#21516;&#26102;&#32771;&#34385;&#27491;&#24120;&#23395;&#33410;&#24615;RSV&#20256;&#25773;&#27169;&#24335;&#21644;NPI&#12290;&#22312;&#30495;&#23454;&#30340;RSV&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DeCom&#27604;&#29616;&#26377;RSV&#39044;&#27979;&#31639;&#27861;&#26356;&#20934;&#30830;&#65292;&#24182;&#21487;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory syncytial virus (RSV) is one of the most dangerous respiratory diseases for infants and young children. Due to the nonpharmaceutical intervention (NPI) imposed in the COVID-19 outbreak, the seasonal transmission pattern of RSV has been discontinued in 2020 and then shifted months ahead in 2021 in the northern hemisphere. It is critical to understand how COVID-19 impacts RSV and build predictive algorithms to forecast the timing and intensity of RSV reemergence in post-COVID-19 seasons. In this paper, we propose a deep coupled tensor factorization machine, dubbed as DeCom, for post COVID-19 RSV prediction. DeCom leverages tensor factorization and residual modeling. It enables us to learn the disrupted RSV transmission reliably under COVID-19 by taking both the regular seasonal RSV transmission pattern and the NPI into consideration. Experimental results on a real RSV dataset show that DeCom is more accurate than the state-of-the-art RSV prediction algorithms and achieves up 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.01764</link><description>&lt;p&gt;
&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#22240;&#26524;&#25552;&#31034;&#35821;
&lt;/p&gt;
&lt;p&gt;
Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#25968;&#25454;&#38598;&#19981;&#20165;&#20165;&#21547;&#26377;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#36824;&#21253;&#21547;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#20197;&#24773;&#24863;&#20998;&#31867;&#20026;&#20363;&#65292;&#25506;&#35752;&#35780;&#35770;&#65288;X&#65289;&#21644;&#24773;&#24863;&#65288;Y&#65289;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#21487;&#20197;&#24433;&#21709;&#24773;&#32490;&#65292;&#24403;&#19968;&#20010;&#20154;&#39318;&#27425;&#36827;&#34892;&#35780;&#20998;&#24182;&#22312;&#35780;&#35770;&#20013;&#36827;&#34892;&#33258;&#25105;&#21512;&#29702;&#21270;&#26102;&#65288;&#24773;&#24863;&#24341;&#36215;&#35780;&#35770;&#65292;&#21363;Y-&gt;X&#65289;&#65292;&#19982;&#39318;&#20808;&#25551;&#36848;&#33258;&#24049;&#30340;&#32463;&#21382;&#24182;&#26435;&#34913;&#21033;&#24330;&#20197;&#20570;&#20986;&#26368;&#21518;&#35780;&#20998;&#26102;&#65288;&#35780;&#35770;&#24341;&#36215;&#24773;&#24863;&#65292;&#21363;X-&gt;Y&#65289;&#65292;&#20250;&#24341;&#21457;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#35780;&#27880;&#32773;&#36890;&#36807;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25512;&#26029;&#29992;&#25143;&#30340;&#21407;&#22987;&#35780;&#20998;&#65292;&#21017;&#36825;&#20063;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#35780;&#35770;&#24341;&#36215;&#35780;&#20998;&#65292;&#21363;X-ToM-&gt; Y&#65289;&#12290;&#26412;&#25991;&#23558;&#36825;&#19977;&#31181;&#24773;&#24863;&#20998;&#31867;&#30340;&#20154;&#31867;&#24515;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21270;&#20026;&#19977;&#20010;&#25552;&#31034;&#35821;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#25552;&#31034;&#35821;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP datasets are richer than just input-output pairs; rather, they carry causal relations between the input and output variables. In this work, we take sentiment classification as an example and look into the causal relations between the review (X) and sentiment (Y). As psychology studies show that language can affect emotion, different psychological processes are evoked when a person first makes a rating and then self-rationalizes their feeling in a review (where the sentiment causes the review, i.e., Y -&gt; X), versus first describes their experience, and weighs the pros and cons to give a final rating (where the review causes the sentiment, i.e., X -&gt; Y ). Furthermore, it is also a completely different psychological process if an annotator infers the original rating of the user by theory of mind (ToM) (where the review causes the rating, i.e., X -ToM-&gt; Y ). In this paper, we verbalize these three causal mechanisms of human psychological processes of sentiment classification into three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.01761</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#32593;&#32476;&#29992;&#20110;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Networks for Antibiogram Pattern Prediction. (arXiv:2305.01761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#26159;&#23545;&#24863;&#26579;&#24739;&#32773;&#30340;&#25239;&#29983;&#32032;&#32784;&#33647;&#24615;&#26816;&#27979;&#32467;&#26524;&#36827;&#34892;&#21608;&#26399;&#24615;&#24635;&#32467;&#12290;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#26377;&#21161;&#20110;&#21307;&#29983;&#20102;&#35299;&#22320;&#21306;&#32784;&#33647;&#24615;&#29575;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;&#22788;&#26041;&#25239;&#29983;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#12290;&#23613;&#31649;&#35813;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22788;&#29702;&#35813;&#38382;&#39064;&#20250;&#36935;&#21040;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#30001;&#20110;&#22522;&#22240;&#30456;&#20284;&#24615;&#32780;&#24444;&#27492;&#32039;&#23494;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
An antibiogram is a periodic summary of antibiotic resistance results of organisms from infected patients to selected antimicrobial drugs. Antibiograms help clinicians to understand regional resistance rates and select appropriate antibiotics in prescriptions. In practice, significant combinations of antibiotic resistance may appear in different antibiograms, forming antibiogram patterns. Such patterns may imply the prevalence of some infectious diseases in certain regions. Thus it is of crucial importance to monitor antibiotic resistance trends and track the spread of multi-drug resistant organisms. In this paper, we propose a novel problem of antibiogram pattern prediction that aims to predict which patterns will appear in the future. Despite its importance, tackling this problem encounters a series of challenges and has not yet been explored in the literature. First of all, antibiogram patterns are not i.i.d as they may have strong relations with each other due to genomic similariti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;NMF&#22522;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#24378;&#30417;&#30563;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#37325;&#26500;&#20449;&#21495;&#36136;&#37327;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01758</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Generative NMF for Single Channel Source Separation. (arXiv:2305.01758v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;NMF&#22522;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#24378;&#30417;&#30563;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#37325;&#26500;&#20449;&#21495;&#36136;&#37327;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#23398;&#20064;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#24605;&#24819;&#26368;&#36817;&#34987;&#24341;&#20837;&#20102;&#26356;&#24191;&#27867;&#30340;&#21453;&#38382;&#39064;&#32972;&#26223;&#19979;&#12290;&#35813;&#26041;&#27861;&#30340;&#28789;&#24863;&#22312;&#20110;&#24847;&#35782;&#21040;&#19981;&#20165;&#38656;&#35201;&#23398;&#20064;&#32452;&#25104;&#25152;&#38656;&#20449;&#21495;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#32780;&#19988;&#25110;&#32773;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#23398;&#20064;&#36991;&#20813;&#34920;&#31034;&#20013;&#30340;&#21738;&#20123;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36890;&#36807;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#36827;&#34892;&#28304;&#20998;&#31163;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;NMF&#22522;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#36824;&#26159;&#38899;&#39057;&#20998;&#31163;&#65292;&#22312;&#27809;&#26377;&#24378;&#30417;&#30563;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#37117;&#20250;&#26126;&#26174;&#25552;&#39640;&#37325;&#26500;&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of adversarial learning of regularization functionals has recently been introduced in the wider context of inverse problems. The intuition behind this method is the realization that it is not only necessary to learn the basic features that make up a class of signals one wants to represent, but also, or even more so, which features to avoid in the representation. In this paper, we will apply this approach to the problem of source separation by means of non-negative matrix factorization (NMF) and present a new method for the adversarial training of NMF bases. We show in numerical experiments, both for image and audio separation, that this leads to a clear improvement of the reconstructed signals, in particular in the case where little or no strong supervision data is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;NNIP&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#38598;&#21512;&#26041;&#27861;&#19968;&#33268;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.01754</link><description>&lt;p&gt;
&#21333;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#20013;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles. (arXiv:2305.01754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;NNIP&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#38598;&#21512;&#26041;&#27861;&#19968;&#33268;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24120;&#24120;&#23558;&#39640;&#32622;&#20449;&#24230;&#20998;&#37197;&#32473;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#36828;&#31163;&#25968;&#25454;&#20998;&#24067;&#30340;&#28857;&#65292;&#36825;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#24403;&#23427;&#20204;&#34987;&#29992;&#20110;&#27169;&#25311;&#26448;&#26009;&#31995;&#32479;&#20013;&#30340;&#21407;&#23376;&#38388;&#21183;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#30495;&#23454;&#30340;&#32467;&#26500;&#30772;&#22351;&#27169;&#25311;&#65292;&#25110;&#32773;&#23548;&#33268;&#20559;&#35823;&#30340;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#19981;&#33021;&#21453;&#26144;&#30495;&#27491;&#30340;&#29289;&#29702;&#23398;&#12290;&#21487;&#24494;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#21487;&#20197;&#21457;&#29616;&#26032;&#30340;&#20449;&#24687;&#25968;&#25454;&#65292;&#25512;&#21160;&#40065;&#26834;&#21183;&#30340;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21407;&#23376;&#27169;&#25311;&#23384;&#22312;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#26032;&#24320;&#21457;&#30340;&#25216;&#26415;&#65292;&#19981;&#23384;&#22312;&#28165;&#26224;&#30340;&#25351;&#23548;&#26041;&#38024;&#20197;&#30830;&#23450;&#21738;&#31181;&#25216;&#26415;&#26368;&#20026;&#26377;&#25928;&#25110;&#36866;&#21512;&#29305;&#23450;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;UQ&#26041;&#26696;&#26469;&#25913;&#36827;NN&#21407;&#23376;&#38388;&#21183;&#65288;NNIPs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#19982;&#20351;&#29992;&#21333;&#19968;&#30340;&#30830;&#23450;&#24615;NNs&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#24179;&#22343;&#24046;&#24322;&#20272;&#35745;&#65292;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#21644;&#24322;&#26041;&#24046;&#22238;&#24402;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#38598;&#21512;&#26041;&#27861;&#22312;&#22810;&#20010;&#26448;&#26009;&#31995;&#32479;&#21644;UQ&#25351;&#26631;&#19978;&#34920;&#29616;&#19968;&#33268;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#19982;&#26356;&#39640;&#25928;&#30340;&#21333;&#27169;&#22411;&#26367;&#20195;&#21697;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#26377;&#26102;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) often assign high confidence to their predictions, even for points far out-of-distribution, making uncertainty quantification (UQ) a challenge. When they are employed to model interatomic potentials in materials systems, this problem leads to unphysical structures that disrupt simulations, or to biased statistics and dynamics that do not reflect the true physics. Differentiable UQ techniques can find new informative data and drive active learning loops for robust potentials. However, a variety of UQ techniques, including newly developed ones, exist for atomistic simulations and there are no clear guidelines for which are most effective or suitable for a given case. In this work, we examine multiple UQ schemes for improving the robustness of NN interatomic potentials (NNIPs) through active learning. In particular, we compare incumbent ensemble-based methods against strategies that use single, deterministic NNs: mean-variance estimation, deep evidential regression, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01747</link><description>&lt;p&gt;
&#24102;&#26377;&#26377;&#38480;&#27880;&#37322;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#20266;&#26631;&#31614;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#21450;&#20854;&#25512;&#24191;&#65292;&#20266;&#26631;&#31614;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21407;&#22987;&#25512;&#26029;&#20316;&#20026;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20266;&#26631;&#31614;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37096;&#20998;&#35299;&#37322;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#21407;&#29702;&#19979;&#20266;&#26631;&#31614;&#30340;&#23436;&#20840;&#27867;&#21270;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#23398;&#20064;&#36924;&#36817;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38408;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#21644;&#20854;&#25512;&#24191;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21160;&#20316;&#32452;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#21516;&#26102;&#24182;&#19981;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.01738</link><description>&lt;p&gt;
&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. (arXiv:2305.01738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21160;&#20316;&#32452;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#21516;&#26102;&#24182;&#19981;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#21160;&#20316;&#26159;&#23376;&#21160;&#20316;&#30340;&#32452;&#21512;&#12290;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#22266;&#26377;&#30340;&#20998;&#35299;&#32467;&#26500;&#65292;&#23548;&#33268;&#21487;&#33021;&#23545;&#23569;&#35265;&#30340;&#23376;&#21160;&#20316;&#32452;&#21512;&#20570;&#20986;&#30340;&#25512;&#29702;&#27809;&#26377;&#24847;&#20041;&#65307;&#36825;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#23588;&#20854;&#38382;&#39064;&#31361;&#20986;&#65292;&#22240;&#20026;&#25968;&#25454;&#21487;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#24341;&#36215;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#30830;&#23450;&#20102;&#24403;&#29992;&#20110;&#36817;&#20284;Q&#20989;&#25968;&#26102;&#20445;&#35777;&#20135;&#29983;&#38646;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;&#22312;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#26377;&#29992;&#30340;&#65292;&#22240;&#20026;&#23427;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#32780;&#19981;&#19968;&#23450;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#20801;&#35768;&#25105;&#20204;&#23454;&#29616;&#26356;&#22909;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#22312;&#20351;&#29992;&#30001;&#21307;&#30103;&#20445;&#20581;&#21551;&#31034;&#30340;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26631;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many reinforcement learning (RL) applications have combinatorial action spaces, where each action is a composition of sub-actions. A standard RL approach ignores this inherent factorization structure, resulting in a potential failure to make meaningful inferences about rarely observed sub-action combinations; this is particularly problematic for offline settings, where data may be limited. In this work, we propose a form of linear Q-function decomposition induced by factored action spaces. We study the theoretical properties of our approach, identifying scenarios where it is guaranteed to lead to zero bias when used to approximate the Q-function. Outside the regimes with theoretical guarantees, we show that our approach can still be useful because it leads to better sample efficiency without necessarily sacrificing policy optimality, allowing us to achieve a better bias-variance trade-off. Across several offline RL problems using simulators and real-world datasets motivated by healthca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#26694;&#26550;&#65292;&#21033;&#29992;&#26680;&#20989;&#25968;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#32467;&#26500;&#35774;&#35745;&#20102;&#19968;&#20010;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#29305;&#23450;&#20154;&#32676;&#30340;&#24180;&#40836;&#21644;&#24180;&#20221;&#29305;&#23450;&#30340;&#27515;&#20129;&#29575;&#26354;&#38754;&#65292;&#20026;&#19981;&#21516;&#20154;&#32676;&#20013;&#38431;&#21015;&#25928;&#24212;&#30340;&#23384;&#22312;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#23545;&#24179;&#28369;&#31243;&#24230;&#30340;&#20998;&#26512;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.01728</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#26680;&#34920;&#36798;&#23551;&#21629;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expressive Mortality Models through Gaussian Process Kernels. (arXiv:2305.01728v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#26694;&#26550;&#65292;&#21033;&#29992;&#26680;&#20989;&#25968;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#32467;&#26500;&#35774;&#35745;&#20102;&#19968;&#20010;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#29305;&#23450;&#20154;&#32676;&#30340;&#24180;&#40836;&#21644;&#24180;&#20221;&#29305;&#23450;&#30340;&#27515;&#20129;&#29575;&#26354;&#38754;&#65292;&#20026;&#19981;&#21516;&#20154;&#32676;&#20013;&#38431;&#21015;&#25928;&#24212;&#30340;&#23384;&#22312;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#23545;&#24179;&#28369;&#31243;&#24230;&#30340;&#20998;&#26512;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#24180;&#40836;&#21644;&#24180;&#20221;&#29305;&#23450;&#30340;&#27515;&#20129;&#29575;&#26354;&#38754;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;&#21033;&#29992;GP&#26680;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#26469;&#25628;&#32034;&#38024;&#23545;&#32473;&#23450;&#20154;&#32676;&#30340;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#26680;&#12290;&#25105;&#20204;&#30340;&#32452;&#21512;&#25628;&#32034;&#22522;&#20110;&#24180;&#40836;-&#26399;&#38388;-&#38431;&#21015;&#65288;APC&#65289;&#33539;&#20363;&#65292;&#20197;&#26500;&#24314;&#26368;&#33021;&#21305;&#37197;&#27515;&#20129;&#29575;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#21160;&#24577;&#30340;&#21327;&#26041;&#24046;&#20808;&#39564;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#26696;&#20363;&#30740;&#31350;&#20013;&#24212;&#29992;&#24471;&#21040;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#39564;&#35777;GA&#24674;&#22797;APC&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20154;&#31867;&#27515;&#20129;&#25968;&#25454;&#24211;&#30340;&#23454;&#38469;&#22269;&#23478;&#32423;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#19981;&#21516;&#20154;&#32676;&#20013;&#38431;&#21015;&#25928;&#24212;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#30340;&#26032;&#35265;&#35299;&#65292;&#20197;&#21450;&#27839;&#24180;&#40836;&#21644;&#24180;&#20221;&#32500;&#24230;&#30340;&#27515;&#20129;&#29575;&#26354;&#38754;&#30340;&#30456;&#23545;&#24179;&#28369;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24314;&#27169;&#24037;&#20316;&#26159;&#22312;Python&#30340;PyTorch&#24211;&#20013;&#23436;&#25104;&#30340;&#65292;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a flexible Gaussian Process (GP) framework for learning the covariance structure of Age- and Year-specific mortality surfaces. Utilizing the additive and multiplicative structure of GP kernels, we design a genetic programming algorithm to search for the most expressive kernel for a given population. Our compositional search builds off the Age-Period-Cohort (APC) paradigm to construct a covariance prior best matching the spatio-temporal dynamics of a mortality dataset. We apply the resulting genetic algorithm (GA) on synthetic case studies to validate the ability of the GA to recover APC structure, and on real-life national-level datasets from the Human Mortality Database. Our machine-learning based analysis provides novel insight into the presence/absence of Cohort effects in different populations, and into the relative smoothness of mortality surfaces along the Age and Year dimensions. Our modelling work is done with the PyTorch libraries in Python and provides an in-depth 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24930;&#26432;&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#38750;&#20984;&#32422;&#26463;&#20248;&#21270;&#12289;&#33258;&#36866;&#24212;$\ell_2$&#25910;&#32553;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#25928;&#21464;&#37327;&#31579;&#36873;&#21644;&#32479;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01726</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#23398;&#20064;&#30340;&#24930;&#26432;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Slow Kill for Big Data Learning. (arXiv:2305.01726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24930;&#26432;&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#38750;&#20984;&#32422;&#26463;&#20248;&#21270;&#12289;&#33258;&#36866;&#24212;$\ell_2$&#25910;&#32553;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#25928;&#21464;&#37327;&#31579;&#36873;&#21644;&#32479;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#30340;&#35266;&#23519;&#21644;&#29305;&#24449;&#65292;&#36825;&#20026;&#21464;&#37327;&#36873;&#25321;&#21644;&#21442;&#25968;&#20272;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24930;&#26432;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#38750;&#20984;&#32422;&#26463;&#20248;&#21270;&#12289;&#33258;&#36866;&#24212;$\ell_2$&#25910;&#32553;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#24930;&#26432;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#38382;&#39064;&#35268;&#27169;&#21487;&#20197;&#20943;&#23567;&#65292;&#36825;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#21464;&#37327;&#31579;&#36873;&#12290;&#32479;&#35745;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#26377;&#20851;&#25511;&#21046;&#20998;&#20301;&#25968;&#12289;&#27493;&#38271;&#21644;&#25910;&#32553;&#21442;&#25968;&#20197;&#25918;&#26494;&#25152;&#38656;&#30340;&#27491;&#21017;&#24615;&#26465;&#20214;&#20197;&#23454;&#29616;&#25152;&#38656;&#32479;&#35745;&#31934;&#24230;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24930;&#26432;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big-data applications often involve a vast number of observations and features, creating new challenges for variable selection and parameter estimation. This paper presents a novel technique called ``slow kill,'' which utilizes nonconvex constrained optimization, adaptive $\ell_2$-shrinkage, and increasing learning rates. The fact that the problem size can decrease during the slow kill iterations makes it particularly effective for large-scale variable screening. The interaction between statistics and optimization provides valuable insights into controlling quantiles, stepsize, and shrinkage parameters in order to relax the regularity conditions required to achieve the desired level of statistical accuracy. Experimental results on real and synthetic data show that slow kill outperforms state-of-the-art algorithms in various situations while being computationally efficient for large-scale data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#20174;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#20013;&#20998;&#21106;&#27700;&#22495;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01698</link><description>&lt;p&gt;
DeepAqua:&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#20174;SAR&#22270;&#20687;&#33258;&#25105;&#30417;&#30563;&#20998;&#21106;&#28287;&#22320;&#30340;&#35821;&#20041;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation. (arXiv:2305.01698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#20174;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#20013;&#20998;&#21106;&#27700;&#22495;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#25216;&#26415;&#36890;&#36807;&#23558;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#27700;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#22312;&#28287;&#22320;&#26816;&#27979;&#26041;&#38754;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#27700;&#30340;&#33539;&#22260;&#38543;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#23545;&#21516;&#19968;&#21306;&#22495;&#36827;&#34892;&#22810;&#27425;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#28040;&#38500;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;DeepAqua&#21033;&#29992;&#24402;&#19968;&#21270;&#24046;&#24322;&#27700;&#25351;&#25968;&#65288;NDWI&#65289;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20197;&#20998;&#21106;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#20013;&#30340;&#27700;&#12290;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#20809;&#23398;&#21644;&#38647;&#36798;&#27700;&#36136;&#25513;&#34109;&#30456;&#37325;&#21472;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#21450;&#26377;&#26893;&#34987;&#27700;&#38754;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28287;&#22320;&#26816;&#27979;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#36827;&#27493;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing has significantly advanced water detection by applying semantic segmentation techniques to satellite imagery. However, semantic segmentation remains challenging due to the substantial amount of annotated data required. This is particularly problematic in wetland detection, where water extent varies over time and space, necessitating multiple annotations for the same area. In this paper, we present DeepAqua, a self-supervised deep learning model that leverages knowledge distillation to eliminate the need for manual annotations during the training phase. DeepAqua utilizes the Normalized Difference Water Index (NDWI) as a teacher model to train a Convolutional Neural Network (CNN) for segmenting water from Synthetic Aperture Radar (SAR) images. To train the student model, we exploit cases where optical- and radar-based water masks coincide, enabling the detection of both open and vegetated water surfaces. Our model represents a significant advancement in computer vision tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GP-NAS&#21644;&#20132;&#21449;&#39564;&#35777;&#30340;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#22312;NAS&#22810;&#20219;&#21153;&#20013;&#20934;&#30830;&#39044;&#27979;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#26426;&#22411;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.01667</link><description>&lt;p&gt;
&#20351;&#29992;GP-NAS&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;NAS&#22810;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Predict NAS Multi-Task by Stacking Ensemble Models using GP-NAS. (arXiv:2305.01667v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GP-NAS&#21644;&#20132;&#21449;&#39564;&#35777;&#30340;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#22312;NAS&#22810;&#20219;&#21153;&#20013;&#20934;&#30830;&#39044;&#27979;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#26426;&#22411;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#26679;&#26412;&#35757;&#32451;&#20013;&#20934;&#30830;&#39044;&#27979;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#19981;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#22914;&#20309;&#20998;&#26512;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#20811;&#26381;&#36807;&#24230;&#25311;&#21512;&#26159;&#25105;&#20204;&#24212;&#35813;&#22788;&#29702;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#22914;&#26524;&#23384;&#22312;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#24212;&#35813;&#32771;&#34385;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23613;&#24555;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;Super Network&#22522;&#20110;ViT-Base&#26500;&#24314;&#20102;&#19968;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;&#25628;&#32034;&#31354;&#38388;&#21253;&#21547;&#28145;&#24230;&#12289;&#22836;&#25968;&#12289;mpl-ratio&#21644;embed-dim&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#22522;&#20110;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;&#29702;&#35299;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#21487;&#20197;&#38477;&#20302;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#21644;&#36807;&#24230;&#25311;&#21512;&#30340;&#27010;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;GP-NAS&#21644;&#20132;&#21449;&#39564;&#35777;&#30340;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22534;&#21472;&#27169;&#22411;&#22312;CVPR 2022&#36187;&#36947;2&#25361;&#25112;&#36187;&#20013;&#25490;&#21517;&#31532;1&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the performance of architecture with small sample training is an important but not easy task. How to analysis and train dataset to overcome overfitting is the core problem we should deal with. Meanwhile if there is the mult-task problem, we should also think about if we can take advantage of their correlation and estimate as fast as we can. In this track, Super Network builds a search space based on ViT-Base. The search space contain depth, num-heads, mpl-ratio and embed-dim. What we done firstly are pre-processing the data based on our understanding of this problem which can reduce complexity of problem and probability of over fitting. Then we tried different kind of models and different way to combine them. Finally we choose stacking ensemble models using GP-NAS with cross validation. Our stacking model ranked 1st in CVPR 2022 Track 2 Challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.01666</link><description>&lt;p&gt;
BrainNPT&#65306;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#31867;&#30340;Transformer&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33041;&#25104;&#20687;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#26223;&#30340;&#29305;&#24449;&#23398;&#20064;&#25913;&#36827;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;Transformer&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#37325;&#28857;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;BrainNPT&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&lt;cls&gt;&#26631;&#35760;&#20316;&#20026;&#20998;&#31867;&#23884;&#20837;&#21521;&#37327;&#65292;&#20197;&#20415;&#20110;Transformer&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#33719;&#33041;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#65292;&#29992;&#20110;BrainNPT&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged &lt;cls&gt; token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#37327;&#21270;&#34880;&#28082;&#26679;&#26412;&#22270;&#20687;&#20013;&#25197;&#26354;&#21644;&#27491;&#24120;&#24418;&#24577;&#30340;&#32418;&#32454;&#32990;&#65292;&#24182;&#19988;&#22312;&#38256;&#29366;&#32454;&#32990;&#30142;&#30149;&#36825;&#19968;&#27169;&#22411;&#30142;&#30149;&#29366;&#24577;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01663</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38256;&#29366;&#32454;&#32990;&#30142;&#30149;&#32418;&#32454;&#32990;&#20998;&#31867;&#21644;&#23450;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Learning based Model for Erythrocytes Classification and Quantification in Sickle Cell Disease. (arXiv:2305.01663v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#37327;&#21270;&#34880;&#28082;&#26679;&#26412;&#22270;&#20687;&#20013;&#25197;&#26354;&#21644;&#27491;&#24120;&#24418;&#24577;&#30340;&#32418;&#32454;&#32990;&#65292;&#24182;&#19988;&#22312;&#38256;&#29366;&#32454;&#32990;&#30142;&#30149;&#36825;&#19968;&#27169;&#22411;&#30142;&#30149;&#29366;&#24577;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#32454;&#32990;&#25110;&#34880;&#32454;&#32990;&#22312;&#22810;&#31181;&#30149;&#29702;&#26465;&#20214;&#19979;&#24418;&#24577;&#21457;&#29983;&#25913;&#21464;&#65292;&#22240;&#27492;&#37492;&#23450;&#21644;&#37327;&#21270;&#19981;&#21516;&#30340;&#32418;&#32454;&#32990;&#24418;&#24577;&#21487;&#20197;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#30142;&#30149;&#24182;&#21327;&#21161;&#21046;&#23450;&#27835;&#30103;&#31574;&#30053;&#12290;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35782;&#21035;&#21644;&#37327;&#21270;&#21464;&#24418;&#30340;&#32418;&#32454;&#32990;&#24418;&#24577;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#37327;&#21270;&#26469;&#33258;&#24739;&#26377;&#38256;&#29366;&#32454;&#32990;&#30149;(SCD)&#30340;&#24739;&#32773;&#34880;&#28082;&#26679;&#26412;&#22270;&#20687;&#20013;&#25197;&#26354;&#21644;&#27491;&#24120;&#24418;&#24577;&#30340;&#32418;&#32454;&#32990;&#12290;&#25105;&#20204;&#36873;&#25321;SCD&#20316;&#20026;&#27169;&#22411;&#30142;&#30149;&#29366;&#24577;&#65292;&#22240;&#20026;SCD&#24739;&#32773;&#34880;&#26679;&#20013;&#23384;&#22312;&#21508;&#31181;&#32418;&#32454;&#32990;&#24418;&#24577;&#12290;&#20026;&#20102;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;428&#24352;SCD&#34880;&#26679;&#30340;&#21407;&#22987;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#29983;&#25104;&#20102;&#30001;10,377&#24352;&#21333;&#20010;&#32454;&#32990;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19977;&#31181;&#26126;&#30830;&#23450;&#20041;&#30340;&#32418;&#32454;&#32990;&#24418;&#24577;&#65292;&#21253;&#25324;&#30424;&#24418;&#32454;&#32990;&#12289;&#26925;&#22278;&#24418;&#21644;&#38256;&#20992;&#24418;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;18&#20010;&#23618;&#30340;CNN&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shape of erythrocytes or red blood cells is altered in several pathological conditions. Therefore, identifying and quantifying different erythrocyte shapes can help diagnose various diseases and assist in designing a treatment strategy. Machine Learning (ML) can be efficiently used to identify and quantify distorted erythrocyte morphologies. In this paper, we proposed a customized deep convolutional neural network (CNN) model to classify and quantify the distorted and normal morphology of erythrocytes from the images taken from the blood samples of patients suffering from Sickle cell disease ( SCD). We chose SCD as a model disease condition due to the presence of diverse erythrocyte morphologies in the blood samples of SCD patients. For the analysis, we used 428 raw microscopic images of SCD blood samples and generated the dataset consisting of 10, 377 single-cell images. We focused on three well-defined erythrocyte shapes, including discocytes, oval, and sickle. We used 18 layered
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.01661</link><description>&lt;p&gt;
SIA-FTP: &#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework. (arXiv:2305.01661v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#36890;&#35759;&#36827;&#34892;&#22320;&#31354;&#21327;&#21830;&#26159;&#30830;&#20445;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#25805;&#20316;&#23433;&#20840;&#21644;&#25928;&#29575;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#20132;&#36890;&#27969;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#20154;&#20026;&#22240;&#32032;&#23548;&#33268;&#30340;&#38169;&#35823;&#25351;&#20196;&#32473;ATC&#23433;&#20840;&#24102;&#26469;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#65288;FTP&#65289;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#39134;&#34892;&#29366;&#24577;&#65292;&#22312;&#23454;&#26102;&#26426;&#21160;&#25351;&#20196;&#30340;&#39044;&#27979;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#36825;&#19981;&#21033;&#20110;&#20914;&#31361;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIA-FTP&#30340;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;FTP&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#21547;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#26469;&#25903;&#25345;&#39640;&#26426;&#21160;FTP&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#24577;&#24046;&#36317;&#24182;&#26368;&#23567;&#21270;&#25968;&#25454;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#26469;&#34701;&#21512;&#35821;&#38899;&#25351;&#20196;&#23884;&#20837;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;SIA-FTP&#65292;&#19982;&#29616;&#26377;&#30340;FTP&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-air negotiation via speech communication is a vital prerequisite for ensuring safety and efficiency in air traffic control (ATC) operations. However, with the increase in traffic flow, incorrect instructions caused by human factors bring a great threat to ATC safety. Existing flight trajectory prediction (FTP) approaches primarily rely on the flight status of historical trajectory, leading to significant delays in the prediction of real-time maneuvering instruction, which is not conducive to conflict detection. A major reason is that spoken instructions and flight trajectories are presented in different modalities in the current air traffic control (ATC) system, bringing great challenges to considering the maneuvering instruction in the FTP tasks. In this paper, a spoken instruction-aware FTP framework, called SIA-FTP, is innovatively proposed to support high-maneuvering FTP tasks by incorporating instant spoken instruction. To address the modality gap and minimize the data requ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#19977;&#31181;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01660</link><description>&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#65306;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24207; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
Data valuation: The partial ordinal Shapley value for machine learning. (arXiv:2305.01660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#19977;&#31181;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992; Shapley &#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#32570;&#20047;&#20851;&#20110;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#35752;&#35770;&#65292;&#22240;&#27492;&#35299;&#20915;&#25968;&#25454;&#39034;&#24207;&#30340;&#20316;&#29992;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#32676;&#35770;&#20013;&#30340;&#25277;&#35937;&#20195;&#25968;&#30740;&#31350;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20559;&#24207; Shapley &#20540;&#30340;&#35745;&#31639;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;&#25130;&#26029;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12289;&#20998;&#31867;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#21644;&#20998;&#31867;&#25130;&#26029;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#36825;&#19977;&#20010;&#31639;&#27861;&#30340;&#23454;&#29616;&#19981;&#21516;&#65292;&#20294;&#37117;&#21487;&#20197;&#36890;&#36807;&#19968;&#23450;&#31243;&#24230;&#30340;&#36817;&#20284;&#26469;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation using Shapley value has emerged as a prevalent research domain in machine learning applications. However, it is a challenge to address the role of order in data cooperation as most research lacks such discussion. To tackle this problem, this paper studies the definition of the partial ordinal Shapley value by group theory in abstract algebra. Besides, since the calculation of the partial ordinal Shapley value requires exponential time, this paper also gives three algorithms for approximating the results. The Truncated Monte Carlo algorithm is derived from the classic Shapley value approximation algorithm. The Classification Monte Carlo algorithm and the Classification Truncated Monte Carlo algorithm are based on the fact that the data points in the same class provide similar information, then we can accelerate the calculation by leaving out some data points in each class.
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDVal&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#21644;&#32676;&#26234;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#24335;&#25968;&#25454;&#20272;&#20540;&#65292;&#21487;&#20197;&#20272;&#31639;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#12290;DDVal&#22522;&#20110;&#20849;&#20139;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;k&#26368;&#36817;&#37051;&#36924;&#36817;&#26041;&#27861;&#26469;&#20272;&#31639;Shapley&#20540;&#65292;&#21487;&#29992;&#20110;&#21516;&#26102;&#21521;&#26426;&#26500;&#21644;&#20010;&#20154;&#22870;&#21169;&#20026;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;DDVal&#23545;&#26426;&#26500;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#30340;&#32467;&#35770;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20272;&#31639;&#26426;&#26500;&#36129;&#29486;&#30340;&#20934;&#30830;&#24615;&#36739;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;Shapley&#20540;&#36924;&#36817;&#26041;&#27861;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.01657</link><description>&lt;p&gt;
&#22522;&#20110;DDVal&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#28857;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Data Point Valuation in Decentralized Learning. (arXiv:2305.01657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDVal&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#21644;&#32676;&#26234;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#24335;&#25968;&#25454;&#20272;&#20540;&#65292;&#21487;&#20197;&#20272;&#31639;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#12290;DDVal&#22522;&#20110;&#20849;&#20139;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;k&#26368;&#36817;&#37051;&#36924;&#36817;&#26041;&#27861;&#26469;&#20272;&#31639;Shapley&#20540;&#65292;&#21487;&#29992;&#20110;&#21516;&#26102;&#21521;&#26426;&#26500;&#21644;&#20010;&#20154;&#22870;&#21169;&#20026;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;DDVal&#23545;&#26426;&#26500;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#30340;&#32467;&#35770;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20272;&#31639;&#26426;&#26500;&#36129;&#29486;&#30340;&#20934;&#30830;&#24615;&#36739;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;Shapley&#20540;&#36924;&#36817;&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32852;&#37030;&#21644;&#32676;&#26234;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#30740;&#31350;&#65292;&#29616;&#26377;&#25991;&#29486;&#38598;&#20013;&#22312;&#20272;&#31639;&#23458;&#25143;&#31471;&#36129;&#29486;&#19978;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;(IID)&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#24456;&#23569;&#26159;IID&#20998;&#24067;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDVal&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#21644;&#32676;&#26234;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#24335;&#25968;&#25454;&#20272;&#20540;&#65292;&#21487;&#20197;&#20272;&#31639;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#12290;DDVal&#22522;&#20110;&#20849;&#20139;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;k&#26368;&#36817;&#37051;&#36924;&#36817;&#26041;&#27861;&#26469;&#20272;&#31639;Shapley&#20540;&#12290;&#36825;&#20801;&#35768;&#26032;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#21516;&#26102;&#21521;&#26426;&#26500;&#21644;&#20010;&#20154;&#22870;&#21169;&#20026;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;DDVal&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20272;&#20540;&#65292;&#36824;&#33021;&#23545;&#26426;&#26500;&#30340;&#36129;&#29486;&#36827;&#34892;&#23618;&#27425;&#21270;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DDVal&#22312;&#20272;&#31639;&#26426;&#26500;&#36129;&#29486;&#26102;&#30340;&#20934;&#30830;&#24615;&#27604;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;Shapley&#20540;&#36924;&#36817;&#26041;&#27861;&#39640;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#36798;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Existing research on data valuation in federated and swarm learning focuses on valuing client contributions and works best when data across clients is independent and identically distributed (IID). In practice, data is rarely distributed IID. We develop an approach called DDVal for decentralized data valuation, capable of valuing individual data points in federated and swarm learning. DDVal is based on sharing deep features and approximating Shapley values through a k-nearest neighbor approximation method. This allows for novel applications, for example, to simultaneously reward institutions and individuals for providing data to a decentralized machine learning task. The valuation of data points through DDVal allows to also draw hierarchical conclusions on the contribution of institutions, and we empirically show that the accuracy of DDVal in estimating institutional contributions is higher than existing Shapley value approximation methods for federated learning. Specifically, it reach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#31561;&#35745;&#31639;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#30165;&#36857;&#35760;&#24405;&#65292;&#25581;&#31034;&#20102;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20132;&#20114;&#39118;&#26684;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25512;&#26029;&#21644;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#37319;&#29992;&#30340;&#20132;&#20114;&#39118;&#26684;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.01656</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#24418;&#24335;&#21270;&#24314;&#27169;&#30340;&#20132;&#20114;&#39118;&#26684;&#25581;&#31034;&#19982;&#35299;&#37322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Formal Modelling to Uncover and Interpret Interaction Styles. (arXiv:2305.01656v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#31561;&#35745;&#31639;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#30165;&#36857;&#35760;&#24405;&#65292;&#25581;&#31034;&#20102;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20132;&#20114;&#39118;&#26684;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25512;&#26029;&#21644;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#37319;&#29992;&#30340;&#20132;&#20114;&#39118;&#26684;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#28151;&#21512;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#21644;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#65292;&#29992;&#20110;&#25581;&#31034;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20132;&#20114;&#39118;&#26684;&#12290;&#36825;&#20123;&#39118;&#26684;&#29992;&#20110;&#37325;&#26032;&#35774;&#35745;&#24212;&#29992;&#31243;&#24207;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#23454;&#26045;&#65292;&#37096;&#32626;&#65292;&#28982;&#21518;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#29992;&#25143;&#30165;&#36857;&#35760;&#24405;&#65292;&#20998;&#21035;&#22312;&#20004;&#20010;&#29256;&#26412;&#30340;&#20845;&#20010;&#26376;&#37096;&#32626;&#26399;&#38388;&#25910;&#38598;&#65292;&#24182;&#20998;&#20026;&#19981;&#21516;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#38024;&#23545;&#20219;&#21153;&#25110;&#32477;&#23545;&#25351;&#26631;&#65288;&#22914;&#21442;&#19982;&#24230;&#30340;&#24230;&#37327;&#65289;&#65292;&#32780;&#26159;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25512;&#26029;&#21644;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#20998;&#26512;&#26469;&#25581;&#31034;&#39118;&#26684;&#12290;&#23545;&#20110;&#20004;&#20010;&#29256;&#26412;&#65292;&#29992;&#25143;&#22312;&#20351;&#29992;&#30340;&#31532;&#19968;&#22825;/&#31532;&#19968;&#21608;/&#31532;&#19968;&#20010;&#26376;&#21644;&#31532;&#20108;&#20010;&#31532;&#19977;&#20010;&#26376;&#26399;&#38388;&#37319;&#29992;&#30340;&#39118;&#26684;&#26377;&#26126;&#26174;&#30340;&#21306;&#21035;&#65292;&#36825;&#26159;&#25105;&#20204;&#27809;&#26377;&#39044;&#26009;&#21040;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study using new computational methods, based on a novel combination of machine learning for inferring admixture hidden Markov models and probabilistic model checking, to uncover interaction styles in a mobile app. These styles are then used to inform a redesign, which is implemented, deployed, and then analysed using the same methods. The data sets are logged user traces, collected over two six-month deployments of each version, involving thousands of users and segmented into different time intervals. The methods do not assume tasks or absolute metrics such as measures of engagement, but uncover the styles through unsupervised inference of clusters and analysis with probabilistic temporal logic. For both versions there was a clear distinction between the styles adopted by users during the first day/week/month of usage, and during the second and third months, a result we had not anticipated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#22269;&#23478;&#20581;&#24247;&#21644;&#33829;&#20859;&#26816;&#26597;&#35843;&#26597;&#25968;&#25454;&#30740;&#31350;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#37325;&#25554;&#34917;&#26159;&#39044;&#27979;&#34880;&#21387;&#21464;&#21270;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25554;&#34917;&#27169;&#22411;&#24212;&#21253;&#25324;&#20256;&#32479;&#39044;&#27979;&#22240;&#32032;&#21644;&#21487;&#33021;&#19982;&#32570;&#22833;&#25968;&#25454;&#30456;&#20851;&#30340;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.01655</link><description>&lt;p&gt;
&#39044;&#27979;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#34880;&#21387;&#21464;&#21270;&#65306;&#20351;&#29992;NHANES&#20998;&#26512;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#21644;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting blood pressure under circumstances of missing data: An analysis of missing data patterns and imputation methods using NHANES. (arXiv:2305.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#22269;&#23478;&#20581;&#24247;&#21644;&#33829;&#20859;&#26816;&#26597;&#35843;&#26597;&#25968;&#25454;&#30740;&#31350;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#37325;&#25554;&#34917;&#26159;&#39044;&#27979;&#34880;&#21387;&#21464;&#21270;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25554;&#34917;&#27169;&#22411;&#24212;&#21253;&#25324;&#20256;&#32479;&#39044;&#27979;&#22240;&#32032;&#21644;&#21487;&#33021;&#19982;&#32570;&#22833;&#25968;&#25454;&#30456;&#20851;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#23558;&#24515;&#34880;&#31649;&#30142;&#30149;&#23450;&#20041;&#20026;&#8220;&#24515;&#33039;&#21644;&#34880;&#31649;&#30340;&#19968;&#32452;&#30142;&#30149;&#8221;&#65292;&#21253;&#25324;&#20896;&#24515;&#30149;&#21644;&#20013;&#39118;&#12290;&#36825;&#20123;&#30142;&#30149;&#21463;&#21040;&#8220;&#20013;&#38388;&#39118;&#38505;&#22240;&#32032;&#8221;&#30340;&#24433;&#21709;&#65292;&#22914;&#39640;&#34880;&#21387;&#12289;&#39640;&#34880;&#31958;&#12289;&#39640;&#34880;&#33026;&#21644;&#32933;&#32982;&#12290;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#20027;&#35201;&#21463;&#29983;&#27963;&#26041;&#24335;&#21644;&#34892;&#20026;&#24433;&#21709;&#65292;&#21253;&#25324;&#36523;&#20307;&#19981;&#27963;&#21160;&#12289;&#19981;&#20581;&#24247;&#30340;&#39278;&#39135;&#12289;&#39640;&#30416;&#25668;&#20837;&#20197;&#21450;&#21560;&#28895;&#21644;&#39278;&#37202;&#12290;&#28982;&#32780;&#65292;&#36951;&#20256;&#23398;&#21644;&#31038;&#20250;/&#29615;&#22659;&#22240;&#32032;&#22914;&#36139;&#22256;&#12289;&#21387;&#21147;&#21644;&#31181;&#26063;&#27495;&#35270;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20316;&#32773;&#20351;&#29992;&#32654;&#22269;&#22269;&#23478;&#20581;&#24247;&#21644;&#33829;&#20859;&#26816;&#26597;&#35843;&#26597;(NHANES)&#30340;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#65292;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#25554;&#34917;&#26041;&#27861;&#20197;&#39044;&#27979;&#34880;&#21387;&#21464;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#38142;&#24335;&#26041;&#31243;(&#22810;&#37325;&#25554;&#34917;)&#30340;&#25554;&#34917;&#26041;&#27861;&#26159;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#19988;&#25554;&#34917;&#27169;&#22411;&#24212;&#21253;&#25324;&#20256;&#32479;&#30340;&#39044;&#27979;&#22240;&#32032;(&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;/&#26063;&#35028;&#12289;&#20307;&#37325;&#25351;&#25968;)&#65292;&#20197;&#21450;&#21487;&#33021;&#19982;&#32570;&#22833;&#25968;&#25454;&#30456;&#20851;&#30340;&#21464;&#37327;(&#22914;&#25910;&#20837;&#12289;&#25945;&#32946;&#31243;&#24230;&#12289;&#31958;&#23615;&#30149;&#29366;&#24577;)&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Health Organization defines cardio-vascular disease (CVD) as "a group of disorders of the heart and blood vessels," including coronary heart disease and stroke (WHO 21). CVD is affected by "intermediate risk factors" such as raised blood pressure, raised blood glucose, raised blood lipids, and obesity. These are predominantly influenced by lifestyle and behaviour, including physical inactivity, unhealthy diets, high intake of salt, and tobacco and alcohol use. However, genetics and social/environmental factors such as poverty, stress, and racism also play an important role. Researchers studying the behavioural and environmental factors associated with these "intermediate risk factors" need access to high quality and detailed information on diet and physical activity. However, missing data are a pervasive problem in clinical and public health research, affecting both randomized trials and observational studies. Reasons for missing data can vary substantially across studies bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.01507</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning. (arXiv:2305.01507v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#65288;ART&#65289;&#31639;&#27861;&#20013;&#65292;&#33410;&#28857;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#30456;&#20284;&#24230;&#38408;&#20540;&#65288;&#21363;&#35686;&#35273;&#21442;&#25968;&#65289;&#23545;&#32858;&#31867;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#20013;&#30340;&#36793;&#32536;&#21024;&#38500;&#38408;&#20540;&#22312;&#33258;&#32452;&#32455;&#36807;&#31243;&#20013;&#29983;&#25104;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#31639;&#27861;&#22312;&#26080;&#39044;&#35774;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In general, a similarity threshold (i.e., a vigilance parameter) for a node learning process in Adaptive Resonance Theory (ART)-based algorithms has a significant impact on clustering performance. In addition, an edge deletion threshold in a topological clustering algorithm plays an important role in adaptively generating well-separated clusters during a self-organizing process. In this paper, we propose a new parameter-free ART-based topological clustering algorithm capable of continual learning by introducing parameter estimation methods. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to the state-of-the-art clustering algorithms without any parameter pre-specifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01160</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#23454;&#29616;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#38271;&#23614;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#32452;&#21512;&#26159;&#20020;&#26102;&#30340;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#29702;&#35770;&#32972;&#26223;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#32972;&#26223;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#38271;&#23614;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#23427;&#20204;&#35797;&#22270;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#30001;&#20110;&#19981;&#32771;&#34385;&#30495;&#23454;&#26631;&#31614;&#30340;&#26368;&#22823;&#21270;&#65292;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#35299;&#37322;&#20026;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#38598;&#25104;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#28508;&#22312;&#31867;&#21035;&#65288;LC&#65289;&#26041;&#27861;&#65292;&#23427;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#26631;&#31614;&#30340;&#20998;&#24067;&#65292;&#24182;&#32852;&#21512;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#23545;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00312</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#36890;&#24120;&#26159;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20540;&#24471;&#20449;&#36182;&#65292;&#23427;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;/&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12289;&#26368;&#23567;&#21270;&#38544;&#31169;&#27844;&#38706;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#23545;&#24694;&#24847;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26088;&#22312;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#38750;&#24120;&#36866;&#21512;&#35299;&#20915;&#20540;&#24471;&#20449;&#36182;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;TFL&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;MOO&#21644;TFL&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21046;&#23450;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#32852;&#21512;&#23398;&#20064;&#65288;CMOFL&#65289;&#38382;&#39064;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#21046;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#36866;&#29992;&#20110;TFL&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;CMOFL&#20316;&#21697;&#19987;&#27880;&#20110;&#25928;&#29992;&#12289;&#25928;&#29575;&#12289;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36825;&#26159;TFL&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;CMOFL&#31639;&#27861;&#65292;&#23427;&#20204;&#36820;&#22238;&#19968;&#32452;&#24179;&#34913;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#28385;&#36275;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#22522;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;&#65292;&#20943;&#36731;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.00003</link><description>&lt;p&gt;
&#22810;&#26230;&#24494;&#32467;&#26500;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Network Accelerated Process Design of Polycrystalline Microstructures. (arXiv:2305.00003v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00003
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;&#65292;&#20943;&#36731;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#23454;&#39564;&#65292;&#25214;&#21040;&#32500;&#25345;&#29702;&#24819;&#26448;&#26009;&#32467;&#26500;&#30340;&#35774;&#35745;&#36335;&#24452;&#21487;&#20197;&#20248;&#21270;&#25152;&#38656;&#30340;&#26448;&#26009;&#24615;&#36136;&#12290;&#36825;&#38656;&#35201;&#37319;&#29992;&#22810;&#23610;&#24230;&#26041;&#27861;&#26469;&#29702;&#35299;&#21152;&#24037;-&#65288;&#24494;&#35266;&#65289;&#32467;&#26500;-&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23558;&#23439;&#35266;&#23610;&#24230;&#65288;&#24037;&#33402;&#21442;&#25968;&#65289;&#19982;&#20013;&#35266;&#65288;&#22343;&#36136;&#21270;&#30340;&#24615;&#36136;&#65289;&#21644;&#24494;&#35266;&#65288;&#26230;&#20307;&#23398;&#32441;&#29702;&#65289;&#23610;&#24230;&#36830;&#25509;&#36215;&#26469;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#35774;&#32622;&#65292;&#21487;&#33021;&#30340;&#21152;&#24037;&#36335;&#24452;&#36873;&#25321;&#20250;&#38543;&#30528;&#20915;&#31574;&#26641;&#30340;&#21152;&#28145;&#32780;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20256;&#32479;&#27169;&#25311;&#22120;&#30340;&#36895;&#24230;&#36798;&#21040;&#20851;&#38190;&#35745;&#31639;&#38408;&#20540;&#12290;&#20026;&#20102;&#20943;&#36731;&#22312;&#32473;&#23450;&#21152;&#36733;&#26465;&#20214;&#19979;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24102;&#29289;&#29702;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26041;&#27861;&#12290;&#35813;NN&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#22522;&#26412;&#36807;&#31243;&#19979;&#24494;&#35266;&#32467;&#26500;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#26041;&#38754;&#26159;&#26377;&#25928;&#21644;&#40065;&#26834;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23545;&#38109;&#24494;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational experiments are exploited in finding a well-designed processing path to optimize material structures for desired properties. This requires understanding the interplay between the processing-(micro)structure-property linkages using a multi-scale approach that connects the macro-scale (process parameters) to meso (homogenized properties) and micro (crystallographic texture) scales. Due to the nature of the problem's multi-scale modeling setup, possible processing path choices could grow exponentially as the decision tree becomes deeper, and the traditional simulators' speed reaches a critical computational threshold. To lessen the computational burden for predicting microstructural evolution under given loading conditions, we develop a neural network (NN)-based method with physics-infused constraints. The NN aims to learn the evolution of microstructures under each elementary process. Our method is effective and robust in finding optimal processing paths. In this study, our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.12322</link><description>&lt;p&gt;
&#20351;&#29992;Pylogik&#36827;&#34892;&#21307;&#23398;&#24433;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#20449;&#24687;&#26041;&#38754;&#39035;&#27880;&#24847;&#65292;&#24517;&#39035;&#28165;&#27927;&#21644;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#12290;&#24403;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#23884;&#20837;&#22312;&#24433;&#20687;&#20803;&#25968;&#25454;&#20013;&#26102;&#65292;&#20419;&#36827;&#22810;&#20013;&#24515;&#21512;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#35843;&#21464;&#24471;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#26694;&#26550;&#19979;&#30340;&#24211;&#65292;&#31216;&#20026;PyLogik&#65292;&#24110;&#21161;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#22270;&#20687;&#30452;&#25509;&#21253;&#21547;&#24456;&#22810;PHI&#12290;PyLogik&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25991;&#26412;&#26816;&#27979;/&#25552;&#21462;&#12289;&#36807;&#28388;&#12289;&#38408;&#20540;&#21270;&#12289;&#24418;&#24577;&#23398;&#21644;&#36718;&#24275;&#27604;&#36739;&#22788;&#29702;&#22270;&#20687;&#20307;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#21435;&#26631;&#35782;&#21270;&#22270;&#20687;&#65292;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#20934;&#22791;&#22909;&#20102;&#22270;&#20687;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;PyLogik&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#35782;&#21035;&#26377;&#25928;&#24615;&#65292;&#38543;&#26426;&#25277;&#21462;&#20102;50&#24352;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36739;&#20026;&#20934;&#30830;&#22320;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#20855;&#26377;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11182</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automatically identifying dynamical systems from data. (arXiv:2304.11182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36739;&#20026;&#20934;&#30830;&#22320;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#20855;&#26377;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#26159;&#24403;&#20195;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#21435;&#22122;&#25216;&#26415;&#12289;&#31232;&#30095;&#22238;&#24402;&#21644;&#33258;&#21161;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#32452;&#65292;&#26102;&#24207;&#21576;&#25351;&#25968;&#22686;&#38271;&#21644;&#21508;&#31181;&#20449;&#22122;&#27604;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19968;&#33268;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#36866;&#24230;&#30340;&#21644;&#39640;&#20449;&#21495;&#36136;&#37327;&#30456;&#23545;&#20110;&#32972;&#26223;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#12290;&#36890;&#36807;&#20934;&#30830;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#24433;&#21709;&#21508;&#31181;&#39046;&#22495;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#23398;&#20197;&#21450;&#24037;&#31243;&#23398;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering nonlinear differential equations that describe system dynamics from empirical data is a fundamental challenge in contemporary science. Here, we propose a methodology to automatically identify dynamical laws by integrating denoising techniques, sparse regression, and bootstrap confidence intervals. We evaluate our method on well-known ordinary differential equations with an ensemble of random initial conditions, time series of increasing length, and varying signal-to-noise ratios. Our algorithm consistently identifies three-dimensional systems, given moderately-sized time series and high signal quality levels relative to background noise. By accurately identifying dynamical systems, our methodology has the potential to impact diverse fields, such as the physical and biological sciences, as well as engineering, where understanding complex systems is crucial.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.09961</link><description>&lt;p&gt;
&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#32463;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#23454;&#26102;&#21709;&#24212;&#65292;&#30001;&#20110;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#33021;&#21147;&#26377;&#38480;&#65292;&#25903;&#25345;&#27492;&#31867;&#23454;&#26102;&#20998;&#26512;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#22788;&#29702;&#31163;&#32447;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#12290;&#26412;&#25991;&#32771;&#23519;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#36816;&#34892;&#30456;&#21516;DNN&#30340;&#25152;&#26377;&#35831;&#27714;&#30340;&#25209;&#22788;&#29702;&#20248;&#21183;&#12290;&#36825;&#24456;&#26377;&#35828;&#26381;&#21147;&#65292;&#22240;&#20026;&#21482;&#26377;&#23569;&#25968;DNN&#65292;&#35768;&#22810;&#35831;&#27714;&#20542;&#21521;&#20110;&#20351;&#29992;&#21516;&#19968;&#20010;DNN&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#30446;&#26631;&#65292;&#22914;&#26368;&#23567;&#21270;&#23436;&#25104;&#26102;&#38388;&#25110;&#26368;&#22823;&#21270;&#21450;&#26102;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#22788;&#29702;&#20351;&#29992;&#19981;&#21516;DNN&#30340;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#20849;&#20139;&#23618;&#30340;&#35831;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HEAT&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;SimpleX&#22312;CPU&#19978;&#30340;&#25805;&#20316;&#65292;&#23454;&#29616;&#21327;&#21516;&#36807;&#28388;&#30340;&#39640;&#25928;&#29575;&#35757;&#32451;</title><link>http://arxiv.org/abs/2304.07334</link><description>&lt;p&gt;
HEAT&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#22522;&#20110;CPU&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#35757;&#32451;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HEAT: A Highly Efficient and Affordable Training System for Collaborative Filtering Based Recommendation on CPUs. (arXiv:2304.07334v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07334
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HEAT&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;SimpleX&#22312;CPU&#19978;&#30340;&#25805;&#20316;&#65292;&#23454;&#29616;&#21327;&#21516;&#36807;&#28388;&#30340;&#39640;&#25928;&#29575;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#24050;&#34987;&#35777;&#26126;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#25152;&#26377;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#20013;&#65292;SimpleX&#26159;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#36866;&#24403;&#25968;&#37327;&#30340;&#36127;&#26679;&#26412;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;SimpleX&#22312;&#22810;&#26680;CPU&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#23548;&#33268;&#24615;&#33021;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;SimpleX&#23454;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21253;&#25324;(1)&#19981;&#35268;&#21017;&#30340;&#20869;&#23384;&#35775;&#38382;&#65292;(2)&#19981;&#24517;&#35201;&#30340;&#20869;&#23384;&#22797;&#21046;&#65292;(3)&#20887;&#20313;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;CF&#35757;&#32451;&#31995;&#32479;(&#21517;&#20026;HEAT)&#65292;&#23427;&#20805;&#20998;&#21457;&#25381;&#20102;&#29616;&#20195;CPU&#30340;&#22810;&#32423;&#32531;&#23384;&#21644;&#22810;&#32447;&#31243;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HEAT&#30340;&#20248;&#21270;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#20351;&#29992;&#29926;&#29255;&#21270;&#25216;&#26415;&#22686;&#21152;&#25968;&#25454;&#23616;&#37096;&#24615;&#21644;&#20943;&#23569;&#32531;&#23384;&#22833;&#25928;(&#20174;&#32780;&#20943;&#23569;&#35835;&#21462;&#24310;&#36831;)&#65307;(2)&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#21644;&#37319;&#26679;&#20248;&#21270;&#65307;(3)&#20351;&#29992;&#21151;&#33021;&#20998;&#21306;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) has been proven to be one of the most effective techniques for recommendation. Among all CF approaches, SimpleX is the state-of-the-art method that adopts a novel loss function and a proper number of negative samples. However, there is no work that optimizes SimpleX on multi-core CPUs, leading to limited performance. To this end, we perform an in-depth profiling and analysis of existing SimpleX implementations and identify their performance bottlenecks including (1) irregular memory accesses, (2) unnecessary memory copies, and (3) redundant computations. To address these issues, we propose an efficient CF training system (called HEAT) that fully enables the multi-level caching and multi-threading capabilities of modern CPUs. Specifically, the optimization of HEAT is threefold: (1) It tiles the embedding matrix to increase data locality and reduce cache misses (thus reduce read latency); (2) It optimizes stochastic gradient descent (SGD) with sampling by par
&lt;/p&gt;</description></item><item><title>HGWaveNet&#26159;&#19968;&#31181;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#21253;&#25324;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215;&#21644;&#23567;&#27874;&#26102;&#38388;&#21367;&#31215;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#21487;&#26377;&#25928;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#21644;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07302</link><description>&lt;p&gt;
HGWaveNet: &#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#30340;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction. (arXiv:2304.07302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07302
&lt;/p&gt;
&lt;p&gt;
HGWaveNet&#26159;&#19968;&#31181;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#21253;&#25324;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215;&#21644;&#23567;&#27874;&#26102;&#38388;&#21367;&#31215;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#21487;&#26377;&#25928;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#21644;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#25104;&#23545;&#33410;&#28857;&#20043;&#38388;&#30340;&#26410;&#26469;&#36793;&#32536;&#65292;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#24314;&#31435;&#22312;&#22343;&#21248;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#65292;&#36825;&#19982;&#29616;&#23454;&#19990;&#30028;&#22270;&#24418;&#30340;&#24130;&#24459;&#20998;&#24067;&#30456;&#30683;&#30462;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20998;&#23618;&#36830;&#25509;&#12290;&#38024;&#23545;&#36825;&#31181;&#29305;&#27530;&#30340;&#25968;&#25454;&#29305;&#24449;&#65292;&#21452;&#26354;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#24819;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#25351;&#25968;&#25193;&#23637;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGWaveNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36229;&#20960;&#20309;&#31354;&#38388;&#19982;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#36866;&#24212;&#24615;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#26469;&#20998;&#21035;&#23398;&#20064;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#28436;&#21464;&#20449;&#24687;&#12290;&#19968;&#26041;&#38754;&#65292;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215; (HDGC) &#27169;&#22359;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#26356;&#24191;&#27867;&#30340;&#37051;&#23621;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#23567;&#27874;&#30340;&#26102;&#38388;&#21367;&#31215; (WTC) &#27169;&#22359;&#36890;&#36807;&#36716;&#25442;&#26102;&#38388;&#22495;&#29305;&#24449;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HGWaveNet&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the ot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#30446;&#26631;GAN&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35843;&#30340;$\alpha$-loss&#26469;&#24314;&#27169;&#27599;&#20010;&#30446;&#26631;&#12290;&#22312;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#25968;&#21644;&#23481;&#37327;&#19979;&#65292;&#36825;&#31867;GAN&#30340;&#38750;&#38646;&#21644;&#28216;&#25103;&#31616;&#21270;&#20026;&#26368;&#23567;&#21270;$f$-&#25955;&#24230;&#12290;&#26368;&#21518;&#65292;&#35843;&#25972;$(\alpha_D,\alpha_G)$&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14320</link><description>&lt;p&gt;
$(\alpha_D,\alpha_G)$-GANs&#65306;&#36890;&#36807;&#21452;&#37325;&#30446;&#26631;&#26469;&#35299;&#20915;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
$(\alpha_D,\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives. (arXiv:2302.14320v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#30446;&#26631;GAN&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35843;&#30340;$\alpha$-loss&#26469;&#24314;&#27169;&#27599;&#20010;&#30446;&#26631;&#12290;&#22312;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#25968;&#21644;&#23481;&#37327;&#19979;&#65292;&#36825;&#31867;GAN&#30340;&#38750;&#38646;&#21644;&#28216;&#25103;&#31616;&#21270;&#20026;&#26368;&#23567;&#21270;$f$-&#25955;&#24230;&#12290;&#26368;&#21518;&#65292;&#35843;&#25972;$(\alpha_D,\alpha_G)$&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;GAN&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#20989;&#25968;&#65288;&#30446;&#26631;&#65289;&#30340;&#21452;&#37325;&#30446;&#26631;GAN&#65292;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35843;&#30340;&#20998;&#31867;&#25439;&#22833;&#8212;&#8212;$\alpha$-loss&#26469;&#24314;&#27169;&#27599;&#20010;&#30446;&#26631;&#65292;&#20197;&#24471;&#21040;&#30001;$(\alpha_D,\alpha_G)\in(0,\infty]^2$&#21442;&#25968;&#21270;&#30340;$(\alpha_D,\alpha_G)$-GAN&#12290;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#25968;&#21644;G&#12289;D&#30340;&#23481;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;$(\alpha_D,\alpha_G)$&#30340;&#36866;&#24403;&#26465;&#20214;&#19979;&#65292;&#23548;&#33268;&#30340;&#38750;&#38646;&#21644;&#28216;&#25103;&#31616;&#21270;&#20026;&#26368;&#23567;&#21270;$f$-&#25955;&#24230;&#12290;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#25968;&#21644;&#23481;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#20272;&#35745;&#35823;&#24046;&#65292;&#20197;&#37327;&#21270;&#30456;&#23545;&#20110;&#26080;&#38480;&#26679;&#26412;&#19979;&#30340;&#26368;&#20248;&#35774;&#23450;&#32780;&#35328;&#29983;&#25104;&#22120;&#24615;&#33021;&#30340;&#24046;&#36317;&#65292;&#24182;&#24471;&#21040;&#20102;&#36825;&#20010;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#23427;&#30340;&#38454;&#26377;&#25928;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35843;&#25972;$(\alpha_D,\alpha_G)$&#22312;&#32531;&#35299;&#21512;&#25104;2D&#39640;&#26031;&#28151;&#21512;&#38382;&#39064;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to address the training instabilities of GANs, we introduce a class of dual-objective GANs with different value functions (objectives) for the generator (G) and discriminator (D). In particular, we model each objective using $\alpha$-loss, a tunable classification loss, to obtain $(\alpha_D,\alpha_G)$-GANs, parameterized by $(\alpha_D,\alpha_G)\in (0,\infty]^2$. For sufficiently large number of samples and capacities for G and D, we show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\alpha_D,\alpha_G)$. In the finite sample and capacity setting, we define estimation error to quantify the gap in the generator's performance relative to the optimal setting with infinite samples and obtain upper bounds on this error, showing it to be order optimal under certain conditions. Finally, we highlight the value of tuning $(\alpha_D,\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26410;&#30693;&#30340;&#21160;&#24577;&#21644;&#27979;&#37327;&#27169;&#22411;&#65292;&#24182;&#36319;&#36394;&#29366;&#24577;&#21518;&#39564;&#27010;&#29575;&#65292;&#22312;&#30456;&#20851;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10319</link><description>&lt;p&gt;
&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#29366;&#24577;&#36716;&#31227;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentiable Bootstrap Particle Filters for Regime-Switching Models. (arXiv:2302.10319v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26410;&#30693;&#30340;&#21160;&#24577;&#21644;&#27979;&#37327;&#27169;&#22411;&#65292;&#24182;&#36319;&#36394;&#29366;&#24577;&#21518;&#39564;&#27010;&#29575;&#65292;&#22312;&#30456;&#20851;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#21644;&#23398;&#20064;&#21442;&#25968;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26032;&#20852;&#31890;&#23376;&#28388;&#27874;&#26041;&#27861;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29366;&#24577;&#21160;&#24577;&#21644;&#27979;&#37327;&#32467;&#26524;&#37117;&#21487;&#20197;&#22312;&#19968;&#32452;&#21487;&#36873;&#27169;&#22411;&#20013;&#20999;&#25442;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#29366;&#24577;&#36716;&#31227;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26410;&#30693;&#30340;&#21160;&#24577;&#21644;&#27979;&#37327;&#27169;&#22411;&#65292;&#24182;&#36319;&#36394;&#29366;&#24577;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#22312;&#30456;&#20851;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#31454;&#20105;&#31639;&#27861;&#30456;&#27604;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable particle filters are an emerging class of particle filtering methods that use neural networks to construct and learn parametric state-space models. In real-world applications, both the state dynamics and measurements can switch between a set of candidate models. For instance, in target tracking, vehicles can idle, move through traffic, or cruise on motorways, and measurements are collected in different geographical or weather conditions. This paper proposes a new differentiable particle filter for regime-switching state-space models. The method can learn a set of unknown candidate dynamic and measurement models and track the state posteriors. We evaluate the performance of the novel algorithm in relevant models, showing its great performance compared to other competitive algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2302.09173</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#36716;&#24405;&#20013;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#20219;&#21153;&#22270;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#25552;&#20379;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#65288;&#22914;&#21046;&#20316;&#21654;&#21857;&#65289;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#38190;&#27493;&#39588;&#21450;&#20854;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#38754;&#21521;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#32858;&#31867;&#21644;&#25490;&#24207;&#32452;&#20214;&#65292;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#20219;&#21153;&#22270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2302.08893</link><description>&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08893
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#25968;&#25454;&#20165;&#20197;&#26410;&#26631;&#35760;&#24418;&#24335;&#21487;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#26368;&#23567;&#21270;&#19982;&#25910;&#38598;&#26631;&#35760;&#35266;&#27979;&#30456;&#20851;&#30340;&#25104;&#26412;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26631;&#27880;&#27599;&#20010;&#35266;&#27979;&#21487;&#20197;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#20351;&#24471;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#25552;&#20986;&#65292;&#26088;&#22312;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35266;&#27979;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#22320;&#20998;&#20026;&#20004;&#31867;&#65306;&#38745;&#24577;&#22522;&#20110;&#27744;&#30340;&#21644;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;&#28041;&#21450;&#20174;&#23553;&#38381;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;&#35266;&#27979;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#35843;&#26597;&#21644;&#25991;&#29486;&#32508;&#36848;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#27169;&#22411;&#36866;&#24212;&#26032;&#36827;&#25968;&#25454;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#20363;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30123;&#24773;&#26399;&#38388;&#26816;&#27979;&#23398;&#29983;&#26159;&#21542;&#20018;&#36890;&#20316;&#24330;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#36828;&#31243;&#32771;&#35797;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#32676;&#20307;&#20316;&#24330;&#34892;&#20026;&#25110;&#32773;&#24322;&#24120;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.07014</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#32771;&#35797;&#20013;&#26816;&#27979;&#20018;&#36890;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30123;&#24773;&#26399;&#38388;&#26816;&#27979;&#23398;&#29983;&#26159;&#21542;&#20018;&#36890;&#20316;&#24330;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#36828;&#31243;&#32771;&#35797;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#32676;&#20307;&#20316;&#24330;&#34892;&#20026;&#25110;&#32773;&#24322;&#24120;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;COVID-19&#30123;&#24773;&#26399;&#38388;&#30340;&#39044;&#38450;&#24615;&#25514;&#26045;&#65292;&#35768;&#22810;&#22823;&#23398;&#25552;&#20379;&#20102;&#26080;&#30417;&#32771;&#30340;&#36828;&#31243;&#32771;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#27979;&#23398;&#29983;&#38388;&#21487;&#33021;&#30340;&#20018;&#36890;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30123;&#24773;&#26399;&#38388;&#36828;&#31243;&#32771;&#35797;&#30340;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#30528;&#26126;&#26174;&#30456;&#20284;&#32771;&#35797;&#30340;&#23398;&#29983;&#32676;&#20307;&#65292;&#21516;&#26102;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#34987;&#30417;&#32771;&#25511;&#21046;&#32452;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#8220;&#26497;&#20854;&#30456;&#20284;&#8221;&#30340;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the precautionary measures during the COVID-19 pandemic many universities offered unproctored take-home exams. We propose methods to detect potential collusion between students and apply our approach on event log data from take-home exams during the pandemic. We find groups of students with suspiciously similar exams. In addition, we compare our findings to a proctored control group. By this, we establish a rule of thumb for evaluating which cases are "outstandingly similar", i.e., suspicious cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2302.05658</link><description>&lt;p&gt;
DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#25991;&#20214;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#30340;&#21830;&#21153;&#25991;&#20214;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290; &#23427;&#21253;&#21547;6.7k&#20010;&#24102;&#27880;&#37322;&#30340;&#21830;&#21153;&#25991;&#20214;&#65292;100k&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;&#25991;&#26723;&#20197;&#21450;&#36817;1M&#20010;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290; &#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#29305;&#23450;&#20110;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20855;&#26377;&#20197;&#19979;&#20851;&#38190;&#29305;&#24449;&#65306;&#65288;i&#65289;&#22312;55&#20010;&#31867;&#21035;&#20013;&#27880;&#37322;&#65292;&#20854;&#31890;&#24230;&#36828;&#36828;&#36229;&#36807;&#20197;&#21069;&#21457;&#24067;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;; &#65288;ii&#65289;&#34892;&#39033;&#30446;&#35782;&#21035;&#34920;&#31034;&#19968;&#39033;&#26497;&#20855;&#23454;&#29992;&#24615;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22312;&#34920;&#26684;&#20013;&#24517;&#39035;&#23558;&#20851;&#38190;&#20449;&#24687;&#20998;&#37197;&#32473;&#39033;&#30446;; &#65288;iii&#65289;&#25991;&#26723;&#26469;&#33258;&#20247;&#22810;&#24067;&#23616;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#26696;&#20363;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#24120;&#35265;&#30340;&#24067;&#23616;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#37197;&#26377;&#22810;&#20010;&#22522;&#32447;&#65292;&#21253;&#25324;RoBERTa&#12289; LayoutLMv3&#21644;&#22522;&#20110;DETR&#30340;&#34920;&#26684;Transformer&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domainand task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; app
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03857</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65306;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#20294;&#21487;&#20197;&#36755;&#20986;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#40065;&#26834;&#24615;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;ACL&#38656;&#35201;&#24040;&#22823;&#30340;&#36816;&#34892;&#26102;&#38388;&#25165;&#33021;&#29983;&#25104;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#21152;&#36895;ACL&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#12290;RCS&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#65292;&#25628;&#32034;&#26368;&#23567;&#21270;&#34920;&#31034;&#20998;&#27495;&#30340;&#20449;&#24687;&#23376;&#38598;&#65292;&#21363;&#33258;&#28982;&#25968;&#25454;&#21644;&#20854;&#34394;&#25311;&#23545;&#25239;&#21464;&#20307;&#20043;&#38388;&#34920;&#31034;&#30340;&#36317;&#31163;&#12290;RCS&#30340;&#22522;&#26412;&#35299;&#27861;&#26159;&#36941;&#21382;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#38598;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;RCS&#36716;&#21270;&#20026;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#38382;&#39064;&#65292;&#21033;&#29992;&#36138;&#24515;&#25628;&#32034;&#26159;&#21407;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20855;&#26377;&#21407;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RCS&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21152;&#36895;ACL&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.02865</link><description>&lt;p&gt;
&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#24674;&#22797;&#20102;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#21487;&#20197;&#32763;&#36716;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65306;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#32534;&#30721;&#25104;&#29983;&#25104;&#35813;&#22270;&#20687;&#30340;&#30495;&#23454;&#28508;&#21464;&#37327;&#65288;Zimmermann&#31561;&#20154;&#65292;2021&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#32467;&#26524;&#36890;&#24120;&#23384;&#22312;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#20687;&#21487;&#33021;&#27169;&#31946;&#25110;&#21482;&#26174;&#31034;3D&#29289;&#20307;&#30340;2D&#35270;&#22270;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#28508;&#21464;&#37327;&#29983;&#25104;&#23427;&#20204;&#12290;&#36825;&#20351;&#24471;&#28508;&#21464;&#37327;&#30340;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#20855;&#26377;&#24322;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24120;&#35265;&#30340;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#65292;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#20998;&#24067;&#24674;&#22797;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#27491;&#30830;&#21518;&#39564;&#20998;&#24067;&#65292;&#21253;&#25324;&#20854;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#20272;&#35745;&#65292;&#35813;&#20272;&#35745;&#23384;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#26059;&#36716;&#12290;&#38500;&#20102;&#25552;&#20379;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20043;&#22806;&#65292;&#36825;&#20123;&#21518;&#39564;&#20998;&#24067;&#36824;&#20801;&#35768;&#22312;&#22270;&#20687;&#26816;&#32034;&#20013;&#35745;&#31639;&#21487;&#20449;&#21306;&#38388;&#12290;&#23427;&#20204;&#21253;&#25324;&#20855;&#26377;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#21516;&#30340;&#28508;&#21464;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#21464;&#37327;&#21644;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#24191;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#21487;&#36776;&#35782;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02672</link><description>&lt;p&gt;
&#28508;&#21464;&#37327;&#21644;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#20174;&#32447;&#24615;&#21040;&#38750;&#32447;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of latent-variable and structural-equation models: from linear to nonlinear. (arXiv:2302.02672v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#21464;&#37327;&#21644;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#24191;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#21487;&#36776;&#35782;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#21464;&#37327;&#32479;&#35745;&#23398;&#20013;&#65292;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#21487;&#36776;&#35782;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#24191;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#21487;&#36776;&#35782;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#25110;&#35266;&#27979;&#21040;&#30340;&#36741;&#21161;&#21464;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#20811;&#26381;&#38750;&#39640;&#26031;&#24615;&#26159;&#21542;&#36275;&#22815;&#30340;&#38480;&#21046;&#65292;&#26469;&#23454;&#29616;&#36825;&#26679;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
An old problem in multivariate statistics is that linear Gaussian models are often unidentifiable, i.e. some parameters cannot be uniquely estimated. In factor (component) analysis, an orthogonal rotation of the factors is unidentifiable, while in linear regression, the direction of effect cannot be identified. For such linear models, non-Gaussianity of the (latent) variables has been shown to provide identifiability. In the case of factor analysis, this leads to independent component analysis, while in the case of the direction of effect, non-Gaussian versions of structural equation modelling solve the problem. More recently, we have shown how even general nonparametric nonlinear versions of such models can be estimated. Non-Gaussianity is not enough in this case, but assuming we have time series, or that the distributions are suitably modulated by some observed auxiliary variables, the models are identifiable. This paper reviews the identifiability theory for the linear and nonlinear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;Top-K&#25512;&#33616;&#19979;&#30340;&#31454;&#20105;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#21463;&#23567;&#24120;&#25968;&#19978;&#30028;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01971</link><description>&lt;p&gt;
&#31454;&#20105;&#24615;&#20869;&#23481;&#21019;&#20316;&#32773;&#19979;&#30340;Top-K&#25512;&#33616;&#26377;&#22810;&#31967;&#31957;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Bad is Top-$K$ Recommendation under Competing Content Creators?. (arXiv:2302.01971v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;Top-K&#25512;&#33616;&#19979;&#30340;&#31454;&#20105;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#21463;&#23567;&#24120;&#25968;&#19978;&#30028;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;&#25512;&#33616;&#24179;&#21488;&#19978;&#31454;&#20105;&#26333;&#20809;&#29575;&#65292;&#36825;&#31181;&#25112;&#30053;&#34892;&#20026;&#23548;&#33268;&#20102;&#20869;&#23481;&#20998;&#24067;&#30340;&#21160;&#24577;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#21019;&#20316;&#32773;&#30340;&#31454;&#20105;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#31119;&#21033;&#65292;&#20197;&#21450;&#30456;&#20851;&#25512;&#33616;&#22914;&#20309;&#24433;&#21709;&#38271;&#26399;&#21160;&#24577;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36825;&#20123;&#30740;&#31350;&#38382;&#39064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#20551;&#35774;&#19979;&#24314;&#27169;&#21019;&#20316;&#32773;&#30340;&#31454;&#20105;&#65306;1&#65289;&#24179;&#21488;&#37319;&#29992;&#26080;&#23475;&#30340;top-K&#25512;&#33616;&#31574;&#30053;&#65307;2&#65289;&#29992;&#25143;&#20915;&#31574;&#36981;&#24490;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65307;3&#65289;&#20869;&#23481;&#21019;&#20316;&#32773;&#31454;&#20105;&#29992;&#25143;&#20114;&#21160;&#65292;&#19981;&#30693;&#36947;&#20107;&#20808;&#20182;&#20204;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#22240;&#27492;&#24212;&#29992;&#20219;&#24847;&#30340;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#26469;&#26356;&#26032;&#20182;&#20204;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#27931;&#22478;&#20215;&#26684;&#30340;&#35282;&#24230;&#30740;&#31350;&#29992;&#25143;&#31119;&#21033;&#30340;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#30001;&#20110;&#21019;&#20316;&#32773;&#31454;&#20105;&#23548;&#33268;&#30340;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#20221;&#39069;&#22987;&#32456;&#21463;&#21040;$K$&#21644;&#29992;&#25143;&#20915;&#31574;&#38543;&#26426;&#24615;&#24433;&#21709;&#30340;&#23567;&#24120;&#25968;&#30340;&#19978;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creators compete for exposure on recommendation platforms, and such strategic behavior leads to a dynamic shift over the content distribution. However, how the creators' competition impacts user welfare and how the relevance-driven recommendation influences the dynamics in the long run are still largely unknown.  This work provides theoretical insights into these research questions. We model the creators' competition under the assumptions that: 1) the platform employs an innocuous top-$K$ recommendation policy; 2) user decisions follow the Random Utility model; 3) content creators compete for user engagement and, without knowing their utility function in hindsight, apply arbitrary no-regret learning algorithms to update their strategies. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on $K$ and randomness in user decisions; w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01567</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#26159;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20027;&#35201;&#30340;&#35774;&#35745;&#26631;&#20934;&#20043;&#19968;&#12290;&#36825;&#26159;&#30001;&#20110;CPS&#20013;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20204;&#30340;&#22833;&#25928;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;CPS&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26426;&#21046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20256;&#32479;&#30340;&#23481;&#38169;&#26041;&#27861;&#21253;&#25324;&#20887;&#20313;&#26102;&#38388;&#12289;&#30828;&#20214;&#12289;&#20449;&#24687;&#21644;/&#25110;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38500;&#20102;&#20302;&#38169;&#35823;&#35206;&#30422;&#29575;&#22806;&#65292;&#36824;&#20250;&#24102;&#26469;&#26497;&#22823;&#30340;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20010;&#29983;&#29289;&#32593;&#32476;&#30340;EMGNN&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#22522;&#22240;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08831</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30284;&#30151;&#22522;&#22240;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Multilayer Graph Neural Network for Cancer Gene Prediction. (arXiv:2301.08831v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08831
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20010;&#29983;&#29289;&#32593;&#32476;&#30340;EMGNN&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#22522;&#22240;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22522;&#22240;&#30340;&#35782;&#21035;&#26159;&#30284;&#30151;&#22522;&#22240;&#32452;&#23398;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26410;&#33021;&#21033;&#29992;&#22810;&#23618;&#22522;&#22240;-&#22522;&#22240;&#20132;&#20114;&#25110;&#25552;&#20379;&#26377;&#38480;&#30340;&#39044;&#27979;&#35299;&#37322;&#12290;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#19968;&#29983;&#29289;&#32593;&#32476;&#65292;&#26080;&#27861;&#25429;&#33719;&#32959;&#30244;&#21457;&#29983;&#30340;&#23436;&#25972;&#22797;&#26434;&#24615;&#12290;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#32593;&#32476;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#19981;&#21516;&#29978;&#33267;&#30456;&#21453;&#30340;&#30284;&#30151;&#22522;&#22240;&#39044;&#27979;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#21487;&#20449;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;EMGNN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#22522;&#22240;-&#22522;&#22240;&#20132;&#20114;&#32593;&#32476;&#21644;&#20840;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#26469;&#35782;&#21035;&#30284;&#30151;&#22522;&#22240;&#12290;&#19982;&#20256;&#32479;&#30340;&#22312;&#21333;&#19968;&#29983;&#29289;&#32593;&#32476;&#19978;&#36827;&#34892;&#22270;&#23398;&#20064;&#19981;&#21516;&#65292;EMGNN &#20351;&#29992;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#22810;&#20010;&#29983;&#29289;&#32593;&#32476;&#20013;&#23398;&#20064;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#30284;&#30151;&#22522;&#22240;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
The identification of cancer genes is a critical yet challenging problem in cancer genomics research. Existing computational methods, including deep graph neural networks, fail to exploit the multilayered gene-gene interactions or provide limited explanation for their predictions. These methods are restricted to a single biological network, which cannot capture the full complexity of tumorigenesis. Models trained on different biological networks often yield different and even opposite cancer gene predictions, hindering their trustworthy adaptation. Here, we introduce an Explainable Multilayer Graph Neural Network (EMGNN) approach to identify cancer genes by leveraging multiple genegene interaction networks and pan-cancer multi-omics data. Unlike conventional graph learning on a single biological network, EMGNN uses a multilayered graph neural network to learn from multiple biological networks for accurate cancer gene prediction. Our method consistently outperforms all existing methods,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;PlasmoFAB&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#25506;&#32034;Plasmodium falciparum&#34507;&#30333;&#25239;&#21407;&#20505;&#36873;&#29289;&#12290;</title><link>http://arxiv.org/abs/2301.06454</link><description>&lt;p&gt;
PlasmoFAB&#65306;&#20419;&#36827;Plasmodium falciparum&#34507;&#30333;&#25239;&#21407;&#20505;&#36873;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PlasmoFAB: A Benchmark to Foster Machine Learning for Plasmodium falciparum Protein Antigen Candidate Prediction. (arXiv:2301.06454v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;PlasmoFAB&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#25506;&#32034;Plasmodium falciparum&#34507;&#30333;&#25239;&#21407;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#31185;&#23398;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#26377;&#22312;&#33021;&#22815;&#22522;&#20110;&#39640;&#36136;&#37327;&#21644;&#31574;&#21010;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#25165;&#33021;&#21487;&#38752;&#22320;&#20351;&#29992;&#12290;&#30446;&#21069;&#65292;&#36824;&#19981;&#23384;&#22312;&#29992;&#20110;&#25506;&#32034;Plasmodium falciparum&#34507;&#30333;&#25239;&#21407;&#20505;&#36873;&#29289;&#30340;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#23492;&#29983;&#34411;Plasmodium falciparum&#24341;&#21457;&#20256;&#26579;&#30149;&#30111;&#30142;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;&#25239;&#21407;&#23545;&#20110;&#24320;&#21457;&#25239;&#30111;&#30142;&#33647;&#29289;&#21644;&#30123;&#33495;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#23454;&#39564;&#25506;&#32034;&#25239;&#21407;&#20505;&#36873;&#29289;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#26377;&#21487;&#33021;&#21152;&#36895;&#24320;&#21457;&#25239;&#30111;&#30142;&#33647;&#29289;&#21644;&#30123;&#33495;&#65292;&#36825;&#26159;&#25511;&#21046;&#30111;&#30142;&#25152;&#24517;&#38656;&#30340;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;PlasmoFAB&#65292;&#19968;&#20010;&#31574;&#21010;&#33391;&#22909;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#25506;&#32034;Plasmodium falciparum&#34507;&#30333;&#25239;&#21407;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Machine learning methods can be used to support scientific discovery in healthcare-related research fields. However, these methods can only be reliably used if they can be trained on high-quality and curated datasets. Currently, no such dataset for the exploration of Plasmodium falciparum protein antigen candidates exists. The parasite Plasmodium falciparum causes the infectious disease malaria. Thus, identifying potential antigens is of utmost importance for the development of antimalarial drugs and vaccines. Since exploring antigen candidates experimentally is an expensive and time-consuming process, applying machine learning methods to support this process has the potential to accelerate the development of drugs and vaccines, which are needed for fighting and controlling malaria. Results: We developed PlasmoFAB, a curated benchmark that can be used to train machine learning methods for the exploration of Plasmodium falciparum protein antigen candidates. We combined an ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05785</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#23454;&#29616;&#39640;&#25928;&#30340;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24456;&#38590;&#26500;&#24314;&#26368;&#20248;&#28608;&#27963;&#20989;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#31639;&#27861;&#36807;&#20110;&#26114;&#36149;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;2,913&#20010;&#31995;&#32479;&#29983;&#25104;&#30340;&#28608;&#27963;&#20989;&#25968;&#20174;&#22836;&#35757;&#32451;&#21367;&#31215;&#12289;&#27531;&#24046;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#21019;&#24314; Act-Bench-CNN&#12289;Act-Bench-ResNet &#21644; Act-Bench-ViT &#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20934;&#31354;&#38388;&#65292;&#21457;&#29616;&#19982;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#21644;&#28608;&#27963;&#20989;&#25968;&#36755;&#20986;&#20998;&#24067;&#30456;&#20851;&#32852;&#30340; Fisher &#20449;&#24687;&#30697;&#38453;&#30340;&#39057;&#35889;&#23545;&#24615;&#33021;&#30340;&#39044;&#27979;&#24615;&#24456;&#39640;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#20195;&#29702;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#20004;&#30456;&#27969;&#30340;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#20174;&#32780;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.12731</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27169;&#24577;&#20998;&#35299;&#30340;&#20004;&#30456;&#21516;&#24515;&#21943;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting through deep learning and modal decomposition in two-phase concentric jets. (arXiv:2212.12731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#20004;&#30456;&#27969;&#30340;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#20174;&#32780;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#27745;&#26579;&#29289;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#38656;&#35201;&#24320;&#21457;&#20801;&#35768;&#36825;&#31181;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25152;&#20570;&#30340;&#24037;&#20316;&#28041;&#21450;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#65288;&#38590;&#20197;&#27979;&#37327;&#65289;&#25110;&#23436;&#25972;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#65288;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65289;&#12290;&#21518;&#32773;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#32479;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#24320;&#21457;&#23454;&#26102;&#39044;&#27979;&#24037;&#20855;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#65288;&#30456;&#23545;&#26356;&#20415;&#23452;&#30340;&#65289;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#23384;&#22312;&#20999;&#21521;&#19981;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20004;&#30456;&#27969;&#20013;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#20316;&#20026;PDE&#20195;&#29702;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;NN&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to improve fuel chamber injectors' performance in turbofan engines, thus implying improved performance and reduction of pollutants. This requires the development of models that allow real-time prediction and improvement of the fuel/air mixture. However, the work carried out to date involves using experimental data (complicated to measure) or the numerical resolution of the complete problem (computationally prohibitive). The latter involves the resolution of a system of partial differential equations (PDE). These problems make difficult to develop a real-time prediction tool. Therefore, in this work, we propose using machine learning in conjunction with (complementarily cheaper) single-phase flow numerical simulations in the presence of tangential discontinuities to estimate the mixing process in two-phase flows. In this meaning we study the application of two proposed neural network (NN) models as PDE surrogate models. Where the future dynamics is predicted by the NN, gi
&lt;/p&gt;</description></item><item><title>COmic&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21367;&#31215;&#26680;&#32593;&#32476;&#21644;&#36890;&#36335;&#35825;&#23548;&#26680;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#65288;&#22810;&#65289;&#32452;&#23398;&#25968;&#25454;&#30340;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2212.02504</link><description>&lt;p&gt;
COmic&#65306;&#29992;&#20110;&#65288;&#22810;&#65289;&#32452;&#23398;&#25968;&#25454;&#35299;&#37322;&#24615;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#21367;&#31215;&#26680;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
COmic: Convolutional Kernel Networks for Interpretable End-to-End Learning on (Multi-)Omics Data. (arXiv:2212.02504v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02504
&lt;/p&gt;
&lt;p&gt;
COmic&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21367;&#31215;&#26680;&#32593;&#32476;&#21644;&#36890;&#36335;&#35825;&#23548;&#26680;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#65288;&#22810;&#65289;&#32452;&#23398;&#25968;&#25454;&#30340;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#21487;&#29992;&#32452;&#23398;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31283;&#27493;&#22686;&#21152;&#12290;&#34429;&#28982;&#36825;&#31181;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#20013;&#30456;&#20851;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#38024;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#25805;&#20316;&#12290;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#20351;&#29992;&#40657;&#30418;&#27169;&#22411;&#20250;&#24102;&#26469;&#23433;&#20840;&#21644;&#20445;&#23494;&#38382;&#39064;&#12290;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#24433;&#21709;&#39044;&#27979;&#30340;&#20998;&#23376;&#22240;&#32032;&#21644;&#34920;&#22411;&#30340;&#35299;&#37322;&#65292;&#20445;&#20581;&#25552;&#20379;&#32773;&#21482;&#33021;&#30450;&#30446;&#20449;&#20219;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65292;&#21517;&#20026;&#21367;&#31215;&#32452;&#23398;&#26680;&#32593;&#32476;&#65288;COmic&#65289;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#26680;&#32593;&#32476;&#19982;&#36890;&#36335;&#35825;&#23548;&#26680;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20174;&#20960;&#30334;&#20010;&#21040;&#20960;&#21313;&#19975;&#20010;&#26679;&#26412;&#30340;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;COmic&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#22810;&#32452;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: The size of available omics datasets is steadily increasing with technological advancement in recent years. While this increase in sample size can be used to improve the performance of relevant prediction tasks in healthcare, models that are optimized for large datasets usually operate as black boxes. In high stakes scenarios, like healthcare, using a black-box model poses safety and security issues. Without an explanation about molecular factors and phenotypes that affected the prediction, healthcare providers are left with no choice but to blindly trust the models. We propose a new type of artificial neural network, named Convolutional Omics Kernel Network (COmic). By combining convolutional kernel networks with pathway-induced kernels, our method enables robust and interpretable end-to-end learning on omics datasets ranging in size from a few hundred to several hundreds of thousands of samples. Furthermore, COmic can be easily adapted to utilize multi-omics data.  Result
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#21435;&#38500;&#21407;&#23376;&#27169;&#25311;&#36807;&#31243;&#20013;&#30340;&#28909;&#25391;&#21160;&#22122;&#22768;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#24182;&#33021;&#25552;&#39640;&#20998;&#31867;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02421</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#21435;&#22122;&#31639;&#27861;&#22312;&#21407;&#23376;&#32467;&#26500;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Score-based denoising for atomic structure identification. (arXiv:2212.02421v3 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#21435;&#38500;&#21407;&#23376;&#27169;&#25311;&#36807;&#31243;&#20013;&#30340;&#28909;&#25391;&#21160;&#22122;&#22768;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#24182;&#33021;&#25552;&#39640;&#20998;&#31867;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#23376;&#27169;&#25311;&#36807;&#31243;&#20013;&#20998;&#26512;&#22797;&#26434;&#21160;&#24577;&#26102;&#21435;&#38500;&#28909;&#25391;&#21160;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37319;&#29992;&#21435;&#22122;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#22122;&#22768;&#20294;&#22312;&#23436;&#32654;&#26230;&#26684;&#19978;&#26080;&#24178;&#25200;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36845;&#20195;&#22320;&#28040;&#38500;&#21407;&#23376;&#20301;&#32622;&#20013;&#30340;&#22122;&#22768;&#21644;&#25200;&#21160;&#12290;&#21435;&#22122;&#21518;&#30340;&#32467;&#26500;&#28165;&#26224;&#22320;&#23637;&#29616;&#20102;&#26230;&#20307;&#30340;&#20869;&#22312;&#26377;&#24207;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#19982;&#26230;&#20307;&#32570;&#38519;&#30456;&#20851;&#30340;&#26080;&#24207;&#24615;&#12290;&#25105;&#20204;&#30340;&#21435;&#22122;&#22120;&#26080;&#38656;&#20381;&#36182;&#20110;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#26174;&#24335;&#27169;&#25311;&#36755;&#20837;&#65292;&#23436;&#20840;&#22522;&#20110;&#20960;&#20309;&#23398;&#29702;&#35770;&#65292;&#19988;&#33021;&#22815;&#36866;&#29992;&#20110;&#22823;&#37327;&#19981;&#21516;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#36825;&#19968;&#21435;&#22122;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#29616;&#26377;&#20998;&#31867;&#26041;&#27861;&#65288;&#22914;&#20844;&#20849;&#37051;&#23621;&#20998;&#26512;&#21644;&#22810;&#38754;&#20307;&#27169;&#26495;&#21305;&#37197;&#65289;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26368;&#36817;&#30340;&#28909;&#25200;&#21160;&#32467;&#26500;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective method for removing thermal vibrations that complicate the task of analyzing complex dynamics in atomistic simulation of condensed matter. Our method iteratively subtracts thermal noises or perturbations in atomic positions using a denoising score function trained on synthetically noised but otherwise perfect crystal lattices. The resulting denoised structures clearly reveal underlying crystal order while retaining disorder associated with crystal defects. Purely geometric, agnostic to interatomic potentials, and trained without inputs from explicit simulations, our denoiser can be applied to simulation data generated from vastly different interatomic interactions. The denoiser is shown to improve existing classification methods such as common neighbor analysis and polyhedral template matching, reaching perfect classification accuracy on a recent benchmark dataset of thermally perturbed structures up to the melting point. Demonstrated here in a wide variety of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#20302;&#36164;&#28304;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20219;&#21153;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243; (NMR) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#38899;&#20048;&#20998;&#31867;&#12290;NMR&#26088;&#22312;&#36890;&#36807;&#20462;&#25913;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#28304;&#22495;&#37325;&#26032;&#35843;&#25972;&#29992;&#20110;&#30446;&#26631;&#22495;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#19982;&#36755;&#20837;&#26080;&#20851;&#30340;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#37325;&#26032;&#32534;&#31243;&#33539;&#24335;&#65306;&#36755;&#20837;&#20381;&#36182;NMR&#65292;&#20197;&#22686;&#21152;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#38899;&#39057;&#65289;&#30340;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#25104;&#21151;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#20004;&#31181;&#36755;&#20837;&#30456;&#20851;&#30340;NMR&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#36827;&#34892;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#65292;&#25299;&#23637;&#20102;&#945;&#25237;&#36164;&#35268;&#21017;&#20197;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#26469;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#24182;&#25552;&#20379;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#35268;&#21017;&#33021;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2210.17514</link><description>&lt;p&gt;
&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#29992;&#20110;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing. (arXiv:2210.17514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#36827;&#34892;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#65292;&#25299;&#23637;&#20102;&#945;&#25237;&#36164;&#35268;&#21017;&#20197;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#26469;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#24182;&#25552;&#20379;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#35268;&#21017;&#33021;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22312;&#25968;&#25454;&#25910;&#38598;&#20855;&#26377;&#38750;&#24179;&#20961;&#25104;&#26412;&#26102;&#36827;&#34892;&#39034;&#24207;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#35782;&#21035;&#30142;&#30149;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#34920;&#36798;&#22522;&#22240;&#31561;&#29983;&#29289;&#23454;&#39564;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#26500;&#24314;&#22312;&#36890;&#29992;&#945;&#25237;&#36164;&#26694;&#26550;&#19978;&#65292;&#33021;&#22312;&#39034;&#24207;&#26816;&#39564;&#29615;&#22659;&#19979;&#36827;&#34892;&#35823;&#21457;&#29616;&#29575;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#945;-&#36130;&#23500;&#30340;&#38271;&#26399;&#28176;&#36827;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24341;&#20986;&#20102;&#22312;&#945;&#25237;&#36164;&#20915;&#31574;&#35268;&#21017;&#20013;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#30340;&#24605;&#32771;&#12290;&#23558;&#26816;&#39564;&#36807;&#31243;&#35270;&#20026;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#31181;&#20915;&#31574;&#35268;&#21017;&#65292;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#65292;&#24182;&#20026;&#27979;&#35797;&#25552;&#20379;&#20102;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25104;&#26412;&#24863;&#30693;&#30340;ERO&#20915;&#31574;&#35268;&#21017;&#27604;&#20854;&#20182;&#26041;&#27861;&#27491;&#30830;&#22320;&#25298;&#32477;&#20102;&#26356;&#22810;&#30340;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;ERO&#25237;&#36164;&#33267;&#26377;&#38480;&#26102;&#38388;&#26816;&#39564;&#65292;&#20351;&#20915;&#31574;&#35268;&#21017;&#33021;&#22815;&#36328;&#36234;&#22810;&#20010;&#26816;&#39564;&#20998;&#37197;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequential multiple hypothesis testing with nontrivial data collection cost. This problem appears, for example, when conducting biological experiments to identify differentially expressed genes in a disease process. This work builds on the generalized $\alpha$-investing framework that enables control of the false discovery rate in a sequential testing setting. We make a theoretical analysis of the long term asymptotic behavior of $\alpha$-wealth which motivates a consideration of sample size in the $\alpha$-investing decision rule. Posing the testing process as a game with nature, we construct a decision rule that optimizes the expected return (ERO) of $\alpha$-wealth and provides an optimal sample size for the test. Empirical results show that a cost-aware ERO decision rule correctly rejects more false null hypotheses than other methods. We extend cost-aware ERO investing to finite-horizon testing which enables the decision rule to allocate samples across ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#24615;&#23567;&#27874;&#26694;&#26550;&#30340;&#26377;&#21521;&#22270;&#35889;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Framelet-MagNet&#65292;&#22312;&#28388;&#27874;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#65292;&#19988;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#36335;&#39044;&#27979;&#21644;&#21435;&#22122;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.10993</link><description>&lt;p&gt;
&#22522;&#20110;&#30913;&#24615;&#23567;&#27874;&#26694;&#26550;&#30340;&#26377;&#21521;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Magnetic Framelet-Based Convolutional Neural Network for Directed Graphs. (arXiv:2210.10993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#24615;&#23567;&#27874;&#26694;&#26550;&#30340;&#26377;&#21521;&#22270;&#35889;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Framelet-MagNet&#65292;&#22312;&#28388;&#27874;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#65292;&#19988;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#36335;&#39044;&#27979;&#21644;&#21435;&#22122;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#22270;&#21367;&#31215;&#32593;&#32476;&#26159;&#20998;&#26512;&#21644;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36890;&#24120;&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#36827;&#34892;&#39057;&#29575;&#36807;&#28388;&#20197;&#33719;&#24471;&#20855;&#26377;&#36873;&#25321;&#24615;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#30740;&#31350;&#34920;&#26126;&#35889;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#23567;&#27874;&#26694;&#26550;&#30340;&#28388;&#27874;&#24471;&#21040;&#22686;&#24378;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#32771;&#34385;&#26080;&#21521;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30913;&#24615;&#23567;&#27874;&#26694;&#26550;&#30340;&#26377;&#21521;&#22270;&#35889;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Framelet-MagNet&#12290;&#35813;&#27169;&#22411;&#23558;&#23567;&#27874;&#21464;&#25442;&#24212;&#29992;&#20110;&#26377;&#21521;&#22270;&#20449;&#21495;&#65292;&#24418;&#25104;&#26356;&#22797;&#26434;&#30340;&#34920;&#31034;&#29992;&#20110;&#28388;&#27874;&#12290;&#26377;&#21521;&#22270;&#23567;&#27874;&#22522;&#20351;&#29992;&#22797;&#20540;&#30913;&#25289;&#26222;&#25289;&#26031;&#26500;&#36896;&#65292;&#21516;&#26102;&#22312;&#23454;&#25968;&#21644;&#22797;&#25968;&#22495;&#20013;&#36827;&#34892;&#20449;&#21495;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#36335;&#39044;&#27979;&#21644;&#21435;&#22122;&#31561;&#22810;&#26041;&#38754;&#23545;Framelet-MagNet&#30340;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#22810;&#31181;&#26368;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral Graph Convolutional Networks (spectral GCNNs), a powerful tool for analyzing and processing graph data, typically apply frequency filtering via Fourier transform to obtain representations with selective information. Although research shows that spectral GCNNs can be enhanced by framelet-based filtering, the massive majority of such research only considers undirected graphs. In this paper, we introduce Framelet-MagNet, a magnetic framelet-based spectral GCNN for directed graphs (digraphs). The model applies the framelet transform to digraph signals to form a more sophisticated representation for filtering. Digraph framelets are constructed with the complex-valued magnetic Laplacian, simultaneously leading to signal processing in both real and complex domains. We empirically validate the predictive power of Framelet-MagNet over a range of state-of-the-art models in node classification, link prediction, and denoising.
&lt;/p&gt;</description></item><item><title>ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10953</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21457;&#29616;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10953
&lt;/p&gt;
&lt;p&gt;
ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#23547;&#27714;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#22312;&#35299;&#20915;&#26041;&#26696;&#21518;&#26399;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROBOT&#30340;&#25490;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;ROBOT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#21457;&#29616;&#22823;&#37327;&#39640;&#24615;&#33021;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#19982;&#23547;&#25214;&#21333;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#24456;&#23569;&#30340;&#39069;&#22806;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#39057;&#29575;&#22686;&#24378;&#22120;&#65292;&#23558;&#39640;&#39057;&#20449;&#24687;&#34701;&#20837;&#21040;GNN&#20013;&#65292;&#25552;&#39640;&#24322;&#36136;&#24615;&#22270;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;&#27969;&#34892;&#30340;&#22522;&#20934;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#20102;3.6%&#12290;</title><link>http://arxiv.org/abs/2210.08251</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#65306;&#19968;&#31181;&#39640;&#39057;&#29575;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#39057;&#29575;&#22686;&#24378;&#22120;&#65292;&#23558;&#39640;&#39057;&#20449;&#24687;&#34701;&#20837;&#21040;GNN&#20013;&#65292;&#25552;&#39640;&#24322;&#36136;&#24615;&#22270;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;&#27969;&#34892;&#30340;&#22522;&#20934;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#20102;3.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20855;&#26377;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#26159;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#31561;&#38382;&#39064;&#65292;GNN&#26694;&#26550;&#24448;&#24448;&#20250;&#22833;&#36133;&#12290;&#26368;&#27969;&#34892;&#30340;GNN&#34987;&#35748;&#20026;&#38598;&#20013;&#20110;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#20449;&#21495;&#22788;&#29702;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;GNN&#36890;&#24120;&#21463;&#21040;&#20302;&#36890;&#28388;&#27874;&#22120;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#39640;&#39057;&#20449;&#24687;&#34701;&#20837;&#21040;GNN&#20013;&#20197;&#32531;&#35299;&#36825;&#20010;&#26222;&#36941;&#38382;&#39064;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21407;&#22987;&#22270;&#30340;&#34917;&#38598;&#21253;&#25324;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#24182;&#19988;&#25552;&#20986;&#34917;&#38598;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;(CLAR)&#20197;&#26377;&#25928;&#22686;&#24378;&#39640;&#39057;&#20998;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;CLAR&#26377;&#21161;&#20110;GNN&#24212;&#23545;&#36807;&#24230;&#24179;&#28369;&#65292;&#25552;&#39640;&#24322;&#36136;&#24615;&#22270;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#27604;&#27969;&#34892;&#30340;&#22522;&#20934;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;3.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) hold the promise of learning efficient representations of graph-structured data, and one of its most important applications is semi-supervised node classification. However, in this application, GNN frameworks tend to fail due to the following issues: over-smoothing and heterophily. The most popular GNNs are known to be focused on the message-passing framework, and recent research shows that these GNNs are often bounded by low-pass filters from a signal processing perspective. We thus incorporate high-frequency information into GNNs to alleviate this genetic problem. In this paper, we argue that the complement of the original graph incorporates a high-pass filter and propose Complement Laplacian Regularization (CLAR) for an efficient enhancement of high-frequency components. The experimental results demonstrate that CLAR helps GNNs tackle over-smoothing, improving the expressiveness of heterophilic graphs, which adds up to 3.6% improvement over popular basel
&lt;/p&gt;</description></item><item><title>KAIROS&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#24322;&#26500;&#35745;&#31639;&#30828;&#20214;&#27744;&#21644;&#20248;&#21270;&#30340;&#25512;&#26029;&#26597;&#35810;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#26597;&#35810;&#21534;&#21520;&#37327;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#36136;&#37327;&#21644;&#25104;&#26412;&#39044;&#31639;&#38480;&#21046;&#12290;&#22312;&#20135;&#19994;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;KAIROS&#30456;&#27604;&#20110;&#26368;&#20248;&#24322;&#26500;&#26041;&#26696;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;2&#20493;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.05889</link><description>&lt;p&gt;
KAIROS&#65306;&#21033;&#29992;&#24322;&#26500;&#20113;&#36164;&#28304;&#26500;&#24314;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KAIROS: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources. (arXiv:2210.05889v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05889
&lt;/p&gt;
&lt;p&gt;
KAIROS&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#24322;&#26500;&#35745;&#31639;&#30828;&#20214;&#27744;&#21644;&#20248;&#21270;&#30340;&#25512;&#26029;&#26597;&#35810;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#26597;&#35810;&#21534;&#21520;&#37327;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#36136;&#37327;&#21644;&#25104;&#26412;&#39044;&#31639;&#38480;&#21046;&#12290;&#22312;&#20135;&#19994;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;KAIROS&#30456;&#27604;&#20110;&#26368;&#20248;&#24322;&#26500;&#26041;&#26696;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;2&#20493;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#26029;&#27491;&#25104;&#20026;&#35768;&#22810;&#20225;&#19994;&#30340;&#20851;&#38190;&#26381;&#21153;&#20135;&#21697;&#65292;&#37096;&#32626;&#22312;&#20113;&#24179;&#21488;&#19978;&#20197;&#28385;&#36275;&#23458;&#25143;&#38656;&#27714;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#21033;&#28070;&#20135;&#29983;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26381;&#21153;&#38656;&#35201;&#22312;&#20005;&#26684;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#25104;&#26412;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KAIROS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36816;&#34892;&#26102;&#26694;&#26550;&#65292;&#26368;&#22823;&#21270;&#26597;&#35810;&#21534;&#21520;&#37327;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#36136;&#37327;&#30446;&#26631;&#21644;&#25104;&#26412;&#39044;&#31639;&#12290;KAIROS&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24322;&#26500;&#35745;&#31639;&#30828;&#20214;&#27744;&#65292;&#36991;&#20813;&#22312;&#32447;&#36164;&#28304;&#25506;&#32034;&#30340;&#39069;&#22806;&#24320;&#38144;&#65292;&#24182;&#20248;&#21270;&#22320;&#22312;&#36816;&#34892;&#26102;&#20998;&#37197;&#25512;&#26029;&#26597;&#35810;&#12290;&#25105;&#20204;&#20351;&#29992;&#20135;&#19994;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;KAIROS&#20135;&#29983;&#30340;&#21534;&#21520;&#37327;&#27604;&#26368;&#20248;&#24322;&#26500;&#26041;&#26696;&#39640;&#20986;2&#20493;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#36890;&#36807;&#24573;&#30053;&#23427;&#20204;&#30340;&#25506;&#32034;&#24320;&#38144;&#33719;&#24471;&#20102;&#20248;&#21183;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade deep learning (DL) models shows that KAIROS yields up to 2X the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05643</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LMs) &#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#35299;&#20915; NLP &#20219;&#21153;&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#23545;&#20110;&#32463;&#39564;&#25104;&#21151;&#32972;&#21518;&#30340;&#29702;&#35770;&#26426;&#21046;&#20102;&#35299;&#24456;&#23569;&#65292;&#20363;&#22914;&#20026;&#20160;&#20040;&#22312;&#20960;&#21313;&#20010;&#35757;&#32451;&#28857;&#19978;&#24494;&#35843;&#19968;&#20010;&#26377; $10^8$ &#20010;&#25110;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#19981;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102; NTK &#24418;&#24335;&#21270;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110; Adam&#65292;&#24182;&#20351;&#29992; Tensor Programs &#25551;&#36848;&#20102; NTK &#36866;&#29992;&#20110;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26356;&#26032;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312; 14 &#20010; NLP &#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25552;&#31034;&#23558;&#19979;&#28216;&#20219;&#21153;&#34920;&#36848;&#20026;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22810;&#27493;&#23618;&#27425;&#31574;&#30053;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#22522;&#23398;&#20064;&#22120;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14488</link><description>&lt;p&gt;
&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#8212;&#8212;&#19968;&#31181;&#22810;&#27493;&#23618;&#27425;&#31574;&#30053;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Reinforcement Learning in Continuous Spaces -- A Hierarchical Multi-Step Approach for Policy Training. (arXiv:2209.14488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22810;&#27493;&#23618;&#27425;&#31574;&#30053;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#22522;&#23398;&#20064;&#22120;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#21508;&#31181;&#20855;&#26377;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#26377;&#25928;&#22320;&#25506;&#32034;&#20854;&#23398;&#20064;&#29615;&#22659;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#22686;&#21152;&#25506;&#32034;&#24182;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#38598;&#25104;&#31639;&#27861;&#27809;&#26377;&#26126;&#30830;&#22320;&#35757;&#32451;&#25152;&#26377;&#22522;&#23398;&#20064;&#22120;&#20197;&#20849;&#21516;&#20248;&#21270;&#38598;&#25104;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21019;&#26032;&#30340;&#22810;&#27493;&#38598;&#25104;&#26041;&#27861;&#26469;&#35757;&#32451;&#22522;&#23398;&#20064;&#22120;&#38598;&#25104;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#35757;&#32451;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic deep reinforcement learning (DRL) algorithms have recently achieved prominent success in tackling various challenging reinforcement learning (RL) problems, particularly complex control tasks with high-dimensional continuous state and action spaces. Nevertheless, existing research showed that actor-critic DRL algorithms often failed to explore their learning environments effectively, resulting in limited learning stability and performance. To address this limitation, several ensemble DRL algorithms have been proposed lately to boost exploration and stabilize the learning process. However, most of existing ensemble algorithms do not explicitly train all base learners towards jointly optimizing the performance of the ensemble. In this paper, we propose a new technique to train an ensemble of base learners based on an innovative multi-step integration method. This training technique enables us to develop a new hierarchical learning algorithm for ensemble DRL that effectively p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2207.06316</link><description>&lt;p&gt;
&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377; $\beta$-&#24046;&#24322;&#21644;&#20004;&#20010;&#22240;&#23376;&#20013;&#30340;&#19968;&#20010;&#65288;&#27604;&#22914;&#35828;&#65292;&#28608;&#27963;&#30697;&#38453;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12290;&#26631;&#20934;&#30340;&#20570;&#27861;&#26159;&#38480;&#21046;&#23383;&#20856;&#30340;&#21015;&#20855;&#26377;&#21333;&#20301;&#33539;&#25968;&#65292;&#20174;&#32780;&#25511;&#21046;&#21478;&#19968;&#20010;&#22240;&#23376;&#65288;&#23383;&#20856;&#30697;&#38453;&#65289;&#30340;&#33539;&#25968;&#65292;&#20197;&#36991;&#20813;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21407;&#38382;&#39064;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#31561;&#20215;&#30340;&#26631;&#24230;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#22359;&#19979;&#38477;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110; $\ell_{1}$-&#27491;&#21017;&#21270;&#25110;&#26356; "&#28608;&#36827;" &#30340;&#23545;&#25968;&#27491;&#21017;&#21270;&#37117;&#21487;&#20197;&#20135;&#29983;&#31616;&#21333;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20219;&#20309; $\beta$-&#24046;&#24322;&#65288;&#21363;&#20219;&#20309; $\beta$ &#30340;&#20540;&#65289;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#19978;&#20063;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24471;&#20998;&#27169;&#22411;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#22312;$L^2(p)$&#20934;&#30830;&#20272;&#35745;$\nabla \ln p$&#21518;&#21487;&#20197;&#22810;&#39033;&#24335;&#25910;&#25947;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20026;&#20351;&#29992;&#36864;&#28779;&#31243;&#24207;&#29983;&#25104;&#26679;&#26412;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2206.06227</link><description>&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#30340;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#37325;&#35201;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence for score-based generative modeling with polynomial complexity. (arXiv:2206.06227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24471;&#20998;&#27169;&#22411;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#22312;$L^2(p)$&#20934;&#30830;&#20272;&#35745;$\nabla \ln p$&#21518;&#21487;&#20197;&#22810;&#39033;&#24335;&#25910;&#25947;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20026;&#20351;&#29992;&#36864;&#28779;&#31243;&#24207;&#29983;&#25104;&#26679;&#26412;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#65288;SGM&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#26412;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;SGM&#32972;&#21518;&#30340;&#26680;&#24515;&#26426;&#21046;&#21363;&#22312;$L^2(p)$&#20934;&#30830;&#20272;&#35745;$\nabla \ln p$&#21518;&#20174;&#27010;&#29575;&#23494;&#24230;$p$&#20013;&#25277;&#26679;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#20445;&#35777;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20250;&#20135;&#29983;&#38543;&#26102;&#38388;&#25351;&#25968;&#22686;&#38271;&#25110;&#36973;&#21463;&#32500;&#25968;&#28798;&#38590;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#36866;&#29992;&#20110;&#20219;&#20309;&#24179;&#28369;&#20998;&#24067;&#65292;&#19988;&#19982;&#20854;&#23545;&#25968;Sobolev&#24120;&#25968;&#22810;&#39033;&#24335;&#30456;&#20851;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#20445;&#35777;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#23558;&#30333;&#22122;&#22768;&#36755;&#20837;&#36716;&#25442;&#20026;&#20174;&#19981;&#21516;&#22122;&#22768;&#23610;&#24230;&#32473;&#23450;&#24471;&#20998;&#20272;&#35745;&#30340;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20351;&#29992;&#36864;&#28779;&#31243;&#24207;&#29983;&#25104;&#22909;&#30340;&#26679;&#26412;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#35777;&#26126;&#22522;&#26412;&#19978;&#26159;&#20381;&#36182;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\nabla \ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#35265;&#26102;&#29616;&#26377;&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#65292;&#25506;&#31350;&#20102;&#26631;&#35760;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20854;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19968;&#20123;&#32422;&#26463;&#21487;&#20197;&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;</title><link>http://arxiv.org/abs/2206.00137</link><description>&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#36935;&#21040;&#25968;&#25454;&#20559;&#35265;: &#26631;&#27880;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20844;&#24179;&#26631;&#20934;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria. (arXiv:2206.00137v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#35265;&#26102;&#29616;&#26377;&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#65292;&#25506;&#31350;&#20102;&#26631;&#35760;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20854;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19968;&#20123;&#32422;&#26463;&#21487;&#20197;&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20844;&#24179;&#26631;&#20934;&#26469;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#20250;&#34920;&#29616;&#20986;&#25110;&#25918;&#22823;&#25105;&#20204;&#29616;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#26159;&#22312;&#26412;&#36523;&#21487;&#33021;&#23384;&#22312;&#32479;&#35745;&#20559;&#24046;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#35757;&#32451;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#26102;&#19968;&#20123;&#29616;&#26377;(&#20154;&#21475;)&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#65306;&#26631;&#35760;&#36807;&#31243;&#20013;&#30340;&#20808;&#21069;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#38169;&#35823;&#21644;&#23545;&#21155;&#21183;&#20010;&#20307;&#29305;&#24449;&#30340;&#27979;&#37327;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#20123;&#32422;&#26463;(&#20363;&#22914;&#20154;&#21475;&#22343;&#31561;&#24615;)&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#21487;&#20197;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;(&#20363;&#22914;&#24179;&#31561;&#26426;&#20250;)&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36825;&#20123;&#26631;&#20934;&#21644;&#20915;&#31574;&#21046;&#23450;&#32773;&#25928;&#29992;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;(FICO&#12289;&#25104;&#20154;&#21644;&#24503;&#22269;&#20449;&#29992;&#35780;&#20998;&#25968;&#25454;&#38598;)&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of a number of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We also analyze the sensitivity of these criteria and the decision maker's utility to biases. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31185;&#23398;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;14&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#25552;&#39640;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.11342</link><description>&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20013;&#30340;&#20943;&#24369;&#25910;&#30410;
&lt;/p&gt;
&lt;p&gt;
The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31185;&#23398;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;14&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#25552;&#39640;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#19968;&#33324;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36824;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#22312;&#26356;&#22810;&#25968;&#25454;&#19978;&#26356;&#38271;&#26102;&#38388;&#22320;&#39044;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#36825;&#20123;&#32467;&#26524;&#22312;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;14&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#21253;&#25324;ScholarBERT&#65292;&#19968;&#20010;&#26032;&#30340; 7.7 &#20159;&#21442;&#25968;&#30340;&#31185;&#23398;&#32858;&#28966;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102; &#39640;&#36798; 225B &#20010;&#20196;&#29260;&#65289;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26102;&#38388;&#23545;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#30340;&#25552;&#39640;&#65288;&#21363; &gt;1% F1&#65289;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e., &gt;1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ImGCL"&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#20174;&#26080;&#26631;&#31614;&#33410;&#28857;&#65288;&#22270;&#65289;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25972;&#21512;&#22312;&#32447;&#32858;&#31867;&#21644;&#36880;&#27493;&#24179;&#34913;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.11332</link><description>&lt;p&gt;
ImGCL&#65306;&#37325;&#35775;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#22312;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification. (arXiv:2205.11332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ImGCL"&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#20174;&#26080;&#26631;&#31614;&#33410;&#28857;&#65288;&#22270;&#65289;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25972;&#21512;&#22312;&#32447;&#32858;&#31867;&#21644;&#36880;&#27493;&#24179;&#34913;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30001;&#20110;&#20854;&#22312;&#26080;&#26631;&#31614;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36229;&#20961;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290; &#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#32473;&#23450;&#22270;&#24418;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#28508;&#22312;&#31867;&#21035;&#20998;&#24067;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#36825;&#31181;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#31867;&#20998;&#24067;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;GCL&#20013;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290; &#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;GCL&#26041;&#27861;&#26080;&#27861;&#33719;&#24471;&#26377;&#21306;&#21035;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#24046;&#21170;&#30340;&#24615;&#33021;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#27491;&#24335;GCL&#26694;&#26550;&#65288;ImGCL&#65289;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#20174;GCL&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has attracted a surge of attention due to its superior performance for learning node/graph representations without labels. However, in practice, the underlying class distribution of unlabeled nodes for the given graph is usually imbalanced. This highly imbalanced class distribution inevitably deteriorates the quality of learned node representations in GCL. Indeed, we empirically find that most state-of-the-art GCL methods cannot obtain discriminative representations and exhibit poor performance on imbalanced node classification. Motivated by this observation, we propose a principled GCL framework on Imbalanced node classification (ImGCL), which automatically and adaptively balances the representations learned from GCL without labels. Specifically, we first introduce the online clustering based progressively balanced sampling (PBS) method with theoretical rationale, which balances the training sets based on pseudo-labels obtained from learned representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22870;&#21169;&#31995;&#32479;&#38450;&#27490;&#21307;&#30103;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30340;&#22870;&#21169;&#31995;&#32479;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#38477;&#20302;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2205.00470</link><description>&lt;p&gt;
&#21487;&#20449;&#21307;&#30103;&#32852;&#37030;&#23398;&#20064;&#30340;&#22870;&#21169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Reward Systems for Trustworthy Medical Federated Learning. (arXiv:2205.00470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22870;&#21169;&#31995;&#32479;&#38450;&#27490;&#21307;&#30103;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30340;&#22870;&#21169;&#31995;&#32479;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#38477;&#20302;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#39640;&#24230;&#20851;&#27880;&#65292;&#29992;&#20110;&#35757;&#32451;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#20559;&#24046;&#65288;&#23450;&#20041;&#20026;&#27169;&#22411;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#39044;&#27979;&#24615;&#33021;&#30340;&#24046;&#24322;&#65289;&#21487;&#33021;&#23548;&#33268;&#23545;&#29305;&#23450;&#23376;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#29616;&#35937;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21307;&#30103;FL&#20013;&#21040;&#24213;&#23384;&#22312;&#22810;&#23569;&#20559;&#24046;&#65292;&#24182;&#22914;&#20309;&#36890;&#36807;&#22870;&#21169;&#31995;&#32479;&#38450;&#27490;&#36807;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;Shapley&#20540;&#36817;&#20284;&#26041;&#27861;&#35780;&#20272;&#20102;&#36328;silomedical FL&#20013;&#34913;&#37327;&#26426;&#26500;&#39044;&#27979;&#24615;&#33021;&#21644;&#20559;&#24046;&#30340;&#36129;&#29486;&#26041;&#27861;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#22870;&#21169;&#31995;&#32479;&#65292;&#28608;&#21169;&#23545;&#39640;&#39044;&#27979;&#24615;&#33021;&#25110;&#20302;&#20559;&#24046;&#20316;&#20986;&#36129;&#29486;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30340;&#22870;&#21169;&#31995;&#32479;&#65292;&#28608;&#21169;&#20316;&#20986;&#36129;&#29486;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#38477;&#20302;&#20559;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#21307;&#23398;&#33016;&#36879;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has received high interest from researchers and practitioners to train machine learning (ML) models for healthcare. Ensuring the trustworthiness of these models is essential. Especially bias, defined as a disparity in the model's predictive performance across different subgroups, may cause unfairness against specific subgroups, which is an undesired phenomenon for trustworthy ML models. In this research, we address the question to which extent bias occurs in medical FL and how to prevent excessive bias through reward systems. We first evaluate how to measure the contributions of institutions toward predictive performance and bias in cross-silo medical FL with a Shapley value approximation method. In a second step, we design different reward systems incentivizing contributions toward high predictive performance or low bias. We then propose a combined reward system that incentivizes contributions toward both. We evaluate our work using multiple medical chest X-ray
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#39640;&#32500;&#20581;&#22766;&#32479;&#35745;&#23398;&#27969;&#31639;&#27861;&#65292;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#30340;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#36941;&#27969;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.12399</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#20581;&#22766;&#32479;&#35745;&#23398;&#30340;&#27969;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Streaming Algorithms for High-Dimensional Robust Statistics. (arXiv:2204.12399v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#39640;&#32500;&#20581;&#22766;&#32479;&#35745;&#23398;&#27969;&#31639;&#27861;&#65292;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#30340;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#36941;&#27969;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27969;&#27169;&#22411;&#19979;&#30340;&#39640;&#32500;&#20581;&#22766;&#32479;&#35745;&#23398;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#38024;&#23545;&#19968;&#31995;&#21015;&#39640;&#32500;&#20581;&#22766;&#20272;&#35745;&#20219;&#21153;&#25552;&#20986;&#20102;&#35745;&#31639;&#26377;&#25928;&#29575;&#30340;&#31639;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25152;&#26377;&#20043;&#21069;&#30340;&#31639;&#27861;&#37117;&#38656;&#35201;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20869;&#23384;&#33267;&#23569;&#21576;&#20108;&#27425;&#26041;&#19982;&#32500;&#24230;&#21516;&#38454;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#39640;&#32500;&#20581;&#22766;&#32479;&#35745;&#23398;&#27969;&#31639;&#27861;&#65292;&#20854;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#23384;&#20648;&#38656;&#27714;&#65288;&#20197;&#23545;&#25968;&#22240;&#23376;&#34920;&#31034;&#65289;&#12290;&#26412;&#25991;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#30340;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#36941;&#27969;&#31639;&#27861;&#65292;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20960;&#20046;&#32447;&#24615;&#20110;&#32500;&#24230;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20960;&#20010;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#20960;&#20046;&#26368;&#20248;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#65292;&#21253;&#25324;&#20581;&#22766;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#20581;&#22766;&#22238;&#24402;&#65292;&#26356;&#26222;&#36941;&#30340;&#20581;&#22766;&#38543;&#26426;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#24182;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#30340;&#26597;&#35810;&#23601;&#33021;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#36798;&#25104;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#26159;&#19968;&#31181;&#21487;&#24597;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.00853</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#29289;&#29702;&#19990;&#30028;&#23545;&#25239;&#25915;&#20987;&#65306;Adversarial Neon Beam&#65288;arXiv:2204.00853v2 [cs.CV] &#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#24182;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#30340;&#26597;&#35810;&#23601;&#33021;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#36798;&#25104;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#26159;&#19968;&#31181;&#21487;&#24597;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#20809;&#32447;&#20250;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20135;&#21697;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#28982;&#32780;&#65292;&#20809;&#32447;&#20135;&#29983;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#23545;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#26497;&#20854;&#21361;&#38505;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#26597;&#35810;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#26469;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25968;&#23383;&#27979;&#35797;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#20197;&#36798;&#21040;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;99.3&#65285;&#65292;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;100&#65285;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#29702;&#25200;&#21160;&#38544;&#34109;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
In the physical world, light affects the performance of deep neural networks. Nowadays, many products based on deep neural network have been put into daily life. There are few researches on the effect of light on the performance of deep neural network models. However, the adversarial perturbations generated by light may have extremely dangerous effects on these systems. In this work, we propose an attack method called adversarial neon beam (AdvNB), which can execute the physical attack by obtaining the physical parameters of adversarial neon beams with very few queries. Experiments show that our algorithm can achieve advanced attack effect in both digital test and physical test. In the digital environment, 99.3% attack success rate was achieved, and in the physical environment, 100% attack success rate was achieved. Compared with the most advanced physical attack methods, our method can achieve better physical perturbation concealment. In addition, by analyzing the experimental data, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#25506;&#31350;&#65292;&#21462;&#24471;&#20102;&#22312;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21457;&#29616;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2202.06828</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of SARSA with Linear Function Approximation. (arXiv:2202.06828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#25506;&#31350;&#65292;&#21462;&#24471;&#20102;&#22312;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21457;&#29616;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SARSA&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30456;&#32467;&#21512;&#26102;&#20250;&#20986;&#29616;&#38663;&#33633;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21462;&#24471;&#20102;&#22312;&#27492;&#26041;&#38754;&#30340;&#19968;&#23450;&#36827;&#23637;&#12290;&#24778;&#20154;&#30340;&#26159;&#65292;&#21482;&#35201;&#22870;&#21169;&#30340;&#22823;&#23567;&#19981;&#26159;&#22826;&#22823;&#65292;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#20013;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#32447;&#24615;SARSA&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;SARSA&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;Lipschitz&#24120;&#25968;&#36275;&#22815;&#23567;&#65307;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#20219;&#24847;Lipschitz&#24120;&#25968;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#32447;&#24615;SARSA&#30340;&#26032;&#34892;&#20026;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
SARSA, a classical on-policy control algorithm for reinforcement learning, is known to chatter when combined with linear function approximation: SARSA does not diverge but oscillates in a bounded region. However, little is known about how fast SARSA converges to that region and how large the region is. In this paper, we make progress towards this open problem by showing the convergence rate of projected SARSA to a bounded region. Importantly, the region is much smaller than the region that we project into, provided that the magnitude of the reward is not too large. Existing works regarding the convergence of linear SARSA to a fixed point all require the Lipschitz constant of SARSA's policy improvement operator to be sufficiently small; our analysis instead applies to arbitrary Lipschitz constants and thus characterizes the behavior of linear SARSA for a new regime.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#31232;&#30095;&#21487;&#21152;&#20989;&#25968;&#30340;&#30828;&#23725;&#38543;&#26426;&#29305;&#24449;&#25193;&#23637;&#26041;&#27861;&#65288;HARFE&#65289;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#30828;&#38408;&#20540;&#36861;&#36394;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#36817;&#20284;&#35745;&#31639;&#65292;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#23725;&#22238;&#24402;&#65288;SRR&#65289;&#34920;&#36798;&#24335;&#26469;&#21462;&#24471;&#31232;&#30095;&#27169;&#22411;&#36873;&#25321;&#21644;&#23725;&#22238;&#24402;&#24179;&#28369;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65292;HARFE&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.02877</link><description>&lt;p&gt;
HARFE: &#30828;&#23725;&#38543;&#26426;&#29305;&#24449;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HARFE: Hard-Ridge Random Feature Expansion. (arXiv:2202.02877v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02877
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#31232;&#30095;&#21487;&#21152;&#20989;&#25968;&#30340;&#30828;&#23725;&#38543;&#26426;&#29305;&#24449;&#25193;&#23637;&#26041;&#27861;&#65288;HARFE&#65289;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#30828;&#38408;&#20540;&#36861;&#36394;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#36817;&#20284;&#35745;&#31639;&#65292;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#23725;&#22238;&#24402;&#65288;SRR&#65289;&#34920;&#36798;&#24335;&#26469;&#21462;&#24471;&#31232;&#30095;&#27169;&#22411;&#36873;&#25321;&#21644;&#23725;&#22238;&#24402;&#24179;&#28369;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65292;HARFE&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#39640;&#32500;&#31232;&#30095;&#21487;&#21152;&#20989;&#25968;&#65292;&#25552;&#20986;&#19968;&#31181;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#8212;&#8212;&#30828;&#23725;&#38543;&#26426;&#29305;&#24449;&#25193;&#23637;&#26041;&#27861;&#65288;HARFE&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#30828;&#38408;&#20540;&#36861;&#36394;&#30340;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#31232;&#30095;&#23725;&#22238;&#24402;&#65288;SRR&#65289;&#38382;&#39064;&#65292;&#26469;&#36817;&#20284;&#35745;&#31639;&#30456;&#23545;&#20110;&#38543;&#26426;&#29305;&#24449;&#30697;&#38453;&#30340;&#31995;&#25968;&#12290;&#35813;SRR&#34920;&#36798;&#24335;&#22312;&#31232;&#30095;&#27169;&#22411;&#36873;&#25321;&#21644;&#23725;&#22238;&#24402;&#24179;&#28369;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20174;&#32780;&#26377;&#21033;&#20110;&#22788;&#29702;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21305;&#37197;&#21152;&#24615;&#20989;&#25968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#29305;&#24449;&#30697;&#38453;&#20013;&#37319;&#29992;&#20102;&#38543;&#26426;&#31232;&#30095;&#36830;&#25509;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HARFE&#26041;&#27861;&#20250;&#25910;&#25947;&#33267;&#32473;&#23450;&#35823;&#24046;&#30028;&#38480;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#22122;&#22768;&#21644;&#31232;&#30095;&#23725;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#12290;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;HARFE&#26041;&#27861;&#30340;&#35823;&#24046;&#20302;&#20110;&#65288;&#25110;&#19982;&#65289;&#20854;&#20182;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a random feature model for approximating high-dimensional sparse additive functions called the hard-ridge random feature expansion method (HARFE). This method utilizes a hard-thresholding pursuit-based algorithm applied to the sparse ridge regression (SRR) problem to approximate the coefficients with respect to the random feature matrix. The SRR formulation balances between obtaining sparse models that use fewer terms in their representation and ridge-based smoothing that tend to be robust to noise and outliers. In addition, we use a random sparse connectivity pattern in the random feature matrix to match the additive function assumption. We prove that the HARFE method is guaranteed to converge with a given error bound depending on the noise and the parameters of the sparse ridge regression model. Based on numerical results on synthetic data as well as on real datasets, the HARFE approach obtains lower (or comparable) error than other state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#36731;&#37327;&#32423;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#35759;&#24102;&#23485;&#38480;&#21046;&#19979;&#23436;&#25104;&#20102;&#36890;&#29992;&#22270;&#19978;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#22270;&#19981;&#23436;&#20840;&#21644;&#38750;&#33391;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.12482</link><description>&lt;p&gt;
&#26377;&#38480;&#35760;&#24518;&#19979;&#30340;&#36890;&#29992;&#22270;&#21327;&#20316;&#23398;&#20064;&#65306;&#22797;&#26434;&#24230;&#12289;&#21487;&#23398;&#20064;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability. (arXiv:2201.12482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#36731;&#37327;&#32423;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#35759;&#24102;&#23485;&#38480;&#21046;&#19979;&#23436;&#25104;&#20102;&#36890;&#29992;&#22270;&#19978;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#22270;&#19981;&#23436;&#20840;&#21644;&#38750;&#33391;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#20219;&#24847;&#36830;&#25509;&#22270;&#20013;&#26377;&#38480;&#35760;&#24518;&#21644;&#36890;&#20449;&#24102;&#23485;&#30340;&#26234;&#33021;&#20307;&#23436;&#25104; K-&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#36890;&#20449;&#22270;&#24517;&#39035;&#26159;&#23436;&#20840;&#25110;&#33391;&#26500;&#30340;&#65292;&#28982;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#32780;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#20449;&#24102;&#23485;&#20063;&#20250;&#38480;&#21046;&#24050;&#26377;&#32463;&#39564;&#30340;&#20849;&#20139;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#21521;&#20854;&#20182;&#26234;&#33021;&#20307;&#20256;&#36882;&#34394;&#20551;&#30340;&#32463;&#39564;&#20449;&#24687;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;&#38024;&#23545;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#30340;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#27599;&#20010;&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#38543;&#26426;&#28216;&#36208;&#23454;&#29616;&#26368;&#26032;&#32463;&#39564;&#30340;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a K-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorizing capabilities and communication bandwidth. The goal is to let each of the agents eventually learn the best arm. It is assumed in these studies that the communication graph should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since the agents memorize and communicate very few experiences. Additionally, an agent may be corrupted to share falsified experiences to its peers, while the resource limit in terms of memorization and communication may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their latest experiences with each other through light-weight random walks in a ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2201.11989</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#30340;&#23384;&#22312;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#65292;&#22914;&#19981;&#21516;&#30340;&#24658;&#23450;&#29575;&#25110;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#31561;&#65292;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#26377;&#21161;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#27492;&#22806;&#65292;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#20063;&#24456;&#37325;&#35201;&#65292;&#20004;&#32773;&#37117;&#24433;&#21709;&#20102;&#35757;&#32451;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#37327;&#12290;&#26412;&#25991;&#22522;&#20110;&#24658;&#23450;&#23398;&#20064;&#29575;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#19982;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;TTUR&#65292;&#20026;&#20102;&#25214;&#21040;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;&#25152;&#38656;&#27493;&#39588;&#25968;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Fr'echet Inception Distance&#65288;FID&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.08066</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning. (arXiv:2111.08066v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm for offline reinforcement learning that exploits the Action Impact Regularity (AIR) property, which holds in many real-world domains including financial markets, and outperforms existing algorithms across different data collection policies in simulated and real world.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#8212;&#8212;&#20174;&#19968;&#25209;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#8212;&#8212;&#24050;&#30693;&#23545;&#20110;&#19968;&#33324;&#30340;MDP&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#21487;&#34892;&#30340;&#29305;&#23450;&#31867;&#21035;&#30340;MDP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31867;&#21463;&#38480;&#21046;&#30340;MDP&#65292;&#20197;&#33719;&#24471;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#65288;AIR&#65289;&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#65292;&#34892;&#21160;&#20027;&#35201;&#24433;&#21709;&#29366;&#24577;&#30340;&#19968;&#37096;&#20998;&#65288;&#20869;&#29983;&#32452;&#20214;&#65289;&#65292;&#24182;&#19988;&#23545;&#29366;&#24577;&#30340;&#20854;&#20313;&#37096;&#20998;&#65288;&#22806;&#29983;&#32452;&#20214;&#65289;&#24433;&#21709;&#26377;&#38480;&#12290;AIR&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#65292;&#20294;&#23427;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#20173;&#28982;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;AIR&#23646;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20026;&#22522;&#20110;Fitted-Q&#36845;&#20195;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning -- learning a policy from a batch of data -is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;(RF-GNN)&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#22810;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2107.05729</link><description>&lt;p&gt;
&#39640;&#38454;&#22270;&#24418;&#27169;&#22411;&#20013;&#22270;&#32593;&#32476;&#25512;&#26029;&#30340;&#19968;&#33324;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of graph network inferences in higher-order graphical models. (arXiv:2107.05729v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;(RF-GNN)&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#22810;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#22797;&#26434;&#32479;&#35745;&#32467;&#26500;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20855;&#26377;&#20174;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#33218;&#21040;&#29702;&#35299;&#31070;&#32463;&#35745;&#31639;&#31561;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#22312;&#19968;&#33324;&#22270;&#24418;&#26465;&#20214;&#19979;&#65292;&#22914;&#36793;&#38469;&#21270;&#31561;&#25512;&#26029;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#20123;&#25512;&#26029;&#36890;&#24120;&#30001;&#20998;&#24067;&#24335;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65288;&#20363;&#22914;&#20449;&#20219;&#20256;&#25773;&#65289;&#36817;&#20284;&#65292;&#20294;&#26159;&#23427;&#19981;&#24635;&#33021;&#22312;&#20855;&#26377;&#24490;&#29615;&#30340;&#22270;&#24418;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#19981;&#24635;&#33021;&#36731;&#26494;&#25351;&#23450;&#22797;&#26434;&#30340;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#30340;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#65292;&#36825;&#20123;&#22256;&#38590;&#32463;&#24120;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RF-GNN&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#20960;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;RF-GNN&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#20998;&#24067;&#19979;&#30340;&#19968;&#33324;&#21270;&#65292;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic graphical models provide a powerful tool to describe complex statistical structure, with many real-world applications in science and engineering from controlling robotic arms to understanding neuronal computations. A major challenge for these graphical models is that inferences such as marginalization are intractable for general graphs. These inferences are often approximated by a distributed message-passing algorithm such as Belief Propagation, which does not always perform well on graphs with cycles, nor can it always be easily specified for complex continuous probability distributions. Such difficulties arise frequently in expressive graphical models that include intractable higher-order interactions. In this paper we define the Recurrent Factor Graph Neural Network (RF-GNN) to achieve fast approximate inference on graphical models that involve many-variable interactions. Experimental results on several families of graphical models demonstrate the out-of-distribution g
&lt;/p&gt;</description></item></channel></rss>