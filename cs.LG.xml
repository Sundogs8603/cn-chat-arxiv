<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35777;&#26126;signSGD&#31639;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#20998;&#26512;&#20013;&#30340;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;SignRVR&#21644;SignRVM&#31639;&#27861;&#65292;&#24182;&#19988;&#37117;&#20197;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.15976</link><description>&lt;p&gt;
&#38750;&#20984;&#20248;&#21270;&#30340;&#22522;&#20110;&#31526;&#21495;&#38543;&#26426;&#37325;&#25490;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization. (arXiv:2310.15976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15976
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35777;&#26126;signSGD&#31639;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#20998;&#26512;&#20013;&#30340;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;SignRVR&#21644;SignRVM&#31639;&#27861;&#65292;&#24182;&#19988;&#37117;&#20197;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#36890;&#20449;&#25928;&#29575;&#36739;&#39640;&#65292;signSGD&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23545;signSGD&#30340;&#20998;&#26512;&#22522;&#20110;&#20551;&#35774;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25968;&#25454;&#37117;&#26159;&#26377;&#25918;&#22238;&#37319;&#26679;&#30340;&#65292;&#36825;&#19982;&#23454;&#38469;&#23454;&#29616;&#20013;&#25968;&#25454;&#30340;&#38543;&#26426;&#37325;&#25490;&#21644;&#39034;&#24207;&#39304;&#36865;&#36827;&#31639;&#27861;&#30340;&#24773;&#20917;&#30456;&#30683;&#30462;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;signSGD&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#39318;&#20010;&#25910;&#25947;&#32467;&#26524;&#12290;&#32473;&#23450;&#25968;&#25454;&#38598;&#22823;&#23567;$n$&#65292;&#25968;&#25454;&#36845;&#20195;&#27425;&#25968;$T$&#65292;&#21644;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#38480;&#21046;$\sigma^2$&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SignRR&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;signSGD&#30456;&#21516;&#65292;&#20026;$O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ \citep{bernstein2018signsgd}&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; SignRVR &#21644; SignRVM&#65292;&#20998;&#21035;&#21033;&#29992;&#20102;&#26041;&#24046;&#32422;&#20943;&#26799;&#24230;&#21644;&#21160;&#37327;&#26356;&#26032;&#65292;&#37117;&#20197;$O(\log(nT)/\sqrt{nT})$&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;&#19982;signSGD&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#20013;&#26497;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#19982;&#21516;&#31561;&#25968;&#37327;&#30340;&#26799;&#24230;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
signSGD is popular in nonconvex optimization due to its communication efficiency. Yet, existing analyses of signSGD rely on assuming that data are sampled with replacement in each iteration, contradicting the practical implementation where data are randomly reshuffled and sequentially fed into the algorithm. We bridge this gap by proving the first convergence result of signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the dataset size $n$, the number of epochs of data passes $T$, and the variance bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD \citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which leverage variance-reduced gradients and momentum updates respectively, both converging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis of signSGD, our results do not require an extremely large batch size in each iteration to be of the same order a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.15202</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#30340;&#32467;&#21512;&#20301;&#28857;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#22914;&#20309;&#35843;&#25511;&#22522;&#22240;&#34920;&#36798;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#27835;&#30103;&#25163;&#27573;&#36827;&#34892;&#35843;&#33410;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#21033;&#29992;ChIP-seq&#25968;&#25454;&#38598;&#25366;&#25496;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;DNABERT-Cap&#26159;&#19968;&#20010;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#32463;&#36807;&#22823;&#37327;&#22522;&#22240;&#32452;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33014;&#22218;&#23618;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#23545;&#21253;&#21547;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#20010;&#32454;&#32990;&#31995;&#30340;&#22522;&#20934;ChIP-seq&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;A54&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#36890;&#36807;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#26469;&#34913;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#22256;&#24785;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.11094</link><description>&lt;p&gt;
&#37325;&#23398;&#24050;&#36951;&#24536;&#30693;&#35782;&#65306;&#20851;&#20110;&#36951;&#24536;&#65292;&#36807;&#25311;&#21512;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs. (arXiv:2310.11094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#36890;&#36807;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#26469;&#34913;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#22256;&#24785;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#19981;&#32463;&#24120;&#21457;&#29983;&#20196;&#20154;&#22256;&#24785;&#12290;&#29702;&#35770;&#39044;&#27979;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#65292;&#23427;&#20204;&#26368;&#32456;&#24212;&#35813;&#21464;&#24471;&#36807;&#24230;&#36866;&#24212;&#26576;&#20010;&#29305;&#23450;&#30340;&#35757;&#32451;&#38598;&#65292;&#20174;&#32780;&#23548;&#33268;&#27867;&#21270;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#35777;&#32467;&#26524;&#20013;&#65292;&#22686;&#21152;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#25110;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#20960;&#20046;&#20174;&#19981;&#25439;&#23475;&#27867;&#21270;&#12290;&#36825;&#26159;&#22240;&#20026;&#25105;&#20204;&#34913;&#37327;&#36807;&#25311;&#21512;&#30340;&#26041;&#24335;&#22826;&#36807;&#26377;&#38480;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#31243;&#24230;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#12290;&#36825;&#20010;&#20998;&#25968;&#34920;&#26126;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#24403;&#29992;&#36825;&#31181;&#26041;&#24335;&#27979;&#37327;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#25311;&#21512;&#21487;&#20197;&#22312;&#39564;&#35777;&#31934;&#24230;&#38477;&#20302;&#21644;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#21457;&#29983;&#65292;&#24182;&#19988;&#21487;&#33021;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#24120;&#35265;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#26377;&#21161;&#20110;&#28548;&#28165;&#21069;&#36848;&#30340;&#22256;&#24785;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our obs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#35777;&#21457;&#29616;&#36807;&#25311;&#21512;&#26102;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11077</link><description>&lt;p&gt;
&#22242;&#32467;&#19968;&#33268;&#65306;&#21033;&#29992;&#20998;&#26102;&#19968;&#33268;&#24615;&#38598;&#21512;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#35777;&#21457;&#29616;&#36807;&#25311;&#21512;&#26102;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25311;&#21512;&#21407;&#22987;&#22270;&#20687;&#19978;&#23450;&#20041;&#30340;&#38750;&#24120;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#24378;&#22823;&#23398;&#20064;&#22120;&#30340;&#32570;&#28857;&#26159;&#36807;&#25311;&#21512;&#35757;&#32451;&#38598;&#30340;&#21361;&#38505;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#36890;&#24120;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#35757;&#32451;&#30340;&#8220;&#25552;&#21069;&#20572;&#27490;&#8221;&#26469;&#36991;&#20813;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#23427;&#23545;&#25239;&#36807;&#25311;&#21512;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24403;&#21457;&#29983;&#36807;&#25311;&#21512;&#26102;&#65292;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#36825;&#19968;&#28857;&#24050;&#22312;&#24120;&#29992;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#23454;&#35777;&#12290;&#22312;&#36825;&#20123;&#32467;&#26524;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#25239;&#36807;&#25311;&#21512;&#65292;&#20854;&#20013;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#30830;&#23450;&#30340;&#12290;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24120;&#35268;&#38598;&#25104;&#36973;&#21463;&#36807;&#25311;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05858</link><description>&lt;p&gt;
DSAC-T: &#24102;&#26377;&#19977;&#20010;&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#8212;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#25152;&#24341;&#36215;&#30340;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#35780;&#35770;&#32773;&#65288;DSAC&#25110;DSAC-v1&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#30340;&#39640;&#26031;&#20540;&#20998;&#24067;&#26469;&#26377;&#25928;&#25552;&#39640;&#20540;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;DSAC&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#32780;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#32553;&#25918;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#19968;&#20123;&#29305;&#27530;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19977;&#20010;&#23545;&#26631;&#20934;DSAC&#30340;&#37325;&#35201;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#12290;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31216;&#20026;DSAC-T&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>Ophiuchus&#26159;&#19968;&#20010;&#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#23427;&#33021;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#25805;&#20316;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#32467;&#26500;&#30340;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02508</link><description>&lt;p&gt;
Ophiuchus: &#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02508
&lt;/p&gt;
&lt;p&gt;
Ophiuchus&#26159;&#19968;&#20010;&#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#23427;&#33021;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#25805;&#20316;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#32467;&#26500;&#30340;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#28982;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#21407;&#29983;&#24577;&#29366;&#24577;&#26174;&#31034;&#20986;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#36890;&#24120;&#23616;&#38480;&#20110;&#22312;&#21333;&#19968;&#31934;&#32454;&#21270;&#20998;&#36776;&#29575;&#20869;&#25805;&#20316;&#65292;&#24182;&#19988;&#32570;&#20047;&#25429;&#25417;&#39640;&#32423;&#26500;&#24314;&#27169;&#22359;&#30340;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Ophiuchus&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#23427;&#26159;&#19968;&#20010;SO(3)-&#31561;&#21464;&#31895;&#31890;&#21270;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25805;&#20316;&#26631;&#20934;&#34507;&#30333;&#36136;&#27531;&#22522;&#30340;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#24182;&#21516;&#26102;&#23562;&#37325;&#23427;&#20204;&#30340;&#30456;&#20851;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#24403;&#21069;&#37319;&#29992;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#23616;&#37096;&#21367;&#31215;&#31895;&#21270;&#65292;&#20197;&#22312;&#23545;&#25968;&#32447;&#24615;&#38271;&#24230;&#22797;&#26434;&#24230;&#19979;&#27169;&#25311;&#24207;&#21015;&#27169;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;PDB&#21333;&#20307;&#30340;&#36830;&#32493;&#29255;&#27573;&#23545;Ophiuchus&#36827;&#34892;&#35757;&#32451;&#65292;&#30740;&#31350;&#20854;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#23398;&#20064;&#21040;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#26500;&#35937;&#25554;&#20540;&#20013;&#30340;&#24555;&#36895;&#20351;&#29992;&#65292;&#23558;&#25554;&#20540;&#36712;&#36857;&#19982;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional native states of natural proteins display recurring and hierarchical patterns. Yet, traditional graph-based modeling of protein structures is often limited to operate within a single fine-grained resolution, and lacks hourglass neural architectures to learn those high-level building blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries. Our model departs from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. We train Ophiuchus on contiguous fragments of PDB monomers, investigating its reconstruction capabilities across different compression rates. We examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure
&lt;/p&gt;</description></item><item><title>Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.01824</link><description>&lt;p&gt;
Mini-BEHAVIOR&#65306;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#20915;&#31574;&#21046;&#23450;&#30340;&#36807;&#31243;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01824
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Mini-BEHAVIOR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#25361;&#25112;&#26234;&#33021;&#20307;&#21033;&#29992;&#25512;&#29702;&#21644;&#20915;&#31574;&#25216;&#33021;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#20154;&#31867;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#12290;Mini-BEHAVIOR&#29615;&#22659;&#26159;&#19968;&#20010;&#24555;&#36895;&#65292;&#29616;&#23454;&#30340;Gridworld&#29615;&#22659;&#65292;&#26082;&#20855;&#26377;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20445;&#30041;&#20102;&#22797;&#26434;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#22522;&#20934;&#20013;&#31526;&#21495;&#32423;&#30340;&#29289;&#29702;&#29616;&#23454;&#24863;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#38190;&#29305;&#24615;&#65292;&#22914;&#36807;&#31243;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;Mini-BEHAVIOR&#25552;&#20379;&#20102;&#21407;&#22987;BEHAVIOR&#22522;&#20934;&#20013;&#21508;&#31181;&#23478;&#21153;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35757;&#32451;&#30340;&#20837;&#38376;&#20195;&#30721;&#12290;&#24635;&#20043;&#65292;Mini-BEHAVIOR&#20026;&#35780;&#20272;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#24320;&#25918;&#24335;&#30340;&#22522;&#20934;&#12290;&#23427;&#20316;&#20026;&#30740;&#31350;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#20837;&#21475;&#28857;&#65292;&#20419;&#36827;&#20102;&#35780;&#20272;&#21644;&#21457;&#23637;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00806</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#35823;&#24046;&#27714;&#21644;&#30340;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#21407;&#21017;&#24471;&#21040;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#36718;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#21019;&#24314;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#31639;&#27861;&#20449;&#24687;&#27604;&#8221;&#65292;&#26377;&#25928;&#22320;&#34920;&#24449;&#20102;&#20219;&#20309;&#31639;&#27861;&#30340;&#39057;&#29575;&#35823;&#24046;&#30340;&#20869;&#22312;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#24335;&#31639;&#27861;&#26080;&#20808;&#39564;&#22320;&#24182;&#19988;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#20197;&#36890;&#29992;&#21644;&#26368;&#20248;&#26041;&#24335;&#24212;&#29992;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31639;&#27861;&#31616;&#21333;&#19988;&#36890;&#24120;&#23481;&#26131;&#23454;&#29616;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#12289;&#23545;&#25239;&#21644;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13108</link><description>&lt;p&gt;
&#25968;&#25454;&#21152;&#36733;&#36890;&#24120;&#20855;&#26377;&#30701;&#28145;&#24230;&#65306;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13108
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24320;&#21457;&#29992;&#20110;&#30740;&#31350;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31616;&#21333;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#25104;&#26412;&#26159;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#38556;&#30861;&#12290;&#24403;&#20351;&#29992;&#25391;&#24133;&#32534;&#30721;&#26102;&#65292;&#21152;&#36733;&#20219;&#24847;&#32463;&#20856;&#21521;&#37327;&#38656;&#35201;&#19982;&#27604;&#29305;&#25968;&#25104;&#25351;&#25968;&#20851;&#31995;&#30340;&#30005;&#36335;&#28145;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20010;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#29702;&#35770;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;AMLET&#65288;&#33258;&#21160;&#22810;&#23618;&#21152;&#36733;&#22120;&#21033;&#29992;TNs&#65289;&#8212;&#8212;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#29305;&#23450;&#30340;TN&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#35843;&#25972;&#30005;&#36335;&#28145;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#34507;&#30333;&#36136;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#30495;&#23454;&#32463;&#20856;&#25968;&#25454;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20851;&#20110;&#23558;&#32463;&#20856;&#25968;&#25454;&#21152;&#36733;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#30340;&#26368;&#24191;&#27867;&#30340;&#25968;&#20540;&#20998;&#26512;&#12290;&#19982;&#36825;&#19968;&#39046;&#22495;&#26368;&#36817;&#30340;&#20854;&#20182;&#24037;&#20316;&#19968;&#33268;&#65292;&#25152;&#38656;&#30340;
&lt;/p&gt;
&lt;p&gt;
Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#30340;&#26089;&#26399;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#23547;&#25214;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#65292;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.11646</link><description>&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of autism spectrum disorder using machine learning approaches. (arXiv:2309.11646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#30340;&#26089;&#26399;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#23547;&#25214;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#65292;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#26159;&#19968;&#31181;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#34920;&#29616;&#20026;&#31038;&#20132;&#20114;&#21160;&#22256;&#38590;&#12289;&#35821;&#35328;&#27807;&#36890;&#22256;&#38590;&#21644;&#37325;&#22797;&#34892;&#20026;&#12290;&#36825;&#20123;&#22256;&#38590;&#30340;&#20005;&#37325;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#65292;&#34987;&#35786;&#26029;&#20026;ASD&#30340;&#20154;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#26089;&#35782;&#21035;&#21644;&#22788;&#29702;ASD&#21487;&#20197;&#20419;&#36827;&#35813;&#30142;&#30149;&#30340;&#25913;&#21892;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#35786;&#26029;&#20316;&#20026;&#20256;&#32479;&#20020;&#24202;&#26041;&#27861;&#30340;&#34917;&#20805;&#20986;&#29616;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#28508;&#22312;&#32570;&#28857;&#12290;&#26412;&#25991;&#21033;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23547;&#25214;ASD&#30340;&#26368;&#26174;&#33879;&#29305;&#24449;&#24182;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20845;&#31181;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#25214;&#21040;&#26368;&#36866;&#21512;&#35782;&#21035;ASD&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#23545;&#36825;&#20123;ASD&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we 
&lt;/p&gt;</description></item><item><title>PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2309.08140</link><description>&lt;p&gt;
PromptTTS++&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#25552;&#31034;&#24335;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08140
&lt;/p&gt;
&lt;p&gt;
PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PromptTTS++&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;TTS&#26694;&#26550;&#20013;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#35813;&#25552;&#31034;&#25551;&#36848;&#20102;&#35821;&#38899;&#29305;&#24449;&#65288;&#22914;&#20013;&#24615;&#12289;&#24180;&#36731;&#12289;&#32769;&#24180;&#21644;&#27785;&#38391;&#65289;&#65292;&#26088;&#22312;&#19982;&#35828;&#35805;&#39118;&#26684;&#22823;&#33268;&#29420;&#31435;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#21253;&#21547;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LibriTTS-R&#35821;&#26009;&#24211;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22768;&#23398;&#27169;&#22411;&#19982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26469;&#24314;&#27169;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#21270;&#35828;&#35805;&#32773;&#22240;&#32032;&#12290;&#19982;&#20043;&#21069;&#20165;&#20381;&#36182;&#26679;&#24335;&#25552;&#31034;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26679;&#24335;&#25552;&#31034;&#20165;&#25551;&#36848;&#20102;&#35828;&#35805;&#32773;&#20010;&#24615;&#21270;&#30340;&#26377;&#38480;&#26041;&#38754;&#65292;&#22914;&#38899;&#35843;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39069;&#22806;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21040;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic f
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.02842</link><description>&lt;p&gt;
&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random postprocessing for combinatorial Bayesian optimization. (arXiv:2309.02842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02842
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#39034;&#24207;&#26041;&#27861;&#29992;&#20110;&#31163;&#25955;&#30340;&#8220;&#40657;&#30418;&#8221;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#24120;&#20250;&#23545;&#32473;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#35775;&#38382;&#22810;&#27425;&#30456;&#21516;&#30340;&#28857;&#65292;&#23548;&#33268;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#25165;&#33021;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#12290;&#25105;&#20204;&#21457;&#29616;&#21518;&#22788;&#29702;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#25152;&#38656;&#30340;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#37319;&#26679;&#20989;&#25968;&#26159;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#26102;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.15478</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#22312;&#20999;&#21521;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#20854;&#20013;&#29305;&#24449;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#32771;&#34385;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#20174;&#32780;&#36890;&#36807;&#21452;&#32447;&#24615;&#25554;&#20540;&#32422;&#26463;&#22312;&#21442;&#25968;&#21644;&#21464;&#25442;&#19978;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#19982;&#20855;&#26377;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#32447;&#24615;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#36817;&#20284;&#20302;&#31209;&#35299;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25105;&#20204;&#23545;&#29305;&#24449;&#20197;&#21450;&#26680;&#20989;&#25968;&#30340;&#21464;&#21270;&#33719;&#24471;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20026;&#24403;&#30446;&#26631;&#20989;&#25968;&#22312;&#20999;&#21521;&#29305;&#24449;&#19978;&#34920;&#24449;&#19981;&#22909;&#26102;&#30340;&#26680;&#23545;&#40784;&#29616;&#35937;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#38500;&#20102;&#22312;&#31616;&#21333;&#22238;&#24402;&#38382;&#39064;&#19978;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#23519;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20999;&#21521;&#29305;&#24449;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411; (RDDM)&#65292;&#29992;&#20110;&#23558;PPG&#36716;&#25442;&#20026;ECG&#20449;&#21495;&#12290;RDDM&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21521;&#24863;&#20852;&#36259;&#21306;&#22495;&#28155;&#21152;&#22122;&#22768;&#26469;&#25429;&#25417;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.13568</link><description>&lt;p&gt;
&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;PPG&#21040;ECG&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation. (arXiv:2308.13568v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13568
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411; (RDDM)&#65292;&#29992;&#20110;&#23558;PPG&#36716;&#25442;&#20026;ECG&#20449;&#21495;&#12290;RDDM&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21521;&#24863;&#20852;&#36259;&#21306;&#22495;&#28155;&#21152;&#22122;&#22768;&#26469;&#25429;&#25417;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39640;&#21457;&#29575;&#38656;&#35201;&#20415;&#25463;&#19988;&#32463;&#27982;&#26377;&#25928;&#30340;&#36830;&#32493;&#24515;&#33039;&#30417;&#27979;&#24037;&#20855;&#12290;&#23613;&#31649;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#36830;&#32493;&#30417;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#27492;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20809;&#30005;&#23481;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#22522;&#26412;&#30340;&#21487;&#29992;&#20110;&#28040;&#36153;&#32773;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#31181;&#24819;&#27861;&#26368;&#36817;&#24341;&#36215;&#20102;&#23558;PPG&#36716;&#21270;&#20026;ECG&#20449;&#21495;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;ECG&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#25429;&#25417;&#36825;&#31181;&#32454;&#24494;&#24046;&#21035;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#25972;&#20010;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#28155;&#21152;&#36807;&#31243;&#26159;&#19981;&#21152;&#36873;&#25321;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;RDDM&#36890;&#36807;&#23558;&#22122;&#22768;&#26377;&#36873;&#25321;&#22320;&#28155;&#21152;&#21040;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65288;&#22914;ECG&#20449;&#21495;&#20013;&#30340;QRS&#22797;&#21512;&#20307;&#65289;&#30340;&#26032;&#39062;&#27491;&#21521;&#36807;&#31243;&#21644;&#21453;&#21521;&#36807;&#31243;&#26469;&#20811;&#26381;&#27492;&#31867;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high prevalence of cardiovascular diseases (CVDs) calls for accessible and cost-effective continuous cardiac monitoring tools. Despite Electrocardiography (ECG) being the gold standard, continuous monitoring remains a challenge, leading to the exploration of Photoplethysmography (PPG), a promising but more basic alternative available in consumer wearables. This notion has recently spurred interest in translating PPG to ECG signals. In this work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel diffusion model designed to capture the complex temporal dynamics of ECG. Traditional Diffusion models like Denoising Diffusion Probabilistic Models (DDPM) face challenges in capturing such nuances due to the indiscriminate noise addition process across the entire signal. Our proposed RDDM overcomes such limitations by incorporating a novel forward process that selectively adds noise to specific regions of interest (ROI) such as QRS complex in ECG signals, and a reverse proces
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#21442;&#25968;&#35268;&#21010;&#36873;&#25321;&#24615;&#25512;&#26029;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#26469;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11351</link><description>&lt;p&gt;
&#21442;&#25968;&#35268;&#21010;&#30340;&#36873;&#25321;&#24615;&#25512;&#26029;&#20013;&#30340;&#26377;&#30028;P&#20540;
&lt;/p&gt;
&lt;p&gt;
Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#21442;&#25968;&#35268;&#21010;&#36873;&#25321;&#24615;&#25512;&#26029;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#26469;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#25512;&#26029;&#65288;SI&#65289;&#20316;&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20551;&#35774;&#26816;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26694;&#26550;&#65292;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#20851;&#27880;&#12290;SI&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#20551;&#35774;&#34987;&#36873;&#20013;&#30340;&#20107;&#20214;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#20102;&#36827;&#34892;SI&#65292;&#24517;&#39035;&#20197;&#21487;&#36861;&#36394;&#30340;&#24418;&#24335;&#23545;&#36825;&#20010;&#20107;&#20214;&#36827;&#34892;&#25551;&#36848;&#12290;&#24403;&#36873;&#25321;&#20107;&#20214;&#38590;&#20197;&#25551;&#36848;&#26102;&#65292;&#21487;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#20197;&#20351;&#20854;&#21487;&#22788;&#29702;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#26465;&#20214;&#24448;&#24448;&#20250;&#23548;&#33268;&#21151;&#25928;&#30340;&#25439;&#22833;&#65292;&#36825;&#19968;&#38382;&#39064;&#34987;&#31216;&#20026;&#36807;&#24230;&#26465;&#20214;&#21270;&#12290;&#22522;&#20110;&#21442;&#25968;&#35268;&#21010;&#30340;SI&#65288;PP-based SI&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36807;&#24230;&#26465;&#20214;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;PP-based SI&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#30001;&#20110;&#38656;&#35201;&#23436;&#20840;&#22320;&#25506;&#32034;&#25968;&#25454;&#31354;&#38388;&#32780;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#65292;&#36890;&#36807;&#25552;&#20986;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search str
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10422</link><description>&lt;p&gt;
PreDiff: &#20351;&#29992;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26102;&#31354;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#31354;&#21069;&#22686;&#21152;&#20351;&#24471;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#35201;&#20040;&#38590;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#24573;&#35270;&#29305;&#23450;&#39046;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#27169;&#31946;&#25110;&#20135;&#29983;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26102;&#31354;&#39044;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;PreDiff&#30340;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#65307;2&#65289;&#25105;&#20204;&#34701;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#20351;&#39044;&#27979;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20272;&#35745;&#19982;&#25152;&#26045;&#21152;&#32422;&#26463;&#30340;&#20559;&#24046;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#28857;&#20113;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#20351;&#24471;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09259</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#30340;&#33258;&#36866;&#24212;&#25299;&#25169;&#29305;&#24449;&#65306;&#28857;&#20113;&#30340;&#28388;&#27874;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#28857;&#20113;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#20351;&#24471;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24418;&#29366;&#35782;&#21035;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#20250;&#24341;&#20837;&#20840;&#23616;&#25299;&#25169;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#25552;&#21462;&#12290;&#22312;&#23545;&#28857;&#20113;&#36827;&#34892;&#25345;&#20037;&#21516;&#35843;&#35745;&#31639;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#28857;&#20113;&#30340;&#28388;&#27874;&#22120;&#65292;&#21363;&#19968;&#20010;&#36880;&#28176;&#22686;&#21152;&#30340;&#31354;&#38388;&#24207;&#21015;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#25345;&#20037;&#21516;&#35843;&#30340;&#32467;&#21512;&#21463;&#21040;&#28388;&#27874;&#22120;&#36873;&#25321;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#21644;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20351;&#24471;&#24471;&#21040;&#30340;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#26377;&#38480;&#32500;&#36817;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#65306;&#27169;&#22411;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#28145;&#24230;&#22270;&#32593;&#32476;&#65288;DGNs&#65289;&#30340;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#25512;&#21160;&#20102;&#22270;&#19978;&#23398;&#20064;&#30340;&#39046;&#22495;&#25104;&#29087;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#24613;&#38656;&#20351;DGNs&#36866;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38543;&#26102;&#38388;&#25512;&#31227;&#19981;&#26029;&#28436;&#21270;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20419;&#36827;&#21160;&#24577;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#26368;&#26032;&#27010;&#35272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17778</link><description>&lt;p&gt;
&#30475;&#30475;&#12289;&#35760;&#20303;&#21644;&#25512;&#29702;&#65306;&#22522;&#20110;&#26426;&#29702;&#30340;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17778
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36827;&#34892;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#22312;&#35768;&#22810;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38656;&#35201;&#23558;&#35270;&#35273;&#20449;&#24687;&#32039;&#23494;&#34701;&#21512;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36825;&#20010;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#31181;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#12290;&#23427;&#36890;&#24120;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#30475;&#65292;&#35760;&#20303;&#65292;&#25512;&#29702;&#8221;&#30340;&#19977;&#20010;&#27493;&#39588;&#36807;&#31243;&#65306;&#36890;&#36807;&#36880;&#27493;&#36827;&#34892;&#20302;&#32423;&#35270;&#35273;&#36807;&#31243;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#30452;&#21040;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#25105;&#20204;&#36981;&#24490;&#30456;&#21516;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#26550;&#26500;&#26356;&#25913;&#65292;&#20351;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#30340;&#21407;&#29702;&#65292;&#20801;&#35768;&#25105;&#20204;&#38598;&#25104;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#65292;&#22914;&#23545;&#35937;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#25913;&#36827;&#20102;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#36824;&#20855;&#26377;&#24182;&#34892;&#24615;&#65292;&#20026;&#27169;&#22411;&#24182;&#34892;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17648</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#25913;&#36827;&#20102;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#36824;&#20855;&#26377;&#24182;&#34892;&#24615;&#65292;&#20026;&#27169;&#22411;&#24182;&#34892;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#21644;&#20056;&#24615;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;L-BFGS&#20248;&#21270;&#22120;&#12290;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#26159;&#36890;&#36807;&#21033;&#29992;Schwarz&#22495;&#20998;&#35299;&#26694;&#26550;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#21442;&#25968;&#20197;&#36880;&#23618;&#26041;&#24335;&#36827;&#34892;&#20998;&#35299;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21152;&#24615;&#21644;&#20056;&#24615;&#39044;&#26465;&#20214;&#22120;&#37117;&#33021;&#26174;&#33879;&#25913;&#21892;&#26631;&#20934;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#27492;&#22806;&#65292;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#26412;&#36136;&#19978;&#26159;&#24182;&#34892;&#30340;&#65292;&#22240;&#27492;&#20026;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17052</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#20849;&#20139;&#20132;&#36890;&#65292;&#38656;&#35201;&#21327;&#35843;&#22823;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20248;&#21270;&#20195;&#34920;&#24615;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#26469;&#24212;&#23545;&#30001;&#27492;&#24102;&#26469;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20998;&#24067;&#23384;&#22312;&#20840;&#23616;&#32422;&#26463;&#30340;&#24773;&#20917;&#65288;&#20363;&#22914;&#38656;&#35201;&#28385;&#36275;&#23481;&#37327;&#32422;&#26463;&#25110;&#26368;&#23567;&#35206;&#30422;&#35201;&#27714;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23427;&#22312;&#20445;&#35777;&#24754;&#35266;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#36716;&#31227;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#30830;&#20445;&#39640;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20849;&#20139;&#20132;&#36890;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#36710;&#36742;&#37325;&#23450;&#20301;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#22323;&#20986;&#31199;&#36710;&#36712;&#36857;&#25968;&#25454;&#30340;&#20223;&#30495;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#28385;&#36275;&#20851;&#38190;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25506;&#32034; GPnn &#30340;&#40065;&#26834;&#24615;&#21644;&#26497;&#38480;&#34892;&#20026;&#23454;&#29616;&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#21363;&#20351;&#22312;&#20986;&#29616;&#37325;&#22823;&#23567;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#21482;&#38656;&#35201;&#33457;&#36153;&#23569;&#37327;&#30340;&#24037;&#20316;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21363;&#21487;&#23454;&#29616;&#39640; MSE &#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#21152;&#24615;&#22122;&#22768;&#26041;&#24046;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24230;&#26657;&#20934;&#21644; NLL &#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14731</link><description>&lt;p&gt;
&#21033;&#29992;&#26412;&#22320;&#24615;&#21644;&#40065;&#26834;&#24615;&#23454;&#29616;&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression. (arXiv:2306.14731v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25506;&#32034; GPnn &#30340;&#40065;&#26834;&#24615;&#21644;&#26497;&#38480;&#34892;&#20026;&#23454;&#29616;&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#21363;&#20351;&#22312;&#20986;&#29616;&#37325;&#22823;&#23567;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#21482;&#38656;&#35201;&#33457;&#36153;&#23569;&#37327;&#30340;&#24037;&#20316;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21363;&#21487;&#23454;&#29616;&#39640; MSE &#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#21152;&#24615;&#22122;&#22768;&#26041;&#24046;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24230;&#26657;&#20934;&#21644; NLL &#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25152;&#25552;&#20379;&#30340;&#31934;&#30830;&#39044;&#27979;&#21644;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#20250;&#20135;&#29983; O(n^3) &#30340;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#29616;&#20195;&#22823;&#35268;&#27169;&#24212;&#29992;&#26469;&#35828;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#20851;&#20110;&#35745;&#31639;&#25928;&#29575;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034; GP &#26368;&#36817;&#37051;&#39044;&#27979;(GPnn) &#30340;&#40065;&#26834;&#24615;&#21644;&#26497;&#38480;&#34892;&#20026;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#27169;&#25311;&#35777;&#26126;&#65292;&#38543;&#30528;&#25968;&#25454;&#37327; n &#30340;&#22686;&#21152;&#65292;&#20272;&#35745;&#21442;&#25968;&#21644; GP &#27169;&#22411;&#20551;&#35774;&#30340;&#20934;&#30830;&#24615;&#23545; GPnn &#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36880;&#28176;&#20943;&#23567;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640; MSE &#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#20986;&#29616;&#37325;&#22823;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;, &#21482;&#38656;&#35201;&#33457;&#36153;&#23569;&#37327;&#30340;&#24037;&#20316;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21363;&#21487;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38543;&#30528; n &#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#30830;&#23450;&#24230;&#26657;&#20934;&#21644; NLL &#20173;&#23545;&#19968;&#20010;&#21442;&#25968;&#25935;&#24863;&#65292;&#21363;&#21152;&#24615;&#22122;&#22768;&#26041;&#24046;&#65307;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24230;&#26657;&#20934;&#21644; NLL&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate predictions and principled uncertainty measures provided by GP regression incur O(n^3) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size n increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as n tends to infinity, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03502</link><description>&lt;p&gt;
&#20420;&#20044;&#25112;&#20105;&#65306;&#39044;&#27979;&#21644;&#35299;&#37322;Twitter&#30340;&#23553;&#31105;
&lt;/p&gt;
&lt;p&gt;
Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;2&#26376;24&#26085;&#65292;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#65292;&#24320;&#22987;&#20102;&#29616;&#22312;&#24050;&#30693;&#30340;&#20420;&#20044;&#25112;&#20105;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24341;&#21457;&#20102;&#22312;&#32447;&#35805;&#35821;&#12290;Twitter&#20316;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#20043;&#19968;&#65292;&#20197;&#20854;&#24320;&#25918;&#21644;&#27665;&#20027;&#30340;&#29305;&#28857;&#65292;&#22312;&#20854;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20013;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#35752;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;Twitter&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#28389;&#29992;&#34892;&#20026;&#12289;&#20405;&#29359;&#20844;&#27665;&#26435;&#21033;&#65292;&#22240;&#27492;&#23548;&#33268;&#29992;&#25143;&#36134;&#25143;&#34987;&#23553;&#31105;&#21644;&#21024;&#38500;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#20102;Twitter&#30340;&#23553;&#31105;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#21487;&#33021;&#23548;&#33268;&#36134;&#25143;&#34987;&#23553;&#31105;&#30340;&#20849;&#20139;&#20869;&#23481;&#21644;&#29992;&#25143;&#36134;&#25143;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;Twitter API&#33719;&#24471;&#20102;&#21253;&#21547;107.7M&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;980&#19975;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#34987;&#23553;&#31105;&#36134;&#25143;&#30340;&#20849;&#20139;&#20869;&#23481;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#32858;&#31867;&#26469;&#35299;&#37322;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#28389;&#29992;Twitter&#25919;&#31574;&#26631;&#20934;&#30340;&#39575;&#23376;&#27963;&#21160;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#23459;&#20256;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#31881;&#19997;&#25968;&#36739;&#23569;&#30340;&#36134;&#25143;&#65292;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#26377;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#20026;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#65292;&#26368;&#23567;&#21270;&#26377;&#23475;&#20869;&#23481;&#30340;&#20256;&#25773;&#25552;&#20379;&#26377;&#29992;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
&lt;/p&gt;</description></item><item><title>SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19442</link><description>&lt;p&gt;
SimFBO&#65306;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19442
&lt;/p&gt;
&lt;p&gt;
SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#12289;&#24494;&#35843;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#20013;&#23884;&#22871;&#20248;&#21270;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#65288;FBO&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FBO&#31639;&#27861;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#24182;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#22810;&#20010;&#23376;&#24490;&#29615;&#65292;&#27599;&#20010;&#23376;&#24490;&#29615;&#21253;&#21547;&#22810;&#20010;&#36890;&#20449;&#36718;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimFBO&#30340;&#31616;&#21333;&#28789;&#27963;&#30340;FBO&#26694;&#26550;&#65292;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#23376;&#24490;&#29615;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#24191;&#20041;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#21644;&#26356;&#26032;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31995;&#32479;&#32423;&#24322;&#26500;&#40065;&#26834;FBO&#65288;ShroFBO&#65289;&#20316;&#20026;SimFBO&#30340;&#21464;&#20307;&#65292;&#20854;&#23545;&#26412;&#22320;&#35745;&#31639;&#30340;&#24322;&#26500;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#21644;&#26080;&#26367;&#25442;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#19979;&#65292;SimFBO&#21644;ShroFBO&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#21152;&#36895;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20803;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#37327;&#20102;Barron&#31354;&#38388;&#21644;&#35889;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#23884;&#20837;&#19981;&#31561;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.19082</link><description>&lt;p&gt;
Barron&#22411;&#31354;&#38388;&#30340;&#23884;&#20837;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#37327;&#20102;Barron&#31354;&#38388;&#21644;&#35889;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#23884;&#20837;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#29702;&#35299;&#39640;&#32500;&#26465;&#20214;&#19979;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#21644;&#27867;&#21270;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;Barron&#31354;&#38388;$\mathcal{B}_s(\Omega)$&#21644;&#35889;Barron&#31354;&#38388;$\mathcal{F}_s(\Omega)$&#65292;&#20854;&#20013;&#25351;&#25968;$s$&#34920;&#24449;&#20102;&#36825;&#20123;&#31354;&#38388;&#20013;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#65292;$\Omega\subset\mathbb{R}^d$&#34920;&#31034;&#36755;&#20837;&#22495;&#12290;&#28982;&#32780;&#65292;&#20004;&#31181;&#31867;&#22411;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#19981;&#31561;&#24335;&#24314;&#31435;&#20102;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#36830;&#32493;&#23884;&#20837;&#65306;&#23545;&#20110;&#20219;&#24847;$\delta\in(0,1),s\in\mathbb{N}^{+}$&#21644;$f:\Omega \mapsto \mathbb{R}$&#65292;&#37117;&#26377;\[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]&#20854;&#20013;$\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$&#65292;$\lesssim_s$&#34920;&#31034;&#20165;&#19982;&#24179;&#28369;&#21442;&#25968;$s$&#26377;&#20851;&#30340;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \] where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12467</link><description>&lt;p&gt;
&#29702;&#35299;ReLU&#32593;&#32476;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#21160;&#24577;&#21644;&#20016;&#23500;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#25439;&#22833;&#30340;&#38750;&#20984;&#24615;&#20026;&#29702;&#35770;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25429;&#33719;&#20102;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#21040;&#26368;&#32456;&#25910;&#25947;&#30340;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#20063;&#21487;&#20197;&#34987;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#29702;&#35770;&#19978;&#25429;&#33719;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12066</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models. (arXiv:2305.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064; (MTL) &#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#12290;&#34429;&#28982;&#21333;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#23384;&#22312;&#30528;&#20960;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;: 1&#65289;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#21333;&#20219;&#21153;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#22914;&#20309;&#65311;2&#65289;&#33021;&#21542;&#35774;&#35745;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#21516;&#26102;&#25915;&#20987;&#22810;&#20010;&#20219;&#21153;&#65311; 3&#65289;&#20219;&#21153;&#20849;&#20139;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#21152;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65311;&#26412;&#25991;&#36890;&#36807;&#20180;&#32454;&#20998;&#26512;&#21644;&#20005;&#26684;&#30340;&#23454;&#39564;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21333;&#20219;&#21153;&#30333;&#30418;&#25915;&#20987;&#30340;&#21021;&#32423;&#36716;&#21270;&#24182;&#20998;&#26512;&#20102;&#20854;&#22266;&#26377;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25226;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#38382;&#39064;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#24179;&#22343;&#30456;&#23545;&#25439;&#22833;&#21464;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) creates a single machine learning model called multi-task model to simultaneously perform multiple tasks. Although the security of single task classifiers has been extensively studied, there are several critical security research questions for multi-task models including 1) How secure are multi-task models to single task adversarial machine learning attacks, 2) Can adversarial attacks be designed to attack multiple tasks simultaneously, and 3) Does task sharing and adversarial training increase multi-task model robustness to adversarial attacks? In this paper, we answer these questions through careful analysis and rigorous experimentation. First, we develop na\"ive adaptation of single-task white-box attacks and analyze their inherent drawbacks. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking a multi-task model as an optimization problem based on averaged relative loss change, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#20248;&#21270;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#25928;&#24212;&#22823;&#23567;&#30340;&#22686;&#30410;&#21644;&#25439;&#22833;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08559</link><description>&lt;p&gt;
&#35774;&#35745;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing Discontinuities. (arXiv:2305.08559v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#20248;&#21270;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#25928;&#24212;&#22823;&#23567;&#30340;&#22686;&#30410;&#21644;&#25439;&#22833;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36830;&#32493;&#24615;&#21487;&#20197;&#26159;&#30456;&#24403;&#20219;&#24847;&#30340;&#65292;&#20294;&#20063;&#20250;&#22312;&#31038;&#20250;&#31995;&#32479;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#30340;&#20219;&#24847;&#24615;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#34987;&#29992;&#20110;&#25512;&#26029;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#30340;&#22238;&#24402;&#19981;&#36830;&#32493;&#24615;&#20551;&#23450;&#23384;&#22312;&#19968;&#20010;&#19981;&#36830;&#32493;&#30340;&#21464;&#37327;&#65292;&#23558;&#20154;&#21475;&#20998;&#25104;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#20197;&#20272;&#35745;&#32473;&#23450;&#29616;&#35937;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20026;&#32473;&#23450;&#30340;&#19981;&#36830;&#32493;&#21464;&#37327;&#35774;&#35745;&#20998;&#21306;&#20197;&#20248;&#21270;&#20197;&#21069;&#20351;&#29992;&#22238;&#24402;&#19981;&#36830;&#32493;&#24615;&#30740;&#31350;&#36807;&#30340;&#26576;&#31181;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#26469;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;&#25928;&#26524;&#65292;&#39318;&#20808;&#23398;&#20064;&#32473;&#23450;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#22240;&#26524;&#25928;&#24212;&#22823;&#23567;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20248;&#21270;&#19981;&#36830;&#32493;&#24615;&#30340;&#37327;&#21270;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#22686;&#30410;&#21644;&#25439;&#22833;&#30340;&#25928;&#24212;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#24418;&#25104;&#21160;&#24577;&#35268;&#21010;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discontinuities can be fairly arbitrary but also cause a significant impact on outcomes in social systems. Indeed, their arbitrariness is why they have been used to infer causal relationships among variables in numerous settings. Regression discontinuity from econometrics assumes the existence of a discontinuous variable that splits the population into distinct partitions to estimate the causal effects of a given phenomenon. Here we consider the design of partitions for a given discontinuous variable to optimize a certain effect previously studied using regression discontinuity. To do so, we propose a quantization-theoretic approach to optimize the effect of interest, first learning the causal effect size of a given discontinuous variable and then applying dynamic programming for optimal quantization design of discontinuities that balance the gain and loss in the effect size. We also develop a computationally-efficient reinforcement learning algorithm for the dynamic programming formul
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14530</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#31181;&#23376;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#26032;&#30340;&#32452;&#21512;&#21644;&#22330;&#26223;&#20013;&#21512;&#25104;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#29983;&#25104;&#19981;&#24120;&#35265;&#30340;&#27010;&#24565;&#12289;&#32597;&#35265;&#30340;&#19981;&#23547;&#24120;&#32452;&#21512;&#25110;&#32467;&#26500;&#21270;&#27010;&#24565;&#65288;&#22914;&#25163;&#25484;&#65289;&#26041;&#38754;&#26377;&#22256;&#38590;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#37096;&#20998;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#38271;&#23614;&#24615;&#65306;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#20998;&#24067;&#23614;&#37096;&#30340;&#27010;&#24565;&#19978;&#34920;&#29616;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19981;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#31934;&#24515;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#65292;&#21487;&#20197;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#25216;&#26415;&#34987;&#31216;&#20026;SeedSelect&#12290;SeedSelect&#26159;&#39640;&#25928;&#30340;&#65292;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;SeedSelect&#30340;&#25928;&#30410;&#12290;&#39318;&#20808;&#65292;&#22312;&#23569;&#26679;&#26412;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#20013;&#65292;&#25105;&#20204;&#20026;&#23569;&#26679;&#26412;&#21644;&#38271;&#23614;&#22522;&#20934;&#29983;&#25104;&#20102;&#35821;&#20041;&#27491;&#30830;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can synthesize a large variety of concepts in new compositions and scenarios. However, they still struggle with generating uncommon concepts, rare unusual combinations, or structured concepts like hand palms. Their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. Here we characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, a technique that we call SeedSelect. SeedSelect is efficient and does not require retraining the diffusion model. We evaluate the benefit of SeedSelect on a series of problems. First, in few-shot semantic data augmentation, where we generate semantically correct images for few-shot and long-tail benchmarks. We show classificati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09870</link><description>&lt;p&gt;
&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23558;&#23427;&#20204;&#38480;&#21046;&#22312;&#21516;&#36136;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#19968;&#33324;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26680;&#24515;&#26159;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;&#26080;&#21442;&#25968;&#20849;&#20139;&#32422;&#26463;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335;&#24471;&#20986;&#20102;HATRPO&#21644;HAPPO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#24182;&#22312;&#22270;&#20687;&#24207;&#21015;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#30340;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08818</link><description>&lt;p&gt;
&#23558;&#28508;&#21464;&#37327;&#23545;&#40784;&#65306;&#20351;&#29992;&#28508;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#24182;&#22312;&#22270;&#20687;&#24207;&#21015;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#30340;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#36890;&#36807;&#22312;&#21387;&#32553;&#30340;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21512;&#25104;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#36807;&#22810;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#23558;LDM&#24212;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21333;&#29420;&#30340;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#22312;&#32534;&#30721;&#30340;&#22270;&#20687;&#24207;&#21015;&#65288;&#21363;&#35270;&#39057;&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23558;&#29983;&#25104;&#22120;&#20174;&#22270;&#20687;&#29983;&#25104;&#22120;&#36716;&#25442;&#20026;&#35270;&#39057;&#29983;&#25104;&#22120;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#22312;&#26102;&#38388;&#19978;&#23545;&#40784;&#25193;&#25955;&#27169;&#22411;&#19978;&#37319;&#26679;&#22120;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#30456;&#20851;&#30340;&#23454;&#38469;&#24212;&#29992;&#65306;&#37326;&#22806;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#24314;&#27169;&#30340;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#20998;&#36776;&#29575;&#20026;512 x 1024&#30340;&#30495;&#23454;&#39550;&#39542;&#35270;&#39057;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35270;&#39057;LDM&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trai
&lt;/p&gt;</description></item><item><title>ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.05977</link><description>&lt;p&gt;
ImageReward&#65306;&#23398;&#20064;&#21644;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;
&lt;/p&gt;
&lt;p&gt;
ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05977
&lt;/p&gt;
&lt;p&gt;
ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;ImageReward&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#22522;&#20110;&#25105;&#20204;&#30340;&#31995;&#32479;&#27880;&#37322;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20998;&#21644;&#25490;&#21517;&#32452;&#20214;&#65292;&#36804;&#20170;&#24050;&#25910;&#38598;&#20102;137k&#30340;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#38598;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;ImageReward&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#20363;&#22914;&#27604;CLIP&#39640;38.6\%&#65289;&#65292;&#22240;&#27492;&#23427;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;\texttt {image-reward}&#31243;&#24207;&#21253;&#20844;&#24320;&#25552;&#20379;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/THUDM/ImageReward}&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#30340;&#33258;&#21160;&#35843;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05430</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#30340;&#33258;&#21160;&#35843;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#38656;&#35201;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20026;&#32473;&#23450;&#31243;&#24207;&#25628;&#32034;&#21508;&#31181;&#21487;&#33021;&#30340;&#31243;&#24207;&#36716;&#25442;&#32452;&#21512;&#65292;&#20197;&#20248;&#21270;&#24352;&#37327;&#31243;&#24207;&#30340;&#25191;&#34892;&#12290;&#30001;&#20110;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#25351;&#25968;&#32423;&#21035;&#30340;&#21464;&#25442;&#32452;&#21512;&#65292;&#33258;&#21160;&#35843;&#25972;&#24352;&#37327;&#31243;&#24207;&#30340;&#29983;&#25104;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#24403;&#38656;&#35201;&#38754;&#23545;&#24322;&#26500;&#30340;&#30446;&#26631;&#26102;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#31227;&#21040;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;TenSet&#65292;&#22312;&#27979;&#35797;&#38598;&#20998;&#21106;&#31574;&#30053;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#27880;&#24847;&#21147;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20026;&#35843;&#25972;&#24352;&#37327;&#31243;&#24207;&#25552;&#20379;&#25903;&#25345;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#31934;&#31616;&#39640;&#36798;45&#65285;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;Pairwise Comparis&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\% of the baseline without compromising the Pairwise Comparis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04027</link><description>&lt;p&gt;
NeBLa: &#20351;&#29992;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#37325;&#24314;&#21475;&#33108;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;X&#32447;&#29255;&#65288;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#65292;PX&#65289;&#26159;&#24120;&#29992;&#20110;&#29273;&#31185;&#26816;&#26597;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;3D&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#30456;&#27604;&#65292;PX&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;PX&#21482;&#25552;&#20379;&#21475;&#33108;&#32467;&#26500;&#30340;&#20108;&#32500;&#25153;&#24179;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#30340;PX&#22270;&#20687;&#20272;&#35745;3D&#21475;&#33108;&#32467;&#26500;&#12290;&#30001;&#20110;PX&#21644;CBCT&#25968;&#25454;&#30340;&#21305;&#37197;&#19981;&#22810;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#20102;&#20174;CBCT&#27169;&#25311;&#30340;PX&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#20102;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#32447;&#37319;&#26679;&#26041;&#27861;&#65292;&#21463;&#21040;&#20840;&#26223;&#25918;&#23556;&#32447;&#25104;&#20687;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#21860;&#37202;-&#20848;&#20271;&#29305;&#23450;&#24459;&#23548;&#20986;&#28210;&#26579;&#20989;&#25968;&#29983;&#25104;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#36716;&#25442;&#27169;&#22359;&#65292;&#29983;&#25104;&#27169;&#22359;&#21644;&#31934;&#28860;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23558;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;&#35757;&#32451;&#22270;&#20687;&#39118;&#26684;&#12290;&#29983;&#25104;&#27169;&#22359;&#21033;&#29992;&#23556;&#32447;&#37319;&#26679;&#26041;&#27861;&#24471;&#21040;&#30340;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#32422;&#26463;&#19979;&#30340;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#32467;&#26500;&#12290;&#31934;&#28860;&#27169;&#22359;&#25913;&#21892;&#20102;3D&#32467;&#26500;&#30340;&#24179;&#28369;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#29273;&#31185;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.14582</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#30456;&#20851;&#28304;&#20219;&#21153;&#26469;&#35757;&#32451;&#20302;&#36164;&#28304;&#30446;&#26631;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#25152;&#26377;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#31616;&#21333;&#32452;&#21512;&#24182;&#19981;&#24635;&#26159;&#33021;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#20026;&#20250;&#23384;&#22312;&#36127;&#36801;&#31227;&#12290;&#22240;&#27492;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#30410;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23376;&#38598;&#30340;&#25968;&#37327;&#38543;&#30528;&#28304;&#20219;&#21153;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#20195;&#29702;&#24314;&#27169;&#20013;&#65292;&#25105;&#20204;&#23545;&#28304;&#20219;&#21153;&#36827;&#34892;&#37319;&#26679;&#65288;&#38543;&#26426;&#65289;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#23427;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#34920;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36924;&#36817;&#39044;&#20808;&#35745;&#31639;&#30340;&#34920;&#29616;&#65292;&#35813;&#27169;&#22411;&#20063;&#21487;&#29992;&#20110;&#39044;&#27979;&#26410;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#31034;&#20363;&#21644;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03428</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#65292;&#20854;&#29942;&#39048;&#21253;&#25324;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12289;&#21151;&#32791;&#21644;&#26102;&#38388;&#65292;&#26082;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#20063;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#21487;&#33021;&#20250;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#32553;&#25918;&#20026; $\mathcal{O}(T^2 \times \text{polylog}(n))$&#65292;&#20854;&#20013; $n$ &#26159;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;$T$ &#26159;&#35757;&#32451;&#20013;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#12290;&#22522;&#20110;&#26089;&#26399;&#29992;&#20110;&#32791;&#25955;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#31639;&#27861;&#21487;&#29992;&#20110;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23545;&#25317;&#26377;&#20174;700&#19975;&#21040;1.03&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#26174;&#28982;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;MDF-Net&#65292;&#23558;&#20020;&#24202;&#25968;&#25454;&#19982;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30142;&#30149;&#23450;&#20301;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.13390</link><description>&lt;p&gt;
MDF-Net&#65306;&#32467;&#21512;X&#23556;&#32447;&#19982;&#20020;&#24202;&#25968;&#25454;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data. (arXiv:2302.13390v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;MDF-Net&#65292;&#23558;&#20020;&#24202;&#25968;&#25454;&#19982;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30142;&#30149;&#23450;&#20301;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#24739;&#32773;&#30340;&#20020;&#24202;&#20449;&#24687;&#21152;&#20837;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30142;&#30149;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#20998;&#31867;&#22120;&#22312;&#20351;&#29992;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#26102;&#20855;&#26377;&#39640;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#23545;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35775;&#35848;&#34920;&#26126;&#65292;&#20020;&#24202;&#25968;&#25454;&#23545;&#20110;&#35299;&#37322;&#22270;&#20687;&#21644;&#20570;&#20986;&#27491;&#30830;&#30340;&#35786;&#26029;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#20855;&#26377;&#24456;&#39640;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#30001;&#20004;&#31181;&#34701;&#21512;&#26041;&#27861;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#65288;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#21644;&#33016;&#37096;X&#23556;&#32447;&#65288;&#22270;&#20687;&#25968;&#25454;&#65289;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#25968;&#25454;&#27169;&#24577;&#22312;&#19981;&#21516;&#30340;&#32500;&#24230;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#25490;&#21015;&#31574;&#30053;&#31354;&#38388;&#21270;&#65292;&#20197;&#20415;&#22312;Mask R-CNN&#27169;&#22411;&#20013;&#20419;&#36827;&#22810;&#27169;&#24577;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;MIMIC-Eye&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65306;MIMIC-CXR&#65288;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65289;&#12289;MIMIC IV-ED&#65288;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#65289;&#21644;REFLACX&#65288;&#35780;&#20272;&#27880;&#37322;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MDF-Net&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the effects of including patients' clinical information on the performance of deep learning (DL) classifiers for disease location in chest X-ray images. Although current classifiers achieve high performance using chest X-ray images alone, our interviews with radiologists indicate that clinical data is highly informative and essential for interpreting images and making proper diagnoses.  In this work, we propose a novel architecture consisting of two fusion methods that enable the model to simultaneously process patients' clinical data (structured data) and chest X-rays (image data). Since these data modalities are in different dimensional spaces, we propose a spatial arrangement strategy, spatialization, to facilitate the multimodal learning process in a Mask R-CNN model. We performed an extensive experimental evaluation using MIMIC-Eye, a dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED (patients' clinical data), and REFLACX (annotatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#31216;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03791</link><description>&lt;p&gt;
&#22914;&#20309;&#20449;&#20219;&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#31181;&#20984;&#20248;&#21270;&#26041;&#27861;&#24212;&#23545;&#31526;&#21512;&#39118;&#38505;&#25511;&#21046;&#30340;&#22240;&#24335;&#20998;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#31216;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31616;&#31216;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#35201;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#32487;&#32493;&#22686;&#38271;&#12290;&#23613;&#31649;&#23427;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;&#32463;&#39564;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#26679;&#26412;&#65292;&#20294;&#22312;&#20854;&#36127;&#36131;&#20219;&#22320;&#29992;&#20110;&#20851;&#38190;&#22330;&#26223;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#20173;&#23384;&#22312;&#37325;&#35201;&#38382;&#39064;&#12290;&#25910;&#25947;&#39044;&#27979;&#26159;&#19968;&#31181;&#29616;&#20195;&#24037;&#20855;&#65292;&#29992;&#20110;&#20026;&#20219;&#20309;&#40657;&#30418;&#23376;&#39044;&#27979;&#22120;&#26500;&#24314;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;$(i)$&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;$(ii)$&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#25910;&#25947;&#39118;&#38505;&#25511;&#21046;&#36807;&#31243;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#20381;&#38752;&#19968;&#31181;&#26032;&#22411;&#30340;&#20984;&#20248;&#21270;&#20844;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#20687;&#21040;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#19978;&#20351;&#29992;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#31243;&#24207;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#26657;&#20934;&#21644;&#33391;&#22909;&#25511;&#21046;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise calibrated intervals for future samples of any diffusion model, and $(ii)$ control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02288</link><description>&lt;p&gt;
gRoMA: &#19968;&#31181;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02288
&lt;/p&gt;
&lt;p&gt;
gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#21069;&#27839;&#25216;&#26415;&#30340;&#20195;&#34920;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33322;&#31354;&#25110;&#27773;&#36710;&#39046;&#22495;&#65289;&#26102;&#65292;&#30001;&#20110;&#23545;&#25239;&#24615;&#36755;&#20837;&#65288;&#21363;&#21487;&#33021;&#23548;&#33268;DNN&#29359;&#38169;&#30340;&#36755;&#20837;&#25200;&#21160;&#65289;&#30340;&#23041;&#32961;&#65292;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#21363;&#20415;&#26159;&#29616;&#20195;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24517;&#39035;&#27979;&#37327;&#24182;&#38477;&#20302;&#36825;&#31181;&#39118;&#38505;&#25165;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#37096;&#32626;DNN&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;gRoMA&#65288;&#20840;&#23616;&#40065;&#26834;&#24615;&#27979;&#37327;&#21644;&#35780;&#20272;&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#26469;&#27979;&#37327;DNN&#30340;&#20840;&#23616;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;gRoMA&#27979;&#37327;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36935;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#20135;&#29983;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;DNN&#22312;&#28909;&#38376;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2301.00812</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23454;&#29616;&#20102;&#25163;&#26415;&#25216;&#33021;&#30340;&#33258;&#21160;&#21644;&#23458;&#35266;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#12290;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#36807;&#28193;&#21040;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#27169;&#25311;&#22120;&#19978;&#24320;&#21457;&#20102;A-VBANet&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#30340;&#25163;&#26415;&#23460;&#35270;&#39057;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.5%&#65288;&#19968;&#27425;&#24615;&#65289;&#21644;99.9%&#65288;&#23569;&#37327;&#26679;&#26412;&#65289;&#65292;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;89.7%&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#25163;&#26415;&#27169;&#25311;&#22120;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25163;&#26415;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#27979;&#35797;&#36807;&#31243;SHAP-XRT&#65292;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2207.07038</link><description>&lt;p&gt;
SHAP-XRT: Shapley Value&#36935;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SHAP-XRT: The Shapley Value Meets Conditional Independence Testing. (arXiv:2207.07038v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#27979;&#35797;&#36807;&#31243;SHAP-XRT&#65292;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;Shapley&#20540;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26368;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#20043;&#19968;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#29305;&#24449;&#37325;&#35201;&#24615;&#26159;&#36890;&#36807;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23450;&#20041;&#30340;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20004;&#31181;&#35299;&#37322;&#26041;&#27861;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#34987;&#35748;&#20026;&#26159;&#20998;&#24320;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SHAPley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT)&#65292;&#19968;&#31181;&#21463;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;(CRT)&#21551;&#21457;&#30340;&#27979;&#35797;&#36807;&#31243;&#65292;&#29992;&#20110;&#26816;&#39564;&#29305;&#23450;&#27010;&#24565;&#30340;&#23616;&#37096;&#65288;&#22312;&#26679;&#26412;&#19978;&#30340;&#65289;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;SHAP-XRT&#65292;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the $\textbf{SHAP}$ley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley valu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#20013;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#65292;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#65292;&#24182;&#25552;&#20986;&#21333;&#27874;&#23454;&#39564;&#21644;&#22810;&#27874;&#23454;&#39564;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28322;&#20986;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.08174</link><description>&lt;p&gt;
&#26410;&#30693;&#24178;&#25200;&#23454;&#39564;&#20013;&#30340;&#25919;&#31574;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Policy design in experiments with unknown interference. (arXiv:2011.08174v7 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#20013;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#65292;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#65292;&#24182;&#25552;&#20986;&#21333;&#27874;&#23454;&#39564;&#21644;&#22810;&#27874;&#23454;&#39564;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28322;&#20986;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28322;&#20986;&#25928;&#24212;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#23558;&#21333;&#20803;&#32452;&#32455;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#65292;&#24182;&#22312;&#27599;&#20010;&#31751;&#20869;&#20197;&#26410;&#30693;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#12290;&#20316;&#20026;&#31532;&#19968;&#39033;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27874;&#23454;&#39564;&#65292;&#36890;&#36807;&#22312;&#31751;&#23545;&#38388;&#20180;&#32454;&#21464;&#21270;&#38543;&#26426;&#21270;&#65292;&#32771;&#34385;&#28322;&#20986;&#25928;&#24212;&#20272;&#35745;&#27835;&#30103;&#27010;&#29575;&#21464;&#21270;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#21033;&#29992;&#36825;&#20010;&#36793;&#38469;&#25928;&#24212;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26816;&#39564;&#25919;&#31574;&#26368;&#20248;&#24615;&#30340;&#27979;&#35797;&#12290;&#20316;&#20026;&#31532;&#20108;&#39033;&#36129;&#29486;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27874;&#23454;&#39564;&#65292;&#20272;&#35745;&#27835;&#30103;&#35268;&#21017;&#24182;&#26368;&#22823;&#21270;&#31119;&#21033;&#12290;&#26412;&#25991;&#23545;&#26368;&#22823;&#21487;&#36798;&#31119;&#21033;&#20110;&#25152;&#20272;&#35745;&#25919;&#31574;&#35780;&#20272;&#19979;&#31119;&#21033;&#20043;&#38388;&#30340;&#24046;&#24322;&#32473;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#23567;&#26679;&#26412;&#20445;&#35777;&#12290;&#20316;&#32773;&#22312;&#26681;&#25454;&#29616;&#26377;&#20851;&#20110;&#20449;&#24687;&#20256;&#25773;&#21644;&#29616;&#37329;&#36716;&#31227;&#35745;&#21010;&#30340;&#23454;&#39564;&#27169;&#25311;&#21644;&#22823;&#35268;&#27169;&#29616;&#22330;&#23454;&#39564;&#20013;&#25552;&#20379;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies experimental designs for estimation and inference on welfare-maximizing policies in the presence of spillover effects. Units are organized into a finite number of large clusters and interact in unknown ways within each cluster. As a first contribution, I introduce a single-wave experiment that, by carefully varying the randomization across cluster pairs, estimates the marginal effect of a change in treatment probabilities, taking spillover effects into account. Using the marginal effect, I propose a test for policy optimality. As a second contribution, I design a multiple-wave experiment to estimate treatment rules and maximize welfare. I derive strong small-sample guarantees on the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. I illustrate the method's properties in simulations calibrated to existing experiments on information diffusion and cash-transfer programs, and in a large scale field experiment implemente
&lt;/p&gt;</description></item><item><title>Gasper&#26159;&#19968;&#20010;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2007.10642</link><description>&lt;p&gt;
Gasper&#65306;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Gasper: GrAph Signal ProcEssing in R. (arXiv:2007.10642v4 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.10642
&lt;/p&gt;
&lt;p&gt;
Gasper&#26159;&#19968;&#20010;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;R&#35821;&#35328;&#30340;gasper&#21253;&#30340;&#31616;&#30701;&#25945;&#31243;&#12290;Gasper&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#36824;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a short tutorial on to the use of the \proglang{R} \pkg{gasper} package. Gasper is a package dedicated to signal processing on graphs. It also provides an interface to the SuiteSparse Matrix Collection.
&lt;/p&gt;</description></item></channel></rss>