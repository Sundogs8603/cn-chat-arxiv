<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; $\infty$-Diff &#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.18242</link><description>&lt;p&gt;
&#26080;&#31351;&#20998;&#36776;&#29575;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States. (arXiv:2303.18242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18242
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; $\infty$-Diff &#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;$\infty$-Diff&#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#38543;&#26426;&#25277;&#26679;&#25968;&#25454;&#30340;&#23376;&#38598;&#24182;&#23398;&#20064;&#21435;&#22122;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#21040;&#19968;&#20010;&#36830;&#32493;&#30340;&#20989;&#25968;&#65292;&#20801;&#35768;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;&#37319;&#26679;&#12290;&#19982;&#20854;&#20182;&#26368;&#36817;&#30340;&#26080;&#38480;&#20998;&#36776;&#29575;&#29983;&#25104;&#24335;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#25805;&#20316;&#21407;&#22987;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#20363;&#22914;&#38477;&#20302;FID&#20998;&#25968;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\infty$-Diff, a generative diffusion model which directly operates on infinite resolution data. By randomly sampling subsets of coordinates during training and learning to denoise the content at those coordinates, a continuous function is learned that allows sampling at arbitrary resolutions. In contrast to other recent infinite resolution generative models, our approach operates directly on the raw data, not requiring latent vector compression for context, using hypernetworks, nor relying on discrete components. As such, our approach achieves significantly higher sample quality, as evidenced by lower FID scores, as well as being able to effectively scale to higher resolutions than the training data while retaining detail.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18240</link><description>&lt;p&gt;
&#23547;&#25214;&#20855;&#26377;&#36523;&#20307;&#26234;&#33021;&#30340;&#20154;&#24037;&#35270;&#35273;&#30382;&#23618;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#65288;PVR&#65289;&#25110;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#29992;&#20110;&#36523;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102; CortexBench&#65292;&#20854;&#20013;&#21253;&#25324;&#28085;&#30422;&#21160;&#21147;&#23398;&#12289;&#23548;&#33322;&#12289;&#29087;&#32451;&#21644;&#31227;&#21160;&#25805;&#20316;&#30340;17&#31181;&#19981;&#21516;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;PVR&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26469;&#33258;7&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#36229;&#36807;4000&#23567;&#26102;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65288;&#36229;&#36807;560&#19975;&#24352;&#22270;&#20687;&#65289;&#21644;ImageNet&#65292;&#20351;&#29992;&#20999;&#29255;&#25968;&#25454;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;&#65288;MAE&#65289;&#26469;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#24418;&#22120;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#25512;&#26029;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#65288;&#20294;&#24179;&#22343;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#65289;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#21517;&#20026;VC-1&#65292;&#24179;&#22343;&#34920;&#29616;&#36229;&#36807;&#25152;&#26377;&#20808;&#21069;&#30340;PVR&#65292;&#20294;&#20063;&#27809;&#26377;&#26222;&#36941;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VC-1&#30340;&#29305;&#23450;&#20110;&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;&#36866;&#24212;&#20250;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#19981;&#21464;&#20998;&#21464;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#35299;&#20915;&#22312;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31163;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#21464;&#37327;&#22240;&#32032;&#26469;&#21457;&#29616;&#29289;&#29702;&#21644;&#21270;&#23398;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.18236</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#32422;&#34920;&#31034;&#33719;&#21462;&#29289;&#29702;&#21644;&#21270;&#23398;&#20449;&#24687;&#65306;&#19981;&#21464;&#20998;&#21464;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Physics and Chemistry from Parsimonious Representations: Image Analysis via Invariant Variational Autoencoders. (arXiv:2303.18236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#19981;&#21464;&#20998;&#21464;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#35299;&#20915;&#22312;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31163;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#21464;&#37327;&#22240;&#32032;&#26469;&#21457;&#29616;&#29289;&#29702;&#21644;&#21270;&#23398;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#12289;&#20809;&#23398;&#21644;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#38236;&#25216;&#26415;&#27491;&#22312;&#20135;&#29983;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#21407;&#23376;&#21644;&#20013;&#31561;&#23610;&#24230;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#20449;&#24687;&#12290;&#36825;&#38656;&#35201;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29289;&#29702;&#21644;&#21270;&#23398;&#29616;&#35937;&#65292;&#20363;&#22914;&#30005;&#23376;&#21644;&#25195;&#25551;&#38567;&#36947;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#34920;&#29616;&#65292;&#32435;&#31859;&#39063;&#31890;&#30340;&#21464;&#24322;&#24615;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#20998;&#26512;&#33539;&#20363;&#65292;&#20801;&#35768;&#20998;&#35299;&#21464;&#37327;&#22240;&#32032;&#24182;&#21457;&#29616;&#26368;&#20339;&#31616;&#27905;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;VAEs&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#28085;&#30422;&#20102;VAEs&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#30452;&#35273;&#12290;&#20171;&#32461;&#20102;&#19981;&#21464;VAEs&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#20110;&#25104;&#20687;&#25968;&#25454;&#20013;&#30340;&#23610;&#24230;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#24182;&#23558;&#24050;&#30693;&#21464;&#21270;&#22240;&#32032;&#19982;&#24453;&#21457;&#29616;&#22240;&#32032;&#20998;&#24320;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#19981;&#21464;VAEs&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#21457;&#29616;&#23545;&#31216;&#24615;&#30772;&#32570;&#29616;&#35937;&#21644;&#35782;&#21035;&#21270;&#23398;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electron, optical, and scanning probe microscopy methods are generating ever increasing volume of image data containing information on atomic and mesoscale structures and functionalities. This necessitates the development of the machine learning methods for discovery of physical and chemical phenomena from the data, such as manifestations of symmetry breaking in electron and scanning tunneling microscopy images, variability of the nanoparticles. Variational autoencoders (VAEs) are emerging as a powerful paradigm for the unsupervised data analysis, allowing to disentangle the factors of variability and discover optimal parsimonious representation. Here, we summarize recent developments in VAEs, covering the basic principles and intuition behind the VAEs. The invariant VAEs are introduced as an approach to accommodate scale and translation invariances present in imaging data and separate known factors of variations from the ones to be discovered. We further describe the opportunities ena
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.18211</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#25490;&#24207;&#26631;&#20934;&#26377;&#21161;&#20110;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models. (arXiv:2303.18211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18211
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21151;&#33021;&#20551;&#35774;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#30001;&#20110;&#32570;&#20047;&#31526;&#21512;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21512;&#25104;ANM&#25968;&#25454;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;Reisach&#31561;&#20154;&#65288;2021&#65289;&#34920;&#26126;&#65292;&#23545;&#20110;&#24120;&#35265;&#30340;&#27169;&#25311;&#21442;&#25968;&#65292;&#25353;&#22686;&#22823;&#26041;&#24046;&#30340;&#39034;&#24207;&#21464;&#37327;&#25490;&#21015;&#19982;&#22240;&#26524;&#39034;&#24207;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#24341;&#20837;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#26469;&#37327;&#21270;&#36825;&#31181;&#23545;&#40784;&#31243;&#24230;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#38500;&#20102;&#26041;&#24046;&#65292;&#36824;&#26377;&#21464;&#37327;&#30340;&#26041;&#24046;&#34987;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#35299;&#37322;&#30340;&#27604;&#20363;&#65288;&#30001;&#20915;&#23450;&#31995;&#25968;$R^2$&#25429;&#33719;&#65289;&#20542;&#21521;&#20110;&#27839;&#30528;&#22240;&#26524;&#39034;&#24207;&#22686;&#21152;&#12290;&#31616;&#21333;&#30340;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;$R^2$-sortability&#26469;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;$R^2$&#21487;&#25490;&#24207;&#24615;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26631;&#20934;&#21270;&#25110;&#37325;&#26032;&#32553;&#25918;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#30340;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive Noise Models (ANM) encode a popular functional assumption that enables learning causal structure from observational data. Due to a lack of real-world data meeting the assumptions, synthetic ANM data are often used to evaluate causal discovery algorithms. Reisach et al. (2021) show that, for common simulation parameters, a variable ordering by increasing variance is closely aligned with a causal order and introduce var-sortability to quantify the alignment. Here, we show that not only variance, but also the fraction of a variable's variance explained by all others, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. Simple baseline algorithms can use $R^2$-sortability to match the performance of established methods. Since $R^2$-sortability is invariant under data rescaling, these algorithms perform equally well on standardized or rescaled data, addressing a key limitation of algorithms exploiting var-sortability. We characterize and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20551;&#35774;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;SimTS&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18205</link><description>&lt;p&gt;
SimTS:&#37325;&#26032;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting. (arXiv:2303.18205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20551;&#35774;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;SimTS&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#25110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#23398;&#20064;&#24847;&#20041;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#19981;&#22815;&#26126;&#26174;&#65292;&#22240;&#20026;&#23545;&#23454;&#20363;&#37492;&#21035;&#30340;&#20248;&#21270;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#20174;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#25216;&#26415;&#20013;&#27491;&#36127;&#31034;&#20363;&#23545;&#30340;&#26500;&#24314;&#24378;&#28872;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimTS&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;SimTS&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SimTS&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning methods have shown an impressive ability to learn meaningful representations for image or time series classification. However, these methods are less effective for time series forecasting, as optimization of instance discrimination is not directly applicable to predicting the future state from the history context. Moreover, the construction of positive and negative pairs in current technologies strongly relies on specific time series characteristics, restricting their generalization across diverse types of time series data. To address these limitations, we propose SimTS, a simple representation learning approach for improving time series forecasting by learning to predict the future from the past in the latent space. SimTS does not rely on negative pairs or specific assumptions about the characteristics of the particular time series. Our extensive experiments on several benchmark time series forecasting datasets show that SimTS achieves competitive performance comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.18201</link><description>&lt;p&gt;
TPMCF: &#20351;&#29992;&#22810;&#28304;&#21327;&#21516;&#29305;&#24449;&#36827;&#34892;&#26102;&#38388;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#26381;&#21153;API&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#22686;&#38271;&#20013;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20915;&#23450;&#26381;&#21153;&#24615;&#33021;&#30340;&#26381;&#21153;&#36136;&#37327;(QoS)&#21442;&#25968;&#32463;&#24120;&#34987;&#29992;&#20110;&#25512;&#33616;&#65292;&#20294;&#38543;&#26102;&#38388;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;QoS&#30340;&#39044;&#27979;&#23545;&#20110;&#22312;&#31561;&#20215;&#26381;&#21153;&#20013;&#35782;&#21035;&#21512;&#36866;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20195;&#30340;&#26102;&#38388;QoS&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#21508;&#31181;&#38480;&#21046;&#32780;&#24456;&#38590;&#36798;&#21040;&#26399;&#26395;&#30340;&#31934;&#24230;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#20197;&#21450;&#25429;&#33719;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#24314;&#27169;QoS&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21327;&#20316;&#29305;&#24449;&#65289;&#26469;&#29702;&#35299;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#27979;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;TPMCF&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;TPMCF&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#65288;&#21253;&#25324;&#26102;&#38388;&#12289;&#29992;&#25143;&#21644;&#26381;&#21153;&#29305;&#24449;&#65289;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#39062;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#21033;&#29992;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TPMCF&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
&lt;/p&gt;</description></item><item><title>PADME-SoSci&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#22312;&#21407;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#23398;&#20064;&#26469;&#23454;&#29616;&#25968;&#25454;&#25152;&#26377;&#26435;&#20445;&#25252;&#21644;&#36328;&#20301;&#32622;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2303.18200</link><description>&lt;p&gt;
PADME-SoSci&#65306;&#31038;&#20250;&#31185;&#23398;&#20013;&#29992;&#20110;&#20998;&#26512;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences. (arXiv:2303.18200v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18200
&lt;/p&gt;
&lt;p&gt;
PADME-SoSci&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#22312;&#21407;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#23398;&#20064;&#26469;&#23454;&#29616;&#25968;&#25454;&#25152;&#26377;&#26435;&#20445;&#25252;&#21644;&#36328;&#20301;&#32622;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#25968;&#25454;&#38544;&#31169;&#21644;&#25152;&#26377;&#26435;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#24403;&#19981;&#21516;&#26041;&#25317;&#26377;&#25968;&#25454;&#30340;&#19981;&#21516;&#37096;&#20998;&#26102;&#65292;&#20849;&#20139;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#19968;&#31181;&#24212;&#23545;&#25361;&#25112;&#30340;&#26041;&#27861;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#20043;&#21069;&#23558;&#25968;&#25454;&#24212;&#29992;&#21435;&#35782;&#21035;&#21270;&#25110;&#21311;&#21517;&#21270;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#38477;&#20302;&#25968;&#25454;&#25928;&#29992;&#24182;&#22686;&#21152;&#37325;&#26032;&#35782;&#21035;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PADME&#65292;&#36825;&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#32852;&#37030;&#20102;&#27169;&#22411;&#23454;&#29616;&#21644;&#35757;&#32451;&#12290;PADME&#20351;&#29992;&#32852;&#37030;&#26041;&#27861;&#65292;&#27169;&#22411;&#30001;&#25152;&#26377;&#26041;&#23454;&#29616;&#21644;&#37096;&#32626;&#65292;&#24182;&#36880;&#27493;&#35775;&#38382;&#27599;&#20010;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36328;&#20301;&#32622;&#20998;&#26512;&#25968;&#25454;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#20687;&#25152;&#26377;&#25968;&#25454;&#37117;&#22312;&#21516;&#19968;&#20301;&#32622;&#19968;&#26679;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25968;&#25454;&#30340;&#21407;&#22987;&#20301;&#32622;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20445;&#30041;&#25968;&#25454;&#25152;&#26377;&#26435;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#25968;&#25454;&#20301;&#32622;&#19978;&#30340;&#20998;&#26512;&#37117;&#23436;&#25104;&#21518;&#65292;&#25165;&#20250;&#25552;&#20379;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and ownership are significant in social data science, raising legal and ethical concerns. Sharing and analyzing data is difficult when different parties own different parts of it. An approach to this challenge is to apply de-identification or anonymization techniques to the data before collecting it for analysis. However, this can reduce data utility and increase the risk of re-identification. To address these limitations, we present PADME, a distributed analytics tool that federates model implementation and training. PADME uses a federated approach where the model is implemented and deployed by all parties and visits each data location incrementally for training. This enables the analysis of data across locations while still allowing the model to be trained as if all data were in a single location. Training the model on data in its original location preserves data ownership. Furthermore, the results are not provided until the analysis is completed on all data locations to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Generative Volumetric Primitives (GVP)&#29983;&#25104;&#20307;&#32032;&#20803;&#32032;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#37319;&#26679;&#21644;&#28210;&#26579;512&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#32431;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;2D&#21367;&#31215;&#32593;&#32476;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#20010;&#20307;&#31215;&#20803;&#32032;&#21644;&#23427;&#20204;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#33021;&#22815;&#25429;&#25417;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#23545;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#23454;&#29616;&#39640;&#33258;&#30001;&#24230;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;GVP&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#20174;&#22270;&#20687;&#36136;&#37327;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#20004;&#26041;&#38754;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.18193</link><description>&lt;p&gt;
GVP: &#29983;&#25104;&#20307;&#32032;&#20803;&#32032;
&lt;/p&gt;
&lt;p&gt;
GVP: Generative Volumetric Primitives. (arXiv:2303.18193v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Generative Volumetric Primitives (GVP)&#29983;&#25104;&#20307;&#32032;&#20803;&#32032;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#37319;&#26679;&#21644;&#28210;&#26579;512&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#32431;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;2D&#21367;&#31215;&#32593;&#32476;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#20010;&#20307;&#31215;&#20803;&#32032;&#21644;&#23427;&#20204;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#33021;&#22815;&#25429;&#25417;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#23545;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#23454;&#29616;&#39640;&#33258;&#30001;&#24230;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;GVP&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#20174;&#22270;&#20687;&#36136;&#37327;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#20004;&#26041;&#38754;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#20855;&#26377;&#26174;&#24335;&#25668;&#20687;&#26426;&#25511;&#21046;&#30340;&#22270;&#20687;&#21512;&#25104;&#36793;&#30028;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#65292;&#24050;&#32463;&#23581;&#35797;&#35774;&#35745;&#20102;&#19968;&#20123;&#39640;&#25928;&#30340;&#21457;&#29983;&#22120;&#65292;&#20363;&#22914;&#20855;&#26377;3D&#21644;2D&#32452;&#20214;&#30340;&#28151;&#21512;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#20250;&#25439;&#23475;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#65292;&#32780;&#39640;&#20998;&#36776;&#29575;&#30340;&#32431;3D&#29983;&#25104;&#22120;&#30340;&#35774;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#20307;&#32032;&#20803;&#32032;&#65288;GVP&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#37319;&#26679;&#21644;&#28210;&#26579;512&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#32431;3D&#29983;&#25104;&#27169;&#22411;&#12290;GVP&#20849;&#21516;&#27169;&#25311;&#20102;&#22810;&#20010;&#20307;&#31215;&#20803;&#32032;&#21644;&#23427;&#20204;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#36825;&#20004;&#20010;&#20803;&#32032;&#37117;&#21487;&#20197;&#36890;&#36807;2D&#21367;&#31215;&#32593;&#32476;&#39640;&#25928;&#22320;&#29983;&#25104;&#12290;&#36825;&#20123;&#20803;&#32032;&#30340;&#28151;&#21512;&#33258;&#28982;&#22320;&#25429;&#25417;&#20102;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#23545;&#24212;&#24615;&#12290;&#36825;&#31181;&#39640;&#24230;&#33258;&#30001;&#24230;&#30340;&#21457;&#29983;&#22120;&#30340;&#35757;&#32451;&#26159;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#23454;&#29616;&#30340;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GVP&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in 3D-aware generative models have pushed the boundary of image synthesis with explicit camera control. To achieve high-resolution image synthesis, several attempts have been made to design efficient generators, such as hybrid architectures with both 3D and 2D components. However, such a design compromises multiview consistency, and the design of a pure 3D generator with high resolution is still an open problem. In this work, we present Generative Volumetric Primitives (GVP), the first pure 3D generative model that can sample and render 512-resolution images in real-time. GVP jointly models a number of volumetric primitives and their spatial information, both of which can be efficiently generated via a 2D convolutional network. The mixture of these primitives naturally captures the sparsity and correspondence in the 3D volume. The training of such a generator with a high degree of freedom is made possible through a knowledge distillation technique. Experiments on several datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#36890;&#36807;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#65292;&#19981;&#38656;&#35201;&#20854;&#20182;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18191</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#22351;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#30340;&#25512;&#29702;&#38454;&#27573;&#21518;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. (arXiv:2303.18191v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#36890;&#36807;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#65292;&#19981;&#38656;&#35201;&#20854;&#20182;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#26816;&#27979;&#35302;&#21457;&#26679;&#26412;&#65292;&#21363;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#65292;&#21487;&#20197;&#38450;&#27490;&#21518;&#38376;&#34987;&#35302;&#21457;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#38450;&#24481;&#32773;&#23545;&#21463;&#23475;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12289;&#39069;&#22806;&#30340;&#28165;&#27905;&#25968;&#25454;&#25110;&#20102;&#35299;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#22806;&#35266;&#30693;&#35782;&#31561;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#19968;&#39033;&#26377;&#36259;&#30340;&#35266;&#23519;&#24320;&#22987;&#65292;&#21363;&#34987;&#24863;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#23545;&#20110;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21516;&#22270;&#20687;&#30772;&#22351;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#35302;&#21457;&#26679;&#26412;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102; TeCo &#26469;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#30340;&#20559;&#24046;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality. In this paper, we propose the test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra information. Our journey begins with the intriguing observation that the backdoor-infected models have similar performance across different image corruptions for the clean images, but perform discrepantly for the trigger samples. Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36882;&#24402;&#30005;&#36335;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#26159;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#20998;&#24067;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.18187</link><description>&lt;p&gt;
&#21033;&#29992;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#21069;&#21521;&#36807;&#31243;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Spiking Neural Systems with the Event-Driven Forward-Forward Process. (arXiv:2303.18187v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36882;&#24402;&#30005;&#36335;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#26159;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#20998;&#24067;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20351;&#29992;&#23574;&#23792;&#31070;&#32463;&#20803;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20998;&#20998;&#37197;&#31639;&#27861;&#65292;&#26080;&#38656;&#21453;&#39304;&#31361;&#35302;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36845;&#20195;&#22788;&#29702;&#24863;&#35273;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#36882;&#24402;&#30005;&#36335;&#20250;&#26681;&#25454;&#23616;&#37096;&#33258;&#19979;&#21521;&#19978;&#12289;&#33258;&#19978;&#32780;&#19979;&#21644;&#20391;&#38754;&#30340;&#20449;&#21495;&#35745;&#31639;&#27599;&#23618;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#65292;&#20419;&#36827;&#19968;&#31181;&#21160;&#24577;&#30340;&#12289;&#36880;&#23618;&#24182;&#34892;&#30340;&#31070;&#32463;&#35745;&#31639;&#24418;&#24335;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#36825;&#26679;&#23601;&#33021;&#22815;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#21069;&#21521;&#65288;ED-FF&#65289;&#26694;&#26550;&#24037;&#20316;&#27491;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel credit assignment algorithm for information processing with spiking neurons without requiring feedback synapses. Specifically, we propose an event-driven generalization of the forward-forward and the predictive forward-forward learning processes for a spiking neural system that iteratively processes sensory input over a stimulus window. As a result, the recurrent circuit computes the membrane potential of each neuron in each layer as a function of local bottom-up, top-down, and lateral signals, facilitating a dynamic, layer-wise parallel form of neural computation. Unlike spiking neural coding, which relies on feedback synapses to adjust neural electrical activity, our model operates purely online and forward in time, offering a promising way to learn distributed representations of sensory data patterns with temporal spike signals. Notably, our experimental results on several pattern datasets demonstrate that the even-driven forward-forward (ED-FF) framework works we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65292;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20043;&#21518;&#21487;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18181</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#36827;&#19968;&#27493;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65292;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20043;&#21518;&#21487;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#22914;Stable Diffusion&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#34920;&#29616;&#20986;&#36275;&#22815;&#30340;&#24378;&#22823;&#65292;&#28982;&#32780;&#22312;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#24494;&#35843;&#26102;&#21364;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20869;&#23384;&#21644;&#26102;&#38388;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36817;&#26399;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;(&#31216;&#20316;&#36866;&#37197;&#22120;)&#26469;&#30740;&#31350;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36866;&#37197;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20998;&#35299;&#20026;&#27491;&#20132;&#22240;&#23376;&#8212;&#8212;&#36755;&#20837;&#20301;&#32622;&#12289;&#36755;&#20986;&#20301;&#32622;&#20197;&#21450;&#20989;&#25968;&#24418;&#24335;&#65292;&#24182;&#36827;&#34892;ANOVA&#20998;&#26512;&#65292;&#19968;&#31181;&#20998;&#26512;&#31163;&#25955;(&#35774;&#35745;&#36873;&#39033;)&#19982;&#36830;&#32493;&#21464;&#37327;(&#35780;&#20272;&#25351;&#26631;)&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#36755;&#20837;&#20301;&#32622;&#30340;&#36873;&#25321;&#65292;&#21457;&#29616;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#21518;&#21487;&#20197;&#20351;&#24615;&#33021;&#36798;&#21040;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Party-wise Dropout&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#23545;&#34987;&#21160;&#26041;&#24847;&#22806;&#36864;&#20986;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIMIP&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#27963;&#21160;&#26041;&#22312;&#37096;&#32626;&#38454;&#27573;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;</title><link>http://arxiv.org/abs/2303.18178</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#19982;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#38450;&#27490;&#21442;&#19982;&#26041;&#24847;&#22806;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties. (arXiv:2303.18178v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Party-wise Dropout&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#23545;&#34987;&#21160;&#26041;&#24847;&#22806;&#36864;&#20986;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIMIP&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#27963;&#21160;&#26041;&#22312;&#37096;&#32626;&#38454;&#27573;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#20351;&#20855;&#26377;&#26631;&#35760;&#29305;&#24449;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;&#21363;&#27963;&#21160;&#26041;&#65289;&#33021;&#22815;&#19982;&#25317;&#26377;&#36741;&#21161;&#29305;&#24449;&#30340;&#34987;&#21160;&#26041;&#21512;&#20316;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#28431;&#27934;&#65292;&#24403;&#34987;&#21160;&#26041;&#22312;VFL&#37096;&#32626;&#38454;&#27573;&#24847;&#22806;&#36864;&#20986;&#26102; - &#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#21644;&#27963;&#21160;&#26041;&#26631;&#31614;&#30340;&#30693;&#35782;&#20135;&#26435;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Party-wise Dropout&#26469;&#25552;&#39640;VFL&#27169;&#22411;&#23545;&#34987;&#21160;&#26041;&#24847;&#22806;&#36864;&#20986;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;DIMIP&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#27963;&#21160;&#26041;&#22312;&#37096;&#32626;&#38454;&#27573;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#19981;&#21516;&#30340;&#25512;&#29702;&#25915;&#20987;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Party-wise Dropout&#21487;&#20197;&#22312;&#34987;&#21160;&#26041;&#36864;&#20986;&#21518;&#26377;&#25928;&#22320;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#65292;DIMIP&#21487;&#20197;&#25104;&#21151;&#22320;&#25513;&#30422;&#34987;&#21160;&#26041;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#36731;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables a service provider (i.e., active party) who owns labeled features to collaborate with passive parties who possess auxiliary features to improve model performance. Existing VFL approaches, however, have two major vulnerabilities when passive parties unexpectedly quit in the deployment phase of VFL - severe performance degradation and intellectual property (IP) leakage of the active party's labels. In this paper, we propose \textbf{Party-wise Dropout} to improve the VFL model's robustness against the unexpected exit of passive parties and a defense method called \textbf{DIMIP} to protect the active party's IP in the deployment phase. We evaluate our proposed methods on multiple datasets against different inference attacks. The results show that Party-wise Dropout effectively maintains model performance after the passive party quits, and DIMIP successfully disguises label information from the passive party's feature extractor, thereby mitigating I
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.18171</link><description>&lt;p&gt;
&#20170;&#22825;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26377;&#22810;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#36845;&#20195;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#27969;&#20013;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#19978;&#65292;&#20294;&#36845;&#20195;&#23398;&#20064;&#32972;&#21518;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#35299;&#20915;&#20102;&#28798;&#38590;&#36951;&#24536;&#38382;&#39064;&#65292;&#20294;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#29575;&#20851;&#27880;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#26377;&#20123;&#26041;&#27861;&#29978;&#33267;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23436;&#25104;&#35757;&#32451;&#65281;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#36845;&#20195;&#23398;&#20064;&#19981;&#20165;&#20165;&#26159;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18158</link><description>&lt;p&gt;
&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#31209;&#19968;&#20989;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#36890;&#36807;&#32422;&#26463;&#26469;&#24314;&#27169;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#25351;&#26631;&#21464;&#37327;&#26469;&#35782;&#21035;&#36830;&#32493;&#21464;&#37327;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25903;&#25345;&#20989;&#25968;&#21442;&#25968;&#21644;&#31163;&#25955;&#35268;&#21010;&#25216;&#26415;&#20197;&#25552;&#20379;&#20984;&#21253;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#36879;&#35270;&#20989;&#25968;&#24341;&#36215;&#30340;&#38544;&#34255;&#22278;&#38181;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#27599;&#20010;&#22278;&#38181;&#32422;&#26463;&#28041;&#21450;&#29420;&#31435;&#36830;&#32493;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#21644;&#19968;&#32452;&#20108;&#20803;&#21464;&#37327;&#30340;&#19968;&#33324;&#22278;&#38181;&#28151;&#21512;&#20108;&#36827;&#21046;&#38598;&#21512;&#24314;&#31435;&#20102;&#19968;&#20010;&#20984;&#21253;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#24212;&#23545;epi&#30456;&#20851;&#30340;&#38598;&#21512;&#30340;&#25193;&#23637;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;ML&#26694;&#26550;MAGNNETO&#65292;&#36816;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;ISP&#32593;&#32476;&#30340;TE&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;OSPF&#20013;&#30340;&#38142;&#36335;&#26435;&#37325;&#35843;&#25972;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.18157</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#24037;&#31243;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;MAGNNETO
&lt;/p&gt;
&lt;p&gt;
MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic Engineering. (arXiv:2303.18157v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18157
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;ML&#26694;&#26550;MAGNNETO&#65292;&#36816;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;ISP&#32593;&#32476;&#30340;TE&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;OSPF&#20013;&#30340;&#38142;&#36335;&#26435;&#37325;&#35843;&#25972;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#32593;&#32476;&#20248;&#21270;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#32593;&#32476;&#20248;&#21270;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#35768;&#22810;&#38754;&#21521;&#20132;&#36890;&#24037;&#31243;(Traffic Engineering, TE)&#30340;&#23646;&#20110;ML&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#26159;ISP&#32593;&#32476;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#26368;&#20808;&#36827;&#30340;TE&#20248;&#21270;&#22120;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#27604;&#22914;&#23616;&#37096;&#25628;&#32034;(Local search)&#12289;&#32422;&#26463;&#31243;&#24207;&#35774;&#35745;(Constraint Programming)&#25110;&#32447;&#24615;&#31243;&#24207;&#35774;&#35745;(Linear programming)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAGNNETO&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;ML&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(Multi-Agent Reinforcement Learning)&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(Graph Neural Networks)&#36827;&#34892;&#20998;&#24067;&#24335;TE&#20248;&#21270;&#12290;MAGNNETO&#22312;&#32593;&#32476;&#20013;&#37096;&#32626;&#20102;&#19968;&#32452;&#26234;&#33021;&#20307;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#36890;&#36807;&#37051;&#36817;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#28040;&#24687;&#20132;&#25442;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#36890;&#20449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20248;&#21270;OSPF&#20013;&#30340;&#38142;&#36335;&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#26368;&#23567;&#21270;&#32593;&#32476;&#25317;&#22622;&#30340;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23558;MAGNNETO&#19982;&#22810;&#20010;&#26368;&#26032;&#30340;TE&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#36229;&#36807;75&#20010;&#25299;&#25169;&#32467;&#26500;(&#36798;&#21040;153&#20010;&#33410;&#28857;)&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends in networking propose the use of Machine Learning (ML) for a wide variety of network optimization tasks. As such, many efforts have been made to produce ML-based solutions for Traffic Engineering (TE), which is a fundamental problem in ISP networks. Nowadays, state-of-the-art TE optimizers rely on traditional optimization techniques, such as Local search, Constraint Programming, or Linear programming. In this paper, we present MAGNNETO, a distributed ML-based framework that leverages Multi-Agent Reinforcement Learning and Graph Neural Networks for distributed TE optimization. MAGNNETO deploys a set of agents across the network that learn and communicate in a distributed fashion via message exchanges between neighboring agents. Particularly, we apply this framework to optimize link weights in OSPF, with the goal of minimizing network congestion. In our evaluation, we compare MAGNNETO against several state-of-the-art TE optimizers in more than 75 topologies (up to 153 node
&lt;/p&gt;</description></item><item><title>BERT4ETH&#26159;&#19968;&#20010;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#38024;&#23545;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.18138</link><description>&lt;p&gt;
BERT4ETH&#65306;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18138
&lt;/p&gt;
&lt;p&gt;
BERT4ETH&#26159;&#19968;&#20010;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#38024;&#23545;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;&#22826;&#22346;&#19978;&#21508;&#31181;&#27450;&#35784;&#34892;&#20026;&#30340;&#28608;&#22686;&#65292;&#20445;&#25252;&#26131;&#21463;&#25915;&#20987;&#29992;&#25143;&#20813;&#21463;&#21463;&#23475;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#21487;&#33021;&#19981;&#36866;&#21512;&#22788;&#29702;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BERT4ETH&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#21508;&#31181;&#27450;&#35784;&#34892;&#20026;&#12290;BERT4ETH&#20855;&#26377;Transformer&#30340;&#20248;&#31168;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25429;&#25417;&#20197;&#22826;&#22346;&#20132;&#26131;&#20013;&#22266;&#26377;&#30340;&#21160;&#24577;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#37325;&#22797;&#24615;&#12289;&#20943;&#36731;&#20559;&#26012;&#21644;&#24314;&#27169;&#24322;&#26500;&#24615;&#31561;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#20026;&#20197;&#22826;&#22346;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;BERT4ETH&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.18136</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#30001;&#20110;&#32463;&#27982;&#21644;&#20851;&#38190;&#24615;&#30340;&#21407;&#22240;&#65292;&#25925;&#38556;&#26816;&#27979;&#20219;&#21153;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26234;&#33021;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#32570;&#38519;&#26816;&#27979;&#21644;&#36127;&#36733;&#39044;&#27979;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#26234;&#33021;&#30005;&#32593;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#25361;&#25112;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#23578;&#26410;&#19982;&#25152;&#26377;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#22320;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#31361;&#20986;&#23637;&#31034;&#20102;&#25925;&#38556;&#23450;&#20301;&#21644;&#31867;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33073;&#21516;&#27493;&#30340;&#23545;&#25239;&#25514;&#26045;&#65292;&#20351;&#28608;&#27963;&#20989;&#25968;&#30340;&#26102;&#24207;&#20998;&#26512;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#25552;&#21462;&#21644;&#31070;&#32463;&#32593;&#32476;&#20391;&#20449;&#36947;&#20998;&#26512;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;4096&#20010;&#31070;&#32463;&#20803;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#20165;&#20135;&#29983;&#19981;&#21040;1%&#30340;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2303.18132</link><description>&lt;p&gt;
&#22522;&#20110;&#33073;&#21516;&#27493;&#30340;&#31070;&#32463;&#32593;&#32476;&#20391;&#20449;&#36947;&#25915;&#20987;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks. (arXiv:2303.18132v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33073;&#21516;&#27493;&#30340;&#23545;&#25239;&#25514;&#26045;&#65292;&#20351;&#28608;&#27963;&#20989;&#25968;&#30340;&#26102;&#24207;&#20998;&#26512;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#25552;&#21462;&#21644;&#31070;&#32463;&#32593;&#32476;&#20391;&#20449;&#36947;&#20998;&#26512;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;4096&#20010;&#31070;&#32463;&#20803;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#20165;&#20135;&#29983;&#19981;&#21040;1%&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36890;&#24120;&#21487;&#20197;&#29992;&#20110;&#24674;&#22797;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#23494;&#21442;&#25968;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#21363;&#20351;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#28145;&#23618;&#32593;&#32476;&#30340;&#32593;&#32476;&#20063;&#21487;&#20197;&#36827;&#34892;&#21442;&#25968;&#25552;&#21462;&#65292;&#24182;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#19968;&#23450;&#31243;&#24230;&#30340;&#20445;&#25252;&#20197;&#25269;&#24481;&#36825;&#20123;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33073;&#21516;&#27493;&#30340;&#23545;&#25239;&#25514;&#26045;&#65292;&#20351;&#28608;&#27963;&#20989;&#25968;&#30340;&#26102;&#24207;&#20998;&#26512;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#31181;&#28608;&#27963;&#20989;&#25968;&#30340;&#26102;&#24207;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#33073;&#21516;&#27493;&#26041;&#24335;&#65292;&#20197;&#38544;&#34255;&#36755;&#20837;&#21644;&#28608;&#27963;&#31867;&#22411;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#22312;32&#20301;ARM Cortex-M4&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#39564;&#39564;&#35777;&#20102;&#23545;&#25239;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;t&#26816;&#39564;&#26174;&#31034;&#20102;&#20391;&#20449;&#36947;&#20449;&#24687;&#27844;&#28431;&#12290;&#26368;&#32456;&#24320;&#38144;&#21462;&#20915;&#20110;&#23436;&#20840;&#36830;&#25509;&#23618;&#20013;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;4096&#20010;&#31070;&#32463;&#20803;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#24320;&#38144;&#19981;&#21040;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdvCheck&#65292;&#36890;&#36807;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.18131</link><description>&lt;p&gt;
AdvCheck&#65306;&#36890;&#36807;&#26412;&#22320;&#26799;&#24230;&#26816;&#26597;&#34920;&#24449;&#23545;&#25239;&#29983;&#25104;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (arXiv:2303.18131v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdvCheck&#65292;&#36890;&#36807;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#26469;&#34920;&#24449;&#23545;&#25239;&#29983;&#25104;&#26679;&#26412;&#30340;&#29305;&#24449;&#21807;&#19968;&#24615;&#65292;&#25110;&#21306;&#20998;&#30001;&#23545;&#25239;&#24615;&#26679;&#26412;&#35302;&#21457;&#30340;DNN&#30340;&#34892;&#20026;&#12290;&#22522;&#20110;&#29305;&#24449;&#30340;&#26816;&#27979;&#26041;&#27861;&#19981;&#33021;&#22788;&#29702;&#21463;&#21040;&#22823;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#21478;&#19968;&#20010;&#20027;&#27969;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34892;&#20026;&#34920;&#24449;&#36755;&#20837;&#23646;&#24615;&#65292;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26412;&#22320;&#26799;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#25581;&#31034;&#20986;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26412;&#22320;&#26799;&#24230;&#36739;&#27491;&#24120;&#26679;&#26412;&#26377;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;AdvCheck&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#20174;&#19968;&#20123;&#27491;&#24120;&#26679;&#26412;&#21644;&#28155;&#21152;&#22122;&#22768;&#30340;&#26434;&#39033;&#26679;&#26412;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#27491;&#24120;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvCheck-LIME&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#24615;&#26469;&#22788;&#29702;&#26412;&#22320;&#26799;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#25928;&#29575;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial examples, which may lead to catastrophe in security-critical domains. Numerous detection methods are proposed to characterize the feature uniqueness of adversarial examples, or to distinguish DNN's behavior activated by the adversarial examples. Detections based on features cannot handle adversarial examples with large perturbations. Besides, they require a large amount of specific adversarial examples. Another mainstream, model-based detections, which characterize input properties by model behaviors, suffer from heavy computation cost. To address the issues, we introduce the concept of local gradient, and reveal that adversarial examples have a quite larger bound of local gradient than the benign ones. Inspired by the observation, we leverage local gradient for detecting adversarial examples, and propose a general framework AdvCheck. Specifically, by calculating the local gradient from a few benign examples and noise-added misc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#22836;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#24179;&#22343;K&#20998;&#31867;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#22836;&#34987;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#27169;&#22411;&#37319;&#29992;&#20102;&#36719;&#38408;&#20540;&#35757;&#32451;&#20197;&#20445;&#35777;&#24179;&#22343;&#36820;&#22238;K&#20010;&#31867;&#65292;&#23454;&#39564;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20854;&#20248;&#20110;softmax&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.18118</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24179;&#22343;K&#20998;&#31867;&#30340;&#21452;&#22836;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A two-head loss function for deep Average-K classification. (arXiv:2303.18118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#22836;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#24179;&#22343;K&#20998;&#31867;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#22836;&#34987;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#27169;&#22411;&#37319;&#29992;&#20102;&#36719;&#38408;&#20540;&#35757;&#32451;&#20197;&#20445;&#35777;&#24179;&#22343;&#36820;&#22238;K&#20010;&#31867;&#65292;&#23454;&#39564;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20854;&#20248;&#20110;softmax&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;K&#20998;&#31867;&#26159;&#19968;&#31181;&#22312;&#36755;&#20837;&#22270;&#20687;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#19979;&#36820;&#22238;&#25968;&#37327;&#21464;&#21270;&#30340;K&#20998;&#31867;&#26041;&#27861;&#65292;&#20294;&#25152;&#26377;&#26679;&#26412;&#30340;&#36820;&#22238;&#32467;&#26524;&#38656;&#24179;&#22343;&#20026;K&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#26159;&#29992;&#32463;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;softmax&#36755;&#20986;&#36827;&#34892;&#38408;&#20540;&#22788;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#29702;&#35770;&#19978;&#35777;&#26126;&#28176;&#36817;&#19968;&#33268;&#65292;&#20294;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#38598;&#21512;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22522;&#20110;&#23545;&#32463;&#20856;softmax&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#32780;&#28155;&#21152;&#30340;&#31532;&#20108;&#20010;&#22836;&#12290;&#29992;&#36719;&#38408;&#20540;&#23545;softmax&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#20445;&#35777;&#24179;&#22343;&#36820;&#22238;K&#20010;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#31867;&#21035;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22240;&#27492;&#36820;&#22238;&#26356;&#19968;&#33268;&#30340;&#21487;&#33021;&#31867;&#21035;&#38598;&#12290;&#23454;&#39564;&#22312;&#20004;&#20010;&#24050;&#26377;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;softmax&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent, but it is not guaranteed to be optimal for a finite set of samples. In this paper, we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and, as a result, to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline,
&lt;/p&gt;</description></item><item><title>&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#21457;&#24067;&#65292;&#21253;&#25324;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#21644;&#27599;&#20010;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#22312; EdAcc &#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#31361;&#26174;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#20849;&#20139;&#23558;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.18110</link><description>&lt;p&gt;
&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65306;&#36808;&#21521;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR. (arXiv:2303.18110v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18110
&lt;/p&gt;
&lt;p&gt;
&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#21457;&#24067;&#65292;&#21253;&#25324;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#21644;&#27599;&#20010;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#22312; EdAcc &#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#31361;&#26174;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#20849;&#20139;&#23558;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#27599;&#22825;&#26377;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#31532;&#19968;&#25110;&#31532;&#20108;&#35821;&#35328;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#22659;&#20013;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#33521;&#35821;&#26377;&#35768;&#22810;&#21464;&#20307;&#12290;&#34429;&#28982;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#22522;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#25253;&#21578;&#30340;&#32467;&#26524;&#26410;&#33021;&#20195;&#34920;&#20170;&#22825;&#20840;&#29699;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#33521;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#30340;&#29256;&#26412;&#12290;&#27492;&#25968;&#25454;&#38598;&#35797;&#22270;&#26356;&#22909;&#22320;&#20195;&#34920;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#32422;40&#23567;&#26102;&#30340;&#26379;&#21451;&#20043;&#38388;&#30340;&#20108;&#20803;&#35270;&#39057;&#36890;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EdAcc&#21253;&#25324;&#24191;&#27867;&#30340;&#33521;&#35821;&#31532;&#19968;&#35821;&#35328;&#21644;&#31532;&#20108;&#35821;&#35328;&#21464;&#20307;&#20197;&#21450;&#27599;&#20010;&#20154;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#26368;&#26032;&#20844;&#20849;&#21644;&#21830;&#19994;&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;EdAcc&#24378;&#35843;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#22312;680&#20010;&#36716;&#24405;&#30340;EdAcc&#23545;&#35805;&#19978;&#35757;&#32451;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#30340;&#65292;&#25105;&#20204;&#24076;&#26395;&#23427;&#33021;&#20026;&#27665;&#20027;&#21270;&#33521;&#35821;ASR&#30740;&#31350;&#21644;&#24320;&#21457;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
English is the most widely spoken language in the world, used daily by millions of people as a first or second language in many different contexts. As a result, there are many varieties of English. Although the great many advances in English automatic speech recognition (ASR) over the past decades, results are usually reported based on test datasets which fail to represent the diversity of English as spoken today around the globe. We present the first release of The Edinburgh International Accents of English Corpus (EdAcc). This dataset attempts to better represent the wide diversity of English, encompassing almost 40 hours of dyadic video call conversations between friends. Unlike other datasets, EdAcc includes a wide range of first and second-language varieties of English and a linguistic background profile of each speaker. Results on latest public, and commercial models show that EdAcc highlights shortcomings of current English ASR models. The best performing model, trained on 680 t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.18087</link><description>&lt;p&gt;
&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#30340;&#35780;&#20272;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Evaluation Challenges for Geospatial ML. (arXiv:2303.18087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#30001;&#20854;&#39044;&#27979;&#20135;&#29983;&#30340;&#22320;&#22270;&#22312;&#31185;&#23398;&#21644;&#25919;&#31574;&#30340;&#19979;&#28216;&#20998;&#26512;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#33539;&#20363;&#26377;&#30528;&#20851;&#38190;&#24046;&#24322;&#65292;&#22240;&#27492;&#65292;&#34913;&#37327;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#36755;&#20986;&#24615;&#33021;&#30340;&#27491;&#30830;&#26041;&#27861;&#19968;&#30452;&#26159;&#20105;&#35770;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#20840;&#29699;&#25110;&#36965;&#24863;&#25968;&#25454;&#19979;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.18062</link><description>&lt;p&gt;
&#35299;&#20915;&#24418;&#24577;&#23398;&#31867;&#27604;&#38382;&#39064;&#65306;&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#31181;&#38750;&#20961;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#38590;&#20197;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290; &#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#65288;AR&#65289;&#21463;&#21040;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20854;&#28508;&#21147;&#65292;&#20363;&#22914;&#20998;&#31867;&#65292;&#20915;&#31574;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;AR&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#22312;&#25972;&#20010;Siganalogies&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#21333;&#35789;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#31867;&#27604;&#27604;&#20363;&#65288;APs&#65289;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290; &#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNc&#65289;&#21644;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNr&#65289;&#65292;&#20197;&#21450;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#21333;&#35789;&#19978;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#24635;&#32467;&#24182;&#25193;&#23637;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#22312;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#19981;&#23384;&#22312;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#24314;&#27169;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#23454;&#29616;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26041;&#38754;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.18060</link><description>&lt;p&gt;
NOSTROMO: &#25945;&#35757;&#12289;&#32467;&#35770;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
NOSTROMO: Lessons learned, conclusions and way forward. (arXiv:2303.18060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#24314;&#27169;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#23454;&#29616;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26041;&#38754;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30333;&#30382;&#20070;&#26088;&#22312;&#35299;&#37322;&#20803;&#24314;&#27169;&#23545;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;(ATM)&#30740;&#31350;&#30340;&#20215;&#20540;&#12290;&#23427;&#23558;&#23450;&#20041;&#20803;&#24314;&#27169;&#24182;&#25506;&#35752;&#20854;&#33021;&#21147;&#21644;&#19981;&#33021;&#20570;&#21040;&#30340;&#20107;&#24773;&#12290;&#35835;&#32773;&#20551;&#23450;&#20855;&#26377;SESAR&#30340;&#22522;&#30784;&#30693;&#35782;&#65306;&#21333;&#19968;&#27431;&#27954;&#22825;&#31354;ATM&#30740;&#31350;&#39033;&#30446;&#12290; SESAR&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#24102;&#26469;&#25913;&#36827;&#65292;&#35813;&#39033;&#30446;&#26159;&#21333;&#19968;&#27431;&#27954;&#22825;&#31354;&#20513;&#35758;&#30340;&#25216;&#26415;&#25903;&#26609;&#65292;&#25913;&#36827;&#26159;&#36890;&#36807;&#29305;&#23450;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPIs)&#34913;&#37327;&#30340;&#65292;&#24182;&#30001;&#25152;&#35859;&#30340;SESAR&#8220;&#35299;&#20915;&#26041;&#26696;&#8221;&#31995;&#21015;&#26469;&#23454;&#26045;&#12290;&#36825;&#20123;&#8220;&#35299;&#20915;&#26041;&#26696;&#8221;&#26159;&#26032;&#30340;&#25110;&#25913;&#36827;&#30340;&#25805;&#20316;&#31243;&#24207;&#25110;&#25216;&#26415;&#65292;&#26088;&#22312;&#28385;&#36275;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#25551;&#36848;&#30340;&#25805;&#20316;&#21644;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This White Paper sets out to explain the value that metamodelling can bring to air traffic management (ATM) research. It will define metamodelling and explore what it can, and cannot, do. The reader is assumed to have basic knowledge of SESAR: the Single European Sky ATM Research project. An important element of SESAR, as the technological pillar of the Single European Sky initiative, is to bring about improvements, as measured through specific key performance indicators (KPIs), and as implemented by a series of so-called SESAR 'Solutions'. These 'Solutions' are new or improved operational procedures or technologies, designed to meet operational and performance improvements described in the European ATM Master Plan.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2303.18059</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#25512;&#26029;&#32593;&#32476;&#32467;&#26500;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#32467;&#26500;&#26159;&#35768;&#22810;&#22797;&#26434;&#29616;&#35937;&#30340;&#21160;&#24577;&#22522;&#30784;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#12289;&#39135;&#29289;&#38142;&#12289;&#30005;&#21147;&#32593;&#32476;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#21040;&#65292;&#22240;&#27492;&#24517;&#39035;&#20174;&#20854;&#32039;&#24613;&#21160;&#24577;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#23427;&#20204;&#30340;&#30456;&#20114;&#36830;&#25509;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30456;&#37051;&#30697;&#38453;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#25512;&#26029;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#21644;&#25968;&#25454;&#19978;&#30340;&#22122;&#22768;&#12290;&#36825;&#26159;&#26377;&#29992;&#30340;&#65292;&#22240;&#20026;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#36890;&#24120;&#26159;&#27424;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#20013;&#32570;&#20047;&#36825;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#35266;&#27979;&#20854;&#21709;&#24212;&#26029;&#30005;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#33521;&#22269;&#30005;&#21147;&#32593;&#32476;&#30340;&#32447;&#36335;&#25925;&#38556;&#20301;&#32622;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#29289;&#29702;&#26377;&#38480;&#20803;&#20998;&#26512;&#27169;&#22411;&#65292;&#23398;&#20064;&#20132;&#38169;&#30456;&#22797;&#21512;&#26448;&#26009;&#22312;&#21160;&#24577;&#21152;&#36733;&#19979;&#30340;&#30636;&#24577;&#21709;&#24212;&#65292;&#21152;&#36895;&#20854;&#21147;&#23398;&#24615;&#33021;&#30340;&#29289;&#29702;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.18055</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#21152;&#36733;&#19979;&#20132;&#38169;&#30456;&#22797;&#21512;&#26448;&#26009;&#30340;&#30636;&#24577;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Deep neural operator for learning transient response of interpenetrating phase composites subject to dynamic loading. (arXiv:2303.18055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#29289;&#29702;&#26377;&#38480;&#20803;&#20998;&#26512;&#27169;&#22411;&#65292;&#23398;&#20064;&#20132;&#38169;&#30456;&#22797;&#21512;&#26448;&#26009;&#22312;&#21160;&#24577;&#21152;&#36733;&#19979;&#30340;&#30636;&#24577;&#21709;&#24212;&#65292;&#21152;&#36895;&#20854;&#21147;&#23398;&#24615;&#33021;&#30340;&#29289;&#29702;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#24050;&#34987;&#35748;&#20026;&#26159;&#21046;&#36896;&#19994;&#30340;&#25216;&#26415;&#38761;&#21629;&#65292;&#23427;&#20801;&#35768;&#30452;&#25509;&#20174;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#27169;&#22411;&#20013;&#21046;&#36896;&#20855;&#26377;&#22797;&#26434;&#19977;&#32500;&#32467;&#26500;&#30340;&#26448;&#26009;&#12290;&#20132;&#38169;&#30456;&#22797;&#21512;&#26448;&#26009;&#65288;IPC&#65289;&#30340;&#21147;&#23398;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#23545;&#21160;&#24577;&#21152;&#36733;&#30340;&#21709;&#24212;&#65292;&#39640;&#24230;&#20381;&#36182;&#20110;&#23427;&#20204;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#20026;&#20102;&#21152;&#24555;&#23545;&#21508;&#31181;&#32467;&#26500;&#35774;&#35745;&#30340;IPC&#21147;&#23398;&#24615;&#33021;&#30340;&#29289;&#29702;&#39044;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#65288;DNO&#65289;&#26469;&#23398;&#20064;IPC&#22312;&#21160;&#24577;&#21152;&#36733;&#19979;&#30340;&#30636;&#24577;&#21709;&#24212;&#65292;&#20316;&#20026;&#22522;&#20110;&#29289;&#29702;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#27169;&#22411;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;&#20004;&#31181;Young&#27169;&#37327;&#27604;&#20026;2.7&#30340;&#37329;&#23646;&#32452;&#25104;&#30340;3D IPC&#26753;&#65292;&#20854;&#20013;&#20351;&#29992;&#32452;&#20998;&#26448;&#26009;&#30340;&#38543;&#26426;&#22359;&#26469;&#23637;&#31034;&#23427;&#20204;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has been recognized as an industrial technological revolution for manufacturing, which allows fabrication of materials with complex three-dimensional (3D) structures directly from computer-aided design models. The mechanical properties of interpenetrating phase composites (IPCs), especially response to dynamic loading, highly depend on their 3D structures. In general, for each specified structural design, it could take hours or days to perform either finite element analysis (FEA) or experiments to test the mechanical response of IPCs to a given dynamic load. To accelerate the physics-based prediction of mechanical properties of IPCs for various structural designs, we employ a deep neural operator (DNO) to learn the transient response of IPCs under dynamic loading as surrogate of physics-based FEA models. We consider a 3D IPC beam formed by two metals with a ratio of Young's modulus of 2.7, wherein random blocks of constituent materials are used to demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#23545;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.18047</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#22312;&#65288;&#38750;&#65289;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Convex Optimization in (Non)-Euclidean Space Revisited. (arXiv:2303.18047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#23545;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#20010;&#20173;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#35774;&#32622;: (1) &#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#38480;&#21046;&#19988;&#26377;&#30028;&#30340;&#65288;&#20984;&#65289;&#38598;&#21512;&#19978;&#36827;&#34892;DP-SCO; (2) &#22312;$\ell_p^d$&#31354;&#38388;&#19978;&#26080;&#38480;&#21046;&#30340;DP-SCO; (3) &#22312;&#38480;&#21046;&#19988;&#26377;&#30028;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#12290;&#23545;&#20110;&#38382;&#39064;&#65288;1&#65289;&#65292;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#32780;&#19981;&#26159;&#31354;&#38388;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#65292;&#26368;&#22810;&#30456;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#23545;&#20110;&#38382;&#39064;&#65288;2&#65289;&#21644;&#65288;3&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) in Euclidean and general $\ell_p^d$ spaces. Specifically, we focus on three settings that are still far from well understood: (1) DP-SCO over a constrained and bounded (convex) set in Euclidean space; (2) unconstrained DP-SCO in $\ell_p^d$ space; (3) DP-SCO with heavy-tailed data over a constrained and bounded set in $\ell_p^d$ space. For problem (1), for both convex and strongly convex loss functions, we propose methods whose outputs could achieve (expected) excess population risks that are only dependent on the Gaussian width of the constraint set rather than the dimension of the space. Moreover, we also show the bound for strongly convex functions is optimal up to a logarithmic factor. For problems (2) and (3), we propose several novel algorithms and provide the first theoretical results for both cases when $1&lt;p&lt;2$ and $2\leq p\leq \infty$.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#21487;&#25193;&#23637;&#36830;&#25509;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#20272;&#35745;&#20855;&#26377;&#24378;&#30456;&#20851;&#30340;&#22823;&#22411;&#21644;&#22797;&#26434;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#22522;&#25968;&#65292;&#26377;&#26395;&#22312;&#26597;&#35810;&#20248;&#21270;&#22120;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.18042</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#21487;&#25193;&#23637;&#36830;&#25509;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scardina: Scalable Join Cardinality Estimation by Multiple Density Estimators. (arXiv:2303.18042v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#21487;&#25193;&#23637;&#36830;&#25509;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#20272;&#35745;&#20855;&#26377;&#24378;&#30456;&#20851;&#30340;&#22823;&#22411;&#21644;&#22797;&#26434;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#22522;&#25968;&#65292;&#26377;&#26395;&#22312;&#26597;&#35810;&#20248;&#21270;&#22120;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#27491;&#22312;&#21462;&#20195;&#20256;&#32479;&#26041;&#27861;&#12290;&#36825;&#19968;&#21464;&#21270;&#39044;&#35745;&#23558;&#26377;&#21161;&#20110;&#22522;&#25968;&#20272;&#35745;&#30340;&#26368;&#37325;&#35201;&#24212;&#29992;&#20043;&#19968;&#65292;&#21363;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#20197;&#21152;&#24555;&#26597;&#35810;&#22788;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#24403;&#20851;&#31995;&#27169;&#24335;&#30001;&#35768;&#22810;&#34920;&#26684;&#32452;&#25104;&#65292;&#19988;&#34920;&#26684;/&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#26102;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#27809;&#26377;&#31934;&#30830;&#20272;&#35745;&#22522;&#25968;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22810;&#20010;&#23494;&#24230;&#20272;&#35745;&#22120;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#22320;&#38024;&#23545;&#20855;&#26377;&#24378;&#30456;&#20851;&#30340;&#22823;&#22411;&#21644;&#22797;&#26434;&#27169;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#22522;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Scardina&#65292;&#19968;&#31181;&#26032;&#30340;&#36830;&#25509;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27169;&#24335;&#32467;&#26500;&#30340;&#22810;&#20010;&#20998;&#21306;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning-based cardinality estimation methods are replacing traditional methods. This change is expected to contribute to one of the most important applications of cardinality estimation, the query optimizer, to speed up query processing. However, none of the existing methods do not precisely estimate cardinalities when relational schemas consist of many tables with strong correlations between tables/attributes. This paper describes that multiple density estimators can be combined to effectively target the cardinality estimation of data with large and complex schemas having strong correlations. We propose Scardina, a new join cardinality estimation method using multiple partitioned models based on the schema structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#35299;&#20915;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#8212;&#8212;TSR&#25968;&#25454;&#38598;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;TT100K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#25104;&#26524;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#23450;&#26631;&#24535;&#31867;&#21035;&#30340;&#24615;&#33021;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18037</link><description>&lt;p&gt;
&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#25968;&#25454;&#38598;&#19982;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Traffic Sign Recognition Dataset and Data Augmentation. (arXiv:2303.18037v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#35299;&#20915;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#8212;&#8212;TSR&#25968;&#25454;&#38598;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;TT100K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#25104;&#26524;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#23450;&#26631;&#24535;&#31867;&#21035;&#30340;&#24615;&#33021;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#35768;&#22810;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#24456;&#23569;&#26377;&#25910;&#38598;&#29992;&#20110;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21482;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#33719;&#21462;&#36275;&#22815;&#30340;&#23454;&#20363;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290; &#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20960;&#20046;&#26159;&#21807;&#19968;&#30340;&#36884;&#24452;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#28085;&#30422;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#36890;&#36807;&#39068;&#33394;&#12289;&#24418;&#29366;&#31561;&#65289;&#30456;&#27604;&#39640;&#24230;&#30456;&#20284;&#31867;&#21035;&#30340;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#27169;&#22411;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#26631;&#24535;&#31867;&#21035;&#65292;&#23427;&#20204;&#30340;&#26631;&#24535;&#21547;&#20041;&#27880;&#23450;&#26080;&#27861;&#22312;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#36275;&#22815;&#30340;&#23454;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#25968;&#25454;&#38598;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20132;&#36890;&#26631;&#24535;&#30340;&#26631;&#20934;&#65292;&#31216;&#20043;&#20026;TSR&#25968;&#25454;&#38598;&#22686;&#24378;&#12290;&#25105;&#20204;&#22522;&#20110;&#22522;&#20934;&#28165;&#21326;&#22823;&#23398;-&#33150;&#35759;100K&#65288;TT100K&#65289;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#35813;&#29420;&#29305;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;TT100K&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;&#20027;&#35201;&#36845;&#20195;&#29256;&#26412;&#25968;&#25454;&#38598;&#25191;&#34892;&#35813;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#29305;&#23450;&#26631;&#24535;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;TSR&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there are many datasets for traffic sign classification, there are few datasets collected for traffic sign recognition and few of them obtain enough instances especially for training a model with the deep learning method. The deep learning method is almost the only way to train a model for real-world usage that covers various highly similar classes compared with the traditional way such as through color, shape, etc. Also, for some certain sign classes, their sign meanings were destined to can't get enough instances in the dataset. To solve this problem, we purpose a unique data augmentation method for the traffic sign recognition dataset that takes advantage of the standard of the traffic sign. We called it TSR dataset augmentation. We based on the benchmark Tsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentation method. we performed the method on four main iteration version datasets based on the TT100K dataset and the experimental results showed our meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#22312;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;CORAL&#21644;MMD&#31561;&#31616;&#21333;DG&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#31454;&#20105;&#21147;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.18031</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#26159;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#30340;&#24378;&#22823;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization. (arXiv:2303.18031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#22312;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;CORAL&#21644;MMD&#31561;&#31616;&#21333;DG&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#31454;&#20105;&#21147;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#26410;&#30693;&#31867;&#21035;&#65292;&#20197;&#21450;&#39046;&#22495;&#28418;&#31227;&#65288;domain shift&#65289;&#65292;&#21363;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#22788;&#29702;&#25512;&#29702;&#38454;&#27573;&#30340;&#30446;&#26631;&#39046;&#22495;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#19981;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#30340;&#39046;&#22495;&#28418;&#31227;&#24773;&#20917;&#12290;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#65288;ODG&#65289;&#21516;&#26102;&#32771;&#34385;&#20102;DG&#21644;OSR&#12290;&#39046;&#22495;&#22686;&#24378;&#20803;&#23398;&#20064;&#65288;DAML&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;ODG&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#36739;&#20026;&#22797;&#26434;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25552;&#20986;&#20102;&#21508;&#31181;DG&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;ODG&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#22312;ODG&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;DG&#26041;&#27861;&#65292;&#21363;CORrelation ALignment&#65288;CORAL&#65289;&#21644;Maximum Mean Discrepancy&#65288;MMD&#65289;&#22312;&#33509;&#24178;&#24773;&#20917;&#19979;&#19982;DAML&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23567;&#35843;&#25972;&#65292;&#25552;&#20986;&#20102;CORAL&#21644;MMD&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, a machine learning model is required to handle an open-set recognition (OSR), where unknown classes appear during the inference, in addition to a domain shift, where the distribution of data differs between the training and inference phases. Domain generalization (DG) aims to handle the domain shift situation where the target domain of the inference phase is inaccessible during model training. Open domain generalization (ODG) takes into account both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method targeting ODG but has a complicated learning process. On the other hand, although various DG methods have been proposed, they have not been evaluated in ODG situations. This work comprehensively evaluates existing DG methods in ODG and shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum Mean Discrepancy (MMD), are competitive with DAML in several cases. In addition, we propose simple extensions of CORAL and MMD by introducing th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20943;&#23569;&#20102;&#29289;&#29702;&#23454;&#39564;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.18017</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32452;&#32455;&#26426;&#26800;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rapid prediction of lab-grown tissue properties using deep learning. (arXiv:2303.18017v1 [q-bio.TO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20943;&#23569;&#20102;&#29289;&#29702;&#23454;&#39564;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#21644;&#32454;&#32990;&#22806;&#22522;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#32452;&#32455;&#33258;&#32452;&#32455;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#39044;&#27979;&#22312;&#22266;&#23450;&#27169;&#20855;&#20013;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#33258;&#32452;&#32455;&#20013;&#26426;&#26800;&#29983;&#29289;&#23398;&#20316;&#29992;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;CONTRACT network dipole orientation(CONDOR)&#27169;&#22411;&#22312;&#27169;&#20855;&#20013;&#27169;&#25311;&#20102;6500&#20010;&#32454;&#32990;/&#22522;&#36136;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#20351;&#29992;\texttt{pix2pix}&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#23454;&#29616;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23558;&#20445;&#30041;&#30340;740&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#26696;&#20363;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#29983;&#29289;&#29289;&#29702;&#31639;&#27861;&#30340;&#20445;&#30041;&#39044;&#27979;&#20043;&#38388;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#24555;&#36895;&#39044;&#27979;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#65292;&#20943;&#23569;&#20102;&#32791;&#26102;&#21644;&#26114;&#36149;&#30340;&#29289;&#29702;&#23454;&#39564;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactions between cells and the extracellular matrix are vital for the self-organisation of tissues. In this paper we present proof-of-concept to use machine learning tools to predict the role of this mechanobiology in the self-organisation of cell-laden hydrogels grown in tethered moulds. We develop a process for the automated generation of mould designs with and without key symmetries. We create a large training set with $N=6500$ cases by running detailed biophysical simulations of cell-matrix interactions using the contractile network dipole orientation (CONDOR) model for the self-organisation of cellular hydrogels within these moulds. These are used to train an implementation of the \texttt{pix2pix} deep learning model, reserving $740$ cases that were unseen in the training of the neural network for training and validation. Comparison between the predictions of the machine learning technique and the reserved predictions from the biophysical algorithm show that the machine le
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.18005</link><description>&lt;p&gt;
&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;-&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#21033;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#36827;&#34892;&#21365;&#24034;&#30284;&#35786;&#26029;&#25110;&#39044;&#21518;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;-&#22312;2022&#24180;1&#26376;12&#26085;&#20043;&#21069;&#65292;&#23545;5&#20010;&#26469;&#28304;&#36827;&#34892;&#25628;&#32034;&#12290;&#21253;&#25324;&#26631;&#20934;&#35201;&#27714;&#30740;&#31350;&#35780;&#20272;AI&#22312;&#21365;&#24034;&#30284;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#19978;&#65292;&#23545;&#21365;&#24034;&#30284;&#65292;&#21253;&#25324;&#36755;&#21365;&#31649;&#21365;&#24034;&#21644;&#33145;&#33180;&#32959;&#30244;&#30340;&#35786;&#26029;&#25110;&#39044;&#21518;&#25512;&#26029;&#12290;&#25490;&#38500;&#35780;&#35770;&#21644;&#38750;&#33521;&#35821;&#25991;&#31456;&#12290;&#23545;&#27599;&#20010;&#21253;&#21547;&#30340;&#27169;&#22411;&#20351;&#29992;PROBAST&#35780;&#20272;&#20559;&#20506;&#39118;&#38505;&#12290;&#32467;&#26524;-&#20849;&#21457;&#29616;1434&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#20854;&#20013;36&#31687;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#12290;&#36825;&#20123;&#30740;&#31350;&#25253;&#21578;&#20102;62&#20010;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;35&#20010;&#20998;&#31867;&#22120;&#65292;14&#20010;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;7&#20010;&#20998;&#21106;&#27169;&#22411;&#21644;6&#20010;&#22238;&#24402;&#27169;&#22411;&#12290;&#20351;&#29992;1-1375&#24352;&#20174;1-664&#20010;&#21365;&#24034;&#30284;&#24739;&#32773;&#20013;&#24471;&#21040;&#30340;&#24187;&#28783;&#29255;&#24320;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#39044;&#27979;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24635;&#20307;&#29983;&#23384;&#65288;9/62&#65289;&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#65288;7/62&#65289;&#21644;&#28107;&#24052;&#32467;&#29366;&#24577;&#65288;6/62&#65289;&#12290;&#32467;&#35770;-&#22522;&#20110;&#21487;&#29992;&#30340;&#25991;&#29486;&#65292;AI&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#21463;&#21040;&#26679;&#26412;&#37327;&#23567;&#12289;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17995</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#65306;&#22522;&#20110;&#29109;&#29305;&#24449;&#30340;&#33041;&#30005;&#20449;&#21495;&#21644;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#20998;&#31163;&#65292;&#29992;&#20110;NNetEn&#35745;&#31639;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27979;&#37327;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;&#29109;&#27979;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#39321;&#20892;&#29109;&#65292;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#65292;&#38656;&#35201;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;&#26469;&#34920;&#24449;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#27010;&#24565;&#26159;&#22522;&#20110;&#29305;&#27530;&#25968;&#25454;&#38598;(MNIST-10&#21644;SARS-CoV-2-RBV1)&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#35760;&#24405;&#22312;LogNNet&#31070;&#32463;&#32593;&#32476;&#20648;&#23618;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#29109;&#30456;&#20851;&#12290;NNetEn&#20197;&#21407;&#22987;&#26041;&#24335;&#20272;&#35745;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#22522;&#20110;NNetEn&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20998;&#31867;&#24230;&#37327;&#65306;R2&#25928;&#29575;&#21644;&#30382;&#23572;&#36874;&#25928;&#29575;&#12290;NNetEn&#30340;&#25928;&#29575;&#22312;&#20351;&#29992;&#31163;&#25955;&#20998;&#26512;(ANOVA)&#20998;&#31163;&#27491;&#24358;&#26144;&#23556;&#30340;&#20004;&#20010;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#24471;&#21040;&#39564;&#35777;&#12290;&#23545;&#20110;&#20004;&#20010;&#25509;&#36817;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015; (r=1.1918&#21644;r=1.2243)&#65292;F&#27604;&#20540;&#36798;&#21040;&#20102;124&#30340;&#20540;&#65292;&#21453;&#26144;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17992</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization. (arXiv:2303.17992v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#26131;&#20110;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#26041;&#27861;&#26159;&#30001;&#26446;&#39134;&#39134;&#21644;&#25165;&#21326;&#27178;&#28322;&#20110;2002&#24180;&#25552;&#20986;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#31616;&#21333;&#26131;&#23454;&#29616;&#21644;&#21487;&#36866;&#24212;&#27969;&#34892;&#21464;&#20307;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#24314;&#35758;&#36890;&#36807;&#20026;&#27599;&#20010;&#26367;&#20195;&#23376;&#38382;&#39064;&#21046;&#20316;&#26356;&#32039;&#23494;&#30340;Hessian&#30697;&#38453;&#30340;&#19978;&#38480;&#26469;&#25913;&#36827;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#25152;&#25552;&#20986;&#30340;fastMU&#31639;&#27861;&#36890;&#24120;&#27604;&#21407;&#22987;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21516;&#26102;&#22312;&#36924;&#36817;&#31934;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25910;&#25947;&#20173;&#28982;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization is an important tool in unsupervised machine learning to decompose a data matrix into a product of parts that are often interpretable. Many algorithms have been proposed during the last three decades. A well-known method is the Multiplicative Updates algorithm proposed by Lee and Seung in 2002. Multiplicative updates have many interesting features: they are simple to implement and can be adapted to popular variants such as sparse Nonnegative Matrix Factorization, and, according to recent benchmarks, is state-of-the-art for many problems where the loss function is not the Frobenius norm. In this manuscript, we propose to improve the Multiplicative Updates algorithm seen as an alternating majorization minimization algorithm by crafting a tighter upper bound of the Hessian matrix for each alternate subproblem. Convergence is still ensured and we observe in practice on both synthetic and real world dataset that the proposed fastMU algorithm is often several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;FL4M&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;FL&#23545;&#20110;&#20445;&#25252;&#20803;&#23431;&#23449;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#38477;&#20302;&#26381;&#21153;&#22120;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17987</link><description>&lt;p&gt;
&#38754;&#21521;&#20803;&#23431;&#23449;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Metaverse: A Survey. (arXiv:2303.17987v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;FL4M&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;FL&#23545;&#20110;&#20445;&#25252;&#20803;&#23431;&#23449;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#38477;&#20302;&#26381;&#21153;&#22120;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20803;&#23431;&#23449;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#37319;&#38598;&#21644;&#31169;&#20154;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#25104;&#20026;&#20102;&#21046;&#32422;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#30340;&#35757;&#32451;&#20219;&#21153;&#12290;&#23558;FL&#24212;&#29992;&#20110;&#20803;&#23431;&#23449;&#19981;&#20165;&#21487;&#20197;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;&#26381;&#21153;&#22120;&#19978;&#39640;&#35745;&#31639;&#33021;&#21147;&#21644;&#39640;&#23384;&#20648;&#37327;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;FL4M&#30340;&#19968;&#20123;&#26089;&#26399;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25490;&#24207;&#26041;&#24335;&#65292;&#22312;&#22788;&#32602;&#30340;&#20010;&#20154;&#20013;&#28608;&#21169;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#23545;&#20110;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24635;&#20184;&#27454;&#12290;</title><link>http://arxiv.org/abs/2303.17971</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#24207;&#20419;&#36827;&#38750;&#21512;&#20316;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Promoting Non-Cooperation Through Ordering. (arXiv:2303.17971v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25490;&#24207;&#26041;&#24335;&#65292;&#22312;&#22788;&#32602;&#30340;&#20010;&#20154;&#20013;&#28608;&#21169;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#23545;&#20110;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24635;&#20184;&#27454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#22810;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#22823;&#22478;&#24066;&#20013;&#30340;&#23567;&#20132;&#36890;&#36829;&#35268;&#20107;&#20214;&#65292;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#38656;&#35201;&#23450;&#26399;&#23545;&#22823;&#37327;&#20010;&#20154;&#36827;&#34892;&#24809;&#32602;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#32473;&#27599;&#20010;&#20010;&#20154;&#19968;&#20010;&#26426;&#20250;&#65292;&#25215;&#25285;&#19968;&#23567;&#31508;&#32602;&#27454;&#65292;&#24182;&#20445;&#35777;&#20813;&#38500;&#21487;&#33021;&#20250;&#38754;&#20020;&#30340;&#36739;&#22823;&#22788;&#32602;&#21644;&#27861;&#24459;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36829;&#27861;&#32773;&#25968;&#37327;&#20247;&#22810;&#65292;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#21147;&#26377;&#38480;&#65292;&#20010;&#20154;&#38754;&#20020;&#30340;&#39118;&#38505;&#36890;&#24120;&#24456;&#23567;&#65292;&#29702;&#24615;&#30340;&#20010;&#20154;&#23558;&#36873;&#25321;&#19981;&#25903;&#20184;&#32602;&#27454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#65292;&#22914;&#26524;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#25353;&#29031;&#24050;&#30693;&#30340;&#20844;&#24320;&#39034;&#24207;&#22788;&#29702;&#36829;&#27861;&#32773;&#65292;&#23601;&#33021;&#36866;&#24403;&#22320;&#28608;&#21169;&#36829;&#27861;&#32773;&#25903;&#20184;&#32602;&#27454;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#20013;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20419;&#36827;&#20102;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#28608;&#21169;&#20010;&#20154;&#25903;&#20184;&#32602;&#27454;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#20219;&#24847;&#32852;&#30431;&#20063;&#26159;&#36866;&#29992;&#30340;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#25910;&#21040;&#30340;&#39044;&#26399;&#24635;&#20184;&#27454;&#65292;&#24182;&#26174;&#31034;&#20854;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real world situations, like minor traffic offenses in big cities, a central authority is tasked with periodic administering punishments to a large number of individuals. Common practice is to give each individual a chance to suffer a smaller fine and be guaranteed to avoid the legal process with probable considerably larger punishment. However, thanks to the large number of offenders and a limited capacity of the central authority, the individual risk is typically small and a rational individual will not choose to pay the fine. Here we show that if the central authority processes the offenders in a publicly known order, it properly incentives the offenders to pay the fine. We show analytically and on realistic experiments that our mechanism promotes non-cooperation and incentives individuals to pay. Moreover, the same holds for an arbitrary coalition. We quantify the expected total payment the central authority receives, and show it increases considerably.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HD-GCN&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#37051;&#25509;&#30697;&#38453;&#36896;&#25104;&#30340;&#20449;&#24687;&#25193;&#25955;&#38480;&#21046;&#12290;&#21033;&#29992;&#25193;&#25955;&#26144;&#23556;&#20419;&#36827;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#25193;&#25955;&#65292;&#20877;&#21033;&#29992;&#22270;&#21367;&#31215;&#36827;&#19968;&#27493;&#20256;&#25773;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#33719;&#24471;&#30340;&#25193;&#25955;&#36317;&#31163;&#36827;&#34892;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2303.17966</link><description>&lt;p&gt;
HD-GCN: &#19968;&#31181;&#28151;&#21512;&#25193;&#25955;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HD-GCN:A Hybrid Diffusion Graph Convolutional Network. (arXiv:2303.17966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17966
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HD-GCN&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#37051;&#25509;&#30697;&#38453;&#36896;&#25104;&#30340;&#20449;&#24687;&#25193;&#25955;&#38480;&#21046;&#12290;&#21033;&#29992;&#25193;&#25955;&#26144;&#23556;&#20419;&#36827;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#25193;&#25955;&#65292;&#20877;&#21033;&#29992;&#22270;&#21367;&#31215;&#36827;&#19968;&#27493;&#20256;&#25773;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#33719;&#24471;&#30340;&#25193;&#25955;&#36317;&#31163;&#36827;&#34892;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GCN&#21450;&#20854;&#21464;&#31181;&#27169;&#22411;&#30340;&#20449;&#24687;&#25193;&#25955;&#24615;&#33021;&#21463;&#37051;&#25509;&#30697;&#38453;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#25193;&#25955;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HD-GCN&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#37051;&#25509;&#30697;&#38453;&#36896;&#25104;&#30340;&#20449;&#24687;&#25193;&#25955;&#38480;&#21046;&#12290;&#22312;HD-GCN&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#25193;&#25955;&#26144;&#23556;&#26469;&#20419;&#36827;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#25193;&#25955;&#12290;&#36825;&#20801;&#35768;&#22312;&#27809;&#26377;&#30456;&#37051;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#30456;&#20284;&#28857;&#20043;&#38388;&#36827;&#34892;&#20449;&#24687;&#25193;&#25955;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#26144;&#23556;&#20043;&#21518;&#21033;&#29992;&#22270;&#21367;&#31215;&#36827;&#19968;&#27493;&#20256;&#25773;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22270;&#20013;&#30456;&#37051;&#30456;&#20284;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#33719;&#24471;&#30340;&#25193;&#25955;&#36317;&#31163;&#23545;&#35757;&#32451;&#26631;&#31614;&#30340;&#39044;&#27979;&#36827;&#34892;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information diffusion performance of GCN and its variant models is limited by the adjacency matrix, which can lower their performance. Therefore, we introduce a new framework for graph convolutional networks called Hybrid Diffusion-based Graph Convolutional Network (HD-GCN) to address the limitations of information diffusion caused by the adjacency matrix. In the HD-GCN framework, we initially utilize diffusion maps to facilitate the diffusion of information among nodes that are adjacent to each other in the feature space. This allows for the diffusion of information between similar points that may not have an adjacent relationship. Next, we utilize graph convolution to further propagate information among adjacent nodes after the diffusion maps, thereby enabling the spread of information among similar nodes that are adjacent in the graph. Finally, we employ the diffusion distances obtained through the use of diffusion maps to regularize and constrain the predicted labels of trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21333;&#22120;&#23448;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38480;&#21046;&#19979;&#25918;&#23556;&#27835;&#30103;&#22120;&#23448;&#39118;&#38505;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17956</link><description>&lt;p&gt;
CT&#31995;&#21015;&#20013;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Methods for Multi-Organ Segmentation in CT Series. (arXiv:2303.17956v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21333;&#22120;&#23448;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38480;&#21046;&#19979;&#25918;&#23556;&#27835;&#30103;&#22120;&#23448;&#39118;&#38505;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#65292;&#35821;&#20041;&#20998;&#21106;&#26159;&#30001;&#21307;&#29983;&#25191;&#34892;&#30340;&#26368;&#37325;&#35201;&#12289;&#26368;&#22256;&#38590;&#21644;&#26368;&#32791;&#26102;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#33258;&#21160;&#21270;&#36825;&#31181;&#20219;&#21153;&#30340;&#25215;&#35834;&#21464;&#24471;&#36234;&#26469;&#36234;&#29616;&#23454;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#25968;&#25454;&#30340;&#31232;&#32570;&#21487;&#29992;&#24615;&#20197;&#21450;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#25928;&#29575;&#38590;&#20197;&#25193;&#23637;&#21040;&#19968;&#33324;&#22330;&#26223;&#12290;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#30340;&#39118;&#38505;&#22120;&#23448;&#20998;&#21106;&#23646;&#20110;&#27492;&#31867;&#38382;&#39064;&#65292;&#22240;&#20026;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#20250;&#23545;&#24320;&#21457;&#36890;&#29992;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#31867;&#22411;&#30340;&#21333;&#22120;&#23448;&#27169;&#22411;&#38598;&#25104;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#20854;&#32452;&#20214;&#30340;&#19981;&#21516;&#19987;&#19994;&#25216;&#33021;&#20135;&#29983;&#22810;&#22120;&#23448;&#25513;&#27169;&#12290;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26159;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#25214;&#21040;&#26377;&#25928;&#30340;&#26041;&#27861;&#22312;CT&#31995;&#21015;&#20013;&#25191;&#34892;&#22810;&#22120;&#23448;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the medical images field, semantic segmentation is one of the most important, yet difficult and time-consuming tasks to be performed by physicians. Thanks to the recent advancement in the Deep Learning models regarding Computer Vision, the promise to automate this kind of task is getting more and more realistic. However, many problems are still to be solved, like the scarce availability of data and the difficulty to extend the efficiency of highly specialised models to general scenarios. Organs at risk segmentation for radiotherapy treatment planning falls in this category, as the limited data available negatively affects the possibility to develop general-purpose models; in this work, we focus on the possibility to solve this problem by presenting three types of ensembles of single-organ models able to produce multi-organ masks exploiting the different specialisations of their components. The results obtained are promising and prove that this is a possible solution to finding effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;FP8&#21644;INT8&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#25104;&#26524;&#65292;&#20026;&#36873;&#25321;&#27491;&#30830;&#30340;&#25968;&#23383;&#26684;&#24335;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.17951</link><description>&lt;p&gt;
FP8&#21644;INT8&#22312;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;FP8&#21644;INT8&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#25104;&#26524;&#65292;&#20026;&#36873;&#25321;&#27491;&#30830;&#30340;&#25968;&#23383;&#26684;&#24335;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;FP8&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25968;&#23383;&#26684;&#24335;&#30340;&#24819;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#19990;&#30028;&#20013;&#27969;&#20256;&#12290;&#37492;&#20110;&#30446;&#21069;&#22823;&#37096;&#20998;&#35757;&#32451;&#37117;&#26159;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#30340;FP32&#25110;&#32773;&#26377;&#26102;&#20351;&#29992;&#28151;&#21512;&#31934;&#24230;&#30340;FP16&#36827;&#34892;&#30340;&#65292;&#37096;&#20998;&#32593;&#32476;&#20351;&#29992;8&#20301;&#37325;&#37327;&#32423;&#30340;FP8&#21487;&#20197;&#21152;&#24555;&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#24120;&#32791;&#26102;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#27492;&#21457;&#23637;&#23545;&#20110;&#36793;&#32536;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#30340;&#24433;&#21709;&#30340;&#33258;&#28982;&#38382;&#39064;&#12290;&#22312;&#39640;&#25928;&#25512;&#29702;&#35774;&#22791;&#20013;&#65292;&#24037;&#20316;&#36127;&#36733;&#36890;&#24120;&#22312;INT8&#20013;&#25191;&#34892;&#12290;&#26377;&#26102;&#20026;&#20102;&#20445;&#35777;&#25928;&#29575;&#65292;&#29978;&#33267;&#20302;&#33267;INT4&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;FP8&#21644;INT&#26684;&#24335;&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;INT&#21644;FP&#26684;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#21644;&#35757;&#32451;&#26102;&#37327;&#21270;&#32467;&#26524;&#26469;&#23637;&#31034;&#22914;&#20309;&#22312;&#19981;&#21516;&#26684;&#24335;&#19979;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#38899;&#39057;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#26041;&#27861;AEGAN-AD&#65292;&#24182;&#22312;DCASE 2022 Challenge&#20219;&#21153;2&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17949</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#38899;&#39057;&#24322;&#24120;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Anomaly Detection and Localization of Machine Audio: A GAN-based Approach. (arXiv:2303.17949v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#38899;&#39057;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#26041;&#27861;AEGAN-AD&#65292;&#24182;&#22312;DCASE 2022 Challenge&#20219;&#21153;2&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#24322;&#24120;&#26816;&#27979;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#35748;&#20026;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#33021;&#21147;&#36866;&#29992;&#20110;&#26426;&#22120;&#38899;&#39057;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;AEGAN-AD&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#65288;&#20063;&#26159;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65289;&#34987;&#35757;&#32451;&#29992;&#20110;&#37325;&#26500;&#36755;&#20837;&#39057;&#35889;&#22270;&#65292;&#32780;&#37492;&#21035;&#22120;&#32463;&#36807;&#37325;&#26032;&#35774;&#35745;&#65292;&#20197;&#22312;&#35757;&#32451;&#21644;&#26816;&#27979;&#38454;&#27573;&#26399;&#38388;&#36741;&#21161;&#29983;&#25104;&#22120;&#12290;&#22312;DCASE 2022 Challenge TASK 2&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#34920;&#26126;&#65292;AEGAN-AD&#22312;&#20116;&#31181;&#26426;&#22120;&#31867;&#22411;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of machine anomaly remains challenging for machine learning. We believe the capability of generative adversarial network (GAN) suits the need of machine audio anomaly detection, yet rarely has this been investigated by previous work. In this paper, we propose AEGAN-AD, a totally unsupervised approach in which the generator (also an autoencoder) is trained to reconstruct input spectrograms. It is pointed out that the denoising nature of reconstruction deprecates its capacity. Thus, the discriminator is redesigned to aid the generator during both training stage and detection stage. The performance of AEGAN-AD on the dataset of DCASE 2022 Challenge TASK 2 demonstrates the state-of-the-art result on five machine types. A novel anomaly localization method is also investigated. Source code available at: www.github.com/jianganbai/AEGAN-AD
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;FedAvg&#21644;FedCurv&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#24322;&#36136;&#31995;&#32479;&#21644;&#38750;IID&#25968;&#25454;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.17942</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;FedAvg&#21644;FedCurv&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking FedAvg and FedCurv for Image Classification Tasks. (arXiv:2303.17942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;FedAvg&#21644;FedCurv&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#24322;&#36136;&#31995;&#32479;&#21644;&#38750;IID&#25968;&#25454;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#22312;&#21333;&#20010;&#25968;&#25454;&#28246;&#20013;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#26041;&#20415;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20445;&#23494;&#12290;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#20215;&#20540;&#65292;&#19982;&#20182;&#20154;&#20849;&#20139;&#26102;&#21487;&#33021;&#20250;&#28040;&#22833;&#65307;&#36991;&#20813;&#20849;&#20139;&#25968;&#25454;&#30340;&#33021;&#21147;&#20351;&#24471;&#24037;&#19994;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#23433;&#20840;&#21644;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#21482;&#38656;&#23454;&#29616;&#26412;&#22320;&#31574;&#30053;&#21363;&#21487;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29420;&#31435;&#36816;&#34892;&#29978;&#33267;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#24515;&#36816;&#34892;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20849;&#20139;&#26412;&#22320;AI&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#20998;&#25955;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#31649;&#29702;&#21516;&#19968;&#32852;&#21512;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#31995;&#32479;&#24182;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#36890;&#24120;&#19981;&#26159;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#65288;non-IID&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic Machine Learning techniques require training on data available in a single data lake. However, aggregating data from different owners is not always convenient for different reasons, including security, privacy and secrecy. Data carry a value that might vanish when shared with others; the ability to avoid sharing the data enables industrial applications where security and privacy are of paramount importance, making it possible to train global models by implementing only local policies which can be run independently and even on air-gapped data centres. Federated Learning (FL) is a distributed machine learning approach which has emerged as an effective way to address privacy concerns by only sharing local AI models while keeping the data decentralized. Two critical challenges of Federated Learning are managing the heterogeneous systems in the same federated network and dealing with real data, which are often not independently and identically distributed (non-IID) among the clients
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;GAN&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;CT&#25195;&#25551;&#20013;&#22120;&#23448;&#39118;&#38505;&#65288;OAR&#65289;&#20998;&#21106;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#20284;&#25110;&#20248;&#20110;&#20854;&#23545;&#24212;&#30340;CNN&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#21106;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.17941</link><description>&lt;p&gt;
&#23545;&#27604;&#23545;&#25239;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#22312;CT&#22270;&#20687;&#20013;&#22120;&#23448;&#39118;&#38505;&#20998;&#21106;&#26041;&#38754;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images. (arXiv:2303.17941v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;GAN&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;CT&#25195;&#25551;&#20013;&#22120;&#23448;&#39118;&#38505;&#65288;OAR&#65289;&#20998;&#21106;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#20284;&#25110;&#20248;&#20110;&#20854;&#23545;&#24212;&#30340;CNN&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#21106;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#25195;&#25551;&#20013;&#22120;&#23448;&#39118;&#38505;&#65288;OAR&#65289;&#20998;&#21106;&#26159;&#25918;&#30103;&#27835;&#30103;&#27969;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#21106;OARs&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;GAN&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#30456;&#21516;&#30340;&#29983;&#25104;&#22120;&#32467;&#26500;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#20351;&#29992;&#21253;&#21547;50&#20010;&#24050;&#27880;&#37322;CT&#25195;&#25551;&#30340;StructSeg&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#33391;&#22909;&#30340;CNN&#27169;&#22411;&#65292;&#22914;SE-ResUnet&#21644;DeepLabV3&#65292;&#36825;&#20123;&#25195;&#25551;&#21253;&#21547;&#20845;&#20010;OAR&#30340;&#36718;&#24275;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#22312;OAR&#20998;&#21106;&#29615;&#22659;&#20013;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#20248;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#32467;&#26524;&#38750;&#24120;&#26377;&#24076;&#26395;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#20284;&#25110;&#20248;&#20110;&#20854;&#23545;&#24212;&#30340;CNN&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#21106;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organ at Risk (OAR) segmentation from CT scans is a key component of the radiotherapy treatment workflow. In recent years, deep learning techniques have shown remarkable potential in automating this process. In this paper, we investigate the performance of Generative Adversarial Networks (GANs) compared to supervised learning approaches for segmenting OARs from CT images. We propose three GAN-based models with identical generator architectures but different discriminator networks. These models are compared with well-established CNN models, such as SE-ResUnet and DeepLabV3, using the StructSeg dataset, which consists of 50 annotated CT scans containing contours of six OARs. Our work aims to provide insight into the advantages and disadvantages of adversarial training in the context of OAR segmentation. The results are very promising and show that the proposed GAN-based approaches are similar or superior to their CNN-based counterparts, particularly when segmenting more challenging targe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17940</link><description>&lt;p&gt;
&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#25913;&#36827;&#20102;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (arXiv:2303.17940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102; \citet {cao2022benign} &#30340;&#20449;&#21495;&#22122;&#38899;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102; PEGR &#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#12290;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;PEGR &#21487;&#20197;&#21306;&#20998;&#20449;&#21495;&#21644;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102; PEGR &#24809;&#32602;&#27169;&#24335;&#23398;&#20064;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25233;&#21046;&#20102;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#24518;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient regularization, as described in \citet{barrett2021implicit}, is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from \citet{cao2022benign} and show that PEGR can learn signals effectively while suppressing noise. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#22238;&#36991;&#30340;&#38598;&#25104;&#28176;&#21464;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#38598;&#25104;&#20445;&#35777;&#19981;&#21463;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#30446;&#26631;&#35782;&#21035;&#38598;&#25104;&#20013;&#26080;&#25928;&#26799;&#24230;&#30340;&#21306;&#22495;&#65292;&#24182;&#33258;&#36866;&#24212;&#35843;&#25972;&#36825;&#20123;&#26799;&#24230;&#30340;&#26435;&#37325;&#65292;&#20197;&#36991;&#20813;&#25506;&#32034;&#19981;&#21487;&#34892;&#30340;&#35774;&#35745;&#21306;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17934</link><description>&lt;p&gt;
&#20914;&#31361;&#22238;&#36991;&#30340;&#38598;&#25104;&#28176;&#21464;&#20248;&#21270;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conflict-Averse Gradient Optimization of Ensembles for Effective Offline Model-Based Optimization. (arXiv:2303.17934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#22238;&#36991;&#30340;&#38598;&#25104;&#28176;&#21464;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#38598;&#25104;&#20445;&#35777;&#19981;&#21463;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#30446;&#26631;&#35782;&#21035;&#38598;&#25104;&#20013;&#26080;&#25928;&#26799;&#24230;&#30340;&#21306;&#22495;&#65292;&#24182;&#33258;&#36866;&#24212;&#35843;&#25972;&#36825;&#20123;&#26799;&#24230;&#30340;&#26435;&#37325;&#65292;&#20197;&#36991;&#20813;&#25506;&#32034;&#19981;&#21487;&#34892;&#30340;&#35774;&#35745;&#21306;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#20248;&#21270;&#26159;&#35299;&#20915;&#40657;&#30418;&#35745;&#31639;&#35774;&#35745;&#38382;&#39064;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#22240;&#20026;&#30495;&#23454;&#30340;&#30446;&#26631;&#20989;&#25968;&#26410;&#30693;&#19988;&#26114;&#36149;&#19988;&#38590;&#20197;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#26041;&#27861;&#20248;&#21270;&#35774;&#35745;&#19982;&#22522;&#30784;&#30495;&#23454;&#30446;&#26631;&#30340;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21487;&#33021;&#20250;&#21463;&#21040;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#35774;&#35745;&#20301;&#20110;&#29421;&#31364;&#27969;&#24418;&#19978;&#65292;&#26631;&#20934;&#26041;&#27861;&#26131;&#20110;&#20135;&#29983;&#8220;&#35823;&#23548;&#8221;&#20195;&#29702;&#27169;&#22411;&#36755;&#20986;&#39640;&#20540;&#30340;&#36234;&#30028;&#26080;&#25928;&#35774;&#35745;&#12290;&#20351;&#29992;&#38598;&#21512;&#32780;&#19981;&#26159;&#21333;&#20010;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#26377;&#21161;&#20110;&#32531;&#35299;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#20294;&#26159;&#20174;&#38598;&#21512;&#20013;&#32452;&#21512;&#26799;&#24230;&#20449;&#24687;&#30340;&#26420;&#32032;&#20844;&#24335;&#65288;&#20363;&#22914;&#26368;&#23567;&#21644;&#24179;&#22343;&#26799;&#24230;&#65289;&#20173;&#28982;&#26159;&#27425;&#20248;&#30340;&#24182;&#19988;&#32463;&#24120;&#21463;&#21040;&#19981;&#25910;&#25947;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#38598;&#21512;&#20013;&#32452;&#21512;&#26799;&#24230;&#20449;&#24687;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20998;&#24067;&#21464;&#21270;&#26377;&#40065;&#26834;&#24615;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#40065;&#26834;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#36741;&#21161;&#8220;&#28165;&#27927;&#8221;&#30446;&#26631;&#26469;&#35782;&#21035;&#35774;&#35745;&#31354;&#38388;&#30340;&#21306;&#22495;&#65292;&#20854;&#20013;&#38598;&#21512;&#21487;&#33021;&#20135;&#29983;&#26080;&#25928;&#26799;&#24230;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#20943;&#23567;&#20248;&#21270;&#26399;&#38388;&#30340;&#36825;&#20123;&#26799;&#24230;&#30340;&#26435;&#37325;&#65292;&#20197;&#36991;&#20813;&#25506;&#32034;&#19981;&#21487;&#34892;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#30340;&#35774;&#35745;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26631;&#20934;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20248;&#21270;&#24615;&#33021;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven offline model-based optimization (MBO) is an established practical approach to black-box computational design problems for which the true objective function is unknown and expensive to query. However, the standard approach which optimizes designs against a learned proxy model of the ground truth objective can suffer from distributional shift. Specifically, in high-dimensional design spaces where valid designs lie on a narrow manifold, the standard approach is susceptible to producing out-of-distribution, invalid designs that "fool" the learned proxy model into outputting a high value. Using an ensemble rather than a single model as the learned proxy can help mitigate distribution shift, but naive formulations for combining gradient information from the ensemble, such as minimum or mean gradient, are still suboptimal and often hampered by non-convergent behavior.  In this work, we explore alternate approaches for combining gradient information from the ensemble that are robu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#12289;&#19981;&#30830;&#23450;&#12289;&#27169;&#22411;&#35823;&#24046;&#31561;&#38382;&#39064;&#26041;&#38754;&#32988;&#36807;&#32463;&#20856;&#35266;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#33258;&#34892;&#36710;&#27169;&#22411;&#19978;&#24050;&#24471;&#21040;&#35780;&#20215;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.17933</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#22312;&#21160;&#21147;&#23398;&#33258;&#34892;&#36710;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning-based Observer Evaluated on the Kinematic Bicycle Model. (arXiv:2303.17933v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#12289;&#19981;&#30830;&#23450;&#12289;&#27169;&#22411;&#35823;&#24046;&#31561;&#38382;&#39064;&#26041;&#38754;&#32988;&#36807;&#32463;&#20856;&#35266;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#33258;&#34892;&#36710;&#27169;&#22411;&#19978;&#24050;&#24471;&#21040;&#35780;&#20215;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#29366;&#24577;&#30340;&#30693;&#35782;&#26159;&#36827;&#34892;&#36866;&#24403;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#36825;&#20123;&#37327;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#27979;&#37327;&#33719;&#24471;&#12290;&#25511;&#21046;&#29702;&#35770;&#25552;&#20379;&#20102;&#26497;&#20854;&#26377;&#29992;&#30340;&#26041;&#27861;-&#35266;&#27979;&#22120;-&#29992;&#20110;&#22788;&#29702;&#19981;&#33021;&#30452;&#25509;&#27979;&#37327;&#25110;&#20855;&#26377;&#22122;&#22768;&#27979;&#37327;&#30340;&#25968;&#37327;&#12290;&#32463;&#20856;&#35266;&#23519;&#22120;&#26159;&#20174;&#27169;&#22411;&#20013;&#25968;&#23398;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;&#23427;&#20204;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#22914;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20043;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#22312;&#31995;&#32479;&#34920;&#29616;&#20986;&#39640;&#24230;&#38750;&#32447;&#24615;&#12289;&#24314;&#27169;&#35823;&#24046;&#12289;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#29615;&#22659;&#20132;&#20114;&#22256;&#38590;&#65288;&#20363;&#22914;&#36947;&#36335;&#25509;&#35302;&#65289;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#20854;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#22312;&#23398;&#20064;&#22522;&#30784;&#19978;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#33021;&#22815;&#32988;&#36807;&#32463;&#20856;&#30340;&#35266;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#23450;&#20041;&#20102;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#33258;&#34892;&#36710;&#27169;&#22411;&#19978;&#24471;&#21040;&#35780;&#20215;&#65292;&#36825;&#21487;&#20197;&#26041;&#20415;&#22320;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#25968;&#25454;&#12290;&#36825;&#20010;&#27169;&#22411;&#20063;&#34987;&#29992;&#20110;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EKF&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge of the states of a vehicle is a necessity to perform proper planning and control. These quantities are usually accessible through measurements. Control theory brings extremely useful methods -- observers -- to deal with quantities that cannot be directly measured or with noisy measurements. Classical observers are mathematically derived from models. In spite of their success, such as the Kalman filter, they show their limits when systems display high non-linearities, modeling errors, high uncertainties or difficult interactions with the environment (e.g. road contact). In this work, we present a method to build a learning-based observer able to outperform classical observing methods. We compare several neural network architectures and define the data generation procedure used to train them. The method is evaluated on a kinematic bicycle model which allows to easily generate data for training and testing. This model is also used in an Extended Kalman Filter (EKF) for compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#65292;&#22797;&#26434;&#25299;&#25169;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#20294;&#20195;&#20215;&#26159;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.17925</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#23618;&#24863;&#30693;&#22120;&#65306;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Beyond Multilayer Perceptrons: Investigating Complex Topologies in Neural Networks. (arXiv:2303.17925v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#65292;&#22797;&#26434;&#25299;&#25169;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#20294;&#20195;&#20215;&#26159;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#26500;&#24314;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;Barabasi-Albert&#65292;Erdos-Renyi&#65292;Watts-Strogatz&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#27969;&#24418;&#23398;&#20064;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#26500;&#24314;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#38590;&#24230;&#21644;&#22122;&#22768;&#27700;&#24179;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;MLP&#30456;&#27604;&#65292;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;&#36825;&#31181;&#24615;&#33021;&#20248;&#21183;&#24402;&#22240;&#20110;&#22797;&#26434;&#32593;&#32476;&#21033;&#29992;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#21512;&#24615;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22909;&#22788;&#26159;&#20197;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#20026;&#20195;&#20215;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21508;&#31181;&#25299;&#25169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the impact of network topology on the approximation capabilities of artificial neural networks (ANNs), with a particular focus on complex topologies. We propose a novel methodology for constructing complex ANNs based on various topologies, including Barab\'asi-Albert, Erd\H{o}s-R\'enyi, Watts-Strogatz, and multilayer perceptrons (MLPs). The constructed networks are evaluated on synthetic datasets generated from manifold learning generators, with varying levels of task difficulty and noise. Our findings reveal that complex topologies lead to superior performance in high-difficulty regimes compared to traditional MLPs. This performance advantage is attributed to the ability of complex networks to exploit the compositionality of the underlying target function. However, this benefit comes at the cost of increased forward-pass computation time and reduced robustness to graph damage. Additionally, we investigate the relationship between various topological attribute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.17907</link><description>&lt;p&gt;
&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39044;&#27979;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#37325;&#23450;&#21521;&#27493;&#34892;
&lt;/p&gt;
&lt;p&gt;
Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#27491;&#26397;&#30528;&#22686;&#24378;&#27785;&#28024;&#24863;&#12289;&#25903;&#25345;&#22810;&#29992;&#25143;&#20307;&#39564;&#21644;&#22312;&#34394;&#25311;&#20307;&#39564;&#20013;&#25903;&#25345;&#26080;&#38480;&#21046;&#30340;&#31227;&#21160;&#65292;&#32780;&#36890;&#36807;&#37325;&#23450;&#21521;&#27493;&#34892;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#19987;&#38376;&#30340;VR&#35774;&#32622;&#20869;&#12290;&#20026;&#20102;&#28385;&#36275;&#26410;&#26469;VR&#31995;&#32479;&#30340;&#26497;&#31471;&#25968;&#25454;&#36895;&#29575;&#21644;&#24310;&#36831;&#35201;&#27714;&#65292;&#25903;&#25345;&#26080;&#32447;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#23558;&#22312;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#39057;&#29575;&#19978;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#23454;&#29616;&#39640;&#24230;&#23450;&#21521;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#30701;&#26399;&#39044;&#27979;&#22810;&#29992;&#25143;VR&#35774;&#32622;&#20013;&#29992;&#25143;&#30340;&#27178;&#21521;&#31227;&#21160;&#65292;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#26041;&#21521;&#19978;&#30340;&#30452;&#32447;&#35270;&#36317;&#65288;LoS&#65289;&#8220;&#36319;&#36394;&#8221;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#19968;&#27425;&#24615;&#20998;&#24067;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644; Nesterov &#21160;&#37327;&#26469;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#32531;&#35299;&#20102;&#20174;&#24037;&#20154;&#21040;&#26381;&#21153;&#22120;&#30340;&#19978;&#20256;&#38142;&#36335;&#25104;&#20026;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;PCA-AWFL &#31639;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#27604; PCA-WFL &#31639;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17885</link><description>&lt;p&gt;
&#21033;&#29992; Nesterov &#21160;&#37327;&#21644;&#20998;&#24067;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;&#26469;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis. (arXiv:2303.17885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#19968;&#27425;&#24615;&#20998;&#24067;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644; Nesterov &#21160;&#37327;&#26469;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#32531;&#35299;&#20102;&#20174;&#24037;&#20154;&#21040;&#26381;&#21153;&#22120;&#30340;&#19978;&#20256;&#38142;&#36335;&#25104;&#20026;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;PCA-AWFL &#31639;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#27604; PCA-WFL &#31639;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27491;&#20132;&#26080;&#32447;&#36890;&#36947;&#20801;&#35768;&#26381;&#21153;&#22120;&#21644;&#24037;&#20154;&#20132;&#25442;&#26410;&#32534;&#30721;&#20449;&#24687;&#12290;&#30001;&#20110;&#24037;&#20154;&#39057;&#32321;&#36890;&#36807;&#24102;&#23485;&#21463;&#38480;&#36890;&#36947;&#21521;&#26381;&#21153;&#22120;&#19978;&#20256;&#26412;&#22320;&#26799;&#24230;&#65292;&#22240;&#27492;&#20174;&#24037;&#20154;&#21040;&#26381;&#21153;&#22120;&#30340;&#19978;&#20256;&#38142;&#36335;&#25104;&#20026;&#19968;&#20010;&#36890;&#20449;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#19968;&#27425;&#24615;&#20998;&#24067;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#20943;&#23567;&#19978;&#20256;&#26799;&#24230;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#36890;&#20449;&#29942;&#39048;&#12290;&#22522;&#20110;&#20302;&#32500;&#26799;&#24230;&#21644; Nesterov &#21160;&#37327;&#25552;&#20986;&#20102;&#22522;&#20110; PCA &#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PCA-WFL&#65289;&#31639;&#27861;&#21450;&#20854;&#21152;&#36895;&#29256;&#26412;&#65288;&#21363; PCA-AWFL&#65289;&#12290;&#23545;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#31995;&#32479;&#36229;&#21442;&#25968;&#23545; PCA-WFL &#21644; PCA-AWFL &#31639;&#27861;&#25910;&#25947;&#30340;&#24433;&#21709;&#12290;PCA-AWFL &#31639;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#27604; PCA-WFL &#31639;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#21644;&#38543;&#26426;&#24037;&#20154;&#27169;&#22411;&#30340;&#20849;&#23384;&#65292;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wireless federated learning system is investigated by allowing a server and workers to exchange uncoded information via orthogonal wireless channels. Since the workers frequently upload local gradients to the server via bandwidth-limited channels, the uplink transmission from the workers to the server becomes a communication bottleneck. Therefore, a one-shot distributed principle component analysis (PCA) is leveraged to reduce the dimension of uploaded gradients such that the communication bottleneck is relieved. A PCA-based wireless federated learning (PCA-WFL) algorithm and its accelerated version (i.e., PCA-AWFL) are proposed based on the low-dimensional gradients and the Nesterov's momentum. For the non-convex loss functions, a finite-time analysis is performed to quantify the impacts of system hyper-parameters on the convergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm is theoretically certified to converge faster than the PCA-WFL algorithm. Besides, the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#28145;&#24230;&#20998;&#22359;&#29926;&#29255;&#65288;FDT&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#36816;&#34892;&#26102;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;DNN&#25512;&#29702;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#30340;TinyML&#27169;&#22411;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17878</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34701;&#21512;&#28145;&#24230;&#20998;&#22359;&#29926;&#29255;&#25216;&#26415;&#25552;&#39640;&#20869;&#23384;&#21033;&#29992;&#29575;
&lt;/p&gt;
&lt;p&gt;
Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference. (arXiv:2303.17878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#28145;&#24230;&#20998;&#22359;&#29926;&#29255;&#65288;FDT&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#36816;&#34892;&#26102;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;DNN&#25512;&#29702;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#30340;TinyML&#27169;&#22411;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24494;&#22411;&#12289;&#20302;&#21151;&#32791;&#25511;&#21046;&#22120;&#30340;&#20986;&#29616;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#30340;&#20869;&#23384;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#26679;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#36816;&#34892;DNN&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#38899;&#39057;&#20851;&#38190;&#35789;&#26816;&#27979;&#25110;&#22522;&#20110;&#38647;&#36798;&#30340;&#25163;&#21183;&#35782;&#21035;&#65292;&#30001;&#20110;DNN&#25512;&#29702;&#38656;&#35201;&#22823;&#37327;&#30340;&#20013;&#38388;&#36816;&#34892;&#26102;&#32531;&#20914;&#21306;&#26469;&#23384;&#20648;&#28608;&#27963;&#21644;&#20854;&#20182;&#20013;&#38388;&#25968;&#25454;&#65292;&#20174;&#32780;&#23548;&#33268;&#20869;&#23384;&#20351;&#29992;&#37327;&#39640;&#24230;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#28145;&#24230;&#20998;&#22359;&#29926;&#29255;&#65288;FDT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;DNN&#30340;&#20869;&#23384;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#20998;&#22359;&#26041;&#27861;&#30456;&#27604;&#65292;FDT&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#36816;&#34892;&#26102;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#36866;&#29992;&#20110;&#27604;&#29616;&#26377;&#20998;&#22359;&#26041;&#27861;&#26356;&#22810;&#31181;&#31867;&#30340;&#32593;&#32476;&#23618;&#12290;FDT&#26174;&#33879;&#25552;&#39640;&#20102;TinyML&#30340;&#20869;&#23384;&#20248;&#21270;&#65292;&#20943;&#23569;&#20102;&#20197;&#21069;&#26080;&#27861;&#20943;&#23569;&#20869;&#23384;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#26367;&#20195;&#35774;&#35745;&#25552;&#20379;&#20102;&#20854;&#20182;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory optimization for deep neural network (DNN) inference gains high relevance with the emergence of TinyML, which refers to the deployment of DNN inference tasks on tiny, low-power microcontrollers. Applications such as audio keyword detection or radar-based gesture recognition are heavily constrained by the limited memory on such tiny devices because DNN inference requires large intermediate run-time buffers to store activations and other intermediate data, which leads to high memory usage. In this paper, we propose a new Fused Depthwise Tiling (FDT) method for the memory optimization of DNNs, which, compared to existing tiling methods, reduces memory usage without inducing any run time overhead. FDT applies to a larger variety of network layers than existing tiling methods that focus on convolutions. It improves TinyML memory optimization significantly by reducing memory of models where this was not possible before and additionally providing alternative design points for models th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CAP-VSTNet&#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#20869;&#23481;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;&#22810;&#21151;&#33021;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17867</link><description>&lt;p&gt;
CAP-VSTNet: &#20869;&#23481;&#30456;&#20851;&#24615;&#20445;&#30041;&#30340;&#22810;&#21151;&#33021;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer. (arXiv:2303.17867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CAP-VSTNet&#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#20869;&#23481;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;&#22810;&#21151;&#33021;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#25439;&#22833;&#38382;&#39064;&#19968;&#30452;&#26159;&#24341;&#36215;&#29031;&#29255;&#36924;&#30495;&#21450;&#35270;&#39057;&#39118;&#26684;&#36716;&#21270;&#20013;&#20135;&#29983;&#20266;&#24433;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAP-VSTNet&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20840;&#26032;&#30340;&#21487;&#36870;&#27531;&#24046;&#32593;&#32476;&#21644;&#26080;&#20559;&#30340;&#32447;&#24615;&#36716;&#25442;&#27169;&#22359;&#32452;&#25104;&#65292;&#29992;&#20110;&#23454;&#29616;&#22810;&#21151;&#33021;&#39118;&#26684;&#36716;&#25442;&#12290;&#36825;&#31181;&#21487;&#36870;&#27531;&#24046;&#32593;&#32476;&#19981;&#20165;&#33021;&#22815;&#20445;&#30041;&#20869;&#23481;&#30456;&#20851;&#24615;&#65292;&#32780;&#19988;&#19981;&#20250;&#20687;&#20256;&#32479;&#30340;&#21487;&#36870;&#32593;&#32476;&#19968;&#26679;&#24341;&#20837;&#20887;&#20313;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#26684;&#21270;&#25928;&#26524;&#12290;&#22312;&#35299;&#20915;&#30001;&#32447;&#24615;&#36716;&#25442;&#24341;&#36215;&#30340;&#20687;&#32032;&#30456;&#20851;&#24615;&#25439;&#22833;&#38382;&#39064;&#30340;Matting Laplacian&#35757;&#32451;&#25439;&#22833;&#30340;&#25903;&#25345;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22810;&#21151;&#33021;&#39118;&#26684;&#36716;&#25442;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAP-VSTNet&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealistic and video style transfer. This paper proposes a new framework named CAP-VSTNet, which consists of a new reversible residual network and an unbiased linear transform module, for versatile style transfer. This reversible residual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks, and hence facilitate better stylization. Empowered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform, the proposed framework is applicable and effective on versatile style transfer. Extensive experiments show that CAP-VSTNet can produce better qualitative and quantitative results in comparison with the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#30340;&#21327;&#21464;&#37327;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#27969;&#31243;&#20248;&#21270;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19982;&#35299;&#37322;&#24615;&#21327;&#21464;&#37327;&#30456;&#20851;&#24615;&#26368;&#39640;&#30340;&#28857;&#20113;&#30340;&#20302;&#32500;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2303.17852</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#39062;&#21327;&#21464;&#37327;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#26497;&#22823;&#21327;&#26041;&#24046;&#23637;&#24320;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Maximum Covariance Unfolding Regression: A Novel Covariate-based Manifold Learning Approach for Point Cloud Data. (arXiv:2303.17852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#30340;&#21327;&#21464;&#37327;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#27969;&#31243;&#20248;&#21270;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19982;&#35299;&#37322;&#24615;&#21327;&#21464;&#37327;&#30456;&#20851;&#24615;&#26368;&#39640;&#30340;&#28857;&#20113;&#30340;&#20302;&#32500;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#25968;&#25454;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#30340;&#36807;&#31243;&#26816;&#27979;&#12289;&#24314;&#27169;&#12289;&#30417;&#25511;&#21644;&#20248;&#21270;&#12290;&#30446;&#21069;&#30340;&#24352;&#37327;&#22238;&#24402;&#25216;&#26415;&#24050;&#32463;&#26377;&#25928;&#22320;&#29992;&#20110;&#32467;&#26500;&#21270;&#28857;&#20113;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#20854;&#20013;&#22312;&#22343;&#21248;&#32593;&#26684;&#19978;&#30340;&#27979;&#37327;&#21487;&#20197;&#24418;&#25104;&#19968;&#20010;&#24352;&#37327;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#36866;&#29992;&#20110;&#24120;&#24120;&#24418;&#25104;&#27969;&#24418;&#30340;&#38750;&#32467;&#26500;&#21270;&#28857;&#20113;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21517;&#20026;&#26497;&#22823;&#21327;&#26041;&#24046;&#23637;&#24320;&#22238;&#24402;&#65292;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#26368;&#39640;&#19982;&#35299;&#37322;&#24615;&#21327;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#28857;&#20113;&#30340;&#20302;&#32500;&#27969;&#24418;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#20302;&#32500;&#27969;&#24418;&#29992;&#20110;&#22522;&#20110;&#36807;&#31243;&#21464;&#37327;&#30340;&#22238;&#24402;&#24314;&#27169;&#21644;&#27969;&#31243;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#21644;&#38050;&#21046;&#25903;&#26550;&#21046;&#36896;&#30340;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#24182;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud data are widely used in manufacturing applications for process inspection, modeling, monitoring and optimization. The state-of-art tensor regression techniques have effectively been used for analysis of structured point cloud data, where the measurements on a uniform grid can be formed into a tensor. However, these techniques are not capable of handling unstructured point cloud data that are often in the form of manifolds. In this paper, we propose a nonlinear dimension reduction approach named Maximum Covariance Unfolding Regression that is able to learn the low-dimensional (LD) manifold of point clouds with the highest correlation with explanatory covariates. This LD manifold is then used for regression modeling and process optimization based on process variables. The performance of the proposed method is subsequently evaluated and compared with benchmark methods through simulations and a case study of steel bracket manufacturing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17841</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#30456;&#20851;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#20027;&#35201; bottleneck&#12290;&#32780;&#19988;&#65292;&#20026;&#20102;&#35299;&#20915;&#37326;&#24515;&#21187;&#21187;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#38468;&#24102;&#24102;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#20197;&#20415;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25163;&#21160;&#26631;&#35760;&#20855;&#26377;&#39640;&#36136;&#37327;&#26631;&#31614;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24448;&#24448;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#29942;&#39048;&#12290;&#24369;&#30417;&#30563;&#23398;&#20064; (WSL) &#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#26681;&#25454;&#21551;&#21457;&#24335;&#12289;&#36828;&#31243;&#30417;&#35270;&#21644;&#30693;&#35782;&#24211;&#26469;&#36171;&#20104;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#32422;&#26631;&#31614; (&#20266;&#26631;&#31614;) &#30340;&#33258;&#21160;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#25105;&#20204;&#24212;&#29992;&#27010;&#29575;&#29983;&#25104;&#38544;&#21464;&#37327;&#27169;&#22411; (PLVMs)&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#34920;&#31034;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PLVMs &#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#22810;&#25165;&#22810;&#33402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.17839</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations. (arXiv:2303.17839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20016;&#23500;&#36164;&#28304;&#20026;&#29702;&#35299;&#36807;&#31243;&#27963;&#21160;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20010;&#20307;&#27493;&#39588;&#21450;&#20854;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#12290;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#27493;&#39588;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#24040;&#22823;&#20010;&#20307;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23398;&#20064;&#26102;&#38388;&#25490;&#24207;&#19981;&#20165;&#33021;&#22815;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#21152;&#24378;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#20998;&#31867;&#65288;&#22312;COIN/EPIC-Kitchens&#19978;&#20998;&#21035;&#22686;&#21152;2.8% / 3.3%&#65289;&#21644;&#27493;&#39588;&#39044;&#27979;&#65288;&#22312;COIN&#19978;&#22686;&#21152;7.4%&#65289;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#25552;&#21462;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of instructional videos and their narrations over the Internet offers an exciting avenue for understanding procedural activities. In this work, we propose to learn video representation that encodes both action steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narrations, without using human annotations. Our method jointly learns a video representation to encode individual step concepts, and a deep probabilistic model to capture both temporal dependencies and immense individual variations in the step ordering. We empirically demonstrate that learning temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising results in zero-shot inference for step c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#23427;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29420;&#31435;&#20110;&#36755;&#20837;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#65292;&#19988;&#40065;&#26834;&#24615;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17836</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35299;&#37322;&#65306;&#28145;&#24230;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers. (arXiv:2303.17836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#23427;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29420;&#31435;&#20110;&#36755;&#20837;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#65292;&#19988;&#40065;&#26834;&#24615;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#24402;&#23646;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#25552;&#20379;&#20107;&#21518;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290; &#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#21333;&#20010;&#36755;&#20837;&#26679;&#26412;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#27492;&#26080;&#27861;&#22238;&#31572;&#26377;&#20851;&#27169;&#22411;&#30340;&#29420;&#31435;&#20110;&#36755;&#20837;&#30340;&#26597;&#35810;&#12290; &#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#26174;&#33879;&#24615;&#26144;&#23556;&#26412;&#36136;&#19978;&#23481;&#26131;&#21463;&#21040;&#35823;&#23548;&#24615;&#29305;&#24449;&#24402;&#23646;&#30340;&#24433;&#21709;&#12290;&#35797;&#22270;&#20351;&#29992;&#8220;&#36890;&#29992;&#8221;&#36755;&#20837;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#30340;&#29616;&#26377;&#23581;&#35797;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#36825;&#20123;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#23548;&#33268;&#35299;&#37322;&#30340;&#20559;&#24046;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290; &#36825;&#20123;&#29305;&#24449;&#26159;&#20960;&#20309;&#30456;&#20851;&#30340;&#65292;&#24182;&#36890;&#36807;&#31215;&#32047;&#27169;&#22411;&#30456;&#23545;&#20110;&#26080;&#38480;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#35745;&#31639;&#12290; &#20026;&#20102;&#35745;&#31639;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#27839;&#30528;&#20154;&#31867;&#21487;&#29702;&#35299;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#25512;&#21521;&#27169;&#22411;&#25439;&#22833;&#38754;&#12290; &#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#36755;&#20837;&#20449;&#24687;&#65292;&#24182;&#19988;&#32463;&#36807;&#26816;&#39564;&#65292;&#22312;&#36755;&#20837;&#21464;&#21270;&#12289;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use 'general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#12290;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#21152;&#36895;&#35757;&#32451;&#24182;&#20445;&#25345;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17824</link><description>&lt;p&gt;
&#38544;&#24335;&#27169;&#26495;&#21270;ODE-nets&#30340;&#23454;&#29616;&#21644;(&#21453;&#36716;&#20462;&#27491;)&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets. (arXiv:2303.17824v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#12290;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#21152;&#36895;&#35757;&#32451;&#24182;&#20445;&#25345;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#26469;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#21453;&#36716;&#20462;&#27491;&#35823;&#24046;&#20998;&#26512;&#20197;&#26041;&#20415;&#35299;&#37322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#36820;&#22238;&#20102;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38024;&#23545;&#35757;&#32451;&#27492;&#31867;ODE-nets&#36827;&#34892;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32780;&#24403;&#21069;&#30340;&#31574;&#30053;&#36890;&#24120;&#23558;ODE-nets&#30340;&#25968;&#20540;&#31215;&#20998;&#35270;&#20026;&#40657;&#21283;&#23376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30417;&#27979;&#35823;&#24046;&#32423;&#21035;&#24182;&#35843;&#25972;(&#23637;&#24320;&#30340;)&#38544;&#24335;&#35299;&#27861;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#20197;&#20351;&#23637;&#24320;&#30340;&#36817;&#20284;&#35823;&#24046;&#23567;&#20110;&#24403;&#21069;&#30340;&#23398;&#20064;&#25439;&#22833;&#12290;&#36825;&#26377;&#21161;&#20110;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning hidden dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17823</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#22238;&#24212;&#26377;&#24207;&#22238;&#24402;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;&#65288;N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#20854;&#20013;&#21453;&#24212;&#21464;&#37327;&#19981;&#20165;&#21487;&#20197;&#21462;&#31163;&#25955;&#20540;&#65292;&#20063;&#21487;&#20197;&#21462;&#36830;&#32493;&#20540;&#65292;&#32780;&#22238;&#24402;&#31995;&#25968;&#26681;&#25454;&#39044;&#27979;&#39034;&#24207;&#21453;&#24212;&#20063;&#19981;&#21516;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#20174;&#31163;&#25955;&#21453;&#24212;&#20272;&#35745;&#32447;&#24615;&#31995;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20197;&#21453;&#24212;&#20026;&#36755;&#20837;&#20135;&#29983;&#32447;&#24615;&#31995;&#25968;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;N$^3$POM&#21487;&#20197;&#22312;&#20445;&#30041;&#20256;&#32479;&#26377;&#24207;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#22312;&#25351;&#23450;&#30340;&#29992;&#25143;&#21306;&#22495;&#20869;&#65292;&#39044;&#27979;&#30340;&#26465;&#20214;&#32047;&#31215;&#27010;&#29575;&#65288;CCP&#65289;&#28385;&#36275;&#23616;&#37096;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#21333;&#35843;&#24615;&#30340;&#38543;&#26426;&#65288;MPS&#65289;&#31639;&#27861;&#26469;&#20805;&#20998;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#29305;&#23450;&#30340;&#25345;&#32493;&#28608;&#21169;&#36755;&#20837;&#20316;&#20026;&#25506;&#32034;&#20449;&#21495;&#26469;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;LQR&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21487;&#25910;&#25947;&#21040;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27714;&#35299;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17819</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;LQR&#38382;&#39064;&#30340;&#39640;&#25928;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem. (arXiv:2303.17819v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#29305;&#23450;&#30340;&#25345;&#32493;&#28608;&#21169;&#36755;&#20837;&#20316;&#20026;&#25506;&#32034;&#20449;&#21495;&#26469;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;LQR&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21487;&#25910;&#25947;&#21040;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27714;&#35299;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20165;&#21033;&#29992;&#20174;&#31995;&#32479;&#27979;&#37327;&#21040;&#30340;&#36755;&#20837;&#29366;&#24577;&#25968;&#25454;&#26469;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;LQR&#38382;&#39064;&#12290;&#19981;&#21516;&#20110;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#25968;&#25454;&#25910;&#38598;&#27493;&#39588;&#20013;&#20351;&#29992;&#29305;&#23450;&#30340;&#25345;&#32493;&#28608;&#21169;&#36755;&#20837;&#20316;&#20026;&#25506;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#38543;&#21518;&#23637;&#31034;&#65292;&#21033;&#29992;&#36825;&#31181;&#25345;&#32493;&#28608;&#21169;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#31639;&#27861;&#20013;&#30697;&#38453;&#26041;&#31243;&#30340;&#35299;&#20445;&#35777;&#27599;&#27425;&#36845;&#20195;&#37117;&#23384;&#22312;&#19988;&#21807;&#19968;&#12290;&#21516;&#26102;&#65292;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#20063;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#35780;&#20272;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;Sylvester&#36716;&#32622;&#26041;&#31243;&#30340;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#27714;&#35299;&#30340;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#26469;&#30830;&#23450;&#31283;&#23450;&#31574;&#30053;&#20197;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, an off-policy reinforcement learning algorithm is designed to solve the continuous-time LQR problem using only input-state data measured from the system. Different from other algorithms in the literature, we propose the use of a specific persistently exciting input as the exploration signal during the data collection step. We then show that, using this persistently excited data, the solution of the matrix equation in our algorithm is guaranteed to exist and to be unique at every iteration. Convergence of the algorithm to the optimal control input is also proven. Moreover, we formulate the policy evaluation step as the solution of a Sylvester-transpose equation, which increases the efficiency of its solution. Finally, a method to determine a stabilizing policy to initialize the algorithm using only measured data is proposed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.17807</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#32479;&#38889;&#21307;&#20013;&#30340;&#28508;&#21147;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25991;&#21270;&#36866;&#24212;&#20445;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#38889;&#21307;&#27880;&#37325;&#20010;&#20307;&#21270;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#25968;&#25454;&#26377;&#38480;&#19988;&#36807;&#31243;&#38544;&#24615;&#65292;&#20351;AI&#24314;&#27169;&#22256;&#38590;&#12290;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#32570;&#20047;&#21307;&#23398;&#19987;&#19994;&#22521;&#35757;&#65292;&#20294;&#24050;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#21307;&#30103;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#21462;&#24471;&#20102;42.06%&#21644;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;GPT-4&#25509;&#36817;&#21450;&#26684;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment, making AI modeling difficult due to limited data and implicit processes. GPT-3.5 and GPT-4, large language models, have shown impressive medical knowledge despite lacking medicine-specific training. This study aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the Korean National Licensing Examination for Korean Medicine Doctors. Methods: GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions from the 2022 examination across 12 subjects. Each question was independently evaluated five times in an initialized session. Results: GPT-3.5 and GPT-4 achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing performance. There were significant differences in accuracy by subjects, with 83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine (2). Both models showed high accuracy in recall-based and diagnosis-based questi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#23637;&#31034;&#35813;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#65292;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.17805</link><description>&lt;p&gt;
&#20851;&#20110;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#65306;2&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#23637;&#31034;&#35813;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#65292;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27491;&#21017;&#21270;&#36335;&#24452;&#26377;&#26102;&#34987;&#29992;&#20316;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#36335;&#24452;&#30340;&#26041;&#20415;&#29702;&#35770;&#20195;&#29702;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#20462;&#25913;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;2&#23618;&#32593;&#32476;&#35757;&#32451;&#30340;&#38750;&#20984;&#24615;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#30456;&#24212;&#30340;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#24182;&#35843;&#26597;&#20102;&#20854;&#20027;&#35201;&#29305;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#32553;&#25918;&#36335;&#24452;&#21644;&#20248;&#21270;&#36335;&#24452;&#30340;&#26368;&#32456;&#29366;&#24577;&#34892;&#20026;&#31867;&#20284;&#65292;&#29978;&#33267;&#36229;&#36234;&#20102;&#36825;&#20123;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#20004;&#20010;&#23376;&#31354;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17802</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces. (arXiv:2303.17802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#20004;&#20010;&#23376;&#31354;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#35889;&#20998;&#26512;(SSA)&#21644;&#24046;&#24322;&#23376;&#31354;&#38388;&#27010;&#24565;&#30340;&#26032;&#22411;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#30417;&#25511;&#36807;&#21435;&#21644;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25152;&#23545;&#24212;&#30340;&#20004;&#20010;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#38388;&#36731;&#24494;&#30340;&#26102;&#38388;&#21464;&#21270;&#26469;&#20316;&#20026;&#24322;&#24120;&#35780;&#20998;&#12290;&#36825;&#26159;&#20256;&#32479;SSA&#26041;&#27861;&#30340;&#33258;&#28982;&#25512;&#24191;&#65292;&#20256;&#32479;&#26041;&#27861;&#27979;&#37327;&#30340;&#26159;&#20004;&#20010;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#26368;&#23567;&#35282;&#24230;&#26469;&#34920;&#24449;&#21464;&#21270;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#29992;&#24046;&#24322;&#23376;&#31354;&#38388;&#26367;&#20195;&#26368;&#23567;&#35282;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;SSA&#26694;&#26550;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new method for anomaly detection in time-series data by incorporating the concept of difference subspace into the singular spectrum analysis (SSA). The key idea is to monitor slight temporal variations of the difference subspace between two signal subspaces corresponding to the past and present time-series data, as anomaly score. It is a natural generalization of the conventional SSA-based method which measures the minimum angle between the two signal subspaces as the degree of changes. By replacing the minimum angle with the difference subspace, our method boosts the performance while using the SSA-based framework as it can capture the whole structural difference between the two subspaces in its magnitude and direction. We demonstrate our method's effectiveness through performance evaluations on public time-series datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#32531;&#24930;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#31561;&#27493;&#39588;&#65292;&#21487;&#32531;&#35299;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17782</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20851;&#27880;&#32531;&#24930;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slow-Shifting Concerned Machine Learning Method for Short-term Traffic Flow Forecasting. (arXiv:2303.17782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#32531;&#24930;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#31561;&#27493;&#39588;&#65292;&#21487;&#32531;&#35299;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#25317;&#25380;&#21306;&#22495;&#30340;&#20132;&#36890;&#27969;&#37327;&#21464;&#21270;&#23545;&#24403;&#23616;&#21046;&#23450;&#32531;&#35299;&#25317;&#22581;&#30340;&#25514;&#26045;&#25110;&#23433;&#25490;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#27599;&#26085;&#21644;&#27599;&#21608;&#23792;&#20540;&#20043;&#38388;&#32531;&#24930;&#21464;&#21270;&#30340;&#26102;&#38388;&#28418;&#31227;&#65292;&#23548;&#33268;&#20132;&#36890;&#27969;&#37327;&#20449;&#21495;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#36827;&#32780;&#23548;&#33268;&#20934;&#30830;&#39044;&#27979;&#30340;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#32531;&#24930;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;&#65292;&#20854;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#20316;&#20026;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#20943;&#36731;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24471;&#21040;&#19968;&#31995;&#21015;&#24179;&#31283;&#20998;&#37327;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#25429;&#25417;&#26102;&#38388;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25105;&#20204;&#20197;&#24179;&#31283;&#20998;&#37327;&#20026;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#20808;&#36827;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to predict traffic flow over time for crowded areas during rush hours is increasingly important as it can help authorities make informed decisions for congestion mitigation or scheduling of infrastructure development in an area. However, a crucial challenge in traffic flow forecasting is the slow shifting in temporal peaks between daily and weekly cycles, resulting in the nonstationarity of the traffic flow signal and leading to difficulty in accurate forecasting. To address this challenge, we propose a slow shifting concerned machine learning method for traffic flow forecasting, which includes two parts. First, we take advantage of Empirical Mode Decomposition as the feature engineering to alleviate the nonstationarity of traffic flow data, yielding a series of stationary components. Second, due to the superiority of Long-Short-Term-Memory networks in capturing temporal features, an advanced traffic flow forecasting model is developed by taking the stationary components as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#24418;&#27169;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#31227;&#21160;&#30340;&#20108;&#32500;&#25237;&#24433;&#28857;&#36827;&#34892;&#28145;&#24230;&#25512;&#26029;&#65292;&#23398;&#20064;&#20102;&#19977;&#32500;&#26059;&#36716;&#21464;&#25442;&#65292;&#20026;&#20102;&#29702;&#35299;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#22914;&#20309;&#20869;&#37096;&#34920;&#31034;&#19977;&#32500;&#21464;&#25442;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.17776</link><description>&lt;p&gt;
&#20174;&#20108;&#32500;&#25237;&#24433;&#22270;&#20687;&#20013;&#23398;&#20064;&#19977;&#32500;&#21464;&#25442;&#30340;&#20869;&#37096;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Internal Representations of 3D Transformations from 2D Projected Inputs. (arXiv:2303.17776v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#24418;&#27169;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#31227;&#21160;&#30340;&#20108;&#32500;&#25237;&#24433;&#28857;&#36827;&#34892;&#28145;&#24230;&#25512;&#26029;&#65292;&#23398;&#20064;&#20102;&#19977;&#32500;&#26059;&#36716;&#21464;&#25442;&#65292;&#20026;&#20102;&#29702;&#35299;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#22914;&#20309;&#20869;&#37096;&#34920;&#31034;&#19977;&#32500;&#21464;&#25442;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19977;&#32500;&#19990;&#30028;&#20013;&#20132;&#20114;&#26102;&#65292;&#20154;&#31867;&#24517;&#39035;&#20174;&#25237;&#24433;&#21040;&#20108;&#32500;&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#20272;&#35745;&#19977;&#32500;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#20351;&#29992;&#29289;&#20307;&#24418;&#29366;&#22312;&#36816;&#21160;&#24341;&#36215;&#30340;&#36716;&#25442;&#20013;&#30340;&#25345;&#20037;&#24615;&#20316;&#20026;&#25552;&#31034;&#26469;&#35299;&#20915;&#28145;&#24230;&#27169;&#31946;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#22914;&#20309;&#20869;&#37096;&#34920;&#31034;&#19977;&#32500;&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#24418;&#27169;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#20108;&#32500;&#28857;&#30340;&#36816;&#21160;&#20013;&#25512;&#26029;&#19977;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#23398;&#20064;&#21464;&#25442;&#30340;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35828;&#26126;&#20154;&#31867;&#22914;&#20309;&#22312;&#21457;&#32946;&#25110;&#28436;&#21270;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#21457;&#23637;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#38598;&#20013;&#20110;&#26059;&#36716;&#36816;&#21160;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22914;&#20309;&#20174;&#31227;&#21160;&#30340;&#20108;&#32500;&#25237;&#24433;&#28857;&#20013;&#25512;&#26029;&#28145;&#24230;&#65292;&#20174;2D&#35757;&#32451;&#21050;&#28608;&#20013;&#23398;&#20064;&#19977;&#32500;&#26059;&#36716;&#21464;&#25442;&#65292;&#24182;&#19982;&#20154;&#30340;&#24515;&#29702;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
When interacting in a three dimensional world, humans must estimate 3D structure from visual inputs projected down to two dimensional retinal images. It has been shown that humans use the persistence of object shape over motion-induced transformations as a cue to resolve depth ambiguity when solving this underconstrained problem. With the aim of understanding how biological vision systems may internally represent 3D transformations, we propose a computational model, based on a generative manifold model, which can be used to infer 3D structure from the motion of 2D points. Our model can also learn representations of the transformations with minimal supervision, providing a proof of concept for how humans may develop internal representations on a developmental or evolutionary time scale. Focused on rotational motion, we show how our model infers depth from moving 2D projected points, learns 3D rotational transformations from 2D training stimuli, and compares to human performance on psych
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#19979;&#29190;&#28809;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17769</link><description>&lt;p&gt;
&#29190;&#28809;&#20998;&#31867;&#22120;&#35774;&#35745;&#20013;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Domain Knowledge integrated for Blast Furnace Classifier Design. (arXiv:2303.17769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#19979;&#29190;&#28809;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29190;&#28809;&#24314;&#27169;&#21644;&#25511;&#21046;&#26159;&#24037;&#19994;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#40657;&#21283;&#23376;&#27169;&#22411;&#26159;&#25551;&#36848;&#22797;&#26434;&#29190;&#28809;&#31995;&#32479;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#20026;&#20102;&#28385;&#36275;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#21644;&#33410;&#33021;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#34701;&#20837;&#23398;&#20064;&#26041;&#26696;&#20801;&#35768;&#29992;&#25143;&#26356;&#27491;&#30830;&#22320;&#21019;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20197;&#35782;&#21035;&#8220;&#37325;&#35201;&#26679;&#26412;&#8221;&#65288;&#20854;&#38169;&#35823;&#20998;&#31867;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#24688;&#24403;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#36890;&#36807;&#20004;&#20010;&#30495;&#23454;&#30340;&#29190;&#28809;&#25968;&#25454;&#38598;&#24471;&#21040;&#39564;&#35777;&#65292;&#36825;&#23558;&#25351;&#23548;&#25805;&#20316;&#20154;&#21592;&#26356;&#22909;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#20808;&#21069;&#32463;&#39564;&#26469;&#25511;&#21046;&#29190;&#28809;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blast furnace modeling and control is one of the important problems in the industrial field, and the black-box model is an effective mean to describe the complex blast furnace system. In practice, there are often different learning targets, such as safety and energy saving in industrial applications, depending on the application. For this reason, this paper proposes a framework to design a domain knowledge integrated classification model that yields a classifier for industrial application. Our knowledge incorporated learning scheme allows the users to create a classifier that identifies "important samples" (whose misclassifications can lead to severe consequences) more correctly, while keeping the proper precision of classifying the remaining samples. The effectiveness of the proposed method has been verified by two real blast furnace datasets, which guides the operators to utilize their prior experience for controlling the blast furnace systems better.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#26041;&#27861;(iBaML)&#65292;&#36890;&#36807;&#23558;&#38544;&#24335;&#26799;&#24230;&#30340;&#20248;&#21183;&#24212;&#29992;&#21040;&#27010;&#29575;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#32531;&#35299;&#20102;&#21487;&#25193;&#23637;&#24615;&#30340;&#29942;&#39048;&#65292;&#24182;&#37327;&#21270;&#20102;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#22791;&#33391;&#22909;&#30340;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.17768</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#65306;&#22522;&#20110;&#24191;&#20041;&#38544;&#24335;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian Meta-Learning through Generalized Implicit Gradients. (arXiv:2303.17768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#26041;&#27861;(iBaML)&#65292;&#36890;&#36807;&#23558;&#38544;&#24335;&#26799;&#24230;&#30340;&#20248;&#21183;&#24212;&#29992;&#21040;&#27010;&#29575;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#32531;&#35299;&#20102;&#21487;&#25193;&#23637;&#24615;&#30340;&#29942;&#39048;&#65292;&#24182;&#37327;&#21270;&#20102;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#22791;&#33391;&#22909;&#30340;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#22788;&#29702;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20852;&#20219;&#21153;&#26102;&#20855;&#26377;&#29420;&#29305;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;&#23558;&#20854;&#35270;&#20026;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#25581;&#31034;&#20986;&#23427;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#20869;&#23618;&#20248;&#21270;&#20381;&#36182;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#26102;&#65292;&#20854;&#31639;&#27861;&#35270;&#35282;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#38544;&#24335;&#26799;&#24230;&#30340;&#20248;&#21183;&#24212;&#29992;&#21040;&#27010;&#29575;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#32531;&#35299;&#20102;&#21487;&#25193;&#23637;&#24615;&#30340;&#29942;&#39048;&#12290;&#26032;&#39062;&#30340;&#38544;&#24335;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#65288;iBaML&#65289;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#21487;&#23398;&#20064;&#20808;&#39564;&#30340;&#33539;&#22260;&#65292;&#36824;&#37327;&#21270;&#20102;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#20869;&#23618;&#20248;&#21270;&#36712;&#36857;&#22914;&#20309;&#65292;&#26368;&#32456;&#22797;&#26434;&#24615;&#22343;&#24471;&#21040;&#33391;&#22909;&#25511;&#21046;&#12290;&#20998;&#26512;&#35823;&#24046;&#30028;&#23450;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning owns unique effectiveness and swiftness in tackling emerging tasks with limited data. Its broad applicability is revealed by viewing it as a bi-level optimization problem. The resultant algorithmic viewpoint however, faces scalability issues when the inner-level optimization relies on gradient-based iterations. Implicit differentiation has been considered to alleviate this challenge, but it is restricted to an isotropic Gaussian prior, and only favors deterministic meta-learning approaches. This work markedly mitigates the scalability bottleneck by cross-fertilizing the benefits of implicit differentiation to probabilistic Bayesian meta-learning. The novel implicit Bayesian meta-learning (iBaML) method not only broadens the scope of learnable priors, but also quantifies the associated uncertainty. Furthermore, the ultimate complexity is well controlled regardless of the inner-level optimization trajectory. Analytical error bounds are established to demonstrate the precisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.17765</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20284;&#30340;&#32447;&#24615;&#34920;&#31034;&#65306;&#36866;&#24212;&#24615;&#12289;&#26497;&#23567;&#21270;&#12289;&#20197;&#21450;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#27424;&#32570;&#12290;&#26412;&#25991;&#26088;&#22312;&#29702;&#35299;&#20174;&#20855;&#26377;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#32447;&#24615;&#34920;&#31034;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#24120;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#20219;&#21153;&#25110;&#20165;&#30446;&#26631;&#23398;&#20064;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;TABA&#65289;&#8221;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17764</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;&#25239;&#40065;&#26834;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Continual Learning. (arXiv:2303.17764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;TABA&#65289;&#8221;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#25345;&#32493;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#28789;&#27963;&#24615;&#20351;&#24471;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#34429;&#28982;&#22312;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26377;&#35768;&#22810;&#20851;&#20110;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20294;&#20445;&#25252;&#25345;&#32493;&#23398;&#20064;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#23578;&#26410;&#21463;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;Task-Aware Boundary Augmentation&#65292;TABA&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;TABA&#22312;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that models trained by continual learning can achieve the comparable performances as the standard supervised learning and the learning flexibility of continual learning models enables their wide applications in the real world. Deep learning models, however, are shown to be vulnerable to adversarial attacks. Though there are many studies on the model robustness in the context of standard supervised learning, protecting continual learning from adversarial attacks has not yet been investigated. To fill in this research gap, we are the first to study adversarial robustness in continual learning and propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary \textbf{A}ugmentation (TABA) to boost the robustness of continual learning models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the efficacy of adversarial training and TABA in defending adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#23545;&#20110;&#20256;&#32479;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17745</link><description>&lt;p&gt;
L2&#33539;&#25968;&#19979;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note On Nonlinear Regression Under L2 Loss. (arXiv:2303.17745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17745
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#23545;&#20110;&#20256;&#32479;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;L2&#33539;&#25968;&#65288;&#24179;&#26041;&#25439;&#22833;&#65289;&#20989;&#25968;&#19979;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#23548;&#33268;&#21442;&#25968;&#38598;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#38024;&#23545;&#20256;&#32479;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the nonlinear regression problem under L2 loss (square loss) functions. Traditional nonlinear regression models often result in non-convex optimization problems with respect to the parameter set. We show that a convex nonlinear regression model exists for the traditional least squares problem, which can be a promising towards designing more complex systems with easier to train models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#22686;&#30410;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#31561;&#31639;&#27861;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.17732</link><description>&lt;p&gt;
&#26368;&#20248;&#36755;&#20837;&#22686;&#30410;&#65306;&#36229;&#32423;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network. (arXiv:2303.17732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17732
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#22686;&#30410;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#31561;&#31639;&#27861;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#30340;&#32447;&#24615;&#36716;&#25442;&#25913;&#21464;&#20102;&#31561;&#25928;&#30340;&#21069;&#39304;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32447;&#24615;&#21464;&#25442;&#34987;&#35270;&#20026;&#19982;&#23454;&#38469;&#35757;&#32451;&#20998;&#31163;&#30340;&#39044;&#22788;&#29702;&#25805;&#20316;&#12290;&#20174;&#31561;&#25928;&#32593;&#32476;&#24320;&#22987;&#65292;&#36890;&#36807;&#32447;&#24615;&#36716;&#25442;&#23545;&#36755;&#20837;&#36827;&#34892;&#39044;&#22788;&#29702;&#31561;&#25928;&#20110;&#22312;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#20013;&#23558;&#36127;&#26799;&#24230;&#30697;&#38453;&#19982;&#33258;&#30456;&#20851;&#30697;&#38453;&#30456;&#20056;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#38454;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#22312;&#32473;&#23450;&#36845;&#20195;&#20013;&#26368;&#22823;&#21270;&#23398;&#20064;&#30340;&#33258;&#30456;&#20851;&#30697;&#38453;&#12290;&#24403;&#33258;&#30456;&#20851;&#30697;&#38453;&#20026;&#23545;&#35282;&#32447;&#30697;&#38453;&#26102;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#36755;&#20837;&#22686;&#30410;&#12290;&#35813;&#26368;&#20248;&#36755;&#20837;&#22686;&#30410;&#65288;OIG&#65289;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#20004;&#20010;&#19968;&#38454;&#20108;&#32423;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#65288;HWO&#65289;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#20132;&#26367;&#26356;&#26032;&#36755;&#20837;&#26435;&#37325;&#24182;&#35299;&#32447;&#24615;&#26041;&#31243;&#20197;&#30830;&#23450;&#36755;&#20986;&#26435;&#37325;&#12290;&#32467;&#26524;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;OIG&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31532;&#19968;&#39034;&#24207;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear transformation of the inputs alters the training performance of feed-forward networks that are otherwise equivalent. However, most linear transforms are viewed as a pre-processing operation separate from the actual training. Starting from equivalent networks, it is shown that pre-processing inputs using linear transformation are equivalent to multiplying the negative gradient matrix with an autocorrelation matrix per training iteration. Second order method is proposed to find the autocorrelation matrix that maximizes learning in a given iteration. When the autocorrelation matrix is diagonal, the method optimizes input gains. This optimal input gain (OIG) approach is used to improve two first-order two-stage training algorithms, namely back-propagation (BP) and hidden weight optimization (HWO), which alternately update the input weights and solve linear equations for output weights. Results show that the proposed OIG approach greatly enhances the performance of the first-order al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#31216;&#20026;&#946;4-IRT&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26469;&#22686;&#24378;&#21028;&#21035;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#946;3-IRT&#30340;&#23545;&#31216;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17731</link><description>&lt;p&gt;
&#946;4-IRT&#65306;&#19968;&#31181;&#20855;&#26377;&#22686;&#24378;&#21028;&#21035;&#21147;&#20272;&#35745;&#30340;&#26032;&#946;3-IRT&#12290;
&lt;/p&gt;
&lt;p&gt;
$\beta^{4}$-IRT: A New $\beta^{3}$-IRT with Enhanced Discrimination Estimation. (arXiv:2303.17731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#31216;&#20026;&#946;4-IRT&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26469;&#22686;&#24378;&#21028;&#21035;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#946;3-IRT&#30340;&#23545;&#31216;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#26088;&#22312;&#20174;&#19981;&#21516;&#38590;&#24230;&#31561;&#32423;&#30340;&#39033;&#30446;&#20013;&#25512;&#26029;&#20986;&#21463;&#35797;&#32773;&#26410;&#35266;&#23519;&#21040;&#30340;&#33021;&#21147;&#21644;&#29305;&#24449;&#12290;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#22914;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#21709;&#24212;&#12289;&#21453;&#24212;&#26102;&#38388;&#12289;&#22810;&#37325;&#21709;&#24212;&#31561;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#29256;&#26412;&#30340;&#946;3-IRT&#65292;&#31216;&#20026;&#946;4-IRT&#65292;&#35813;&#29256;&#26412;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;&#946;3-IRT&#20013;&#65292;&#33021;&#21147;&#21644;&#38590;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#27492;&#25105;&#20204;&#37319;&#29992;&#38142;&#25509;&#20989;&#25968;&#23558;&#946;4-IRT&#36716;&#21464;&#20026;&#26080;&#32422;&#26463;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#12290;&#21407;&#22987;&#30340;&#946;3-IRT&#23384;&#22312;&#23545;&#31216;&#38382;&#39064;&#65292;&#21363;&#22914;&#26524;&#26576;&#20010;&#39033;&#30446;&#30340;&#21028;&#21035;&#21147;&#20540;&#31526;&#21495;&#38169;&#35823;&#65288;&#20363;&#22914;&#36127;&#20540;&#65292;&#32780;&#23454;&#38469;&#19978;&#26159;&#27491;&#30340;&#65289;&#65292;&#25311;&#21512;&#36807;&#31243;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#35813;&#39033;&#30446;&#30340;&#27491;&#30830;&#21028;&#21035;&#21147;&#21644;&#38590;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item response theory aims to estimate respondent's latent skills from their responses in tests composed of items with different levels of difficulty. Several models of item response theory have been proposed for different types of tasks, such as binary or probabilistic responses, response time, multiple responses, among others. In this paper, we propose a new version of $\beta^3$-IRT, called $\beta^{4}$-IRT, which uses the gradient descent method to estimate the model parameters. In $\beta^3$-IRT, abilities and difficulties are bounded, thus we employ link functions in order to turn $\beta^{4}$-IRT into an unconstrained gradient descent process. The original $\beta^3$-IRT had a symmetry problem, meaning that, if an item was initialised with a discrimination value with the wrong sign, e.g. negative when the actual discrimination should be positive, the fitting process could be unable to recover the correct discrimination and difficulty values for the item. In order to tackle this limita
&lt;/p&gt;</description></item><item><title>BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.17727</link><description>&lt;p&gt;
BOLT&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17727
&lt;/p&gt;
&lt;p&gt;
BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#23545;&#20110;&#27665;&#20027;&#21270;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#30446;&#21069;&#65292;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#24191;&#27867;&#20351;&#29992;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#20363;&#22914;GPU&#65289;&#65292;&#36825;&#20123;&#21152;&#36895;&#22120;&#20165;&#38480;&#20110;&#23569;&#25968;&#20855;&#26377;&#30456;&#24403;&#36130;&#21153;&#36164;&#28304;&#30340;&#26426;&#26500;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#24102;&#26469;&#24778;&#20154;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BOLT&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;BOLT&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#29992;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#35813;API&#23545;&#20110;&#29616;&#26377;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#29087;&#24713;&#30340;&#12290;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#19987;&#29992;&#36229;&#21442;&#25968;&#65292;BOLT&#20063;&#25277;&#35937;&#25481;&#20102;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#23567;&#25209;&#37327;&#22312;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#22686;&#21152;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#25928;&#26524;&#19979;&#38477;&#65292;&#36827;&#19968;&#27493;&#25552;&#37266;&#20154;&#20204;&#20302;&#20272;&#30495;&#23454;&#30340;&#23545;&#25239;&#25915;&#20987;&#24378;&#24230;&#24182;&#39640;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17720</link><description>&lt;p&gt;
&#22312;&#23567;&#25209;&#37327;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness. (arXiv:2303.17720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#23567;&#25209;&#37327;&#22312;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#22686;&#21152;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#25928;&#26524;&#19979;&#38477;&#65292;&#36827;&#19968;&#27493;&#25552;&#37266;&#20154;&#20204;&#20302;&#20272;&#30495;&#23454;&#30340;&#23545;&#25239;&#25915;&#20987;&#24378;&#24230;&#24182;&#39640;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20294;&#21516;&#26102;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#24102;&#26469;&#30340;&#23041;&#32961;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#19982;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#24378;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26469;&#35299;&#20915;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#22686;&#21152;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25928;&#26524;&#30340;&#19979;&#38477;&#65292;&#24182;&#23558;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#19982;&#26799;&#24230;&#28040;&#22833;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#23567;&#25209;&#37327;&#22823;&#23567;&#19981;&#20250;&#38477;&#20302;&#23545;&#25239;&#24615;&#26679;&#26412;&#24378;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#20302;&#20272;&#23545;&#25239;&#24615;&#25915;&#20987;&#30495;&#23454;&#65288;&#23454;&#38469;&#65289;&#24378;&#24230;&#21644;&#39640;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#25552;&#39640;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been proven to be both highly effective within computer vision, and highly vulnerable to adversarial attacks. Consequently, as the use of neural networks increases due to their unrivaled performance, so too does the threat posed by adversarial attacks. In this work, we build towards addressing the challenge of adversarial robustness by exploring the relationship between the mini-batch size used during adversarial sample generation and the strength of the adversarial samples produced. We demonstrate that an increase in mini-batch size results in a decrease in the efficacy of the samples produced, and we draw connections between these observations and the phenomenon of vanishing gradients. Next, we formulate loss functions such that adversarial sample strength is not degraded by mini-batch size. Our findings highlight a potential risk for underestimating the true (practical) strength of adversarial attacks, and a risk of overestimating a model's robustness. We share 
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17716</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17716
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24403;&#26631;&#31614;&#25968;&#30446;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Multiclass Littlestone&#32500;&#24230;&#65292;&#36825;&#20010;&#27010;&#24565;&#39318;&#27425;&#20986;&#29616;&#22312;\cite{DanielyERMprinciple}&#20013;&#65292;&#32487;&#32493;&#21051;&#30011;&#20102;&#35813;&#22330;&#26223;&#19979;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;\cite{Brukhimetal2022}&#32473;&#20986;&#20102;&#24403;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#24773;&#20917;&#19979;&#25209;&#22788;&#29702;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.17708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#20013;&#30340;&#25925;&#38556;&#21644;&#39118;&#38505;&#20998;&#26512;&#65306;&#20197;ONNX&#29983;&#24577;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem. (arXiv:2303.17708v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#24320;&#21457;&#65292;&#20248;&#21270;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#26694;&#26550;&#20013;&#20351;&#29992;&#21644;&#37325;&#26032;&#20351;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#36816;&#34892;&#26102;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#22312;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#24037;&#31243;&#24072;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#23558;&#27169;&#22411;&#20174;&#26694;&#26550;&#31227;&#21160;&#21040;&#36816;&#34892;&#26102;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36716;&#25442;&#22120;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#24182;&#30772;&#22351;&#37096;&#32626;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#39057;&#29575;&#21644;&#25925;&#38556;&#27169;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;ONNX (Open Neural Network eXchange)&#30456;&#20851;&#30340;&#27169;&#22411;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ONNX&#36716;&#25442;&#22120;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;DL&#26694;&#26550;PyTorch&#21644;TensorFlow&#20013;&#30340;&#36807;&#21435;&#25925;&#38556;&#12290;&#36824;&#25253;&#21578;&#20102;&#25925;&#38556;&#65288;N=200&#20010;&#38382;&#39064;&#65289;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36716;&#25442;8,797&#20010;&#27169;&#22411;&#65288;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#23454;&#20363;&#65289;&#26469;&#35780;&#20272;&#24403;&#20170;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers develop, fine-tune, and deploy deep learning (DL) models. They use and re-use models in a variety of development frameworks and deploy them on a range of runtime environments. In this diverse ecosystem, engineers use DL model converters to move models from frameworks to runtime environments. However, errors in converters can compromise model quality and disrupt deployment. The failure frequency and failure modes of DL model converters are unknown.  In this paper, we conduct the first failure analysis on DL model converters. Specifically, we characterize failures in model converters associated with ONNX (Open Neural Network eXchange). We analyze past failures in the ONNX converters in two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and locations of failures (for N=200 issues), and trends over time are also reported. We also evaluate present-day failures by converting 8,797 models, both real-world and synthetically generated instances. The consis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;U-Net-based&#26550;&#26500;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#25429;&#33719;&#36890;&#36947;&#21644;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#35299;&#20915;&#20102;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17696</link><description>&lt;p&gt;
&#21452;&#37325;&#20132;&#21449;&#27880;&#24847;&#21147;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dual Cross-Attention for Medical Image Segmentation. (arXiv:2303.17696v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;U-Net-based&#26550;&#26500;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#25429;&#33719;&#36890;&#36947;&#21644;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#35299;&#20915;&#20102;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#21452;&#37325;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;&#22522;&#20110;U-Net&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;DCA&#36890;&#36807;&#20381;&#27425;&#25429;&#33719;&#22810;&#23610;&#24230;&#32534;&#30721;&#22120;&#29305;&#24449;&#20013;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#20381;&#36182;&#24615;&#26469;&#35299;&#20915;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#38382;&#39064;&#12290;DCA&#27169;&#22359;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#20013;&#65292;&#27604;&#22914;U-Net&#21450;&#20854;&#21464;&#31181;&#12290;&#26412;&#25991;&#23558;DCA&#27169;&#22359;&#38598;&#25104;&#21040;&#20845;&#31181;U-Net-based&#26550;&#26500;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dual Cross-Attention (DCA), a simple yet effective attention module that is able to enhance skip-connections in U-Net-based architectures for medical image segmentation. DCA addresses the semantic gap between encoder and decoder features by sequentially capturing channel and spatial dependencies across multi-scale encoder features. First, the Channel Cross-Attention (CCA) extracts global channel-wise dependencies by utilizing cross-attention across channel tokens of multi-scale encoder features. Then, the Spatial Cross-Attention (SCA) module performs cross-attention to capture spatial dependencies across spatial tokens. Finally, these fine-grained encoder features are up-sampled and connected to their corresponding decoder parts to form the skip-connection scheme. Our proposed DCA module can be integrated into any encoder-decoder architecture with skip-connections such as U-Net and its variants. We test our DCA module by integrating it into six U-Net-based architectures such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#23548;&#21521;&#20132;&#20114;&#24314;&#27169;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17695</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#20027;&#35266;&#30693;&#35782;&#20132;&#20114;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task Oriented Conversational Modelling With Subjective Knowledge. (arXiv:2303.17695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#23548;&#21521;&#20132;&#20114;&#24314;&#27169;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#35805;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#25968;&#25454;&#24211;&#21644;API&#30340;&#31995;&#32479;&#26469;&#22788;&#29702;&#30340;&#12290;&#20294;&#26159;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#36825;&#20123;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#23458;&#25143;&#35780;&#20215;&#21644;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#20013;&#25214;&#21040;&#12290;DSTC-11&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#30693;&#35782;&#23547;&#27714;&#22238;&#21512;&#26816;&#27979;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#20027;&#35266;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#26356;&#24555;&#30340;&#30693;&#35782;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20102;7&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;4\%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing conversational models are handled by a database(DB) and API based systems. However, very often users' questions require information that cannot be handled by such systems. Nonetheless, answers to these questions are available in the form of customer reviews and FAQs. DSTC-11 proposes a three stage pipeline consisting of knowledge seeking turn detection, knowledge selection and response generation to create a conversational model grounded on this subjective knowledge. In this paper, we focus on improving the knowledge selection module to enhance the overall system performance. In particular, we propose entity retrieval methods which result in an accurate and faster knowledge search. Our proposed Named Entity Recognition (NER) based entity retrieval method results in 7X faster search compared to the baseline model. Additionally, we also explore a potential keyword extraction method which can improve the accuracy of knowledge selection. Preliminary results show a 4 \% improvement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.17674</link><description>&lt;p&gt;
&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#12290;&#21487;&#36798;&#38598;&#22312;&#25511;&#21046;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#35745;&#31639;&#36215;&#26469;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#36807;&#36924;&#36817;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#65292;&#23558;&#20854;&#34920;&#31034;&#25104;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#36825;&#20010;&#26377;&#38480;&#32500;&#30340;&#21051;&#30011;&#24320;&#21551;&#20102;&#19968;&#31181;&#32039;&#23494;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#65292;&#19988;&#25104;&#26412;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20302;&#12289;&#26356;&#31934;&#20934;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#31070;&#32463;&#21453;&#39304;&#29615;&#20998;&#26512;&#21644;&#40065;&#26834;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;ResNets&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23478;&#26063;&#38480;&#21046;&#26680;&#65292;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#22312;&#26080;&#38480;&#28145;&#24230;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#33267;&#31070;&#32463;CDE&#12290;</title><link>http://arxiv.org/abs/2303.17671</link><description>&lt;p&gt;
&#31070;&#32463;&#31614;&#21517;&#26680;&#20316;&#20026;&#21463;&#25511;ResNets&#30340;&#26080;&#38480;&#23485;&#24230;-&#28145;&#24230;&#26497;&#38480;&#12290;(arXiv:2303.17671v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
Neural signature kernels as infinite-width-depth-limits of controlled ResNets. (arXiv:2303.17671v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17671
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;ResNets&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23478;&#26063;&#38480;&#21046;&#26680;&#65292;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#22312;&#26080;&#38480;&#28145;&#24230;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#33267;&#31070;&#32463;CDE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27785;&#31215;&#35745;&#31639;&#33539;&#20363;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#30001;&#31070;&#32463;&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;CDE&#65289;&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#23450;&#20041;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#21463;&#25511;ResNets&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#23485;&#24230;-&#28145;&#24230;&#38480;&#21046;&#21644;&#36866;&#24403;&#30340;&#32553;&#25918;&#19979;&#65292;&#36825;&#20123;&#26550;&#26500;&#24369;&#25910;&#25947;&#21040;&#19968;&#20123;&#36830;&#32493;&#36335;&#24452;&#31354;&#38388;&#19978;&#32034;&#24341;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#28385;&#36275;&#26681;&#25454;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#21464;&#21270;&#30340;&#26576;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26680;&#12290;&#22312;&#28608;&#27963;&#20026;&#24658;&#31561;&#24335;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#35813;&#26041;&#31243;&#24335;&#31616;&#21270;&#20026;&#32447;&#24615;PDE&#65292;&#26497;&#38480;&#26680;&#19982;Salvi&#31561;&#20154;&#30340;&#31614;&#21517;&#26680;&#19968;&#33268;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#23485;&#24230;-&#28145;&#24230;&#26497;&#38480;&#26159;&#21487;&#20132;&#25442;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#30340;&#38480;&#21046;&#26680;&#23478;&#26063;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#28145;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;CDE&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;w&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs). We show that in the infinite-width-then-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function. In the special case where the activation is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021). In this setting, we also show that the width-depth limits commute. We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20851;&#38190;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#65292;&#24182;&#25104;&#21151;&#22312;500&#20221;&#26679;&#26412;&#20013;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#12290;</title><link>http://arxiv.org/abs/2303.17661</link><description>&lt;p&gt;
MetaEnhance:&#22823;&#23398;&#22270;&#20070;&#39302;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20803;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries. (arXiv:2303.17661v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20851;&#38190;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#65292;&#24182;&#25104;&#21151;&#22312;500&#20221;&#26679;&#26412;&#20013;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23545;&#35937;&#30340;&#20803;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#36890;&#36807;&#25968;&#23383;&#24211;&#30028;&#38754;&#36827;&#34892;&#21457;&#29616;&#38750;&#24120;&#37325;&#35201;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#25968;&#23383;&#23545;&#35937;&#30340;&#20803;&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;&#19981;&#23436;&#25972;&#12289;&#19981;&#19968;&#33268;&#21644;&#19981;&#27491;&#30830;&#30340;&#20540;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#26816;&#27979;&#12289;&#32416;&#27491;&#21644;&#35268;&#33539;&#23398;&#26415;&#20803;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#19971;&#20010;&#20851;&#38190;&#23383;&#27573;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#36825;&#20123;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;MetaEnhance&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20803;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;500&#20010;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#65292;&#36890;&#36807;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#37319;&#26679;&#23376;&#38598;&#26469;&#32452;&#21512;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;MetaEnhance&#65292;&#32467;&#26524;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20960;&#20046;&#23436;&#32654;&#22320;&#23454;&#29616;&#20102;&#38169;&#35823;&#26816;&#27979;&#30340;F1&#20998;&#25968;&#20197;&#21450;&#20116;&#20010;&#23383;&#27573;&#20013;&#30340;&#38169;&#35823;&#32416;&#27491;&#30340;F1&#20998;&#25968;&#22312;0.85&#21040;1.00&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.
&lt;/p&gt;</description></item><item><title>CMS&#23454;&#39564;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#31890;&#23376;&#27969;&#31639;&#27861;&#65288;MLPF&#65289;&#24182;&#20248;&#21270;&#20102;&#20854;&#37325;&#26500;&#23454;&#29616;&#65292;&#39318;&#27425;&#20197;&#29983;&#25104;&#22120;/&#27169;&#25311;&#32423;&#31890;&#23376;&#20449;&#24687;&#20026;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#20026;&#25552;&#39640;&#26816;&#27979;&#22120;&#23545;&#29289;&#29702;&#37327;&#30340;&#21709;&#24212;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.17657</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;CMS&#31890;&#23376;&#27969;&#31639;&#27861;&#25913;&#36827;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Progress towards an improved particle flow algorithm at CMS with machine learning. (arXiv:2303.17657v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17657
&lt;/p&gt;
&lt;p&gt;
CMS&#23454;&#39564;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#31890;&#23376;&#27969;&#31639;&#27861;&#65288;MLPF&#65289;&#24182;&#20248;&#21270;&#20102;&#20854;&#37325;&#26500;&#23454;&#29616;&#65292;&#39318;&#27425;&#20197;&#29983;&#25104;&#22120;/&#27169;&#25311;&#32423;&#31890;&#23376;&#20449;&#24687;&#20026;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#20026;&#25552;&#39640;&#26816;&#27979;&#22120;&#23545;&#29289;&#29702;&#37327;&#30340;&#21709;&#24212;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#27969;&#65288;PF&#65289;&#31639;&#27861;&#26159;CMS&#23454;&#39564;&#22312;CERN LHC&#20013;&#20107;&#20214;&#37325;&#24314;&#30340;&#26680;&#24515;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#36712;&#36857;&#21644;&#37327;&#33021;&#22120;&#32858;&#31751;&#25512;&#26029;&#31890;&#23376;&#65292;&#24182;&#22312;&#39044;&#35745;&#20855;&#26377;&#22686;&#21152;&#22534;&#21472;&#21644;&#25506;&#27979;&#22120;&#31890;&#24230;&#30340;&#31532;&#20108;&#38454;&#27573;&#36816;&#34892;&#26465;&#20214;&#20013;&#37325;&#28857;&#24320;&#21457;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31890;&#23376;&#27969;&#65288;MLPF&#65289;&#31639;&#27861;&#24050;&#22312;CMS&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#21487;&#33021;&#20855;&#26377;&#30452;&#25509;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;&#29289;&#29702;&#37327;&#12289;&#39640;&#24230;&#21487;&#37325;&#26500;&#21040;&#26032;&#26465;&#20214;&#21644;&#22825;&#28982;&#36866;&#21512;&#20110;&#24322;&#26500;&#21152;&#36895;&#22120;&#37096;&#32626;&#30340;&#20248;&#28857;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;CMS&#23545;MLPF&#37325;&#26500;&#23454;&#29616;&#30340;&#25913;&#36827;&#36827;&#23637;&#65292;&#39318;&#27425;&#20197;&#29983;&#25104;&#22120;/&#27169;&#25311;&#32423;&#31890;&#23376;&#20449;&#24687;&#20026;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#20026;&#28508;&#22312;&#22320;&#25552;&#39640;&#26816;&#27979;&#22120;&#23545;&#24863;&#20852;&#36259;&#30340;&#29289;&#29702;&#37327;&#30340;&#21709;&#24212;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;
&lt;/p&gt;
&lt;p&gt;
The particle-flow (PF) algorithm, which infers particles based on tracks and calorimeter clusters, is of central importance to event reconstruction in the CMS experiment at the CERN LHC, and has been a focus of development in light of planned Phase-2 running conditions with an increased pileup and detector granularity. In recent years, the machine learned particle-flow (MLPF) algorithm, a graph neural network that performs PF reconstruction, has been explored in CMS, with the possible advantages of directly optimizing for the physical quantities of interest, being highly reconfigurable to new conditions, and being a natural fit for deployment to heterogeneous accelerators. We discuss progress in CMS towards an improved implementation of the MLPF reconstruction, now optimized using generator/simulation-level particle information as the target for the first time. This paves the way to potentially improving the detector response in terms of physical quantities of interest. We describe the
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35797;&#39564;&#26694;&#26550;PEX&#65292;&#21033;&#29992;HTE&#24314;&#27169;&#21644;&#24207;&#21015;&#20915;&#31574;&#31574;&#30053;&#20248;&#21270;&#65292;&#22312;&#29992;&#25143;&#32423;&#21035;&#19978;&#20248;&#21270;&#27835;&#30103;&#32452;&#20998;&#37197;&#65292;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17648</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35797;&#39564;&#19979;&#30340;&#23454;&#29992;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Practical Policy Optimization with Personalized Experimentation. (arXiv:2303.17648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35797;&#39564;&#26694;&#26550;PEX&#65292;&#21033;&#29992;HTE&#24314;&#27169;&#21644;&#24207;&#21015;&#20915;&#31574;&#31574;&#30053;&#20248;&#21270;&#65292;&#22312;&#29992;&#25143;&#32423;&#21035;&#19978;&#20248;&#21270;&#27835;&#30103;&#32452;&#20998;&#37197;&#65292;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32452;&#32455;&#36890;&#36807;&#23454;&#39564;&#24179;&#21488;&#26469;&#27979;&#37327;&#22788;&#29702;&#25928;&#26524;&#65292;&#20197;&#22312;&#20840;&#38754;&#37096;&#32626;&#20043;&#21069;&#35780;&#20272;&#20135;&#21697;&#21464;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#23454;&#39564;&#24179;&#21488;&#23545;&#34920;&#29616;&#20986;&#24322;&#36136;&#22788;&#29702;&#25928;&#24212;&#30340;&#26368;&#32456;&#29992;&#25143;&#20154;&#32676;&#65288;HTE&#65289;&#30340;&#25928;&#26524;&#24182;&#19981;&#29702;&#24819;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35797;&#39564;&#26694;&#26550;&#65292;&#20010;&#24615;&#21270;&#35797;&#39564;&#65288;PEX&#65289;&#65292;&#36890;&#36807;HTE&#24314;&#27169;&#21644;&#39034;&#24207;&#20915;&#31574;&#31574;&#30053;&#20248;&#21270;&#65292;&#20197;&#22312;&#29992;&#25143;&#32423;&#21035;&#19978;&#20248;&#21270;&#27835;&#30103;&#32452;&#20998;&#37197;&#65292;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20174;&#23454;&#36341;&#20013;&#35777;&#26126;&#25104;&#21151;&#24182;&#21487;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;&#36731;&#26494;&#23454;&#29616;&#30340;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many organizations measure treatment effects via an experimentation platform to evaluate the casual effect of product variations prior to full-scale deployment. However, standard experimentation platforms do not perform optimally for end user populations that exhibit heterogeneous treatment effects (HTEs). Here we present a personalized experimentation framework, Personalized Experiments (PEX), which optimizes treatment group assignment at the user level via HTE modeling and sequential decision policy optimization to optimize multiple short-term and long-term outcomes simultaneously. We describe an end-to-end workflow that has proven to be successful in practice and can be readily implemented using open-source software.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24076;&#33098;&#30340;&#29992;&#27700;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#21508;&#31639;&#27861;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.17617</link><description>&lt;p&gt;
&#22522;&#20110;&#24076;&#33098;&#25968;&#25454;&#30340;&#29992;&#27700;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An evaluation of time series forecasting models on water consumption data: A case study of Greece. (arXiv:2303.17617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24076;&#33098;&#30340;&#29992;&#27700;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#21508;&#31639;&#27861;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22478;&#24066;&#21270;&#21644;&#24037;&#19994;&#21270;&#30340;&#21152;&#21095;&#23548;&#33268;&#27700;&#36164;&#28304;&#38656;&#27714;&#19981;&#26029;&#19978;&#21319;&#65292;&#20174;&#32780;&#21152;&#22823;&#20102;&#20379;&#38656;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36866;&#24403;&#30340;&#29992;&#27700;&#20998;&#37197;&#21644;&#29992;&#27700;&#37327;&#39044;&#27979;&#26159;&#32531;&#35299;&#20379;&#38656;&#22833;&#34913;&#12289;&#25913;&#21892;&#27700;&#36164;&#28304;&#36816;&#33829;&#12289;&#35268;&#21010;&#21644;&#31649;&#29702;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#33879;&#21517;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#65292;&#38024;&#23545;&#24076;&#33098;&#30340;&#29992;&#27700;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22810;&#20803;&#31038;&#20250;&#32463;&#27982;&#21644;&#22478;&#24066;&#21270;&#38382;&#39064;&#30340;&#22269;&#23478;&#12290;&#25105;&#20204;&#37319;&#29992;&#30001;&#24076;&#33098;&#33258;&#26469;&#27700;&#21644;&#27745;&#27700;&#20844;&#21496;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#23545;&#39044;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27599;&#31181;&#31639;&#27861;&#21450;&#20854;&#24212;&#29992;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the increased urbanization and industrialization has led to a rising water demand and resources, thus increasing the gap between demand and supply. Proper water distribution and forecasting of water consumption are key factors in mitigating the imbalance of supply and demand by improving operations, planning and management of water resources. To this end, in this paper, several well-known forecasting algorithms are evaluated over time series, water consumption data from Greece, a country with diverse socio-economic and urbanization issues. The forecasting algorithms are evaluated on a real-world dataset provided by the Water Supply and Sewerage Company of Greece revealing key insights about each algorithm and its use.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#31958;&#26102;&#38388;&#24207;&#21015;&#30340;&#39046;&#22495;&#36716;&#25442;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#24110;&#21161;&#36991;&#20813;&#20302;&#34880;&#31958;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.17616</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#36716;&#25442;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#34880;&#31958;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Patterns Detection in Glucose Time Series by Domain Transformations and Deep Learning. (arXiv:2303.17616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#31958;&#26102;&#38388;&#24207;&#21015;&#30340;&#39046;&#22495;&#36716;&#25442;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#24110;&#21161;&#36991;&#20813;&#20302;&#34880;&#31958;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#24739;&#32773;&#38656;&#35201;&#31649;&#29702;&#34880;&#31958;&#27700;&#24179;&#20197;&#20351;&#20854;&#20445;&#25345;&#22312;&#36866;&#24403;&#33539;&#22260;&#20869;&#12290;&#39044;&#27979;&#26410;&#26469;&#30340;&#34880;&#31958;&#20540;&#26159;&#21542;&#20250;&#36229;&#20986;&#20581;&#24247;&#38408;&#20540;&#23545;&#20110;&#37319;&#21462;&#32416;&#27491;&#25514;&#26045;&#20197;&#36991;&#20813;&#28508;&#22312;&#20581;&#24247;&#25439;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#20197;&#20415;&#21487;&#20197;&#39044;&#27979;&#20302;&#34880;&#31958;&#20107;&#20214;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#26159;&#22312;&#34880;&#31958;&#26102;&#38388;&#24207;&#21015;&#19978;&#24212;&#29992;&#21464;&#25442;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20301;&#19981;&#21516;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
People with diabetes have to manage their blood glucose level to keep it within an appropriate range. Predicting whether future glucose values will be outside the healthy threshold is of vital importance in order to take corrective actions to avoid potential health damage. In this paper we describe our research with the aim of predicting the future behavior of blood glucose levels, so that hypoglycemic events may be anticipated. The approach of this work is the application of transformation functions on glucose time series, and their use in convolutional neural networks. We have tested our proposed method using real data from 4 different diabetes patients with promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#39044;&#27979;&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#33647;&#20998;&#23376;&#12290;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#65292;&#32780;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17615</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Utilizing Reinforcement Learning for de novo Drug Design. (arXiv:2303.17615v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#39044;&#27979;&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#33647;&#20998;&#23376;&#12290;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#65292;&#32780;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#26032;&#33647;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#23383;&#31526;&#20018;&#29983;&#25104;&#26032;&#20998;&#23376;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21508;&#31181;on-policy&#21644;off-policy &#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#23398;&#20064;&#22522;&#20110;RNN&#30340;&#31574;&#30053;&#65292;&#29983;&#25104;&#39044;&#27979;&#23545;&#20110;&#22810;&#24052;&#33018;&#21463;&#20307;DRD2&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#23545;&#20110;on-policy&#31639;&#27861;&#65292;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#37325;&#25918;&#39640;&#12289;&#20013;&#21644;&#20302;&#20998;&#23376;&#26102;&#65292;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approaches for generating novel drug molecules with specific properties have gained a lot of interest in the last years. Recent studies have demonstrated promising performance for string-based generation of novel molecules utilizing reinforcement learning. In this paper, we develop a unified framework for using reinforcement learning for de novo drug design, wherein we systematically study various on- and off-policy reinforcement learning algorithms and replay buffers to learn an RNN-based policy to generate novel molecules predicted to be active against the dopamine receptor DRD2. Our findings suggest that it is advantageous to use at least both top-scoring and low-scoring molecules for updating the policy when structural diversity is essential. Using all generated molecules at an iteration seems to enhance performance stability for on-policy algorithms. In addition, when replaying high, intermediate, and low-scoring molecules, off-policy algorithms display the pot
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17610</link><description>&lt;p&gt;
&#37319;&#29992;&#28789;&#27963;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#38598;&#21512;&#22825;&#27668;&#39044;&#25253;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#39044;&#25253;&#21518;&#22788;&#29702;&#26159;&#29983;&#25104;&#20934;&#30830;&#27010;&#29575;&#39044;&#25253;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#26159;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#25110;&#27599;&#20010;&#25552;&#21069;&#26399;&#20272;&#35745;&#21442;&#25968;&#32479;&#35745;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#12290;&#20026;&#20102;&#25918;&#23485;&#35768;&#22810;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#28789;&#27963;&#30340;&#21442;&#25968;&#20998;&#24067;&#20272;&#35745;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#25968;&#23398;&#30830;&#20999;&#30340;&#26041;&#24335;&#27169;&#25311;&#19981;&#21516;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;EUPPBench&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35813;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23545;&#35199;&#27431;&#22320;&#21306;&#23376;&#21306;&#22495;&#30340;&#31449;&#28857;&#36827;&#34892;&#20102;&#28201;&#24230;&#39044;&#25253;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25105;&#20204;&#20043;&#21069;&#30340;&#34920;&#29616;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17607</link><description>&lt;p&gt;
&#21457;&#29616;&#33258;&#28982;&#23450;&#24459;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17607
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#31890;&#23376;&#36981;&#24490;&#37327;&#23376;&#21147;&#23398;&#30340;&#21407;&#29702;&#8212;&#8212;&#37027;&#20040;&#23439;&#35266;&#21644;&#24494;&#35266;&#19990;&#30028;&#20043;&#38388;&#30340;&#26126;&#30830;&#30028;&#38480;&#22312;&#21738;&#37324;&#21602;&#65311;&#27491;&#26159;&#36825;&#20010;&#8220;&#35299;&#37322;&#38382;&#39064;&#8221;&#20419;&#20351;&#34203;&#23450;&#35860;&#25552;&#20986;&#20102;&#20182;&#33879;&#21517;&#30340;&#24605;&#24819;&#23454;&#39564;&#65288;&#19968;&#21482;&#21516;&#26102;&#27515;&#20129;&#21644;&#27963;&#30528;&#30340;&#29483;&#65289;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#37327;&#23376;&#27979;&#37327;&#38382;&#39064;&#30340;&#28608;&#28872;&#20105;&#35770;&#65292;&#20294;&#33267;&#20170;&#20173;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#36825;&#27491;&#26159;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#30340;&#35745;&#31639;&#27169;&#22411;&#26469;&#25551;&#36848;&#21644;&#29702;&#35299;&#33258;&#28982;&#23450;&#24459;&#12290;&#23454;&#38469;&#19978;&#65292;&#26080;&#35770;&#26159;&#23439;&#35266;&#31890;&#23376;&#12289;&#24494;&#35266;&#30005;&#23376;&#36824;&#26159;&#23433;&#20840;&#38382;&#39064;&#65292;&#23427;&#20204;&#37117;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#23454;&#20307;&#65292;&#36825;&#20010;&#23454;&#20307;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#21270;&#65292;&#21487;&#20197;&#29992;&#29366;&#24577;&#21644;&#20540;&#32452;&#25104;&#30340;&#25968;&#25454;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#35266;&#23519;&#32773;&#21487;&#20197;&#20174;&#36825;&#20010;&#25968;&#25454;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#26500;&#24314;&#29702;&#35770;&#65288;&#36890;&#24120;&#30001;&#20989;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#32452;&#25104;&#65289;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#29992;&#25143;&#30340;&#32463;&#39564;&#25110;&#36923;&#36753;&#26469;&#24314;&#27169;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#25454;&#12290;&#35745;&#31639;&#27169;&#22411;&#30340;&#26680;&#24515;&#22522;&#20110;&#20004;&#20010;&#36807;&#31243;&#65306;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#12290;&#20989;&#25968;&#36873;&#25321;&#36807;&#31243;&#31867;&#20284;&#20110;&#36798;&#23572;&#25991;&#30340;&#36827;&#21270;&#65292;&#20801;&#35768;&#20855;&#26377;&#20248;&#21183;&#29305;&#24449;&#30340;&#20989;&#25968;&#29983;&#23384;&#21644;&#32321;&#27542;&#65307;&#32780;&#36816;&#31639;&#31526;&#36873;&#25321;&#36807;&#31243;&#25429;&#25417;&#20102;&#33258;&#28982;&#23450;&#24459;&#30340;&#30456;&#20114;&#20381;&#23384;&#24615;&#65292;&#21487;&#20197;&#24179;&#34913;&#33258;&#28982;&#30028;&#20013;&#19981;&#21516;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#21147;&#23398;&#12289;&#32463;&#20856;&#21147;&#23398;&#21644;&#31995;&#32479;&#29983;&#29289;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.17001</link><description>&lt;p&gt;
G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#38477;&#32500;&#12289;&#32858;&#31867;&#21644;&#21435;&#22122;&#31561;&#39046;&#22495;&#23545;&#27969;&#24418;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#25991;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#12290;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#27839;&#30528;&#20302;&#32500;&#27969;&#24418;&#20256;&#25773;&#30340;&#20307;&#31215;&#65292;&#20854;&#20013;&#27599;&#20010;&#20307;&#31215;&#21487;&#20197;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26059;&#36716;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#38598;&#19978;&#30340;&#32676;&#30340;&#20316;&#29992;&#26469;&#24191;&#20041;&#21270;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#12290;&#25105;&#20204;&#26174;&#31034;&#20102;&#19982;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#31867;&#20284;&#65292;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;Laplace-Beltrami&#31639;&#23376;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#20855;&#26377;&#32676;&#20803;&#32032;&#21644;&#26576;&#20123;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#24352;&#37327;&#31215;&#24418;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;F&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#32954;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#20026;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#22914;COVID-19&#21644;&#32954;&#28814;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16904</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#32954;&#37096;CT&#25195;&#25551;&#23545;&#22320;&#29627;&#29827;&#24433;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65306;&#19968;&#20010;&#19977;&#22825;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration. (arXiv:2303.16904v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#32954;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#20026;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#22914;COVID-19&#21644;&#32954;&#28814;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29627;&#29827;&#24433;&#26159;&#35768;&#22810;&#32954;&#37096;&#30142;&#30149;&#30340;&#26631;&#24535;&#65292;&#21253;&#25324;COVID19&#21644;&#32954;&#28814;&#24739;&#32773;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26032;&#24314;&#30340;&#34394;&#25311;&#29615;&#22659;&#65292;&#22312;2023&#24180;3&#26376;17&#26085;&#21019;&#24314;&#65292;&#36890;&#36807;&#23454;&#39564;&#25506;&#35752;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#12289;ResNet&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20197;&#21450;&#24494;&#35843;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;ADAM&#20248;&#21270;&#31639;&#27861;&#23545;&#25152;&#26377;CNN&#26550;&#26500;&#36827;&#34892;&#26631;&#20934;&#23398;&#20064;&#29575;0.001&#30340;&#24494;&#35843;&#65292;&#24182;&#22312;&#39564;&#35777;&#25439;&#22833;&#36798;&#21040;&#24179;&#21407;&#26102;&#36827;&#34892;&#26089;&#20572;&#12290;&#20026;&#27599;&#20010;&#35757;&#32451;&#30340;CNN&#21046;&#23450;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#35268;&#21017;&#65292;&#24182;&#22312;&#19977;&#22825;&#20869;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-glass opacity is a hallmark of numerous lung diseases, including patients with COVID19 and pneumonia. This brief note presents experimental results of a proof-of-concept framework that got implemented and tested over three days as driven by the third challenge entitled "COVID-19 Competition", hosted at the AI-Enabled Medical Image Analysis Workshop of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). Using a newly built virtual environment (created on March 17, 2023), we investigated various pre-trained two-dimensional convolutional neural networks (CNN) such as Dense Neural Network, Residual Neural Networks (ResNet), and Vision Transformers, as well as the extent of fine-tuning. Based on empirical experiments, we opted to fine-tune them using ADAM's optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and apply early-stopping whenever the validation loss reached a plateau. For each trained CNN, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#26469;&#34917;&#25937;&#21442;&#25968;&#27874;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16066</link><description>&lt;p&gt;
&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#26469;&#34917;&#25937;&#21442;&#25968;&#27874;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#24322;&#26500;&#35774;&#22791;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;&#38750;iid&#65289;&#29305;&#24615;&#65292;&#23548;&#33268;&#26412;&#22320;&#26356;&#26032;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24433;&#21709;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#26412;&#22320;&#35757;&#32451;&#21644;&#32858;&#21512;&#36807;&#31243;&#20197;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#26410;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#21463;&#31070;&#32463;&#34928;&#31469;&#29616;&#35937;&#21551;&#21457;&#65292;&#25105;&#20204;&#24378;&#21046;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20854;&#21021;&#22987;&#21270;&#20026;&#38543;&#26426;&#30340;&#31616;&#21333;&#20845;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#24182;&#22312;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#23558;&#20854;&#20316;&#20026;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#21333;&#20803;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#22266;&#23450;&#12290;&#22312;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#23398;&#20064;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20026;&#27599;&#20010;&#31867;&#21035;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#65292;&#20197;&#34917;&#25937;&#30001;&#20110;&#31867;&#20869;&#26465;&#20214;&#20998;&#24067;&#20559;&#24046;&#24341;&#36215;&#30340;&#21442;&#25968;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15999</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31616;&#32504;&#32433;&#32447;&#25968;&#30446;&#35745;&#31639;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;
&lt;/p&gt;
&lt;p&gt;
Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#23545;&#31616;&#32504;&#32433;&#32447;&#23494;&#24230;&#30340;&#20272;&#35745;&#65292;&#20197;&#36827;&#34892;&#30011;&#24067;&#20998;&#26512;&#12290;&#35813;&#26032;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#20174;&#22270;&#20687;&#20013;&#35745;&#31639;&#32447;&#23494;&#24230;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#22312;&#33402;&#26415;&#20445;&#23384;&#21644;&#20462;&#22797;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.13555</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#24314;&#27169;&#21644;&#21560;&#38468;&#27169;&#22411;&#25506;&#32034;&#65306;&#31995;&#32479;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#12290;&#23427;&#28436;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12289;&#20276;&#38543;&#28789;&#25935;&#24230;&#20998;&#26512;&#21644;JIT&#32534;&#35793;&#30340;&#21521;&#37327;&#38597;&#21508;&#24067;&#20056;&#31215;&#65292;&#32467;&#21512;&#31354;&#38388;&#31163;&#25955;&#21644;&#33258;&#36866;&#24212;&#31215;&#20998;&#22120;&#26469;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#34987;&#29992;&#26469;&#35782;&#21035;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22312;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22266;&#23450;&#24202;&#21560;&#38468;&#30340;&#22122;&#22768;&#31361;&#30772;&#26354;&#32447;&#35266;&#27979;&#32467;&#26524;&#65292;&#24471;&#20986;&#20102;&#25311;&#21512;&#33391;&#22909;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#37325;&#24314;&#20102;&#21560;&#38468;&#25668;&#21462;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#30340;&#22810;&#39033;&#24335;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#31361;&#30772;&#26354;&#32447;&#65292;&#31361;&#26174;&#20102;&#35813;&#26694;&#26550;&#21457;&#29616;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31867;MLP&#26550;&#26500;ALOFT&#65292;&#23427;&#21487;&#20351;&#29992;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;&#12290;&#19982;CNN&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11674</link><description>&lt;p&gt;
ALOFT&#65306;&#19968;&#31181;&#36731;&#37327;&#21270;&#30340;&#31867;MLP&#26550;&#26500;&#65292;&#37197;&#21512;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31867;MLP&#26550;&#26500;ALOFT&#65292;&#23427;&#21487;&#20351;&#29992;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;&#12290;&#19982;CNN&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22810;&#20010;&#28304;&#22495;&#26469;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#24037;&#20316;&#37117;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#26680;&#30340;&#23616;&#37096;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#36807;&#20110;&#20851;&#27880;&#23616;&#37096;&#34920;&#31034;&#65288;&#20363;&#22914;&#32441;&#29702;&#65289;&#65292;&#36825;&#20174;&#26412;&#36136;&#19978;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28304;&#22495;&#24182;&#38459;&#30861;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20960;&#31181;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19981;&#21516;&#22359;&#20043;&#38388;&#30340;&#20840;&#23616;&#20132;&#20114;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#22312;&#27492;&#21463;&#21040;&#21551;&#21457;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;CNN&#21644;MLP&#26041;&#27861;&#22312;DG&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;MLP&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#27604;CNN&#26041;&#27861;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65288;&#20363;&#22914;&#32467;&#26500;&#65289;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26368;&#36817;&#30340;&#19968;&#31181;&#36731;&#37327;&#32423;MLP&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#22823;&#22810;&#25968;&#32479;&#35745;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09483</link><description>&lt;p&gt;
&#36741;&#21161;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33258;&#28982;&#33021;&#21147;&#30456;&#27604;&#65292;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#27169;&#22411;&#22312;&#34987;&#20248;&#21270;&#20026;&#26032;&#20219;&#21153;&#21518;&#65292;&#22312;&#26087;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24613;&#21095;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31038;&#21306;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#65288;&#21487;&#22609;&#24615;&#65289;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20197;&#21069;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#65288;&#31283;&#23450;&#24615;&#65289;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20294;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#36824;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#20854;&#22522;&#26412;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#65288;ANCL&#65289;&#65292;&#23427;&#23558;&#19968;&#20010;&#39069;&#22806;&#30340;&#36741;&#21161;&#32593;&#32476;&#24212;&#29992;&#20110;&#20027;&#35201;&#20851;&#27880;&#31283;&#23450;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#20027;&#35201;&#32593;&#32476;&#21644;&#36741;&#21161;&#32593;&#32476;&#20043;&#38388;&#20449;&#24687;&#30340;&#27969;&#21160;&#26469;&#33258;&#28982;&#22320;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ANCL&#22312;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&lt;p&gt;
&#22522;&#20110;&#30334;&#19975;&#29992;&#25143;&#30340;&#29616;&#23454;&#19990;&#30028;&#20114;&#21160;&#26469;&#22870;&#21169;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23548;&#33268;&#37096;&#32626;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#20854;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#30021;&#24615;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20445;&#35777;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#24456;&#23481;&#26131;&#22833;&#21435;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#21442;&#19982;&#24230;&#20197;&#22686;&#24378;&#30041;&#23384;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20855;&#20307;&#25506;&#35752;&#20102;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#20197;&#39640;&#25928;&#22320;&#24320;&#21457;&#39640;&#24230;&#26377;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#25512;&#29702;&#26102;&#25298;&#32477;&#20302;&#24471;&#20998;&#30340;&#26679;&#26412;&#21709;&#24212;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20363;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#65288;MCL&#65289;&#65292;&#20316;&#20026;&#34913;&#37327;&#24050;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#21442;&#19982;&#24230;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#23545;&#27599;&#26085;&#30340;10,000&#20010;&#26032;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#36827;&#34892;A/B&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20351;MCL&#22686;&#21152;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#30041;&#23384;&#26102;&#38388;&#24310;&#38271;1.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;ADMM&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#24179;&#38754;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.04445</link><description>&lt;p&gt;
&#19968;&#20010;ADMM&#27714;&#35299;MKL-$L_{0/1}$-SVM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ADMM Solver for the MKL-$L_{0/1}$-SVM. (arXiv:2303.04445v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;ADMM&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#24179;&#38754;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#24102;&#26377;&#33261;&#21517;&#26157;&#33879;&#30340;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#21046;&#23450;&#20102;&#22810;&#26680;&#23398;&#20064;(MKL)&#38382;&#39064;&#12290;&#32473;&#20986;&#20102;&#19968;&#20123;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#26465;&#20214;&#26469;&#24320;&#21457;&#19968;&#20010;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MKL-$L_{0/1}$-SVM&#26694;&#26550;&#20855;&#26377;&#24456;&#22909;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate the Multiple Kernel Learning (abbreviated as MKL) problem for the support vector machine with the infamous $(0,1)$-loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver for the nonconvex and nonsmooth optimization problem. A simple numerical experiment on synthetic planar data shows that our MKL-$L_{0/1}$-SVM framework could be promising.
&lt;/p&gt;</description></item><item><title>BO-Muse&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#35753;&#20154;&#31867;&#19987;&#23478;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#65292;&#36890;&#36807;&#27880;&#20837;&#26032;&#39062;&#24615;&#24182;&#21457;&#29616;&#24369;&#28857;&#26469;&#25171;&#30772;&#36807;&#24230;&#24320;&#21457;&#65292;&#20197;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.01684</link><description>&lt;p&gt;
BO-Muse&#65306;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BO-Muse: A human expert and AI teaming framework for accelerated experimental design. (arXiv:2303.01684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01684
&lt;/p&gt;
&lt;p&gt;
BO-Muse&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#35753;&#20154;&#31867;&#19987;&#23478;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#65292;&#36890;&#36807;&#27880;&#20837;&#26032;&#39062;&#24615;&#24182;&#21457;&#29616;&#24369;&#28857;&#26469;&#25171;&#30772;&#36807;&#24230;&#24320;&#21457;&#65292;&#20197;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BO-Muse&#65292;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#21463;&#21040;&#20174;&#19987;&#23478;&#30693;&#35782;&#20013;&#25552;&#21462;&#21644;&#33976;&#39311;&#22238;AI&#27169;&#22411;&#30340;&#20869;&#22312;&#22256;&#38590;&#20197;&#21450;&#23545;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#35774;&#35745;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35753;&#20154;&#31867;&#19987;&#23478;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#21017;&#25198;&#28436;&#30528;&#28789;&#24863;&#30340;&#35282;&#33394;&#65292;&#22312;&#23547;&#25214;&#24369;&#28857;&#30340;&#21516;&#26102;&#27880;&#20837;&#26032;&#39062;&#24615;&#65292;&#20174;&#32780;&#25171;&#30772;&#30001;&#35748;&#30693;&#34701;&#20837;&#24341;&#36215;&#30340;&#36807;&#24230;&#24320;&#21457;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20122;&#32447;&#24615;&#25910;&#25947;&#65292;&#36895;&#24230;&#24555;&#20110;&#21333;&#29420;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#19987;&#23478;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#24314;&#31435;&#23436;&#20840;&#22270;&#21644;&#19977;&#27425;&#30446;&#26631;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#37096;&#20998;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27979;&#35797;&#36825;&#20123;&#26465;&#20214;&#30340;&#31639;&#27861;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#25968;&#20540;&#19978;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04694</link><description>&lt;p&gt;
&#19977;&#27425;&#30456;&#20851;&#32858;&#31867;&#20013;&#30340;&#37096;&#20998;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partial Optimality in Cubic Correlation Clustering. (arXiv:2302.04694v2 [cs.DM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#24314;&#31435;&#23436;&#20840;&#22270;&#21644;&#19977;&#27425;&#30446;&#26631;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#37096;&#20998;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27979;&#35797;&#36825;&#20123;&#26465;&#20214;&#30340;&#31639;&#27861;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#25968;&#20540;&#19978;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#26159;&#19968;&#31181;&#34920;&#36798;&#21147;&#24378;&#30340;&#27169;&#22411;&#65292;&#26368;&#36817;&#38024;&#23545;&#20960;&#20010;&#24212;&#29992;&#25552;&#20986;&#20102;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20445;&#35777;&#26368;&#20248;&#24615;&#26159; NP-hard &#30340;&#65292;&#32780;&#19988;&#22312;&#38382;&#39064;&#38472;&#36848;&#30340;&#22797;&#26434;&#24615;&#19978;&#24050;&#32463;&#21463;&#21040;&#23454;&#38469;&#38459;&#30861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24314;&#31435;&#23436;&#20840;&#22270;&#21644;&#19977;&#27425;&#30446;&#26631;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#37096;&#20998;&#26368;&#20248;&#24615;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#21644;&#23454;&#29616;&#20102;&#27979;&#35797;&#36825;&#20123;&#26465;&#20214;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#25968;&#20540;&#19978;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The higher-order correlation clustering problem is an expressive model, and recently, local search heuristics have been proposed for several applications. Certifying optimality, however, is NP-hard and practically hampered already by the complexity of the problem statement. Here, we focus on establishing partial optimality conditions for the special case of complete graphs and cubic objective functions. In addition, we define and implement algorithms for testing these conditions and examine their effect numerically, on two datasets.
&lt;/p&gt;</description></item><item><title>NeuKron&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#31232;&#30095;&#21487;&#37325;&#25490;&#30697;&#38453;&#21644;&#24352;&#37327;&#21387;&#32553;&#33267;&#24120;&#25968;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24182;&#33021;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26356;&#26032;&#21442;&#25968;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#23545;&#25968;&#26102;&#38388;&#20869;&#26816;&#32034;&#27599;&#20010;&#26465;&#30446;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2302.04570</link><description>&lt;p&gt;
NeuKron: &#31232;&#30095;&#21487;&#37325;&#25490;&#30697;&#38453;&#21644;&#24352;&#37327;&#30340;&#24120;&#25968;&#22823;&#23567;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors. (arXiv:2302.04570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04570
&lt;/p&gt;
&lt;p&gt;
NeuKron&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#31232;&#30095;&#21487;&#37325;&#25490;&#30697;&#38453;&#21644;&#24352;&#37327;&#21387;&#32553;&#33267;&#24120;&#25968;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24182;&#33021;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26356;&#26032;&#21442;&#25968;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#23545;&#25968;&#26102;&#38388;&#20869;&#26816;&#32034;&#27599;&#20010;&#26465;&#30446;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#33258;&#28982;&#22320;&#34987;&#34920;&#31034;&#20026;&#31232;&#30095;&#21487;&#37325;&#25490;&#30697;&#38453;&#65292;&#20854;&#34892;&#21644;&#21015;&#21487;&#20197;&#20219;&#24847;&#25490;&#24207;(&#20363;&#22914;&#21452;&#20998;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;)&#12290;&#23558;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#22312;&#20256;&#32479;&#30340;&#26041;&#24335;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#31354;&#38388;&#26469;&#23384;&#20648;&#38750;&#38646;&#20803;&#32032;&#65292;&#32780;&#31232;&#30095;&#30697;&#38453;&#30340;&#26377;&#25439;&#21387;&#32553;(&#20363;&#22914;&#25130;&#26029;SVD)&#36890;&#24120;&#38656;&#35201;&#19982;&#34892;&#21644;&#21015;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;NeuKron&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#21487;&#37325;&#25490;&#30697;&#38453;&#21387;&#32553;&#33267;&#24120;&#25968;&#22823;&#23567;&#30340;&#31354;&#38388;&#12290;NeuKron&#20351;&#29992;&#20855;&#26377;&#24120;&#25968;&#21442;&#25968;&#25968;&#37327;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;Kronecker&#20056;&#31215;&#12290;NeuKron&#36890;&#36807;&#26356;&#26032;&#21442;&#25968;&#26469;&#36817;&#20284;&#32473;&#23450;&#30697;&#38453;&#30340;&#20056;&#31215;&#65292;&#24182;&#37325;&#26032;&#25490;&#21015;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#20197;&#20415;&#20110;&#36817;&#20284;&#12290;&#26356;&#26032;&#25152;&#38656;&#30340;&#26102;&#38388;&#20026;&#36755;&#20837;&#30697;&#38453;&#20013;&#38750;&#38646;&#20803;&#32032;&#30340;&#32447;&#24615;&#26102;&#38388;&#65292;&#24182;&#19988;&#27599;&#20010;&#26465;&#30446;&#30340;&#36817;&#20284;&#21487;&#20197;&#22312;&#23545;&#25968;&#26102;&#38388;&#20869;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;NeuKron&#25193;&#23637;&#21040;&#21487;&#20197;&#21387;&#32553;&#20855;&#26377;&#24120;&#25968;&#21442;&#25968;&#25968;&#37327;&#30340;&#31232;&#30095;&#21487;&#37325;&#25490;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world data are naturally represented as a sparse reorderable matrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacency matrix of a bipartite graph). Storing a sparse matrix in conventional ways requires an amount of space linear in the number of non-zeros, and lossy compression of sparse matrices (e.g., Truncated SVD) typically requires an amount of space linear in the number of rows and columns. In this work, we propose NeuKron for compressing a sparse reorderable matrix into a constant-size space. NeuKron generalizes Kronecker products using a recurrent neural network with a constant number of parameters. NeuKron updates the parameters so that a given matrix is approximated by the product and reorders the rows and columns of the matrix to facilitate the approximation. The updates take time linear in the number of non-zeros in the input matrix, and the approximation of each entry can be retrieved in logarithmic time. We also extend NeuKron to compress sp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.03421</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#25512;&#23548;&#65288;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#20219;&#20309;&#26102;&#38388;&#37117;&#26377;&#25928;&#30340;&#65288;&#21363;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25353;&#29031;&#20197;&#19979;&#39034;&#24207;&#32467;&#21512;&#20102;&#22235;&#31181;&#24037;&#20855;&#65306;&#65288;a&#65289;&#38750;&#36127;&#36229;&#39532;&#19969;&#26684;&#23572;&#25110;&#21453;&#21521;&#20122;&#39532;&#36874;&#65292;&#65288;b&#65289;&#28151;&#21512;&#27861;&#65292;&#65288;c&#65289;Donsker-Varadhan&#20844;&#24335;&#65288;&#25110;&#20854;&#23427;&#20984;&#24615;&#23545;&#20598;&#21407;&#29702;&#65289;&#21644;&#65288;d&#65289;Ville&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#19968;&#20010;PAC-Bayes&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#22914;&#20309;&#25512;&#20986;&#30693;&#21517;&#30340;&#32463;&#20856;PAC-Bayes&#30028;&#38480;&#65292;&#20363;&#22914;Seeger&#12289;McAllester&#12289;Maurer&#21644;Catoni&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35768;&#22810;&#26368;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25918;&#26494;&#20256;&#32479;&#30340;&#20551;&#35774;&#65307;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoRobust&#30340;&#40657;&#30418;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#26088;&#22312;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#65292;&#24182;&#19988;&#26080;&#35770;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#65292;GeoRobust&#37117;&#33021;&#22815;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#21464;&#25442;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2301.12456</link><description>&lt;p&gt;
&#20026;&#39564;&#35777;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#40065;&#26834;&#24615;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Verifying the Geometric Robustness of Large-scale Neural Networks. (arXiv:2301.12456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoRobust&#30340;&#40657;&#30418;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#26088;&#22312;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#65292;&#24182;&#19988;&#26080;&#35770;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#65292;GeoRobust&#37117;&#33021;&#22815;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#21464;&#25442;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#30693;&#23545;&#25239;&#24615;&#20960;&#20309;&#21464;&#25442;&#26131;&#21463;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#26469;&#39564;&#35777;&#22823;&#35268;&#27169;DNNs&#23545;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GeoRobust&#65292;&#19968;&#20010;&#22522;&#20110;&#26032;&#22411;&#20840;&#23616;&#20248;&#21270;&#31574;&#30053;&#30340;&#40657;&#30418;&#23376;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#29992;&#20110;&#23450;&#20301;&#24433;&#21709;&#29978;&#33267;&#25913;&#21464;&#32593;&#32476;&#36755;&#20986;&#30340;&#26368;&#22351;&#21464;&#25442;&#32452;&#21512;&#12290; GeoRobust&#21487;&#20197;&#26681;&#25454;Lipschitzian&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#25214;&#21040;&#26368;&#22351;&#24773;&#20917;&#32452;&#21512;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#23376;&#24615;&#36136;&#65292;GeoRobust&#21487;&#20197;&#37096;&#32626;&#22312;&#22823;&#35268;&#27169;DNNs&#19978;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#12290;&#23454;&#38469;&#19978;&#65292;GeoRobust&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20197;&#39640;&#31934;&#24230;&#23450;&#20301;ImageNet&#19978;ResNet50&#27169;&#22411;&#30340;&#26368;&#22351;&#20960;&#20309;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are known to be vulnerable to adversarial geometric transformation. This paper aims to verify the robustness of large-scale DNNs against the combination of multiple geometric transformations with a provable guarantee. Given a set of transformations (e.g., rotation, scaling, etc.), we develop GeoRobust, a black-box robustness analyser built upon a novel global optimisation strategy, for locating the worst-case combination of transformations that affect and even alter a network's output. GeoRobust can provide provable guarantees on finding the worst-case combination based on recent advances in Lipschitzian theory. Due to its black-box nature, GeoRobust can be deployed on large-scale DNNs regardless of their architectures, activation functions, and the number of neurons. In practice, GeoRobust can locate the worst-case geometric transformation with high precision for the ResNet50 model on ImageNet in a few seconds on average. We examined 18 ImageNet classifiers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#25209;&#22788;&#29702;&#31574;&#30053;&#21644;&#25209;&#37327;&#22823;&#23567;&#23545;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#22914;&#20309;&#22312;&#23454;&#29616;&#39640;&#32593;&#32476;&#24615;&#33021;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#36164;&#28304;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.10587</link><description>&lt;p&gt;
&#25209;&#37327;&#22788;&#29702;&#21464;&#38271;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#35757;&#32451;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems. (arXiv:2301.10587v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#25209;&#22788;&#29702;&#31574;&#30053;&#21644;&#25209;&#37327;&#22823;&#23567;&#23545;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#22914;&#20309;&#22312;&#23454;&#29616;&#39640;&#32593;&#32476;&#24615;&#33021;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#36164;&#28304;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#30340;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#21033;&#29992;&#21017;&#20027;&#35201;&#21463;&#21040;&#35832;&#22914;&#25209;&#37327;&#22823;&#23567;&#20043;&#31867;&#30340;&#35757;&#32451;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#22122;&#22768;&#21644;&#28151;&#21709;&#35821;&#38899;&#32452;&#21512;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#25345;&#32493;&#26102;&#38388;&#65292;&#22312;&#22521;&#35757;&#26399;&#38388;&#38656;&#35201;&#19968;&#31181;&#25209;&#22788;&#29702;&#31574;&#30053;&#26469;&#22788;&#29702;&#21464;&#38271;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#21147;&#27714;&#22312;&#38646;&#22635;&#20805;&#21644;&#25968;&#25454;&#38543;&#26426;&#21270;&#20043;&#38388;&#21462;&#24471;&#19968;&#31181;&#25240;&#34935;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#25209;&#27425;&#20013;&#19968;&#33268;&#30340;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#23545;&#36164;&#28304;&#21033;&#29992;&#21644;&#26356;&#37325;&#35201;&#30340;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35760;&#24405;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#25209;&#22788;&#29702;&#31574;&#30053;&#21644;&#25209;&#37327;&#22823;&#23567;&#23545;Conv-TasNet&#30340;&#35757;&#32451;&#32479;&#35745;&#25968;&#25454;&#21644;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;&#37197;&#23545;&#21644;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of neural network-based speech enhancement systems is primarily influenced by the model architecture, whereas training times and computational resource utilization are primarily affected by training parameters such as the batch size. Since noisy and reverberant speech mixtures can have different duration, a batching strategy is required to handle variable size inputs during training, in particular for state-of-the-art end-to-end systems. Such strategies usually strive for a compromise between zero-padding and data randomization, and can be combined with a dynamic batch size for a more consistent amount of data in each batch. However, the effect of these strategies on resource utilization and more importantly network performance is not well documented. This paper systematically investigates the effect of different batching strategies and batch sizes on the training statistics and speech enhancement performance of a Conv-TasNet, evaluated in both matched and mismatched co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;TGPSSM&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.08843</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Flexibility and Interpretability of Gaussian Process State-Space Model. (arXiv:2301.08843v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;TGPSSM&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSM&#65289;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;GPSSM&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#36828;&#38750;&#20196;&#20154;&#28385;&#24847;&#12290;&#22823;&#22810;&#25968;GPSSM&#30740;&#31350;&#20381;&#36182;&#20110;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21644;&#39044;&#20808;&#35774;&#32622;&#30340;&#26680;&#24515;&#65292;&#20363;&#22914;&#24179;&#26041;&#25351;&#25968;&#65288;SE&#65289;&#26680;&#24515;&#25110;Matern&#26680;&#24515;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;&#65292;&#31216;&#20026;TGPSSM&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#27491;&#35268;&#21270;&#27969;&#65292;TGPSSM&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20351;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;TGPSSM&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;&#30001;&#20110;GP&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#26159;&#21487;&#35299;&#37322;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process state-space model (GPSSM) has attracted much attention over the past decade. However, the model representation power of the GPSSM is far from satisfactory. Most GPSSM studies rely on the standard Gaussian process (GP) with a preliminary kernel, such as the squared exponential (SE) kernel or Mat\'{e}rn kernel, which limits the model representation power and its application in complex scenarios. To address this issue, this paper proposes a novel class of probabilistic state-space models, called TGPSSMs. By leveraging a parametric normalizing flow, the TGPSSMs enrich the GP priors in the standard GPSSM, rendering the state-space model more flexible and expressive. Additionally, we present a scalable variational inference algorithm for learning and inference in TGPSSMs, which provides a flexible and optimal structure for the variational distribution of latent states. The algorithm is interpretable and computationally efficient owing to the sparse representation of GP a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.08243</link><description>&lt;p&gt;
&#20855;&#26377;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;I-JEPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#22270;&#20687;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#38750;&#29983;&#25104;&#26041;&#27861;&#12290;I-JEPA&#30340;&#26680;&#24515;&#35774;&#35745;&#36873;&#25321;&#26159;&#25513;&#27169;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;I-JEPA&#20135;&#29983;&#35821;&#20041;&#34920;&#31034;&#12290;&#24403;&#19982;Vision Transformers&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#35777;&#26126;I-JEPA&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#21382;&#21490;&#20915;&#31574;&#20026;&#22522;&#30784;&#65292;&#36866;&#24212;&#23454;&#26102;&#26102;&#21464;&#29615;&#22659;&#12290;&#35813;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.00497</link><description>&lt;p&gt;
&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#65306;&#20855;&#26377;&#26377;&#30028;&#21160;&#24577;&#36951;&#25022;&#30340;&#31639;&#27861;&#21644;&#25511;&#21046;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control. (arXiv:2301.00497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#21382;&#21490;&#20915;&#31574;&#20026;&#22522;&#30784;&#65292;&#36866;&#24212;&#23454;&#26102;&#26102;&#21464;&#29615;&#22659;&#12290;&#35813;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#25805;&#20316;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20856;&#22411;&#35745;&#31639;&#29942;&#39048;&#12290;&#26412;&#25991;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35760;&#24518;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;OCO-M&#21453;&#26144;&#20102;&#20915;&#31574;&#21382;&#21490;&#22914;&#20309;&#24433;&#21709;&#24403;&#21069;&#32467;&#26524;&#65292;&#24182;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#20381;&#36182;&#20110;&#24403;&#21069;&#21644;&#36807;&#21435;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#20869;&#23384;&#30340;&#26080;&#25237;&#24433;&#22522;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#21160;&#24577;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#21363;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#26102;&#21464;&#20915;&#31574;&#24207;&#21015;&#30340;&#27425;&#20248;&#24615;&#12290;&#26412;&#31639;&#27861;&#20197;&#22312;&#32447;Frank-Wolfe&#65288;OFW&#65289;&#21644;Hedge&#31639;&#27861;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#38656;&#35201;&#22312;&#23454;&#26102;&#20013;&#36866;&#24212;&#26102;&#21464;&#29615;&#22659;&#65292;&#24182;&#32771;&#34385;&#36807;&#21435;&#20915;&#31574;&#23545;&#29616;&#22312;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#65306;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection operations are a typical computation bottleneck in online learning. In this paper, we enable projection-free online learning within the framework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures how the history of decisions affects the current outcome by allowing the online learning loss functions to depend on both current and past decisions. Particularly, we introduce the first projection-free meta-base learning algorithm with memory that minimizes dynamic regret, i.e., that minimizes the suboptimality against any sequence of time-varying decisions. We are motivated by artificial intelligence applications where autonomous agents need to adapt to time-varying environments in real-time, accounting for how past decisions affect the present. Examples of such applications are: online control of dynamical systems; statistical arbitrage; and time series prediction. The algorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We demonstrate how our 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.00437</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22604;&#38519;:&#20174;&#24179;&#34913;&#21040;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00437
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22797;&#26434;&#31995;&#32479;&#22312;&#35757;&#32451;&#21040;&#25910;&#25947;&#26102;&#65292;&#23427;&#20204;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#32467;&#26500;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;(simplex Equiangular Tight Frame)&#30340;&#39030;&#28857;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#22604;&#38519;(NC)&#12290;&#26368;&#36817;&#30340;&#35770;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#31616;&#21270;&#30340;&#8220;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#8221;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#20986;&#29616;&#20102;$\mathcal{NC}$&#12290;&#22312;&#36825;&#20010;&#35821;&#22659;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24120;&#29992;&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#20132;&#21449;&#29109;(CE)&#25439;&#22833;&#19979;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#20250;&#21457;&#29983;$\mathcal{NC}$&#29616;&#35937;&#65292;&#34920;&#26126;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;$\mathcal{NC}$&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HandsOff&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#29616;&#26377;&#23569;&#37327;&#26631;&#35760;&#22270;&#20687;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#26080;&#38480;&#25968;&#37327;&#30340;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.12645</link><description>&lt;p&gt;
&#26080;&#38656;&#39069;&#22806;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#25216;&#26415;&#8212;&#8212;HandsOff
&lt;/p&gt;
&lt;p&gt;
HandsOff: Labeled Dataset Generation With No Additional Human Annotations. (arXiv:2212.12645v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12645
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HandsOff&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#29616;&#26377;&#23569;&#37327;&#26631;&#35760;&#22270;&#20687;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#26080;&#38480;&#25968;&#37327;&#30340;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#30340;&#34920;&#29616;&#21147;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;&#27880;&#37322;&#65292;&#36825;&#36843;&#20351;&#20174;&#19994;&#32773;&#23547;&#25214;&#27880;&#37322;&#32773;&#65292;&#31574;&#21010;&#19968;&#32452;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HandsOff&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#23569;&#20110;50&#20010;&#29616;&#26377;&#26631;&#35760;&#22270;&#20687;&#19978;&#35757;&#32451;&#21518;&#33021;&#22815;&#20135;&#29983;&#26080;&#38480;&#25968;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;GAN&#21453;&#28436;&#39046;&#22495;&#19982;&#25968;&#25454;&#38598;&#29983;&#25104;&#30456;&#32467;&#21512;&#36991;&#20813;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#23454;&#38469;&#32570;&#28857;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#29983;&#25104;&#20855;&#26377;&#20016;&#23500;&#20687;&#32032;&#32423;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;&#20154;&#33080;&#12289;&#27773;&#36710;&#12289;&#20840;&#36523;&#23039;&#21183;&#21644;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#26041;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#12289;&#20851;&#38190;&#28857;&#26816;&#27979;&#21644;&#28145;&#24230;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work leverages the expressive power of generative adversarial networks (GANs) to generate labeled synthetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We introduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and corresponding labels after being trained on less than 50 pre-existing labeled images. Our framework avoids the practical drawbacks of prior work by unifying the field of GAN inversion with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth estimation compared to prior dataset generation approaches and transfer learning ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.08546</link><description>&lt;p&gt;
&#20351;&#29992;&#37319;&#26679;&#31639;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#30340;&#25130;&#26029;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating truncation effects of quantum bosonic systems using sampling algorithms. (arXiv:2212.08546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#25110;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#27169;&#25311;&#29627;&#33394;&#23376;&#65292;&#24517;&#39035;&#36890;&#36807;&#23558;&#26080;&#38480;&#32500;&#23616;&#37096;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#20026;&#26377;&#38480;&#32500;&#26469;&#35268;&#33539;&#29702;&#35770;&#12290;&#22312;&#23547;&#27714;&#23454;&#38469;&#37327;&#23376;&#24212;&#29992;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#25130;&#26029;&#35823;&#24046;&#26377;&#22810;&#22823;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#25105;&#20204;&#25317;&#26377;&#22909;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#21542;&#21017;&#24456;&#38590;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#32463;&#20856;&#35774;&#22791;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21487;&#20197;&#29992;&#29616;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#36164;&#28304;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#20108;&#32500;&#26684;&#28857;&#19978;&#30340;&#26631;&#37327;&#22330;&#29702;&#35770;&#20026;&#20363;&#28436;&#31034;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#20854;&#22823;&#23567;&#36229;&#36807;&#20102;&#20351;&#29992;&#30830;&#20999;&#23545;&#35282;&#21270;&#26041;&#27861;&#25152;&#33021;&#36798;&#21040;&#30340;&#33539;&#22260;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To simulate bosons on a qubit- or qudit-based quantum computer, one has to regularize the theory by truncating infinite-dimensional local Hilbert spaces to finite dimensions. In the search for practical quantum applications, it is important to know how big the truncation errors can be. In general, it is not easy to estimate errors unless we have a good quantum computer. In this paper we show that traditional sampling methods on classical devices, specifically Markov Chain Monte Carlo, can address this issue with a reasonable amount of computational resources available today. As a demonstration, we apply this idea to the scalar field theory on a two-dimensional lattice, with a size that goes beyond what is achievable using exact diagonalization methods. This method can be used to estimate the resources needed for realistic quantum simulations of bosonic theories, and also, to check the validity of the results of the corresponding quantum simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.06370</link><description>&lt;p&gt;
&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#24212;&#35813;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#20043;&#22806;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;(PIs)&#12290;&#21482;&#35201;Pis&#36275;&#22815;&#31364;&#32780;&#19988;&#25429;&#33719;&#20102;&#22823;&#37096;&#20998;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36825;&#20123;Pis&#23601;&#26159;&#26377;&#29992;&#30340;&#25110;"&#39640;&#36136;&#37327;"&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#21306;&#38388;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#39044;&#27979;&#20043;&#22806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20004;&#20010;&#20276;&#20387;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#20351;&#29992;&#19968;&#20010;&#36755;&#20986;&#65292;&#30446;&#26631;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#36755;&#20986;&#65292;&#30456;&#24212;PI&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#29983;&#25104;PI&#30340;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#32771;&#34385;&#20102;&#30446;&#26631;&#20272;&#35745;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#20004;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#20943;&#23567;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#21644;&#25552;&#39640;Pis&#30340;&#36136;&#37327;(&#36890;&#36807;&#20854;&#35206;&#30422;&#27010;&#29575;&#36827;&#34892;&#27979;&#37327;)&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#19988;&#36136;&#37327;&#26356;&#39640;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2212.04089</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#31639;&#26415;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21464;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#26041;&#24335;&#65288;&#27604;&#22914;&#25552;&#39640;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25110;&#20943;&#36731;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#20559;&#24046;&#65289;&#26159;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22260;&#32469;&#8220;&#20219;&#21153;&#21521;&#37327;&#8221;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#20219;&#21153;&#21521;&#37327;&#25351;&#23450;&#20102;&#19968;&#20010;&#26041;&#21521;&#65292;&#21363;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#32463;&#36807;&#24494;&#35843;&#20219;&#21153;&#21518;&#30340;&#30456;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#20943;&#21435;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#20219;&#21153;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#21542;&#23450;&#21644;&#21152;&#27861;&#31561;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#21542;&#23450;&#20219;&#21153;&#21521;&#37327;&#20250;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#23545;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#24433;&#21709;&#19981;&#22823;&#12290;&#27492;&#22806;&#65292;&#23558;&#20219;&#21153;&#21521;&#37327;&#30456;&#21152;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#27979;&#35797;&#24605;&#24819;&#30340;&#20005;&#26684;&#26694;&#26550;&#26469;&#20998;&#26512;&#26657;&#20934;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36317;&#31163;&#26657;&#20934;&#30340;&#22522;&#30784;&#30495;&#30456;&#65292;&#25552;&#20379;&#20102;&#19977;&#20010;&#19968;&#33268;&#24615;&#30340;&#26657;&#20934;&#24230;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#24230;&#37327;&#30340;&#34920;&#29616;&#22914;&#39044;&#26399;&#12290;</title><link>http://arxiv.org/abs/2211.16886</link><description>&lt;p&gt;
&#19968;&#31181;&#36317;&#31163;&#26657;&#20934;&#30340;&#32479;&#19968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Unifying Theory of Distance from Calibration. (arXiv:2211.16886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#27979;&#35797;&#24605;&#24819;&#30340;&#20005;&#26684;&#26694;&#26550;&#26469;&#20998;&#26512;&#26657;&#20934;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36317;&#31163;&#26657;&#20934;&#30340;&#22522;&#30784;&#30495;&#30456;&#65292;&#25552;&#20379;&#20102;&#19977;&#20010;&#19968;&#33268;&#24615;&#30340;&#26657;&#20934;&#24230;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#24230;&#37327;&#30340;&#34920;&#29616;&#22914;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23450;&#20041;&#21644;&#24230;&#37327;&#27010;&#29575;&#39044;&#27979;&#22120;&#30340;&#36317;&#31163;&#26657;&#20934;&#30340;&#26681;&#26412;&#38382;&#39064;&#12290;&#34429;&#28982;&#23436;&#32654;&#26657;&#20934;&#30340;&#27010;&#24565;&#26159;&#34987;&#29702;&#35299;&#30340;&#65292;&#20294;&#27809;&#26377;&#20849;&#35782;&#22914;&#20309;&#37327;&#21270;&#36317;&#31163;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#26657;&#20934;&#24230;&#37327;&#65292;&#20294;&#19981;&#28165;&#26970;&#23427;&#20204;&#24444;&#27492;&#22914;&#20309;&#27604;&#36739;&#65292;&#35768;&#22810;&#27969;&#34892;&#30340;&#24230;&#37327;&#65288;&#22914;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65289;&#26080;&#27861;&#28385;&#36275;&#36830;&#32493;&#24615;&#31561;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#26657;&#20934;&#24230;&#37327;&#65292;&#21463;&#21040;&#23646;&#24615;&#27979;&#35797;&#30340;&#25991;&#29486;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36317;&#31163;&#26657;&#20934;&#30340;&#22522;&#30784;&#30495;&#30456;&#65306;&#21040;&#26368;&#25509;&#36817;&#23436;&#32654;&#26657;&#20934;&#39044;&#27979;&#22120;&#30340;$\ell_1$&#36317;&#31163;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#26657;&#20934;&#24230;&#37327;&#65292;&#21363;&#19982;&#35813;&#36317;&#31163;&#22810;&#39033;&#24335;&#30456;&#20851;&#30340;&#24230;&#37327;&#12290;&#24212;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#19968;&#33268;&#24615;&#30340;&#26657;&#20934;&#24230;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20272;&#35745;&#65306;&#24179;&#28369;&#26657;&#20934;&#12289;&#21306;&#38388;&#26657;&#20934;&#21644;&#26368;&#22823;&#26657;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#24230;&#37327;&#30340;&#34920;&#29616;&#22914;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental question of how to define and measure the distance from calibration for probabilistic predictors. While the notion of perfect calibration is well-understood, there is no consensus on how to quantify the distance from perfect calibration. Numerous calibration measures have been proposed in the literature, but it is unclear how they compare to each other, and many popular measures such as Expected Calibration Error (ECE) fail to satisfy basic properties like continuity.  We present a rigorous framework for analyzing calibration measures, inspired by the literature on property testing. We propose a ground-truth notion of distance from calibration: the $\ell_1$ distance to the nearest perfectly calibrated predictor. We define a consistent calibration measure as one that is polynomially related to this distance. Applying our framework, we identify three calibration measures that are consistent and can be estimated efficiently: smooth calibration, interval calibratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#31574;&#30053;&#30340;&#30456;&#23545;&#31232;&#30095;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#35299;&#37322;&#20174;&#24403;&#21069;&#30340;&#26631;&#20934;&#25252;&#29702;&#36716;&#21464;&#20026;&#26032;&#30340;&#25252;&#29702;&#25919;&#31574;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2211.16566</link><description>&lt;p&gt;
&#21307;&#30103;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#30456;&#23545;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Relative Sparsity for Medical Decision Problems. (arXiv:2211.16566v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#31574;&#30053;&#30340;&#30456;&#23545;&#31232;&#30095;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#35299;&#37322;&#20174;&#24403;&#21069;&#30340;&#26631;&#20934;&#25252;&#29702;&#36716;&#21464;&#20026;&#26032;&#30340;&#25252;&#29702;&#25919;&#31574;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20272;&#35745;&#19968;&#20010;&#31574;&#30053;&#65292;&#25110;&#32773;&#19968;&#20010;&#20174;&#21327;&#21464;&#37327;&#21040;&#20915;&#31574;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#25351;&#23548;&#20915;&#31574;&#32773;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#21327;&#21464;&#37327;&#34880;&#21387;&#21644;&#24515;&#29575;&#26159;&#21542;&#24212;&#35813;&#36827;&#34892;&#20302;&#34880;&#21387;&#27835;&#30103;&#65289;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#37325;&#35201;&#30340;&#26159;&#21521;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#35299;&#37322;&#22914;&#20309;&#20174;&#24403;&#21069;&#30340;&#26631;&#20934;&#25252;&#29702;&#36716;&#21464;&#20026;&#26032;&#30340;&#25252;&#29702;&#25919;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#20219;&#22495;&#31574;&#30053;&#20248;&#21270;&#65288;TRPO&#65289;&#20013;&#20511;&#37492;&#20102;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#19982;TRPO&#19981;&#21516;&#30340;&#26159;&#65292;&#22312;&#23558;&#26631;&#20934;&#27835;&#30103;&#36716;&#21464;&#20026;&#26032;&#30340;&#24314;&#35758;&#27835;&#30103;&#26102;&#65292;&#38656;&#35201;&#24378;&#21046;&#23558;&#31574;&#30053;&#30340;&#26041;&#38754;&#65288;&#21363;&#34880;&#21387;&#21644;&#24515;&#29575;&#30340;&#21442;&#25968;&#65289;&#22266;&#23450;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#20301;&#32622;&#19978;&#65292;&#20197;&#24110;&#21161;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20135;&#29983;&#20102;&#8220;&#30456;&#23545;&#31232;&#30095;&#8221;&#65292;&#20316;&#20026;&#19968;&#20010;&#35843;&#25972;&#21442;&#25968;$\lambda$&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25919;&#31574;&#24046;&#24322;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing statistical methods can estimate a policy, or a mapping from covariates to decisions, which can then instruct decision makers (e.g., whether to administer hypotension treatment based on covariates blood pressure and heart rate). There is great interest in using such data-driven policies in healthcare. However, it is often important to explain to the healthcare provider, and to the patient, how a new policy differs from the current standard of care. This end is facilitated if one can pinpoint the aspects of the policy (i.e., the parameters for blood pressure and heart rate) that change when moving from the standard of care to the new, suggested policy. To this end, we adapt ideas from Trust Region Policy Optimization (TRPO). In our work, however, unlike in TRPO, the difference between the suggested policy and standard of care is required to be sparse, aiding with interpretability. This yields ``relative sparsity," where, as a function of a tuning parameter, $\lambda$, we can ap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DyNCA &#30340;&#23454;&#26102;&#21487;&#25511;&#21160;&#24577;&#32441;&#29702;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#21512;&#25104;&#26080;&#38480;&#38271;&#21644;&#20219;&#24847;&#22823;&#23567;&#30340;&#36924;&#30495;&#35270;&#39057;&#32441;&#29702;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#36924;&#30495;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#26102;&#35270;&#39057;&#25511;&#21046;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11417</link><description>&lt;p&gt;
DyNCA&#65306;&#20351;&#29992;&#31070;&#32463;&#20803;&#33258;&#21160;&#26426;&#30340;&#23454;&#26102;&#21160;&#24577;&#32441;&#29702;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata. (arXiv:2211.11417v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11417
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DyNCA &#30340;&#23454;&#26102;&#21487;&#25511;&#21160;&#24577;&#32441;&#29702;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#21512;&#25104;&#26080;&#38480;&#38271;&#21644;&#20219;&#24847;&#22823;&#23567;&#30340;&#36924;&#30495;&#35270;&#39057;&#32441;&#29702;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#36924;&#30495;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#26102;&#35270;&#39057;&#25511;&#21046;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#21160;&#24577;&#32441;&#29702;&#21512;&#25104; (DyTS) &#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#36924;&#30495;&#30340;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#32531;&#24930;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#21512;&#25104;&#21333;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#30701;&#35270;&#39057;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#27809;&#26377;&#21518;&#26399;&#25511;&#21046;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#31070;&#32463;&#20803;&#33258;&#21160;&#26426; (DyNCA)&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#21487;&#25511;&#30340;&#21160;&#24577;&#32441;&#29702;&#21512;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340; NCA &#27169;&#22411;&#20043;&#19978;&#65292;&#21487;&#20197;&#23454;&#26102;&#21512;&#25104;&#26080;&#38480;&#38271;&#21644;&#20219;&#24847;&#22823;&#23567;&#30340;&#36924;&#30495;&#35270;&#39057;&#32441;&#29702;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#21512;&#25104;&#30340;&#35270;&#39057;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#36924;&#30495;&#12290;&#25105;&#20204;&#23558; SOTA DyTS &#30340;&#24615;&#33021;&#25552;&#39640;&#20102; $2\sim 4$ &#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#23454;&#26102;&#35270;&#39057;&#25511;&#21046;&#21151;&#33021;&#65292;&#21253;&#25324;&#36816;&#21160;&#36895;&#24230;&#12289;&#36816;&#21160;&#26041;&#21521;&#20197;&#21450;&#32534;&#36753;&#21047;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22312;&#32447;&#20132;&#20114;&#24335;&#28436;&#31034;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#35813;&#28436;&#31034;&#22312;&#26412;&#22320;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#24182;&#21487;&#22312;&#20010;&#20154;&#30005;&#33041;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by $2\sim 4$ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30452;&#25509;&#23545;&#22270;&#35889;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#20102;&#38544;&#24335;&#22270;&#35889;&#31070;&#32463;&#34920;&#31034;&#27861;&#65292;&#23427;&#20855;&#26377;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;&#22270;&#35889;&#12289;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20219;&#24847;&#22823;&#23567;&#20855;&#26377;&#25152;&#38656;&#32467;&#26500;&#30340;&#22270;&#34920;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03329</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#35889;&#31070;&#32463;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Graphon Neural Representation. (arXiv:2211.03329v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30452;&#25509;&#23545;&#22270;&#35889;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#20102;&#38544;&#24335;&#22270;&#35889;&#31070;&#32463;&#34920;&#31034;&#27861;&#65292;&#23427;&#20855;&#26377;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;&#22270;&#35889;&#12289;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20219;&#24847;&#22823;&#23567;&#20855;&#26377;&#25152;&#38656;&#32467;&#26500;&#30340;&#22270;&#34920;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35889;&#26159;&#29983;&#25104;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#34920;&#30340;&#36890;&#29992;&#19988;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#23545;&#22270;&#35889;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#38544;&#24335;&#22270;&#35889;&#31070;&#32463;&#34920;&#31034;&#27861; (IGNR)&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#24314;&#27169;&#21644;&#37325;&#26500;&#22270;&#35889;&#26102;&#24120;&#36890;&#36807;&#22266;&#23450;&#20998;&#36776;&#29575;&#20998;&#27573;&#24120;&#25968;&#34920;&#31034;&#26469;&#36924;&#36817;&#30446;&#26631;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;IGNR&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;&#22270;&#35889;&#65292;&#24182;&#22312;&#23398;&#20064;&#27169;&#22411;&#21518;&#33021;&#22815;&#33258;&#28982;&#39640;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#32467;&#26500;&#30340;&#20219;&#24847;&#22823;&#23567;&#30340;&#22270;&#34920;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;Gromov-Wasserstein&#36317;&#31163;&#65292;&#25105;&#20204;&#20801;&#35768;&#36755;&#20837;&#22270;&#25968;&#25454;&#19981;&#23545;&#40784;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#22312;&#22270;&#35889;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#26469;&#35777;&#26126;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;IGNR&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#34987;&#25972;&#21512;&#21040;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#65292;&#24182;&#22312;&#26356;&#19968;&#33324;&#30340;&#22270;&#35889;&#23398;&#20064;&#22330;&#26223;&#19979;&#23637;&#31034;&#20854;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphons are general and powerful models for generating graphs of varying size. In this paper, we propose to directly model graphons using neural networks, obtaining Implicit Graphon Neural Representation (IGNR). Existing work in modeling and reconstructing graphons often approximates a target graphon by a fixed resolution piece-wise constant representation. Our IGNR has the benefit that it can represent graphons up to arbitrary resolutions, and enables natural and efficient generation of arbitrary sized graphs with desired structure once the model is learned. Furthermore, we allow the input graph data to be unaligned and have different sizes by leveraging the Gromov-Wasserstein distance. We first demonstrate the effectiveness of our model by showing its superior performance on a graphon learning task. We then propose an extension of IGNR that can be incorporated into an auto-encoder framework, and demonstrate its good performance under a more general setting of graphon learning. We al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21327;&#20316;&#22270;&#23454;&#29616;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#25351;&#31034;&#65292;&#36890;&#36807;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#20197;&#26356;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2210.17101</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#23637;&#24320;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unrolled Graph Learning for Multi-Agent Collaboration. (arXiv:2210.17101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21327;&#20316;&#22270;&#23454;&#29616;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#25351;&#31034;&#65292;&#36890;&#36807;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#20197;&#26356;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20132;&#25442;&#21463;&#38480;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#32771;&#34385;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#22266;&#23450;&#24378;&#21046;&#24615;&#30340;&#21327;&#20316;&#20851;&#31995;&#19979;&#30340;&#25968;&#25454;&#34701;&#21512;&#65292;&#36825;&#19981;&#22914;&#20154;&#31867;&#21327;&#20316;&#37027;&#26679;&#28789;&#27963;&#21644;&#33258;&#27835;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#31181;&#36866;&#24212;&#24615;&#21327;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#21327;&#20316;&#22270;&#26469;&#25351;&#31034;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#12290;&#21327;&#20316;&#22270;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#22270;&#23398;&#20064;&#25216;&#26415;&#26469;&#33719;&#24471;&#12290;&#30001;&#20110;&#27169;&#22411;&#30456;&#20284;&#24615;&#19981;&#33021;&#36890;&#36807;&#22266;&#23450;&#30340;&#22270;&#24418;&#20248;&#21270;&#26469;&#25551;&#36848;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#26356;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent learning has gained increasing attention to tackle distributed machine learning scenarios under constrictions of data exchanging. However, existing multi-agent learning models usually consider data fusion under fixed and compulsory collaborative relations among agents, which is not as flexible and autonomous as human collaboration. To fill this gap, we propose a distributed multi-agent learning model inspired by human collaboration, in which the agents can autonomously detect suitable collaborators and refer to collaborators' model for better performance. To implement such adaptive collaboration, we use a collaboration graph to indicate the pairwise collaborative relation. The collaboration graph can be obtained by graph learning techniques based on model similarity between different agents. Since model similarity can not be formulated by a fixed graphical optimization, we design a graph learning network by unrolling, which can learn underlying similar features among potent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#21152;&#20837;&#19981;&#27969;&#30021;&#29305;&#24449;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#36798;&#21040;95.1%&#12290;</title><link>http://arxiv.org/abs/2210.16539</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#21152;&#20837;&#19981;&#27969;&#30021;&#29305;&#24449;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#36798;&#21040;95.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#20419;&#36827;&#39044;&#38450;&#24615;&#25252;&#29702;&#21644;&#24310;&#32531;&#30142;&#30149;&#36827;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#33258;&#21160;AD&#31579;&#26597;&#31995;&#32479;&#20026;&#20854;&#20182;&#20020;&#24202;&#31579;&#26597;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#19988;&#26356;&#20855;&#25193;&#23637;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22914;BERT&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#29305;&#24449;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;PLM&#39046;&#22495;&#24494;&#35843;&#36890;&#24120;&#22522;&#20110;&#25513;&#34109;&#35789;&#25110;&#21477;&#23376;&#39044;&#27979;&#25104;&#26412;&#65292;&#36825;&#19982;&#21518;&#31471;AD&#26816;&#27979;&#20219;&#21153;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;PLM&#24494;&#35843;&#65292;&#36825;&#31181;&#24494;&#35843;&#19968;&#33268;&#22320;&#20351;&#29992;AD&#20998;&#31867;&#38169;&#35823;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;PLM&#24494;&#35843;&#26399;&#38388;&#65292;&#22312;&#25552;&#31034;&#30701;&#35821;&#20013;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#22522;&#20110;&#29369;&#35947;&#25110;&#26242;&#20572;&#22635;&#20805;&#31526;&#20196;&#29260;&#39057;&#29575;&#30340;&#19981;&#27969;&#30021;&#29305;&#24449;&#12290;&#23545;&#20110;&#20351;&#29992;&#19981;&#21516;PLMs&#65288;BERT&#21644;RoBERTa&#65289;&#25110;&#20351;&#29992;&#19981;&#21516;&#24494;&#35843;&#33539;&#20363;&#65288;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#25552;&#31034;&#23398;&#20064;&#65289;&#30340;&#31995;&#32479;&#65292;&#22522;&#20110;&#20915;&#31574;&#25237;&#31080;&#30340;&#32452;&#21512;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;AD&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;AD&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36798;&#21040;&#20102;&#39640;&#36798;95.1%&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#29616;&#22788;&#20110;&#21516;&#31867;&#30740;&#31350;&#30340;&#26368;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and to delay further progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Textual embedding features produced by pre-trained language models (PLMs) such as BERT are widely used in such systems. However, PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task. To this end, this paper investigates the use of prompt-based fine-tuning of PLMs that consistently uses AD classification errors as the training objective function. Disfluency features based on hesitation or pause filler token frequencies are further incorporated into prompt phrases during PLM fine-tuning. The decision voting based combination among systems using different PLMs (BERT and RoBERTa) or systems with different fine-tuning paradigms (conventional ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#33041;&#32959;&#30244;&#20462;&#22797;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20462;&#22797;&#32570;&#22833;&#30340;&#33041;&#32959;&#30244;&#21306;&#22495;&#24182;&#20026;&#25968;&#25454;&#22686;&#21152;&#21512;&#25104;&#30340;&#33041;&#32959;&#30244;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#19988;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12113</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#33041;&#32959;&#30244;&#20462;&#22797;&#65306;&#19968;&#20221;&#26041;&#27861;&#35770;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report. (arXiv:2210.12113v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#33041;&#32959;&#30244;&#20462;&#22797;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20462;&#22797;&#32570;&#22833;&#30340;&#33041;&#32959;&#30244;&#21306;&#22495;&#24182;&#20026;&#25968;&#25454;&#22686;&#21152;&#21512;&#25104;&#30340;&#33041;&#32959;&#30244;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#19988;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#65292;&#20294;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#19981;&#24179;&#34913;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#20462;&#34917;&#31639;&#27861;&#26159;DL&#29983;&#25104;&#27169;&#22411;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#22312;&#21305;&#37197;&#21608;&#22260;&#19978;&#19979;&#25991;&#21644;&#26576;&#20123;&#38750;&#25104;&#20687;&#36755;&#20837;&#26465;&#20214;&#30340;&#21516;&#26102;&#26356;&#25913;&#36755;&#20837;&#22270;&#20687;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#21306;&#22495;&#12290;&#34429;&#28982;&#32477;&#22823;&#22810;&#25968;&#38024;&#23545;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#30340;&#20462;&#34917;&#25216;&#26415;&#37117;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#20294;&#30001;&#20110;GAN&#30340;&#26377;&#38480;&#36755;&#20986;&#31181;&#31867;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#32463;&#24120;&#19981;&#23613;&#20154;&#24847;&#12290;&#25513;&#27169;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPMs)&#26159;&#19968;&#31181;&#26368;&#36817;&#25512;&#20986;&#30340;&#29983;&#25104;&#32593;&#32476;&#31995;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;GANs&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#37492;&#21035;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#30340;&#22810;&#20219;&#21153;&#33041;&#32959;&#30244;&#20462;&#22797;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20462;&#22797;&#32570;&#22833;&#30340;&#33041;&#32959;&#30244;&#21306;&#22495;&#24182;&#20026;&#25968;&#25454;&#22686;&#21152;&#21512;&#25104;&#30340;&#33041;&#32959;&#30244;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ever-increasing interest in applying deep learning (DL) models to medical imaging, the typical scarcity and imbalance of medical datasets can severely impact the performance of DL models. The generation of synthetic data that might be freely shared without compromising patient privacy is a well-known technique for addressing these difficulties. Inpainting algorithms are a subset of DL generative models that can alter one or more regions of an input image while matching its surrounding context and, in certain cases, non-imaging input conditions. Although the majority of inpainting techniques for medical imaging data use generative adversarial networks (GANs), the performance of these algorithms is frequently suboptimal due to their limited output variety, a problem that is already well-known for GANs. Denoising diffusion probabilistic models (DDPMs) are a recently introduced family of generative networks that can generate results of comparable quality to GANs, but with diver
&lt;/p&gt;</description></item><item><title>GINNACER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20135;&#29983;&#20934;&#30830;&#30340;&#19978;&#20272;&#35745;&#36793;&#30028;&#65292;&#21516;&#26102;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#23616;&#37096;&#36755;&#20837;&#20445;&#35777;&#31934;&#30830;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.12054</link><description>&lt;p&gt;
&#21040;&#36798;&#20840;&#23616;&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#19982;&#26412;&#22320;&#31934;&#30830;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Global Neural Network Abstractions with Locally-Exact Reconstruction. (arXiv:2210.12054v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12054
&lt;/p&gt;
&lt;p&gt;
GINNACER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20135;&#29983;&#20934;&#30830;&#30340;&#19978;&#20272;&#35745;&#36793;&#30028;&#65292;&#21516;&#26102;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#23616;&#37096;&#36755;&#20837;&#20445;&#35777;&#31934;&#30830;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#40657;&#21283;&#23376;&#24615;&#36136;&#20351;&#24471;&#35299;&#37322;&#23427;&#20204;&#30340;&#34892;&#20026;&#21644;&#39564;&#35777;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#25277;&#35937;&#25216;&#26415;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#26356;&#31616;&#21333;&#30340;&#12289;&#36807;&#24230;&#36924;&#36817;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25277;&#35937;&#25216;&#26415;&#23384;&#22312;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36755;&#20837;&#22495;&#30340;&#23567;&#23616;&#37096;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20013;&#24515;&#31934;&#30830;&#37325;&#26500;&#30340;&#20840;&#22495;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#65288;GINNACER&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#25277;&#35937;&#25216;&#26415;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20135;&#29983;&#20934;&#30830;&#30340;&#36807;&#24230;&#36924;&#36817;&#36793;&#30028;&#65292;&#21516;&#26102;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#23616;&#37096;&#36755;&#20837;&#20445;&#35777;&#31934;&#30830;&#37325;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GINNACER&#27604;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#25277;&#35937;&#25216;&#26415;&#32039;&#20945;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#19982;&#23616;&#37096;&#25216;&#26415;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#30340;&#19977;&#27493;&#23398;&#20064;&#21644;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36817;&#20284;&#35299;&#26032;&#30340;PDE&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.08140</link><description>&lt;p&gt;
&#29992;&#26680;&#26041;&#27861;&#25506;&#32034;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Kernel Approach for PDE Discovery and Operator Learning. (arXiv:2210.08140v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#30340;&#19977;&#27493;&#23398;&#20064;&#21644;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36817;&#20284;&#35299;&#26032;&#30340;PDE&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#23398;&#20064;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#19977;&#27493;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#21253;&#25324;&#32593;&#26684;&#19978;&#30340;&#22122;&#22768;PDE&#35299;&#20197;&#21450;&#28304;&#39033;/&#36793;&#30028;&#39033;&#30340;&#23545;&#65292;&#21033;&#29992;&#26680;&#24179;&#28369;&#25216;&#26415;&#21435;&#22122;&#24182;&#36817;&#20284;&#35299;&#30340;&#23548;&#25968;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#23398;&#20064;PDE&#30340;&#20195;&#25968;&#24418;&#24335;&#12290;&#23398;&#20064;&#24471;&#21040;&#30340;PDE&#22312;&#22522;&#20110;&#26680;&#30340;&#27714;&#35299;&#22120;&#20013;&#34987;&#29992;&#26469;&#36817;&#20284;&#35299;&#26032;&#30340;&#28304;&#39033;/&#36793;&#30028;&#39033;&#30340;PDE&#65292;&#20174;&#32780;&#26500;&#25104;&#20102;&#19968;&#20010;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#23558;&#35813;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a three-step framework for learning and solving partial differential equations (PDEs) using kernel methods. Given a training set consisting of pairs of noisy PDE solutions and source/boundary terms on a mesh, kernel smoothing is utilized to denoise the data and approximate derivatives of the solution. This information is then used in a kernel regression model to learn the algebraic form of the PDE. The learned PDE is then used within a kernel based solver to approximate the solution of the PDE with a new source/boundary term, thereby constituting an operator learning framework. Numerical experiments compare the method to state-of-the-art algorithms and demonstrate its competitive performance.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#25193;&#20805;&#38024;&#23545;&#22810;&#26679;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#33021;&#22815;&#27604;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26356;&#26377;&#20215;&#20540;&#65292;&#40723;&#21169;&#19981;&#21464;&#24615;&#30340;&#25968;&#25454;&#25193;&#20805;&#21487;&#20197;&#27604;&#21333;&#19968;&#30340;&#19981;&#21464;&#24615;&#26356;&#26377;&#29992;&#65292;&#25968;&#25454;&#25193;&#20805;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#38543;&#26426;&#24615;&#24182;&#33021;&#22815;&#25289;&#24179;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#38754;&#12290;</title><link>http://arxiv.org/abs/2210.06441</link><description>&lt;p&gt;
&#25968;&#25454;&#25193;&#20805;&#21040;&#24213;&#26377;&#22810;&#22823;&#20316;&#29992;&#65311;&#32553;&#25918;&#23450;&#24459;&#12289;&#19981;&#21464;&#24615;&#21450;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization. (arXiv:2210.06441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06441
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25193;&#20805;&#38024;&#23545;&#22810;&#26679;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#33021;&#22815;&#27604;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26356;&#26377;&#20215;&#20540;&#65292;&#40723;&#21169;&#19981;&#21464;&#24615;&#30340;&#25968;&#25454;&#25193;&#20805;&#21487;&#20197;&#27604;&#21333;&#19968;&#30340;&#19981;&#21464;&#24615;&#26356;&#26377;&#29992;&#65292;&#25968;&#25454;&#25193;&#20805;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#38543;&#26426;&#24615;&#24182;&#33021;&#22815;&#25289;&#24179;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#25193;&#20805;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#20294;&#25105;&#20204;&#30693;&#36947;&#30340;&#26377;&#20851;&#25968;&#25454;&#25193;&#20805;&#20026;&#20309;&#22914;&#27492;&#26377;&#25928;&#30340;&#30693;&#35782;&#36824;&#24456;&#23569;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#31163;&#20986;&#25968;&#25454;&#25193;&#20805;&#21457;&#25381;&#20316;&#29992;&#30340;&#20960;&#20010;&#20851;&#38190;&#26426;&#21046;&#12290;&#22312;&#22686;&#24191;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#27719;&#29575;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;&#20135;&#29983;&#22810;&#26679;&#19988;&#19982;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22686;&#24191;&#33021;&#22815;&#27604;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26356;&#26377;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26088;&#22312;&#40723;&#21169;&#19981;&#21464;&#24615;&#30340;&#25968;&#25454;&#25193;&#20805;&#23545;&#20110;&#23567;&#22411;&#21644;&#20013;&#22411;&#35757;&#32451;&#38598;&#21487;&#33021;&#27604;&#21333;&#19968;&#30340;&#19981;&#21464;&#24615;&#26356;&#26377;&#20215;&#20540;&#12290;&#38543;&#30528;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#22686;&#24191;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#39069;&#22806;&#30340;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25289;&#24179;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.00939</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20351;&#29992;&#20998;&#31867;&#25110;&#25991;&#26412;&#26465;&#20214;&#30340;&#25193;&#25955;&#25351;&#23548;&#26041;&#27861;&#65292;&#22914;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25351;&#23548;&#26041;&#27861;&#12290;&#20174;&#36825;&#20010;&#24191;&#20041;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26080;&#26465;&#20214;&#21644;&#26080;&#30417;&#30563;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27169;&#31946;&#25351;&#23548;&#25913;&#21892;&#20102;&#20013;&#38388;&#26679;&#26412;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25351;&#23548;&#23610;&#24230;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#65288;SAG&#65289;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20013;&#38388;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAG&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20165;&#23545;&#25193;&#25955;&#27169;&#22411;&#20851;&#27880;&#30340;&#21306;&#22495;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#31946;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;BSSMF&#65292;&#23427;&#30340;&#30697;&#38453;W&#27599;&#21015;&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#26159;&#38543;&#26426;&#30340;&#65292;&#25512;&#24191;&#20102;NMF&#21644;SSMF&#65292;&#36866;&#29992;&#20110;&#30697;&#38453;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#21644;&#31163;&#25955;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.12638</link><description>&lt;p&gt;
&#26377;&#30028;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65306;&#31639;&#27861;&#12289;&#21487;&#35782;&#21035;&#24615;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded Simplex-Structured Matrix Factorization: Algorithms, Identifiability and Applications. (arXiv:2209.12638v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;BSSMF&#65292;&#23427;&#30340;&#30697;&#38453;W&#27599;&#21015;&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#26159;&#38543;&#26426;&#30340;&#65292;&#25512;&#24191;&#20102;NMF&#21644;SSMF&#65292;&#36866;&#29992;&#20110;&#30697;&#38453;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#21644;&#31163;&#25955;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#30028;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65288;BSSMF&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#30697;&#38453;X&#21644;&#19968;&#20010;&#20998;&#35299;&#31209;r&#65292;BSSMF&#22312;&#30697;&#38453;W&#20013;&#23547;&#25214;&#20855;&#26377;r&#21015;&#30340;&#30697;&#38453;&#21644;&#22312;&#30697;&#38453;H&#20013;&#23547;&#25214;&#20855;&#26377;r&#34892;&#30340;&#30697;&#38453;&#65292;&#20351;&#24471;X&#8776;WH &#65292;&#20854;&#20013;W&#30340;&#27599;&#21015;&#20013;&#30340;&#20803;&#32032;&#37117;&#26159;&#26377;&#30028;&#30340;&#65292;&#21363;&#23427;&#20204;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#23646;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#65292;&#21363;H&#26159;&#21015;&#38543;&#26426;&#30340;&#12290;BSSMF&#25512;&#24191;&#20102;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#21644;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65288;SSMF&#65289;&#12290;BSSMF&#29305;&#21035;&#36866;&#29992;&#20110;&#36755;&#20837;&#30697;&#38453;X&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#65307;&#20363;&#22914;&#65292;&#24403;X&#30340;&#34892;&#34920;&#31034;&#22270;&#20687;&#26102;&#65292;&#25110;&#32773;X&#26159;&#31867;&#20284;Netflix&#21644;MovieLens&#25968;&#25454;&#38598;&#20013;&#30340;&#35780;&#20998;&#30697;&#38453;&#26102;&#65292;&#20854;&#20013;X&#30340;&#20803;&#32032;&#23646;&#20110;&#21306;&#38388;[1,5]&#12290;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;H&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#65292;&#20174;&#32780;&#23545;X&#30340;&#21015;&#31354;&#38388;&#36827;&#34892;&#36719;&#32858;&#31867;&#65292;&#32780;&#19988;&#36824;&#36171;&#20104;H&#30340;&#21015;&#31163;&#25955;&#32467;&#26500;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#22914;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;BSSMF&#20248;&#21270;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;BSSMF&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new low-rank matrix factorization model dubbed bounded simplex-structured matrix factorization (BSSMF). Given an input matrix $X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columns and a matrix $H$ with $r$ rows such that $X \approx WH$ where the entries in each column of $W$ are bounded, that is, they belong to given intervals, and the columns of $H$ belong to the probability simplex, that is, $H$ is column stochastic. BSSMF generalizes nonnegative matrix factorization (NMF), and simplex-structured matrix factorization (SSMF). BSSMF is particularly well suited when the entries of the input matrix $X$ belong to a given interval; for example when the rows of $X$ represent images, or $X$ is a rating matrix such as in the Netflix and MovieLens datasets where the entries of $X$ belong to the interval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easily understandable decomposition providing a soft clustering of the colu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#25216;&#26415;&#39564;&#35777;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#20854;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.06015</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#30340;&#40657;&#30418;&#25968;&#25454;&#38598;&#25152;&#26377;&#26435;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Black-box Dataset Ownership Verification via Backdoor Watermarking. (arXiv:2209.06015v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#25216;&#26415;&#39564;&#35777;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#20854;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#37319;&#29992;&#12290;DNN&#30340;&#24555;&#36895;&#21457;&#23637;&#21463;&#30410;&#20110;&#19968;&#20123;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#30340;&#23384;&#22312;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36731;&#26494;&#39564;&#35777;&#20854;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#37117;&#35201;&#27714;&#23427;&#20204;&#20165;&#33021;&#29992;&#20110;&#23398;&#26415;&#25110;&#25945;&#32946;&#30446;&#30340;&#32780;&#38750;&#21830;&#19994;&#30446;&#30340;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#24456;&#22909;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20445;&#25252;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#24418;&#24335;&#21270;&#20026;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#65292;&#32780;&#38450;&#24481;&#32773;&#21482;&#33021;&#26597;&#35810;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#20854;&#21442;&#25968;&#21644;&#35757;&#32451;&#32454;&#33410;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36830;&#25509;&#20256;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#30721;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20986;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#21442;&#25968;&#30340;NN&#29992;&#20110;&#22810;&#27425;&#36827;&#34892;One-hot&#32534;&#30721;&#65292;&#20197;&#25193;&#23637;&#32534;&#30721;&#32500;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#32416;&#38169;&#33021;&#21147;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.01701</link><description>&lt;p&gt;
&#36830;&#25509;&#20256;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#30721;&#65288;CCN&#65289;&#65306;ConcatenatedAE
&lt;/p&gt;
&lt;p&gt;
Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE. (arXiv:2209.01701v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36830;&#25509;&#20256;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#30721;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20986;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#21442;&#25968;&#30340;NN&#29992;&#20110;&#22810;&#27425;&#36827;&#34892;One-hot&#32534;&#30721;&#65292;&#20197;&#25193;&#23637;&#32534;&#30721;&#32500;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#32416;&#38169;&#33021;&#21147;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#32416;&#38169;&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21487;&#20197;&#25552;&#39640;&#32463;&#20856;&#20449;&#36947;&#32534;&#30721;&#30340;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20449;&#36947;&#27169;&#22411;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#21516;&#19968;&#32452;NN&#26469;&#36827;&#34892;&#19968;&#27425;&#24615;&#32534;&#30721;&#22810;&#20010;One-hot&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#20018;&#32852;&#22312;&#22806;&#23618;&#30340;&#32463;&#20856;&#32534;&#30721;&#20013;&#26469;&#25193;&#23637;&#20219;&#20309;&#36825;&#26679;&#32467;&#26500;&#30340;&#32534;&#30721;&#32500;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#21442;&#25968;&#30340;NN&#65292;&#20854;&#20013;&#27599;&#20010;Reed-Solomon&#30721;&#23383;&#31526;&#37117;&#26159;&#19981;&#21516;NN&#30340;&#36755;&#20837;&#12290;&#30456;&#23545;&#20110;&#23567;&#22411;&#31070;&#32463;&#32534;&#30721;&#65292;&#26174;&#31034;&#20102;&#26174;&#30528;&#30340;&#22359;&#35823;&#30721;&#27010;&#29575;&#25552;&#39640;&#20197;&#21450;&#23545;&#20449;&#36947;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small neural networks (NNs) used for error correction were shown to improve on classic channel codes and to address channel model changes. We extend the code dimension of any such structure by using the same NN under one-hot encoding multiple times, then serially-concatenated with an outer classic code. We design NNs with the same network parameters, where each Reed-Solomon codeword symbol is an input to a different NN. Significant improvements in block error probabilities for an additive Gaussian noise channel as compared to the small neural code are illustrated, as well as robustness to channel model changes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22402;&#30452;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#35757;&#32451;&#20934;&#30830;&#27169;&#22411;&#65292;&#26159;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;k&#22343;&#20540;&#32858;&#31867;&#30340;&#31532;&#19968;&#20010;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.01700</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Vertical Federated Clustering. (arXiv:2208.01700v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22402;&#30452;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#35757;&#32451;&#20934;&#30830;&#27169;&#22411;&#65292;&#26159;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;k&#22343;&#20540;&#32858;&#31867;&#30340;&#31532;&#19968;&#20010;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22810;&#20010;&#26041;&#26377;&#20851;&#21516;&#19968;&#32452;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#23646;&#24615;&#38598;&#19978;&#65292;&#26381;&#21153;&#22120;&#24076;&#26395;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#25968;&#25454;&#20027;&#20307;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#23398;&#20064;&#65292;&#25105;&#20204;&#38656;&#35201;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#25968;&#25454;&#26041;&#20165;&#20849;&#20139;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#20849;&#20139;&#30340;&#20449;&#24687;&#22312;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#30340;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;k&#22343;&#20540;&#32858;&#31867;&#30340;&#31532;&#19968;&#20010;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#21487;&#35777;&#26126;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#20840;&#23616;&#20013;&#24515;&#38598;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20551;&#35774;&#19981;&#21463;&#20449;&#20219;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#20174;&#26412;&#22320;&#25968;&#25454;&#26041;&#32858;&#21512;&#19981;&#21516;ially private&#30340;&#23616;&#37096;&#20013;&#24515;&#21644;&#25104;&#21592;&#32534;&#30721;&#12290;&#23427;&#26500;&#24314;&#19968;&#20010;&#21152;&#26435;&#32593;&#26684;&#20316;&#20026;&#20840;&#23616;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, multiple parties have private data regarding the same set of users but on disjoint sets of attributes, and a server wants to leverage the data to train a model. To enable model learning while protecting the privacy of the data subjects, we need vertical federated learning (VFL) techniques, where the data parties share only information for training the model, instead of the private data. However, it is challenging to ensure that the shared information maintains privacy while learning accurate models. To the best of our knowledge, the algorithm proposed in this paper is the first practical solution for differentially private vertical federated k-means clustering, where the server can obtain a set of global centers with a provable differential privacy guarantee. Our algorithm assumes an untrusted central server that aggregates differentially private local centers and membership encodings from local data parties. It builds a weighted grid as the synopsis of the global
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CondOT &#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#24102;&#26377;&#19978;&#19979;&#25991;&#26631;&#31614; $c_i$ &#30340;&#22810;&#20010;&#27979;&#37327;&#23545; $\left(\mu_i, \nu_i\right)$ &#26469;&#20272;&#35745;&#19968;&#32452;&#26377;&#26465;&#20214;&#30340; OT &#26144;&#23556;&#65292;&#20197;&#20415;&#23558;&#19978;&#19979;&#25991;&#22240;&#32032;&#24341;&#20837; OT &#20272;&#35745;&#20013;&#12290;</title><link>http://arxiv.org/abs/2206.14262</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#33945;&#28909;&#26144;&#23556;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Supervised Training of Conditional Monge Maps. (arXiv:2206.14262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CondOT &#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#24102;&#26377;&#19978;&#19979;&#25991;&#26631;&#31614; $c_i$ &#30340;&#22810;&#20010;&#27979;&#37327;&#23545; $\left(\mu_i, \nu_i\right)$ &#26469;&#20272;&#35745;&#19968;&#32452;&#26377;&#26465;&#20214;&#30340; OT &#26144;&#23556;&#65292;&#20197;&#20415;&#23558;&#19978;&#19979;&#25991;&#22240;&#32032;&#24341;&#20837; OT &#20272;&#35745;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#25551;&#36848;&#20102;&#23558;&#19968;&#20010;&#27010;&#29575;&#27979;&#37327;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#27010;&#29575;&#27979;&#37327;&#30340;&#26368;&#26377;&#25928;&#26041;&#24335;&#30340;&#19968;&#33324;&#21407;&#21017;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#39044;&#27979;&#32454;&#32990;&#23545;&#27835;&#30103;&#30340;&#21453;&#24212;&#65292;&#22312;&#23450;&#20041;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#36755;&#20837;/&#36755;&#20986;&#25968;&#25454;&#27979;&#37327;&#23545; $(\mu, \nu)$ &#19981;&#26159;&#21333;&#29420;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#19982;&#26576;&#20010;&#19978;&#19979;&#25991; $c$ &#30456;&#20851;&#32852;&#65292;&#20363;&#22914;&#27604;&#36739;&#26410;&#32463;&#22788;&#29702;&#21644;&#22788;&#29702;&#30340;&#32454;&#32990;&#32676;&#20307;&#26102;&#30340;&#27835;&#30103;&#12290;&#20026;&#20102;&#23558;&#19978;&#19979;&#25991;&#32771;&#34385;&#36827; OT &#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; CondOT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#24102;&#26377;&#19978;&#19979;&#25991;&#26631;&#31614; $c_i$ &#30340;&#27979;&#37327;&#23545; $\left(\mu_i, \nu_i\right)$ &#26469;&#20272;&#35745;&#19968;&#26063;&#26377;&#26465;&#20214;&#30340; OT &#26144;&#23556;&#12290;CondOT &#23398;&#20064;&#20102;&#19968;&#20010;&#20840;&#23616;&#26144;&#23556; $\mathcal{T}_\theta$&#65292;&#22312;&#19978;&#19979;&#25991;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) theory describes general principles to define and select, among many possible choices, the most efficient way to map a probability measure onto another. That theory has been mostly used to estimate, given a pair of source and target probability measures $(\mu, \nu)$, a parameterized map $T_\theta$ that can efficiently map $\mu$ onto $\nu$. In many applications, such as predicting cell responses to treatments, pairs of input/output data measures $(\mu, \nu)$ that define optimal transport problems do not arise in isolation but are associated with a context $c$, as for instance a treatment when comparing populations of untreated and treated cells. To account for that context in OT estimation, we introduce CondOT, a multi-task approach to estimate a family of OT maps conditioned on a context variable, using several pairs of measures $\left(\mu_i, \nu_i\right)$ tagged with a context label $c_i$. CondOT learns a global map $\mathcal{T}_\theta$ conditioned on context th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24120;&#29992;&#30340;Accuracy&#21644;Dice&#27979;&#37327;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#25506;&#31350;&#20013;&#33719;&#24471;&#20102;&#32467;&#35770;&#65306;&#20004;&#31181;&#27979;&#37327;&#26041;&#27861;&#30340;&#26368;&#20248;&#35299;&#38598;&#20307;&#31215;&#21487;&#33021;&#26174;&#33879;&#20559;&#31163;&#30446;&#26631;&#39044;&#26399;&#20307;&#31215;&#65292;Accuracy&#30340;&#35299;&#30340;&#20307;&#31215;&#22987;&#32456;&#23567;&#20110;&#31561;&#20110;Dice&#30340;&#35299;&#30340;&#20307;&#31215;&#65292;&#20294;&#24403;&#21487;&#34892;&#20998;&#21106;&#38598;&#34987;&#38480;&#21046;&#20026;&#20307;&#31215;&#31561;&#20110;&#30446;&#26631;&#39044;&#26399;&#20307;&#31215;&#30340;&#20998;&#21106;&#38598;&#26102;&#65292;&#20004;&#31181;&#25351;&#26631;&#30340;&#26368;&#20248;&#35299;&#26159;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.06484</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65306;Accuracy&#21644;Dice&#30340;&#26368;&#20248;&#35299;&#30340;&#29305;&#24615;&#21644;&#20307;&#31215;&#23646;&#24615;&#30340;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice. (arXiv:2206.06484v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24120;&#29992;&#30340;Accuracy&#21644;Dice&#27979;&#37327;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#25506;&#31350;&#20013;&#33719;&#24471;&#20102;&#32467;&#35770;&#65306;&#20004;&#31181;&#27979;&#37327;&#26041;&#27861;&#30340;&#26368;&#20248;&#35299;&#38598;&#20307;&#31215;&#21487;&#33021;&#26174;&#33879;&#20559;&#31163;&#30446;&#26631;&#39044;&#26399;&#20307;&#31215;&#65292;Accuracy&#30340;&#35299;&#30340;&#20307;&#31215;&#22987;&#32456;&#23567;&#20110;&#31561;&#20110;Dice&#30340;&#35299;&#30340;&#20307;&#31215;&#65292;&#20294;&#24403;&#21487;&#34892;&#20998;&#21106;&#38598;&#34987;&#38480;&#21046;&#20026;&#20307;&#31215;&#31561;&#20110;&#30446;&#26631;&#39044;&#26399;&#20307;&#31215;&#30340;&#20998;&#21106;&#38598;&#26102;&#65292;&#20004;&#31181;&#25351;&#26631;&#30340;&#26368;&#20248;&#35299;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;Accuracy&#21644;Dice&#22312;&#30446;&#26631;&#26631;&#31614;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#26368;&#20339;&#20998;&#21106;&#38598;&#30340;&#29305;&#24615;&#21644;&#20307;&#31215;&#23646;&#24615;&#26377;&#20851;&#30340;&#22810;&#20010;&#21629;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65306;&#65288;i&#65289;&#20004;&#31181;&#27979;&#37327;&#26041;&#27861;&#30340;&#35299;&#38598;&#30340;&#20307;&#31215;&#21487;&#33021;&#26174;&#33879;&#20559;&#31163;&#30446;&#26631;&#30340;&#39044;&#26399;&#20307;&#31215;&#65292;&#65288;ii&#65289;Accuracy&#30340;&#35299;&#30340;&#20307;&#31215;&#22987;&#32456;&#23567;&#20110;&#31561;&#20110;Dice&#30340;&#35299;&#30340;&#20307;&#31215;&#65292;&#65288;iii&#65289;&#24403;&#21487;&#34892;&#20998;&#21106;&#38598;&#34987;&#38480;&#21046;&#20026;&#20307;&#31215;&#31561;&#20110;&#30446;&#26631;&#39044;&#26399;&#20307;&#31215;&#30340;&#20998;&#21106;&#38598;&#26102;&#65292;&#36825;&#20004;&#31181;&#25351;&#26631;&#30340;&#26368;&#20248;&#35299;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.
&lt;/p&gt;</description></item><item><title>&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#20855;&#26377;&#22256;&#38590;&#24615;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#23384;&#22312;&#36731;&#24494;&#30340;&#36951;&#24536;&#65292;&#38543;&#21518;&#20250;&#26377;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#36319;&#38543;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31283;&#23450;&#24615;&#24046;&#36317;&#8221;&#12290;</title><link>http://arxiv.org/abs/2205.13452</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#30340;&#25345;&#32493;&#35780;&#20272;&#65306;&#35782;&#21035;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13452
&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#20855;&#26377;&#22256;&#38590;&#24615;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#23384;&#22312;&#36731;&#24494;&#30340;&#36951;&#24536;&#65292;&#38543;&#21518;&#20250;&#26377;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#36319;&#38543;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31283;&#23450;&#24615;&#24046;&#36317;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36138;&#23146;&#30340;&#26356;&#26032;&#20250;&#23548;&#33268;&#20197;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#28798;&#38590;&#24615;&#22320;&#34987;&#36951;&#24536;&#12290;&#23613;&#31649;&#32456;&#36523;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#20197;&#20811;&#26381;&#36825;&#31181;&#36951;&#24536;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#24120;&#35265;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#36951;&#24536;&#65292;&#21482;&#26159;&#36825;&#31181;&#36951;&#24536;&#26159;&#26242;&#26102;&#30340;&#65292;&#20250;&#34987;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#25152;&#36319;&#38543;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26377;&#36259;&#20294;&#28508;&#22312;&#26377;&#38382;&#39064;&#30340;&#29616;&#35937;&#31216;&#20026;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#30001;&#20110;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#39046;&#22495;&#20165;&#22312;&#27599;&#20010;&#20219;&#21153;&#20043;&#21518;&#36827;&#34892;&#35780;&#20272;&#30340;&#26631;&#20934;&#20570;&#27861;&#65292;&#22240;&#27492;&#31283;&#23450;&#24615;&#24046;&#36317;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#12290;&#32780;&#25105;&#20204;&#21017;&#24314;&#31435;&#20102;&#19968;&#20010;&#32456;&#36523;&#35780;&#20272;&#26694;&#26550;&#65292;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#30340;&#35780;&#20272;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#39564;&#22238;&#25918;&#12289;&#22522;&#20110;&#32422;&#26463;&#30340;&#22238;&#25918;&#12289;&#30693;&#35782;_distillation&#21644;_expansion&#37117;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RSSI&#21644;&#20247;&#21253;&#24863;&#30693;&#30340;&#22495;&#23545;&#25239;&#22270;&#21367;&#31215;&#32593;&#32476;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23569;&#37327;&#26631;&#35760;&#30340;&#31449;&#28857;&#35843;&#26597;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#20247;&#21253;WiFi&#25351;&#32441;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23450;&#20301;&#31934;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2204.05184</link><description>&lt;p&gt;
&#22522;&#20110;RSSI&#21644;&#20247;&#21253;&#24863;&#30693;&#30340;&#22495;&#23545;&#25239;&#22270;&#21367;&#31215;&#32593;&#32476;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization. (arXiv:2204.05184v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RSSI&#21644;&#20247;&#21253;&#24863;&#30693;&#30340;&#22495;&#23545;&#25239;&#22270;&#21367;&#31215;&#32593;&#32476;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23569;&#37327;&#26631;&#35760;&#30340;&#31449;&#28857;&#35843;&#26597;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#20247;&#21253;WiFi&#25351;&#32441;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23450;&#20301;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;WiFi&#25351;&#32441;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;WiFi&#24191;&#27867;&#21487;&#29992;&#21644;&#31227;&#21160;&#36890;&#20449;&#35774;&#22791;&#30340;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#25351;&#32441;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#32791;&#26102;&#36153;&#21147;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#29702;&#24819;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#65292;&#32780;&#24573;&#30053;&#20102;&#22823;&#22411;&#22810;&#23618;&#24314;&#31569;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;WiDAGCN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#31449;&#28857;&#35843;&#26597;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#20247;&#21253;WiFi&#25351;&#32441;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22522;&#20110;&#27979;&#21521;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#26500;&#24314;&#25910;&#21457;&#28857;&#21644;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#20043;&#38388;&#30340;&#24322;&#26500;&#22270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26469;&#25552;&#21462;&#22270;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22495;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#20943;&#23569;&#31449;&#28857;&#35843;&#26597;&#25968;&#25454;&#21644;&#20247;&#21253;&#25968;&#25454;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23450;&#20301;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the use of WiFi fingerprints for indoor positioning has grown in popularity, largely due to the widespread availability of WiFi and the proliferation of mobile communication devices. However, many existing methods for constructing fingerprint datasets rely on labor-intensive and time-consuming processes of collecting large amounts of data. Additionally, these methods often focus on ideal laboratory environments, rather than considering the practical challenges of large multi-floor buildings. To address these issues, we present a novel WiDAGCN model that can be trained using a small number of labeled site survey data and large amounts of unlabeled crowdsensed WiFi fingerprints. By constructing heterogeneous graphs based on received signal strength indicators (RSSIs) between waypoints and WiFi access points (APs), our model is able to effectively capture the topological structure of the data. We also incorporate graph convolutional networks (GCNs) to extract graph-level 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.00936</link><description>&lt;p&gt;
&#24102;&#22806;&#37096;&#35760;&#24518;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Multi-modal Dynamics with External Memory. (arXiv:2203.00936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#36830;&#32493;&#20986;&#29616;&#26102;&#65292;&#22914;&#20309;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#24847;&#35782;&#21040;&#26032;&#30340;&#27169;&#24335;&#20986;&#29616;&#65292;&#20294;&#23427;&#27809;&#26377;&#35775;&#38382;&#21333;&#20010;&#35757;&#32451;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#65292;&#22240;&#20026;&#21442;&#25968;&#20256;&#36882;&#21463;&#21040;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#24433;&#21709;&#65292;&#32780;&#24773;&#33410;&#35760;&#24518;&#35774;&#35745;&#38656;&#35201;&#30693;&#36947;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#24773;&#33410;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#30340;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#20811;&#26381;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#35760;&#24518;&#30340;&#27880;&#24847;&#26435;&#37325;&#19978;&#20351;&#29992;Dirichlet&#36807;&#31243;&#20808;&#39564;&#65292;&#20197;&#20419;&#36827;&#27169;&#24335;&#25551;&#36848;&#31526;&#30340;&#26377;&#25928;&#23384;&#20648;&#12290;&#36890;&#36807;&#26816;&#32034;&#20808;&#21069;&#20219;&#21153;&#30456;&#20284;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#27492;&#25551;&#36848;&#31526;&#39304;&#20837;&#20854;&#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#26469;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transitio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;VRL3&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2202.10324</link><description>&lt;p&gt;
VRL3&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. (arXiv:2202.10324v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;VRL3&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VRL3&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#31616;&#21333;&#35774;&#35745;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20219;&#21153;&#30340;&#24378;&#22823;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#12290;&#20316;&#32773;&#20998;&#26512;&#20102;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#35774;&#35745;&#21407;&#21017;&#12289;&#26032;&#30340;&#21457;&#29616;&#21644;&#20851;&#20110;&#25968;&#25454;&#39537;&#21160;&#35270;&#35273;DRL&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#20316;&#32773;&#21033;&#29992;&#38750;RL&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#26469;&#23398;&#20064;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#20316;&#32773;&#21033;&#29992;&#31163;&#32447;RL&#25968;&#25454;&#65288;&#20363;&#22914;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#65289;&#23558;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#36716;&#21270;&#20026;&#26356;&#24378;&#22823;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65307;&#22312;&#31532;&#19977;&#38454;&#27573;&#65292;&#20316;&#32773;&#36890;&#36807;&#22312;&#32447;RL&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#19968;&#32452;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25361;&#25112;&#24615;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#65292;&#19982;&#20043;&#21069;&#30340;SOTA&#30456;&#27604;&#65292;VRL3&#30340;&#26679;&#26412;&#25928;&#29575;&#24179;&#22343;&#25552;&#39640;&#20102;780%&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#19978;&#65292;VRL3&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#20102;1220%&#65288;&#20351;&#29992;&#26356;&#23485;&#30340;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#21040;2440%&#65289;&#65292;&#24182;&#20197;&#36229;&#36807;SOTA&#30340;&#24615;&#33021;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Quantus&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35814;&#23613;&#36805;&#36895;&#22320;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#35299;&#37322;&#34920;&#29616;&#65292;&#24182;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.06861</link><description>&lt;p&gt;
Quantus: &#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#36127;&#36131;&#20219;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#21644;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond. (arXiv:2202.06861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Quantus&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35814;&#23613;&#36805;&#36895;&#22320;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#35299;&#37322;&#34920;&#29616;&#65292;&#24182;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26041;&#27861;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#35299;&#37322;&#24615;&#34987;&#35748;&#20026;&#33021;&#22686;&#24378;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#65292;&#26377;&#24517;&#35201;&#31995;&#32479;&#22320;&#23457;&#26680;&#21644;&#27604;&#36739;&#35299;&#37322;&#26041;&#27861;&#20197;&#30830;&#35748;&#20854;&#27491;&#30830;&#24615;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19987;&#27880;&#20110;XAI&#35780;&#20272;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#35814;&#23613;&#36805;&#36895;&#22320;&#35753;&#30740;&#31350;&#32773;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#22686;&#21152;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Quantus&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#12289;&#33391;&#22909;&#32452;&#32455;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#37322;&#26041;&#27861;&#35780;&#20272;&#30340;&#25945;&#31243;&#38598;&#21512;&#12290;&#35813;&#24037;&#20855;&#21253;&#32463;&#36807;&#20102;&#24443;&#24213;&#30340;&#27979;&#35797;&#65292;&#21487;&#22312;PyPi&#19979;&#25110;https://github.com/understandable-machine-intelligence-lab/Quantus/&#19978;&#20197;&#24320;&#28304;&#35768;&#21487;&#35777;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#26031;&#27979;&#24230;&#20043;&#38388;&#30340;Schr\"odinger&#26725;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2202.05722</link><description>&lt;p&gt;
&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;Schr\"odinger&#26725;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
The Schr\"odinger Bridge between Gaussian Measures has a Closed Form. (arXiv:2202.05722v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#26031;&#27979;&#24230;&#20043;&#38388;&#30340;Schr\"odinger&#26725;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;&#38745;&#24577;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65288;$\mathrm{OT}$&#65289;&#23547;&#27714;&#24674;&#22797;&#26368;&#20248;&#26144;&#23556;&#25110;&#26356;&#24191;&#20041;&#30340;&#32806;&#21512;&#65292;&#23558;&#19968;&#20010;&#39640;&#26031;&#20998;&#24067;&#21464;&#20026;&#21478;&#19968;&#20010;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#25991;&#20851;&#27880;OT&#30340;&#21160;&#24577;&#20844;&#24335;&#65292;&#20063;&#31216;&#20026;Schr\"odinger&#26725;&#65288;SB&#65289;&#38382;&#39064;&#65292;&#30001;&#20110;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#24341;&#36215;&#20102;&#20852;&#36259;&#12290;&#19982;&#38745;&#24577;&#35774;&#32622;&#30456;&#27604;&#65292;&#21363;&#20351;&#23545;&#20110;&#39640;&#26031;&#20998;&#24067;&#65292;&#21160;&#24577;&#35774;&#32622;&#20063;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#39640;&#26031;&#27979;&#24230;&#20043;&#38388;&#30340;SB&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#19982;&#38745;&#24577;&#39640;&#26031;OT&#38382;&#39064;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#31616;&#21333;&#22320;&#32553;&#20943;&#20026;&#30740;&#31350;&#20984;&#31243;&#24207;&#65292;&#25105;&#20204;&#27714;&#35299;SB&#30340;&#26694;&#26550;&#38656;&#35201;&#26356;&#22810;&#22797;&#26434;&#30340;&#24037;&#20855;&#65292;&#22914;&#40654;&#26364;&#20960;&#20309;&#21644;&#29983;&#25104;&#22120;&#29702;&#35770;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#36215;&#39640;&#26031;&#27979;&#24230;&#20043;&#38388;SB&#30340;&#35299;&#26412;&#36523;&#26159;&#24102;&#26377;&#39640;&#26031;&#36807;&#31243;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The static optimal transport $(\mathrm{OT})$ problem between Gaussians seeks to recover an optimal map, or more generally a coupling, to morph a Gaussian into another. It has been well studied and applied to a wide variety of tasks. Here we focus on the dynamic formulation of OT, also known as the Schr\"odinger bridge (SB) problem, which has recently seen a surge of interest in machine learning due to its connections with diffusion-based generative models. In contrast to the static setting, much less is known about the dynamic setting, even for Gaussian distributions. In this paper, we provide closed-form expressions for SBs between Gaussian measures. In contrast to the static Gaussian OT problem, which can be simply reduced to studying convex programs, our framework for solving SBs requires significantly more involved tools such as Riemannian geometry and generator theory. Notably, we establish that the solutions of SBs between Gaussian measures are themselves Gaussian processes with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2202.01752</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#35774;&#35745;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#31639;&#27861;&#31995;&#21015;&#65292;&#20165;&#38656;&#35201; $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ &#23616;&#28216;&#25103;&#21363;&#21487;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#20013; $X,Y$ &#26159;&#20449;&#24687;&#38598;&#30340;&#25968;&#37327;&#65292;$A,B$ &#26159;&#20004;&#21517;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#12290;&#36825;&#27604;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230; $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ &#26377;&#30528; $\widetilde{\mathcal{O}}(\max\{X, Y\})$ &#30340;&#24040;&#22823;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#19982;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;&#24179;&#34913;&#22312;&#32447;&#38236;&#38754;&#19979;&#38477;&#21644;&#24179;&#34913;&#21453;&#20107;&#23454;&#21518;&#24724;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#23558;&#8220;&#24179;&#34913;&#25506;&#32034;&#31574;&#30053;&#8221;&#38598;&#25104;&#21040;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#25163;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#25903;&#25345;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#30340;&#20108;&#20154;&#21338;&#24328;&#21644;&#22810;&#20154;&#21338;&#24328;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.14000</link><description>&lt;p&gt;
&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Factor-augmented tree ensembles. (arXiv:2111.14000v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#25552;&#21462;&#28508;&#22312;&#31283;&#24577;&#22240;&#23376;&#26469;&#25193;&#23637;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#20449;&#24687;&#38598;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#35813;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#27979;&#37327;&#35823;&#24046;&#12289;&#38750;&#24179;&#31283;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;/&#25110;&#32570;&#22833;&#35266;&#27979;&#31561;&#19981;&#35268;&#21017;&#30340;&#39044;&#27979;&#21464;&#37327;&#12290;&#31532;&#20108;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26126;&#30830;&#30340;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#29702;&#35770;&#26469;&#25351;&#23548;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#22312;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#32654;&#22269;&#32929;&#31080;&#27874;&#21160;&#29575;&#19982;&#21830;&#19994;&#21608;&#26399;&#20043;&#38388;&#30340;&#20808;&#23548;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript proposes to extend the information set of time-series regression trees with latent stationary factors extracted via state-space methods. In doing so, this approach generalises time-series regression trees on two dimensions. First, it allows to handle predictors that exhibit measurement error, non-stationary trends, seasonality and/or irregularities such as missing observations. Second, it gives a transparent way for using domain-specific theory to inform time-series regression trees. Empirically, ensembles of these factor-augmented trees provide a reliable approach for macro-finance problems. This article highlights it focussing on the lead-lag effect between equity volatility and the business cycle in the United States.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#33021;&#21147;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#20010;&#23458;&#35266;&#27979;&#35797;&#20197;&#35780;&#20272;&#19981;&#21516;GAN&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2111.12577</link><description>&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#24615;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context. (arXiv:2111.12577v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#33021;&#21147;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#20010;&#23458;&#35266;&#27979;&#35797;&#20197;&#35780;&#20272;&#19981;&#21516;GAN&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#21487;&#20197;&#25913;&#21464;&#35786;&#26029;&#25104;&#20687;&#39046;&#22495;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;DGM&#12290;&#28982;&#32780;&#65292;&#23558;GAN&#21644;&#20854;&#20182;DGM&#24212;&#29992;&#20110;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#30340;&#20219;&#20309;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#26222;&#36941;&#23384;&#22312;&#32570;&#20047;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#39046;&#22495;&#30456;&#20851;&#36136;&#37327;&#30340;&#20805;&#20998;&#25110;&#33258;&#21160;&#21270;&#25163;&#27573;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;GAN&#26550;&#26500;&#36755;&#20986;&#30340;&#22270;&#20687;&#30340;&#20960;&#20010;&#23458;&#35266;&#27979;&#35797;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#38543;&#26426;&#19978;&#19979;&#25991;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#24674;&#22797;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#21518;&#21487;&#20197;&#24674;&#22797;&#30340;&#19981;&#21516;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#26159;&#39640;&#38454;&#31639;&#27861;&#20687;&#32032;&#25490;&#21015;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#26131;&#34920;&#36798;&#20026;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#39564;&#35777;&#20102;&#32479;&#35745;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#26816;&#27979;&#24050;&#30693;&#25490;&#21015;&#35268;&#21017;&#30340;&#29305;&#23450;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;GAN&#27491;&#30830;&#22797;&#29616;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) have the potential to revolutionize diagnostic imaging. Generative adversarial networks (GANs) are one kind of DGM which are widely employed. The overarching problem with deploying GANs, and other DGMs, in any application that requires domain expertise in order to actually use the generated images is that there generally is not adequate or automatic means of assessing the domain-relevant quality of generated images. In this work, we demonstrate several objective tests of images output by two popular GAN architectures. We designed several stochastic context models (SCMs) of distinct image features that can be recovered after generation by a trained GAN. Several of these features are high-order, algorithmic pixel-arrangement rules which are not readily expressed in covariance matrices. We designed and validated statistical classifiers to detect specific effects of the known arrangement rules. We then tested the rates at which two different GANs correctly rep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2110.10325</link><description>&lt;p&gt;
&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#21450;&#20854;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#21512;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#35825;&#23548;&#23398;&#20064;&#65292;&#22312;&#21457;&#26126;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;OSAMTL&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#21463;&#35825;&#23548;&#23398;&#20064;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#24335;&#31616;&#21333;&#22320;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21333;&#20010;&#22024;&#26434;&#26631;&#31614;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#20294;&#26159;&#65292;OSAMTL&#19981;&#36866;&#29992;&#20110;&#25552;&#20379;&#22810;&#31181;&#22024;&#26434;&#26679;&#26412;&#65288;DiNS&#65289;&#30340;&#23398;&#20064;&#20219;&#21153;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;DiNS&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#25193;&#23637;&#21407;&#22987;&#30340;OSAMTL&#20197;&#22788;&#29702;DiNS&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#23558;OSAMTL-DiNS&#24212;&#29992;&#20110;MHWSIA&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.04829</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#23884;&#20837;&#24352;&#37327;&#31215;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#19968;&#20010;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#22810;&#36798;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#28857;&#30340;&#26679;&#26412;&#22823;&#23567;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#20943;&#36731;&#20102;RKHS&#24314;&#27169;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#20135;&#29983;&#20102;&#23450;&#20041;&#33391;&#22909;&#30340;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#23884;&#20837;&#35745;&#31639;&#36895;&#24230;&#24555;&#19988;&#36866;&#29992;&#20110;&#20174;&#39044;&#27979;&#21040;&#20998;&#31867;&#30340;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#26377;&#30410;&#30340;&#25968;&#20540;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#22810;&#35821;&#35328;&#31038;&#20132;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#65292;&#23545;&#20110;&#36739;&#23569;&#20351;&#29992;&#30340;&#35821;&#35328;&#20063;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2108.03084</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#24212;&#29992;&#22312;&#22810;&#35821;&#35328;&#31038;&#20132;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#22810;&#35821;&#35328;&#31038;&#20132;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#65292;&#23545;&#20110;&#36739;&#23569;&#20351;&#29992;&#30340;&#35821;&#35328;&#20063;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#31038;&#20132;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#38754;&#21521;&#20855;&#26377;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#35821;&#35328;&#30340;&#21333;&#35821;&#35328;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#20102;&#26356;&#24120;&#35265;&#30340;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#26356;&#23569;&#20351;&#29992;&#30340;&#35821;&#35328;&#30340;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#20197;&#20415;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#27969;&#20013;&#26816;&#27979;&#20107;&#20214;&#12290;&#39318;&#20808;&#30340;&#31361;&#30772;&#22312;&#20110;&#35753;GNN&#33021;&#22815;&#22788;&#29702;&#22810;&#35821;&#35328;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#26500;&#24314;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#33410;&#28857;&#21644;&#35821;&#20041;&#32423;&#21035;&#19978;&#23545;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#28040;&#24687;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#36890;&#36807;&#21512;&#24182;&#30456;&#21516;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#31034;&#30340;&#23454;&#20307;&#26469;&#24314;&#31435;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#38750;&#33521;&#35821;&#28040;&#24687;&#34920;&#31034;&#36890;&#36807;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#35821;&#20041;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;GNN&#27169;&#22411;&#32479;&#19968;&#32534;&#30721;&#29983;&#25104;&#32467;&#26524;&#28040;&#24687;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently published graph neural networks (GNNs) show promising performance at social event detection tasks. However, most studies are oriented toward monolingual data in languages with abundant training samples. This has left the more common multilingual settings and lesser-spoken languages relatively unexplored. Thus, we present a GNN that incorporates cross-lingual word embeddings for detecting events in multilingual data streams. The first exploit is to make the GNN work with multilingual data. For this, we outline a construction strategy that aligns messages in different languages at both the node and semantic levels. Relationships between messages are established by merging entities that are the same but are referred to in different languages. Non-English message representations are converted into English semantic space via the cross-lingual word embeddings. The resulting message graph is then uniformly encoded by a GNN model. In special cases where a lesser-spoken language needs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#23545;&#29031;&#30340;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#25991;&#31456;&#25299;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#24615;&#26356;&#21152;&#24191;&#27867;&#65292;&#24182;&#22312;&#35745;&#31639;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.02780</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24178;&#39044;&#27169;&#24335;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#23545;&#29031;&#30340;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#25991;&#31456;&#25299;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#24615;&#26356;&#21152;&#24191;&#27867;&#65292;&#24182;&#22312;&#35745;&#31639;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#26159;&#19968;&#20010;&#20013;&#24515;&#35745;&#37327;&#32463;&#27982;&#23398;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#26159;&#19968;&#20010;&#22522;&#26412;&#29256;&#26412;&#30340;&#38754;&#26495;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65306;&#35774;$M^*$&#20026;&#20302;&#31209;&#30697;&#38453;&#65292;$E$&#20026;&#38646;&#22343;&#20540;&#22122;&#22768;&#30697;&#38453;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$\{0,1\}$&#20540;&#30340;&#8220;&#27835;&#30103;&#8221;&#30697;&#38453;$Z$&#65292;&#25105;&#20204;&#35266;&#27979;&#21040;&#30697;&#38453;$O$&#65292;&#20854;&#20013;$O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$&#65292;&#20854;&#20013;$\mathcal{T}_{ij}$&#26159;&#26410;&#30693;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#12290;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#25105;&#20204;&#20272;&#35745;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;$\tau^*:=\sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$&#12290;&#21512;&#25104;&#23545;&#29031;&#33539;&#20363;&#25552;&#20379;&#20102;&#19968;&#31181;&#20272;&#35745;$\tau^*$&#30340;&#26041;&#27861;&#65292;&#24403;$Z$&#20165;&#20165;&#25903;&#25345;&#21333;&#20010;&#34892;&#26102;&#12290;&#26412;&#25991;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#20801;&#35768;&#19968;&#33324;&#30340;$Z$&#30340;&#36895;&#29575;&#26368;&#20248;&#24674;&#22797;$\tau^*$&#65292;&#20174;&#32780;&#24191;&#27867;&#25193;&#23637;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#26159;&#22312;&#36825;&#20010;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#31532;&#19968;&#27425;&#20986;&#29616;&#30340;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#31454;&#20105;&#20272;&#35745;&#22120;&#20855;&#26377;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let $M^*$ be a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix $Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to estimating $\tau^*$ when $Z$ places support on a single row. This paper extends that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus broadly expanding its applicability. Our guarantees are the first of their type in this general setting. Computational experiments on synthetic and real-world data show a substantial advantage over competing estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#26469;&#35757;&#32451;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20248;&#21270;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#26041;&#27861;&#21644;&#40723;&#21169;NN&#26356;&#21152;&#31232;&#30095;&#30340;&#27491;&#21017;&#21270;&#39033;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;GPU&#25110;&#22797;&#26434;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2009.03825</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20248;&#21270;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.03825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#26469;&#35757;&#32451;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20248;&#21270;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#26041;&#27861;&#21644;&#40723;&#21169;NN&#26356;&#21152;&#31232;&#30095;&#30340;&#27491;&#21017;&#21270;&#39033;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;GPU&#25110;&#22797;&#26434;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#21487;&#20197;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20294;&#26159;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#30446;&#21069;&#30340;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12289;&#22312;GPU&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#36827;&#34892;&#35757;&#32451;&#19981;&#38656;&#35201;GPU&#25110;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#22788;&#29702;&#23569;&#37327;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#35757;&#32451;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#25928;&#29575;&#24471;&#21040;&#25913;&#21892;&#65292;&#24182;&#21487;&#20197;&#35757;&#32451;&#37325;&#35201;&#30340;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#21457;&#25381;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28508;&#22312;&#37325;&#35201;&#24615;&#65292;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#30340;&#21516;&#26102;&#20248;&#21270;NN&#20013;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#36825;&#20943;&#23569;&#20102;&#22312;&#35757;&#32451;&#20043;&#21069;&#20915;&#23450;&#32593;&#32476;&#32467;&#26500;&#30340;&#38656;&#35201;&#65292;&#24182;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#35757;&#32451;&#30340;NN&#26356;&#21152;&#31232;&#30095;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown potential in using Mixed Integer Programming (MIP) solvers to optimize certain aspects of neural networks (NNs). However the intriguing approach of training NNs with MIP solvers is under-explored. State-of-the-art-methods to train NNs are typically gradient-based and require significant data, computation on GPUs, and extensive hyper-parameter tuning. In contrast, training with MIP solvers does not require GPUs or heavy hyper-parameter tuning, but currently cannot handle anything but small amounts of data. This article builds on recent advances that train binarized NNs using MIP solvers. We go beyond current work by formulating new MIP models which improve training efficiency and which can train the important class of integer-valued neural networks (INNs). We provide two novel methods to further the potential significance of using MIP to train NNs. The first method optimizes the number of neurons in the NN while training. This reduces the need for deciding on netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#30740;&#31350;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#65292;&#21457;&#29616;&#30456;&#27604;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#26080;&#27861;&#26174;&#31034;&#20986;&#20449;&#24687;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/1902.09037</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20272;&#35745;&#22120;&#26174;&#31034;&#20449;&#24687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Adaptive Estimators Show Information Compression in Deep Neural Networks. (arXiv:1902.09037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.09037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#30740;&#31350;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#65292;&#21457;&#29616;&#30456;&#27604;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#26080;&#27861;&#26174;&#31034;&#20986;&#20449;&#24687;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#25552;&#20986;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21387;&#32553;&#23427;&#20204;&#30340;&#34920;&#31034;&#26469;&#24573;&#30053;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#29702;&#35770;&#30340;&#32463;&#39564;&#35777;&#25454;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#32593;&#32476;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#26102;&#25165;&#35266;&#23519;&#21040;&#21387;&#32553;&#12290;&#30456;&#21453;&#65292;&#20855;&#26377;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#26174;&#31034;&#20986;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26356;&#24378;&#22823;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#25216;&#26415;&#65292;&#36866;&#24212;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#27963;&#21160;&#65292;&#24182;&#20135;&#29983;&#26356;&#25935;&#24863;&#30340;&#20174;&#25152;&#26377;&#20989;&#25968;&#20013;&#28608;&#27963;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#26080;&#30028;&#20989;&#25968;&#12290;&#21033;&#29992;&#36825;&#20123;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#20013;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;...
&lt;/p&gt;
&lt;p&gt;
To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we 
&lt;/p&gt;</description></item></channel></rss>