<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#36817;&#20284;&#26041;&#27861;&#23454;&#29616;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01431</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;POMDP&#30340;&#36817;&#20284;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Approximate Control for Continuous-Time POMDPs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#36817;&#20284;&#26041;&#27861;&#23454;&#29616;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20855;&#26377;&#31163;&#25955;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#20915;&#31574;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#30340;&#29366;&#24577;&#25968;&#37327;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#32500;&#36807;&#28388;&#20998;&#24067;&#25237;&#24433;&#21040;&#21442;&#25968;&#20998;&#24067;&#26063;&#19978;&#26469;&#36817;&#20284;&#23427;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#22522;&#20110;&#23436;&#20840;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#25511;&#21046;&#21551;&#21457;&#24335;&#20013;&#65292;&#20197;&#33719;&#24471;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#37096;&#20998;&#35266;&#23519;&#31995;&#32479;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25490;&#38431;&#31995;&#32479;&#21644;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a decision-making framework for partially observable systems in continuous time with discrete state and action spaces. As optimal decision-making becomes intractable for large state spaces we employ approximation methods for the filtering and the control problem that scale well with an increasing number of states. Specifically, we approximate the high-dimensional filtering distribution by projecting it onto a parametric family of distributions, and integrate it into a control heuristic based on the fully observable system to obtain a scalable policy. We demonstrate the effectiveness of our approach on several partially observed systems, including queueing systems and chemical reaction networks.
&lt;/p&gt;</description></item><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#20882;&#29260;&#26679;&#26412;&#30340;&#29702;&#35299;&#36739;&#20026;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#35823;&#23558;&#20882;&#29260;&#26679;&#26412;&#20998;&#31867;&#20026;&#27491;&#30830;&#12289;&#22312;&#25512;&#29702;&#20882;&#29260;&#26679;&#26412;&#30340;&#25191;&#34892;&#34892;&#20026;&#26102;&#34920;&#29616;&#26356;&#24046;&#12289;&#20462;&#22797;&#20882;&#29260;&#26679;&#26412;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#20302;&#20110;&#29983;&#25104;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19475</link><description>&lt;p&gt;
&#20882;&#29260;&#38590;&#39064;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#20854;&#19981;&#27491;&#30830;&#29983;&#25104;&#30340;&#24494;&#22937;&#20043;&#22788;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19475
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#20882;&#29260;&#26679;&#26412;&#30340;&#29702;&#35299;&#36739;&#20026;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#35823;&#23558;&#20882;&#29260;&#26679;&#26412;&#20998;&#31867;&#20026;&#27491;&#30830;&#12289;&#22312;&#25512;&#29702;&#20882;&#29260;&#26679;&#26412;&#30340;&#25191;&#34892;&#34892;&#20026;&#26102;&#34920;&#29616;&#26356;&#24046;&#12289;&#20462;&#22797;&#20882;&#29260;&#26679;&#26412;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#20302;&#20110;&#29983;&#25104;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#23427;&#20204;&#20173;&#28982;&#32463;&#24120;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#35768;&#22810;&#36825;&#20123;&#31243;&#24207;&#26174;&#28982;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#20854;&#20182;&#19968;&#20123;&#21017;&#26356;&#20026;&#24494;&#22937;&#65292;&#21487;&#20197;&#36890;&#36807;&#26356;&#24369;&#30340;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#20363;&#22914;&#33021;&#22815;&#32534;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#20123;&#20266;&#36896;&#30340;&#26679;&#26412;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;1&#65289;&#22312;&#36866;&#24230;&#28201;&#24230;&#19979;&#29983;&#25104;&#30340;&#23545;&#25968;&#27010;&#29575;&#36275;&#22815;&#39640;&#65292;2&#65289;&#36890;&#36807;&#24369;&#27491;&#30830;&#24615;&#26816;&#26597;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23545;&#20266;&#36896;&#21697;&#30340;&#29702;&#35299;&#38750;&#24120;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#38169;&#35823;&#22320;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#27491;&#30830;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#22312;&#25512;&#29702;&#20266;&#36896;&#21697;&#30340;&#25191;&#34892;&#34892;&#20026;&#26041;&#38754;&#26356;&#24046;&#65292;&#36890;&#24120;&#23558;&#23427;&#20204;&#30340;&#25191;&#34892;&#32467;&#26524;&#39044;&#27979;&#20026;&#22914;&#26524;&#23427;&#20204;&#26159;&#27491;&#30830;&#30340;&#19968;&#26679;&#12290;&#31532;&#19977;&#65292;&#22312;&#35201;&#27714;&#27169;&#22411;&#20462;&#22797;&#20266;&#36896;&#21697;&#26102;&#65292;&#27169;&#22411;&#25104;&#21151;&#20462;&#22797;&#20266;&#36896;&#21697;&#30340;&#21487;&#33021;&#24615;&#24448;&#24448;&#29978;&#33267;&#20302;&#20110;&#25277;&#26679;&#29983;&#25104;&#20266;&#36896;&#21697;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19475v1 Announce Type: cross  Abstract: While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of samp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19472</link><description>&lt;p&gt;
&#32456;&#36523;&#22522;&#20934;&#65306;&#22312;&#24555;&#36895;&#36827;&#23637;&#26102;&#20195;&#20013;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#22522;&#20934;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#22797;&#27979;&#35797;&#65292;&#31639;&#27861;&#23545;&#22522;&#20934;&#30340;&#29305;&#27530;&#24615;&#36807;&#24230;&#21033;&#29992;&#65292;&#20250;&#22686;&#21152;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#32534;&#21046;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65288;&#31216;&#20026;&#32456;&#36523;&#22522;&#20934;&#65289;&#26469;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#32456;&#36523;-CIFAR10&#21644;&#32456;&#36523;-ImageNet&#65292;&#20998;&#21035;&#21253;&#21547;&#65288;&#30446;&#21069;&#65289;1.69&#30334;&#19975;&#21644;1.98&#30334;&#19975;&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#23613;&#31649;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#65292;&#32456;&#36523;&#22522;&#20934;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35780;&#20272;&#26085;&#30410;&#22686;&#22810;&#30340;&#27169;&#22411;&#22312;&#19981;&#26029;&#25193;&#22823;&#30340;&#26679;&#26412;&#38598;&#19978;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;Sort \&amp; Search (S&amp;S)&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26377;&#36873;&#25321;&#22320;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#21644;&#23376;&#36873;&#25321;&#65292;&#20351;&#24471;&#32456;&#36523;&#22522;&#20934;&#35780;&#20272;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#36890;&#36807;&#23545;31,000&#20010;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19472v1 Announce Type: new  Abstract: Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \&amp; Search (S&amp;S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#20154;&#24418;&#25511;&#21046;&#38382;&#39064;&#36716;&#21270;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22240;&#26524;&#21464;&#21387;&#22120;&#35757;&#32451;&#23454;&#29616;&#20256;&#24863;&#22120;&#36712;&#36857;&#30340;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#21487;&#22312;&#32570;&#22833;&#27169;&#24577;&#25968;&#25454;&#19979;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#20351;&#20154;&#24418;&#26426;&#22120;&#20154;&#23436;&#25104;&#27493;&#34892;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.19469</link><description>&lt;p&gt;
&#20154;&#24418;&#26426;&#22120;&#20154;&#36816;&#21160;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Humanoid Locomotion as Next Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#20154;&#24418;&#25511;&#21046;&#38382;&#39064;&#36716;&#21270;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22240;&#26524;&#21464;&#21387;&#22120;&#35757;&#32451;&#23454;&#29616;&#20256;&#24863;&#22120;&#36712;&#36857;&#30340;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#21487;&#22312;&#32570;&#22833;&#27169;&#24577;&#25968;&#25454;&#19979;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#20351;&#20154;&#24418;&#26426;&#22120;&#20154;&#23436;&#25104;&#27493;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#20154;&#24418;&#25511;&#21046;&#35299;&#37322;&#20026;&#19968;&#20010;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#39044;&#27979;&#35821;&#35328;&#20013;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#32463;&#36807;&#33258;&#22238;&#24402;&#39044;&#27979;&#20256;&#24863;&#22120;&#36712;&#36857;&#35757;&#32451;&#30340;&#22240;&#26524;&#21464;&#21387;&#22120;&#12290;&#20026;&#20102;&#32771;&#34385;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#25105;&#20204;&#20197;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#24335;&#36827;&#34892;&#39044;&#27979;&#65292;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#65292;&#20174;&#30456;&#21516;&#30340;&#27169;&#24577;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#36825;&#31181;&#19968;&#33324;&#24615;&#26174;&#31034;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#32570;&#22833;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#27809;&#26377;&#21160;&#20316;&#30340;&#35270;&#39057;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#36712;&#36857;&#38598;&#21512;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#36712;&#36857;&#26469;&#33258;&#20808;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#12289;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#12289;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#21644;&#20154;&#31867;&#30340;YouTube&#35270;&#39057;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#19968;&#20010;&#20840;&#23610;&#23544;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26087;&#37329;&#23665;&#34892;&#36208;&#65292;&#29978;&#33267;&#27809;&#26377;&#21463;&#21040;&#22120;&#23448;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#21482;&#26377;27&#23567;&#26102;&#34892;&#36208;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#26102;&#27809;&#26377;&#30475;&#21040;&#30340;&#21629;&#20196;&#65292;&#27604;&#22914;&#21521;&#21518;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19469v1 Announce Type: cross  Abstract: We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backwar
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19464</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22909;&#22855;&#39537;&#21160;&#30340;&#32418;&#38431;&#23545;&#25239;
&lt;/p&gt;
&lt;p&gt;
Curiosity-driven Red-teaming for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19464
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#26377;&#27602;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25506;&#31350;LLM&#20309;&#26102;&#29983;&#25104;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#65292;&#24403;&#21069;&#30340;&#33539;&#20363;&#26159;&#25307;&#21215;&#19968;&#20010;&#20154;&#31867;&#27979;&#35797;&#32773;\textit{&#32418;&#38431;}&#26469;&#35774;&#35745;&#36755;&#20837;&#25552;&#31034;&#65288;&#21363;&#27979;&#35797;&#26696;&#20363;&#65289;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#24341;&#20986;LLMs&#30340;&#19981;&#33391;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#20154;&#31867;&#27979;&#35797;&#32773;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#32418;&#38431;LLM&#33258;&#21160;&#21270;&#32418;&#38431;&#23545;&#25239;&#65292;&#29983;&#25104;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#23569;&#37327;&#26377;&#25928;&#30340;&#27979;&#35797;&#26696;&#20363;&#65292;&#23548;&#33268;&#23545;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#25552;&#31034;&#33539;&#22260;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#22686;&#21152;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#30340;&#38382;&#39064;&#19982;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;ImageNet&#19978;&#30340;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#29702;&#35770;&#21162;&#21147;&#65292;&#20294;&#23454;&#36341;&#20013;&#23578;&#26410;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#24320;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21738;&#20123;&#20272;&#35745;&#22120;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#35265;&#35299;&#24182;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19460</link><description>&lt;p&gt;
&#20026;&#26631;&#20934;&#21270;&#30340;&#20219;&#21153;&#19987;&#38376;&#25351;&#23450;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65306;&#19987;&#38376;&#30340;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#19987;&#38376;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;ImageNet&#19978;&#30340;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#29702;&#35770;&#21162;&#21147;&#65292;&#20294;&#23454;&#36341;&#20013;&#23578;&#26410;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#24320;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21738;&#20123;&#20272;&#35745;&#22120;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#35265;&#35299;&#24182;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#26366;&#32463;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#19968;&#20010;&#21253;&#21547;&#39044;&#27979;&#25233;&#21046;&#12289;&#36234;&#30028;&#26816;&#27979;&#20197;&#21450;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#20869;&#30340;&#20219;&#21153;&#35889;&#31995;&#12290;&#26368;&#26032;&#30340;&#30446;&#26631;&#26159;&#35299;&#24320;&#19981;&#30830;&#23450;&#24615;&#65306;&#26500;&#24314;&#22810;&#20010;&#20272;&#35745;&#22120;&#65292;&#27599;&#20010;&#37117;&#19987;&#38376;&#23450;&#21046;&#20110;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#19981;&#21516;&#24847;&#22270;&#30340;&#26368;&#26032;&#36827;&#23637;&#8212;&#8212;&#36825;&#20123;&#24448;&#24448;&#23436;&#20840;&#20559;&#31163;&#23454;&#38469;&#34892;&#20026;&#12290;&#26412;&#25991;&#22312;ImageNet&#19978;&#23545;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26377;&#30528;&#39047;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#21162;&#21147;&#65292;&#23454;&#36341;&#20013;&#20173;&#26410;&#23454;&#29616;&#35299;&#24320;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21738;&#20123;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#22312;&#21738;&#20123;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#35265;&#35299;&#24182;&#24341;&#23548;&#26410;&#26469;&#30740;&#31350;&#26397;&#30528;&#22522;&#20110;&#20219;&#21153;&#21644;&#35299;&#24320;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/bmucsanyi/bud &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19460v1 Announce Type: new  Abstract: Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one task. Hence, there is a plethora of recent advances with different intentions - that often entirely deviate from practical behavior. This paper conducts a comprehensive evaluation of numerous uncertainty estimators across diverse tasks on ImageNet. We find that, despite promising theoretical endeavors, disentanglement is not yet achieved in practice. Additionally, we reveal which uncertainty estimators excel at which specific tasks, providing insights for practitioners and guiding future research toward task-centric and disentangled uncertainty estimation methods. Our code is available at https://github.com/bmucsanyi/bud.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19455</link><description>&lt;p&gt;
&#21548;&#22122;&#22768;&#65306;&#20351;&#29992;Gibbs&#25193;&#25955;&#36827;&#34892;&#30450;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Listening to the Noise: Blind Denoising with Gibbs Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19455
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#38382;&#39064;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#23494;&#19981;&#21487;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#21435;&#22122;&#22120;&#65292;&#23427;&#20204;&#25152;&#24314;&#27169;&#30340;&#20998;&#24067;&#19982;&#36125;&#21494;&#26031;&#22270;&#20687;&#20013;&#30340;&#21435;&#22122;&#20808;&#39564;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#21518;&#39564;&#37319;&#26679;&#36827;&#34892;&#21435;&#22122;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#65292;&#36825;&#38459;&#30861;&#20102;&#30450;&#21435;&#22122;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#22788;&#29702;&#20449;&#21495;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#20551;&#35774;&#20219;&#24847;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;Gibbs&#31639;&#27861;&#65292;&#20132;&#26367;&#22320;&#20174;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#23558;&#20449;&#21495;&#20808;&#39564;&#26144;&#23556;&#21040;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#19968;&#20010;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#32570;&#38519;&#65292;&#25351;&#23548;&#20102;&#35786;&#26029;&#30340;&#20351;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;Gibbs s&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19455v1 Announce Type: cross  Abstract: In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.19449</link><description>&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;Adam&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#19982;&#19981;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#27604;&#19982;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#24930;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26679;&#26412;&#26469;&#33258;&#30456;&#23545;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65292;&#24179;&#22343;&#25439;&#22833;&#20540;&#22312;&#26799;&#24230;&#19979;&#38477;&#26102;&#19979;&#38477;&#36895;&#24230;&#36739;&#24930;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#21364;&#19981;&#21463;&#27492;&#38382;&#39064;&#24433;&#21709;&#65292;&#24182;&#25913;&#21892;&#20102;&#25152;&#26377;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#34892;&#20026;&#30830;&#23454;&#26159;&#30001;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19449v1 Announce Type: cross  Abstract: Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.19446</link><description>&lt;p&gt;
ArCHer: &#36890;&#36807;&#20998;&#23618;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26696;&#20363;&#26159;&#30446;&#26631;&#23548;&#21521;&#30340;&#20915;&#31574;&#20219;&#21153;&#65288;&#25110;&#8220;&#20195;&#29702;&#8221;&#20219;&#21153;&#65289;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;LLM&#19981;&#20165;&#38656;&#35201;&#20026;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#23436;&#25104;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#20570;&#20986;&#26234;&#33021;&#20915;&#31574;&#20197;&#23436;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#19982;&#32593;&#32476;&#20132;&#20114;&#65292;&#20351;&#29992;&#24037;&#20855;&#25110;&#25552;&#20379;&#23458;&#25143;&#25903;&#25345;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19442</link><description>&lt;p&gt;
&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65306;&#28044;&#29616;&#12289;&#25910;&#25947;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19442
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#32447;&#24615;&#22238;&#24402;&#30340;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#36873;&#25321;&#19979;&#65292;&#26799;&#24230;&#27969;&#21160;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#37117;&#19987;&#27880;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#8212;&#8212;&#28909;&#36523;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#25439;&#22833;&#20943;&#23569;&#36895;&#24230;&#36739;&#24930;&#65292;&#27880;&#24847;&#21147;&#22836;&#36880;&#28176;&#20542;&#21521;&#20110;&#21508;&#33258;&#30340;&#20219;&#21153;&#65307;&#28044;&#29616;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#22836;&#36873;&#25321;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#25439;&#22833;&#36805;&#36895;&#20943;&#23569;&#65307;&#21644;&#25910;&#25947;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27880;&#24847;&#21147;&#21442;&#25968;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#23398;&#20064;&#26497;&#38480;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19442v1 Announce Type: cross  Abstract: We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#25511;&#21046;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19437</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Worst-group Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;$(\epsilon, \delta)$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#23545;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#22312;$p$&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#23376;&#20154;&#32676;&#65288;&#32452;&#65289;&#20013;&#36817;&#20284;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#30340;&#31169;&#26377;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#30340;&#20998;&#24067;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#30340;&#26368;&#22351;&#32452;&#32676;&#20307;&#39118;&#38505;&#36229;&#20986;&#24230;&#20026;$\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} + \sqrt{\frac{p}{K}})$&#65292;&#20854;&#20013;$K$&#26159;&#20174;&#25152;&#26377;&#32452;&#20013;&#25277;&#21462;&#30340;&#26679;&#26412;&#30340;&#24635;&#25968;&#65292;$d$&#26159;&#38382;&#39064;&#32500;&#24230;&#12290;&#24403;&#27599;&#20010;&#20998;&#24067;&#36890;&#36807;&#22823;&#23567;&#20026;$K/p$&#30340;&#22266;&#23450;&#22823;&#23567;&#25968;&#25454;&#38598;&#35266;&#23519;&#26102;&#65292;&#25105;&#20204;&#30340;&#36895;&#29575;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;$\Delta$-&#19968;&#33268;&#24615;&#21442;&#25968;&#31283;&#23450;&#24615;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26368;&#22351;&#32452;&#39118;&#38505;&#30340;$\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$&#27867;&#21270;&#35823;&#24046;&#65292;&#20854;&#20013;$n$&#26159;&#20010;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19437v1 Announce Type: cross  Abstract: We initiate a systematic study of worst-group risk minimization under $(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find a model that approximately minimizes the maximal risk across $p$ sub-populations (groups) with different distributions, where each group distribution is accessed via a sample oracle. We first present a new algorithm that achieves excess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} + \sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from all groups and $d$ is the problem dimension. Our rate is nearly optimal when each distribution is observed via a fixed-size dataset of size $K/p$. Our result is based on a new stability-based analysis for the generalization error. In particular, we show that $\Delta$-uniform argument stability implies $\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. the worst-group risk, where $n$ is the number 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Griffin &#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.19427</link><description>&lt;p&gt;
Griffin: &#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19427
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Griffin &#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#22312;&#38271;&#24207;&#21015;&#19978;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#25928;&#25193;&#23637;&#30340;&#20248;&#21183;&#65292;&#20294;&#35757;&#32451;&#22256;&#38590;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Hawk&#65292;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#30340;RNN&#65292;&#20197;&#21450;Griffin&#65292;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#38376;&#25511;&#32447;&#24615;&#24490;&#29615;&#19982;&#23616;&#37096;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#12290;Hawk&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;Mamba&#65292;&#32780;Griffin&#22312;&#35757;&#32451;&#26102;&#20165;&#20351;&#29992;&#20102;6&#20493;&#23569;&#30340;&#20196;&#29260;&#25968;&#37327;&#21364;&#19982;Llama-2&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Griffin&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#23545;&#27604;&#35757;&#32451;&#26102;&#38271;&#24471;&#22810;&#30340;&#24207;&#21015;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#20855;&#26377;&#19982;Transformer&#30456;&#21305;&#37197;&#30340;&#30828;&#20214;&#25928;&#29575;&#65292;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23558;Griffin&#25193;&#23637;&#21040;&#20102;14B&#21442;&#25968;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20998;&#29255;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19427v1 Announce Type: cross  Abstract: Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.
&lt;/p&gt;</description></item><item><title>PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19411</link><description>&lt;p&gt;
PaECTER&#65306;&#20351;&#29992;&#24341;&#25991;&#20449;&#24687;&#30340;&#19987;&#21033;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PaECTER: Patent-level Representation Learning using Citation-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19411
&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#38754;&#21521;&#19987;&#21033;&#30340;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#23457;&#26680;&#21592;&#28155;&#21152;&#30340;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#19987;&#21033;&#25991;&#26723;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#19982;&#19987;&#21033;&#39046;&#22495;&#20013;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;PaECTER&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19987;&#21033;&#24341;&#25991;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#19981;&#21516;&#30340;&#25490;&#21517;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#19987;&#21033;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#19987;&#21033;BERT&#65289;&#12290;&#19982;25&#20010;&#19981;&#30456;&#20851;&#30340;&#19987;&#21033;&#30456;&#27604;&#65292;PaECTER&#22312;&#24179;&#22343;&#25490;&#21517;1.32&#22788;&#39044;&#27979;&#21040;&#33267;&#23569;&#19968;&#20010;&#26368;&#30456;&#20284;&#30340;&#19987;&#21033;&#12290;PaECTER&#20174;&#19987;&#21033;&#25991;&#26412;&#29983;&#25104;&#30340;&#25968;&#20540;&#34920;&#31034;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#36861;&#36394;&#30693;&#35782;&#27969;&#21160;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#22312;&#21457;&#26126;&#20154;&#21644;&#19987;&#21033;&#30340;&#20808;&#21069;&#25216;&#26415;&#25628;&#32034;&#32972;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Forchestra&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19402</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#36801;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#29992;&#20110;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Forchestra&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#35768;&#22810;&#19994;&#21153;&#38382;&#39064;&#20013;&#26368;&#22522;&#26412;&#19988;&#26368;&#26222;&#36941;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21253;&#25324;&#38656;&#27714;&#39044;&#27979;&#21644;&#29289;&#27969;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38590;&#20197;&#25193;&#23637;&#20854;&#27169;&#22411;&#22823;&#23567;&#65292;&#23548;&#33268;&#20854;&#27169;&#22411;&#35268;&#27169;&#36739;&#23567;&#19988;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Forecasting orchestra (Forchestra)&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#21487;&#25193;&#23637;&#33267;&#39640;&#36798;0.8&#20159;&#20010;&#21442;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#38646;&#26679;&#26412;&#26041;&#24335;&#35780;&#20272;&#19979;&#28216;&#25968;&#25454;&#38598;&#26102;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19402v1 Announce Type: cross  Abstract: Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items. We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters. The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a zero-shot fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.19369</link><description>&lt;p&gt;
&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#35201;&#30340;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#39069;&#22806;&#32467;&#26500;&#65288;&#22914;&#32676;&#23545;&#31216;&#24615;&#65289;&#30340;&#20998;&#24067;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36890;&#36807;&#21046;&#23450;&#25193;&#25955;&#36716;&#25442;&#27493;&#39588;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#29702;&#35770;&#26465;&#20214;&#12290;&#38500;&#20102;&#23454;&#29616;&#31561;&#21464;&#25968;&#25454;&#37319;&#26679;&#36712;&#36857;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#26126;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22266;&#26377;&#23545;&#31216;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#31526;&#21512;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#24182;&#22312;&#26679;&#26412;&#22343;&#31561;&#24615;&#26041;&#38754;&#33021;&#22815;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25552;&#20986;&#30340;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31561;&#21464;&#22270;&#20687;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19369v1 Announce Type: new  Abstract: Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise r
&lt;/p&gt;</description></item><item><title>LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;</title><link>https://arxiv.org/abs/2402.19361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27700;&#21360;&#31363;&#21462;
&lt;/p&gt;
&lt;p&gt;
Watermark Stealing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19361
&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20105;&#36777;&#31216;&#24403;&#21069;&#26041;&#26696;&#21487;&#33021;&#24050;&#32463;&#21487;&#20197;&#37096;&#32626;&#65292;&#25105;&#20204;&#35748;&#20026;&#27700;&#21360;&#31363;&#21462;&#65288;WS&#65289;&#26159;&#36825;&#20123;&#26041;&#26696;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#30340;API&#26469;&#36817;&#20284;&#36870;&#21521;&#27700;&#21360;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#29992;&#30340;&#27450;&#39575;&#25915;&#20987;&#65292;&#21516;&#26102;&#22823;&#24133;&#22686;&#21152;&#20102;&#20043;&#21069;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#25830;&#38500;&#25915;&#20987;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23558;&#20854;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#27450;&#39575;&#21644;&#25830;&#38500;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#38656;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#65292;&#25915;&#20987;&#32773;&#23601;&#33021;&#22815;&#27450;&#39575;&#24182;&#25830;&#38500;&#20043;&#21069;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#20851;&#20110;LLM&#27700;&#21360;&#25216;&#26415;&#30340;&#24120;&#35265;&#20449;&#24565;&#65292;&#24378;&#35843;&#20102;&#26356;&#21152;&#20581;&#22766;&#26041;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19361v1 Announce Type: cross  Abstract: LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We mak
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#35782;&#21035;&#21463;&#23475;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#20808;&#21069;&#30340;&#25915;&#20987;&#31867;&#22411;&#20998;&#31867;&#24037;&#20316;&#21644;&#24341;&#20837;&#26032;&#30340;&#26550;&#26500;&#65292;&#22312;&#25915;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;AUC&#65292;&#25915;&#20987;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;86.48%&#65292;&#21463;&#23475;&#27169;&#22411;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;72.28%&#12290;</title><link>https://arxiv.org/abs/2402.19355</link><description>&lt;p&gt;
&#25581;&#31034;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#23545;&#25239;&#26679;&#26412;--&#25915;&#20987;&#26816;&#27979;&#21644;&#21463;&#23475;&#27169;&#22411;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#35782;&#21035;&#21463;&#23475;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#20808;&#21069;&#30340;&#25915;&#20987;&#31867;&#22411;&#20998;&#31867;&#24037;&#20316;&#21644;&#24341;&#20837;&#26032;&#30340;&#26550;&#26500;&#65292;&#22312;&#25915;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;AUC&#65292;&#25915;&#20987;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;86.48%&#65292;&#21463;&#23475;&#27169;&#22411;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;72.28%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38024;&#23545;&#23427;&#20204;&#30340;&#23545;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#23384;&#22312;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21306;&#20998;&#33391;&#24615;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#25915;&#20987;&#31867;&#22411;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25193;&#23637;&#65292;&#25506;&#32034;&#26032;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21463;&#23475;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#38024;&#23545;&#21508;&#31181;&#21463;&#23475;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#25915;&#20987;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#25915;&#20987;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;0.982&#30340;AUC&#65292;&#22312;&#26410;&#30693;&#25915;&#20987;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#19981;&#36229;&#36807;0.03&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;LightResNet34&#26550;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#22312;&#20843;&#31181;&#25915;&#20987;&#31867;&#22411;&#19978;&#36798;&#21040;86.48%&#30340;&#25915;&#20987;&#20998;&#31867;&#20934;&#30830;&#29575;&#65288;&#38500;&#21435;&#33391;&#24615;&#26679;&#26412;&#65289;&#65292;&#32780;&#25105;&#20204;&#30340;&#21463;&#23475;&#27169;&#22411;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;72.28%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19355v1 Announce Type: cross  Abstract: Adversarial examples have proven to threaten speaker identification systems, and several countermeasures against them have been proposed. In this paper, we propose a method to detect the presence of adversarial examples, i.e., a binary classifier distinguishing between benign and adversarial examples. We build upon and extend previous work on attack type classification by exploring new architectures. Additionally, we introduce a method for identifying the victim model on which the adversarial attack is carried out. To achieve this, we generate a new dataset containing multiple attacks performed against various victim models. We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks. Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19348</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22478;&#24066;&#35745;&#31639;&#20013;&#30340;&#36328;&#22495;&#25968;&#25454;&#34701;&#21512;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#30340;&#19981;&#26029;&#34028;&#21187;&#21457;&#23637;&#65292;&#22478;&#24066;&#35745;&#31639;&#20316;&#20026;&#19968;&#38376;&#20851;&#38190;&#23398;&#31185;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#65288;&#22914;&#22320;&#29702;&#12289;&#20132;&#36890;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#29615;&#22659;&#25968;&#25454;&#65289;&#21644;&#27169;&#24577;&#65288;&#22914;&#26102;&#31354;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65289;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#21147;&#37327;&#65292;&#25104;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#19968;&#31181;&#21033;&#29992;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20221;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19987;&#38376;&#20026;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#35270;&#35282;&#65292;&#20197;&#29702;&#35299;&#27599;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#35770;&#20998;&#31867;&#20026;&#22235;&#22823;&#20027;&#35201;&#31867;&#21035;&#65306;&#22522;&#20110;&#29305;&#24449;&#12289;&#22522;&#20110;&#23545;&#40784;&#12289;&#22522;&#20110;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#34701;&#21512;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22810;&#27169;&#24577;&#22478;&#24066;&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19348v1 Announce Type: cross  Abstract: As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applicatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#20840;&#23616;&#40065;&#26834;&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#39564;&#35777;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.19322</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification of Neural Networks' Global Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#20840;&#23616;&#40065;&#26834;&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#39564;&#35777;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23637;&#31034;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#35768;&#22810;&#39564;&#35777;&#22120;&#26469;&#25512;&#29702;&#32473;&#23450;&#36755;&#20837;&#23545;&#32473;&#23450;&#25200;&#21160;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#23616;&#37096;&#40065;&#26834;&#24615;&#19981;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#12290;&#19968;&#20123;&#24037;&#20316;&#20998;&#26512;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#26080;&#27861;&#25552;&#20379;&#32593;&#32476;&#20998;&#31867;&#22120;&#22312;&#19981;&#25913;&#21464;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#31867;&#22120;&#30340;&#26032;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#23567;&#30340;&#20840;&#23616;&#31283;&#20581;&#36793;&#30028;&#65292;&#20854;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#29992;&#20110;&#20998;&#31867;&#22120;&#30340;&#27969;&#34892;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#21487;&#38543;&#26102;&#20351;&#29992;&#30340;&#39564;&#35777;&#22120;&#12290;VHAGaR&#20381;&#36182;&#20110;&#19977;&#20010;&#20027;&#35201;&#24605;&#24819;&#65306;&#23558;&#38382;&#39064;&#32534;&#30721;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#30001;&#20110;&#27599;&#20010;&#23616;&#37096;&#26356;&#25913;&#32780;&#20135;&#29983;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19322v1 Announce Type: new  Abstract: Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#38024;&#23545;5G&#32593;&#32476;&#20013;&#31227;&#21160;&#24615;&#39044;&#27979;&#30340;&#28508;&#22312;&#25915;&#20987;&#65292;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;1&#19975;&#21517;&#29992;&#25143;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#25163;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19319</link><description>&lt;p&gt;
&#23545;5G&#32593;&#32476;&#20013;&#30340;&#31227;&#21160;&#24615;&#39044;&#27979;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacks Against Mobility Prediction in 5G Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38024;&#23545;5G&#32593;&#32476;&#20013;&#31227;&#21160;&#24615;&#39044;&#27979;&#30340;&#28508;&#22312;&#25915;&#20987;&#65292;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;1&#19975;&#21517;&#29992;&#25143;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#25163;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;5&#20195;&#31227;&#21160;&#32593;&#32476;&#24341;&#20837;&#20102;&#26032;&#30340;&#32593;&#32476;&#21151;&#33021;&#65288;NF&#65289;&#65292;&#21363;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#21151;&#33021;&#65288;NWDAF&#65289;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#32593;&#32476;&#20869;&#30340;&#21508;&#31181;&#23454;&#20307;&#21644;5G&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#22806;&#37096;&#24212;&#29992;&#26381;&#21153;&#25552;&#20379;&#20808;&#36827;&#30340;&#20998;&#26512;&#26381;&#21153;&#12290; NWDAF&#30340;&#19968;&#20010;&#20851;&#38190;&#29992;&#20363;&#26159;&#31227;&#21160;&#36712;&#36857;&#39044;&#27979;&#65292;&#26088;&#22312;&#36890;&#36807;&#21450;&#26102;&#20998;&#37197;&#24517;&#35201;&#30340;&#32593;&#32476;&#36164;&#28304;&#26469;&#20934;&#30830;&#25903;&#25345;&#32593;&#32476;&#20013;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#30340;&#39640;&#25928;&#31227;&#21160;&#31649;&#29702;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#33021;&#25439;&#23475;&#36825;&#20123;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#31227;&#21160;&#24615;&#25915;&#20987;&#12290;&#22312;&#19968;&#20010;&#21322;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25317;&#26377;1&#19975;&#21517;&#29992;&#25143;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20855;&#22791;&#21163;&#25345;&#34562;&#31389;&#31227;&#21160;&#35774;&#22791;&#24182;&#20811;&#38534;&#23427;&#20204;&#33021;&#21147;&#30340;&#23545;&#25163;&#20165;&#20351;&#29992;100&#20010;&#24694;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19319v1 Announce Type: cross  Abstract: The $5^{th}$ generation of mobile networks introduces a new Network Function (NF) that was not present in previous generations, namely the Network Data Analytics Function (NWDAF). Its primary objective is to provide advanced analytics services to various entities within the network and also towards external application services in the 5G ecosystem. One of the key use cases of NWDAF is mobility trajectory prediction, which aims to accurately support efficient mobility management of User Equipment (UE) in the network by allocating ``just in time'' necessary network resources. In this paper, we show that there are potential mobility attacks that can compromise the accuracy of these predictions. In a semi-realistic scenario with 10,000 subscribers, we demonstrate that an adversary equipped with the ability to hijack cellular mobile devices and clone them can significantly reduce the prediction accuracy from 75\% to 40\% using just 100 adve
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21644;&#26631;&#35760;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28789;&#25935;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#31454;&#20105;&#21147;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.19308</link><description>&lt;p&gt;
&#26080;&#25439;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Loss-Free Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19308
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21644;&#26631;&#35760;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28789;&#25935;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#31454;&#20105;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21644;&#26631;&#35760;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#35201;&#27714;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#21024;&#38500;&#20449;&#24687;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#22312;&#35745;&#31639;&#19978;&#28040;&#32791;&#24040;&#22823;&#65292;&#24182;&#19988;&#38656;&#35201;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;&#20379;&#27169;&#22411;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20351;&#29992;&#12290;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#26469;&#33258;&#25439;&#22833;&#20989;&#25968;&#30340;Fisher&#20449;&#24687;&#65292;&#20294;&#36825;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#36825;&#21487;&#33021;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#23558;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#26367;&#25442;&#20026;&#27169;&#22411;&#36755;&#20986;l2&#33539;&#25968;&#30340;&#26799;&#24230;&#20197;&#36817;&#20284;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20351;&#29992;ResNet18&#21644;Vision Transformer&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19308v1 Announce Type: new  Abstract: We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#20219;&#20309;&#21487;&#23398;&#20064;&#30340;&#31867;&#20063;&#21487;&#20197;&#25112;&#30053;&#23398;&#20064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23436;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#30340;&#24773;&#20917;&#65292;&#23398;&#20064;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#21040;&#39044;&#22788;&#29702;&#30340;&#20449;&#24687;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.19303</link><description>&lt;p&gt;
&#25112;&#30053;&#20998;&#31867;&#23398;&#20064;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Learnability Gaps of Strategic Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19303
&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#21487;&#23398;&#20064;&#30340;&#31867;&#20063;&#21487;&#20197;&#25112;&#30053;&#23398;&#20064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23436;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#30340;&#24773;&#20917;&#65292;&#23398;&#20064;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#21040;&#39044;&#22788;&#29702;&#30340;&#20449;&#24687;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26631;&#20934;&#20998;&#31867;&#20219;&#21153;&#30456;&#21453;&#65292;&#25112;&#30053;&#20998;&#31867;&#28041;&#21450;&#20195;&#29702;&#20027;&#21160;&#35843;&#25972;&#20854;&#29305;&#24449;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#39044;&#27979;&#12290;&#22312;&#19968;&#20010;&#20915;&#23450;&#36151;&#27454;&#25209;&#20934;&#30340;&#20998;&#31867;&#22120;&#20013;&#65292;&#30003;&#35831;&#32773;&#21487;&#33021;&#20250;&#24320;&#21551;&#25110;&#20851;&#38381;&#20182;&#20204;&#30340;&#20449;&#29992;&#21345;&#20197;&#24858;&#24324;&#20998;&#31867;&#22120;&#12290;&#23398;&#20064;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#23545;&#25112;&#30053;&#35843;&#25972;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25112;&#30053;&#20998;&#31867;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#22522;&#20110;&#20160;&#20040;&#20449;&#24687;&#22312;&#20309;&#26102;&#21487;&#24471;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#35299;&#20915;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#25112;&#30053;&#20998;&#31867;&#21644;&#26631;&#20934;&#23398;&#20064;&#20043;&#38388;&#30340;&#23398;&#20064;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19303v1 Announce Type: new  Abstract: In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning.   We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\star$) is known and during training time the learner has access to both the pre-man
&lt;/p&gt;</description></item><item><title>RL-GPT &#26159;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#24930;&#36895;&#20195;&#29702;&#21644;&#24555;&#36895;&#20195;&#29702;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32534;&#30721;&#20219;&#21153;&#65292;&#22312;Minecraft&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19299</link><description>&lt;p&gt;
RL-GPT: &#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#20195;&#30721;&#20316;&#20026;&#31574;&#30053;&#36827;&#34892;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
RL-GPT: Integrating Reinforcement Learning and Code-as-policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19299
&lt;/p&gt;
&lt;p&gt;
RL-GPT &#26159;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#24930;&#36895;&#20195;&#29702;&#21644;&#24555;&#36895;&#20195;&#29702;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32534;&#30721;&#20219;&#21153;&#65292;&#22312;Minecraft&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#22312;&#21033;&#29992;&#32534;&#30721;&#26102;&#21508;&#31181;&#24037;&#20855;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#36923;&#36753;&#21644;&#31934;&#30830;&#25511;&#21046;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;&#39640;&#23618;&#35268;&#21010;&#36866;&#23452;&#20110;&#30452;&#25509;&#32534;&#30721;&#65292;&#32780;&#20302;&#23618;&#21160;&#20316;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#32454;&#21270;&#65292;&#27604;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20026;&#20102;&#26080;&#32541;&#25972;&#21512;&#36825;&#20004;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;RL-GPT&#65292;&#21253;&#25324;&#19968;&#20010;&#24930;&#36895;&#20195;&#29702;&#21644;&#19968;&#20010;&#24555;&#36895;&#20195;&#29702;&#12290;&#24930;&#36895;&#20195;&#29702;&#20998;&#26512;&#36866;&#21512;&#32534;&#30721;&#30340;&#21160;&#20316;&#65292;&#32780;&#24555;&#36895;&#20195;&#29702;&#25191;&#34892;&#32534;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#35299;&#26377;&#25928;&#22320;&#20351;&#27599;&#20010;&#20195;&#29702;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#20013;&#35777;&#26126;&#26159;&#38750;&#24120;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;GPT&#20195;&#29702;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;&#22312;Minecraft&#28216;&#25103;&#20013;&#65292;&#23427;&#22312;RTX3090&#19978;&#22312;&#19968;&#22825;&#20869;&#36805;&#36895;&#33719;&#24471;&#20102;&#38075;&#30707;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#25152;&#26377;&#35774;&#35745;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19299v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all desig
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#32959;&#30244;&#20813;&#30123;&#24494;&#29615;&#22659;&#25968;&#23383;&#35780;&#20998;&#25104;&#21151;&#39044;&#27979;&#39135;&#31649;&#32963;&#33146;&#30284;&#24739;&#32773;&#23545;&#32500;&#25345;&#24615;&#20813;&#30123;&#27835;&#30103;&#30340;&#30410;&#22788;&#65292;&#20855;&#26377;&#20020;&#24202;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.19296</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32959;&#30244;&#20813;&#30123;&#24494;&#29615;&#22659;&#25968;&#23383;&#35780;&#20998;&#21487;&#39044;&#27979;&#39135;&#31649;&#32963;&#33146;&#30284;&#26202;&#26399;&#20813;&#30123;&#27835;&#30103;&#32500;&#25345;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19296
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#32959;&#30244;&#20813;&#30123;&#24494;&#29615;&#22659;&#25968;&#23383;&#35780;&#20998;&#25104;&#21151;&#39044;&#27979;&#39135;&#31649;&#32963;&#33146;&#30284;&#24739;&#32773;&#23545;&#32500;&#25345;&#24615;&#20813;&#30123;&#27835;&#30103;&#30340;&#30410;&#22788;&#65292;&#20855;&#26377;&#20020;&#24202;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32963;&#30284;&#21644;&#39135;&#31649;&#30284;&#26159;&#23548;&#33268;&#20840;&#29699;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#39135;&#31649;&#30284;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;PDL1&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;&#65288;ICI&#65289;&#19982;&#21270;&#30103;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23384;&#27963;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#39135;&#31649;&#30284;&#30340;&#32959;&#30244;&#20813;&#30123;&#24494;&#29615;&#22659;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20174;&#25509;&#21463;&#31532;&#19968;&#32447;&#27679;&#34311;&#27688;&#37240;&#22028;&#21604;&#21644;&#38082;&#31867;&#21270;&#30103;&#30340;&#24179;&#21488;&#35797;&#39564;&#65288;NCT02678182&#65289;&#20013;&#30340;&#39135;&#31649;&#32963;&#33146;&#30284;&#65288;OGA&#65289;&#26202;&#26399;&#24739;&#32773;&#30340;&#22810;&#37325;&#20813;&#30123;&#33639;&#20809;&#65288;mIF&#65289;&#22270;&#20687;&#20837;&#25163;&#65292;&#26088;&#22312;&#39044;&#27979;&#35813;&#27835;&#30103;&#30340;&#30103;&#25928;&#24182;&#25506;&#32034;&#23545;&#32500;&#25345;&#24615;&#36798;&#20240;&#21333;&#25239;&#65288;PDL1&#25233;&#21046;&#21058;&#65289;&#26377;&#25928;&#30340;&#24739;&#32773;&#30340;&#29983;&#29289;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26631;&#35760;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#20102;&#21453;&#24212;&#32773;&#21644;&#38750;&#21453;&#24212;&#32773;&#65288;p &lt;0.05&#65289;&#65292;&#20197;&#21450;&#37027;&#20123;&#21487;&#33021;&#20174;ICI&#20013;&#33719;&#30410;&#30340;&#24739;&#32773;&#65288;p &lt;0.05)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19296v1 Announce Type: cross  Abstract: Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival. However, our understanding of the tumour immune microenvironment in OG cancers remains limited. In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p &lt; 0.05) as well as those who could potentially benefit from ICI with statistical significance (p &lt; 0.05) fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28023;&#19978;&#39118;&#21147;&#28065;&#36718;&#32467;&#26500;&#20351;&#29992;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#25512;&#26029;&#22303;&#22756;&#21018;&#24230;&#20998;&#24067;&#24182;&#23454;&#29616;&#20914;&#21047;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.19295</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#32423;&#36125;&#21494;&#26031;&#24314;&#27169;&#22312;&#28023;&#19978;&#39118;&#21147;&#28065;&#36718;&#32467;&#26500;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28023;&#19978;&#39118;&#21147;&#28065;&#36718;&#32467;&#26500;&#20351;&#29992;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#25512;&#26029;&#22303;&#22756;&#21018;&#24230;&#20998;&#24067;&#24182;&#23454;&#29616;&#20914;&#21047;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;PBSHM&#65289;&#26088;&#22312;&#22312;&#32676;&#20307;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#12290;&#19968;&#20010;&#28023;&#19978;&#39118;&#30005;&#22330;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21517;&#20041;&#19978;&#30456;&#21516;&#30340;&#39118;&#21147;&#28065;&#36718;&#32467;&#26500;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#25104;&#21592;&#20043;&#38388;&#23384;&#22312;&#30528;&#33391;&#24615;&#21464;&#21270;&#65292;&#27604;&#22914;&#20960;&#20309;&#24418;&#29366;&#65292;&#28023;&#24202;&#26465;&#20214;&#21644;&#28201;&#24046;&#12290;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#32467;&#26500;&#29305;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21160;&#24577;&#21709;&#24212;&#65292;&#20351;&#24471;&#36890;&#36807;&#20256;&#32479;SHM&#25216;&#26415;&#26356;&#38590;&#20197;&#26816;&#27979;&#32467;&#26500;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#25512;&#26029;&#20154;&#21475;&#21644;&#26412;&#22320;&#27700;&#24179;&#39044;&#26399;&#22303;&#22756;&#21018;&#24230;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#65292;&#20316;&#20026;&#36827;&#34892;&#26032;&#26087;&#28065;&#36718;&#32467;&#26500;&#24322;&#24120;&#26816;&#27979;&#65288;&#22914;&#20914;&#21047;&#65289;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#23558;&#29983;&#25104;&#31867;&#20284;&#20110;&#23567;&#22411;&#39118;&#21147;&#28065;&#36718;&#32676;&#20307;&#30340;&#33258;&#28982;&#39057;&#29575;&#30340;&#35266;&#27979;&#12290;&#20010;&#20307;&#35266;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#23558;&#36890;&#36807;&#20551;&#35774;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19295v1 Announce Type: new  Abstract: Population-based structural health monitoring (PBSHM), aims to share information between members of a population. An offshore wind (OW) farm could be considered as a population of nominally-identical wind-turbine structures. However, benign variations exist among members, such as geometry, sea-bed conditions and temperature differences. These factors could influence structural properties and therefore the dynamic response, making it more difficult to detect structural problems via traditional SHM techniques. This paper explores the use of a hierarchical Bayesian model to infer expected soil stiffness distributions at both population and local levels, as a basis to perform anomaly detection, in the form of scour, for new and existing turbines. To do this, observations of natural frequency will be generated as though they are from a small population of wind turbines. Differences between individual observations will be introduced by postula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UMAP&#25216;&#26415;&#30340;&#26032;&#22411;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#31995;&#32479;&#20013;&#22810;&#31181;&#25925;&#38556;&#27169;&#24335;&#23548;&#33268;&#30340;&#19981;&#21516;&#38477;&#35299;&#36335;&#24452;&#65292;&#25552;&#39640;&#25925;&#38556;&#27169;&#24335;&#30340;&#20934;&#30830;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19294</link><description>&lt;p&gt;
&#26410;&#30693;&#25925;&#38556;&#27169;&#24335;&#19979;&#30340;&#38477;&#35299;&#24314;&#27169;&#19982;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UMAP&#25216;&#26415;&#30340;&#26032;&#22411;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#31995;&#32479;&#20013;&#22810;&#31181;&#25925;&#38556;&#27169;&#24335;&#23548;&#33268;&#30340;&#19981;&#21516;&#38477;&#35299;&#36335;&#24452;&#65292;&#25552;&#39640;&#25925;&#38556;&#27169;&#24335;&#30340;&#20934;&#30830;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#21333;&#20803;&#32463;&#24120;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#32463;&#21382;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#65292;&#23548;&#33268;&#19981;&#21516;&#30340;&#38477;&#35299;&#36335;&#24452;&#12290;&#20381;&#36182;&#20110;&#22312;&#21333;&#19968;&#25925;&#38556;&#27169;&#24335;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36328;&#22810;&#20010;&#25925;&#38556;&#27169;&#24335;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#35782;&#21035;&#25925;&#38556;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#22312;&#38477;&#35299;&#36807;&#31243;&#20013;&#24573;&#30053;&#25925;&#38556;&#27169;&#24335;&#65292;&#35201;&#20040;&#20551;&#23450;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#26631;&#31614;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#36825;&#20123;&#26631;&#31614;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#20449;&#21495;&#30340;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#20851;&#31995;&#20351;&#24471;&#20934;&#30830;&#35782;&#21035;&#25925;&#38556;&#27169;&#24335;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;UMAP&#65288;Uniform Manifold Approximation and Projection&#65289;&#30340;&#38477;&#32500;&#25216;&#26415;&#23558;&#27599;&#20010;&#21333;&#20803;&#30340;&#38477;&#35299;&#36712;&#36857;&#25237;&#24433;&#21644;&#21487;&#35270;&#21270;&#21040;&#36739;&#20302;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#38477;&#35299;&#36712;&#36857;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19294v1 Announce Type: cross  Abstract: Operating units often experience various failure modes in complex systems, leading to distinct degradation paths. Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes. Therefore, accurately identifying the failure mode is of critical importance. Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice. Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately. To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit's degradation trajectory into a lower dimension. Then, using these degradation trajectories, we develop 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#65292;&#28040;&#38500;&#20449;&#21495;&#30340;TF&#25928;&#24212;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#31639;&#27861;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.19290</link><description>&lt;p&gt;
&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#20272;&#35745;&#21644;&#21435;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Estimation and Deconvolution of Second Order Cyclostationary Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#65292;&#28040;&#38500;&#20449;&#21495;&#30340;TF&#25928;&#24212;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#31639;&#27861;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#36890;&#36807;&#20256;&#36755;&#20989;&#25968;(TF)&#20256;&#36755;&#21040;&#20256;&#24863;&#22120;&#30340;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;(CS2)&#20449;&#21495;&#30340;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#65292;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#24182;&#28040;&#38500;&#20102;&#32479;&#35745;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#19978;&#30340;TF&#25928;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#30450;&#30446;&#30340;&#65292;&#21363;&#19981;&#38656;&#35201;&#20851;&#20110;&#20449;&#21495;&#25110;TF&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#27169;&#25311;&#34920;&#26126;&#65292;&#31639;&#27861;&#22312;&#21508;&#31181;&#20449;&#21495;&#31867;&#22411;&#12289;TF&#21644;&#20449;&#22122;&#27604;(SNR)&#19979;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;CS2&#20449;&#21495;&#26063;&#34987;&#38480;&#21046;&#20026;&#30830;&#23450;&#24615;&#21608;&#26399;&#20989;&#25968;&#21644;&#30333;&#22122;&#22768;&#30340;&#20056;&#31215;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#38656;&#35201;&#32858;&#21512;&#26469;&#33258;&#30456;&#21516;&#31995;&#32479;&#20294;&#20855;&#26377;&#19981;&#21516;TF&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19290v1 Announce Type: new  Abstract: This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor. We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time. This method is blind, meaning it does not require prior knowledge about the signals or TF. Simulations demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs). In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise. Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required.
&lt;/p&gt;</description></item><item><title>StiefelGen &#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19287</link><description>&lt;p&gt;
StiefelGen: &#19968;&#31181;&#31616;&#21333;&#30340;&#65292;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#36890;&#36807;&#40654;&#26364;&#27969;&#24418;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19287
&lt;/p&gt;
&lt;p&gt;
StiefelGen &#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#37117;&#24471;&#21040;&#20102;&#31215;&#26497;&#30340;&#21457;&#23637;&#65292;&#27604;&#22914;&#22522;&#20110;&#22270;&#20687;&#30340;&#23398;&#20064;&#27169;&#22411;&#12289;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#21450;&#23545;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#36890;&#29992;&#30340;&#22122;&#22768;&#27880;&#20837;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#65292;&#20173;&#28982;&#26377;&#24456;&#22810;&#38382;&#39064;&#26377;&#24453;&#35299;&#20915;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#20026;&#36825;&#20123;&#27169;&#22411;&#24320;&#21457;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#36328;&#36234;&#12290;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#30340;&#19977;&#31181;&#24120;&#35265;&#26041;&#27861;&#21253;&#25324;&#65306;(i) &#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#31995;&#25968;&#31354;&#38388;&#20013;&#36171;&#20104;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65289;&#65292;(ii) &#21521;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#38598;&#28155;&#21152;&#22122;&#22768;&#65292;&#20197;&#21450;(iii) &#20174;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#31283;&#20581;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#24037;&#19994;&#20013;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#65306;(i) &#36890;&#24120;&#24182;&#27809;&#26377;&#19968;&#20010;&#31283;&#20581;&#30340;&#29289;&#29702;&#27169;&#22411;&#21487;&#20379;&#20351;&#29992;&#65292;(ii)...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19287v1 Announce Type: new  Abstract: Data augmentation is an area of research which has seen active development in many machine learning fields, such as in image-based learning models, reinforcement learning for self driving vehicles, and general noise injection for point cloud data. However, convincing methods for general time series data augmentation still leaves much to be desired, especially since the methods developed for these models do not readily cross-over. Three common approaches for time series data augmentation include: (i) Constructing a physics-based model and then imbuing uncertainty over the coefficient space (for example), (ii) Adding noise to the observed data set(s), and, (iii) Having access to ample amounts of time series data sets from which a robust generative neural network model can be trained. However, for many practical problems that work with time series data in the industry: (i) One usually does not have access to a robust physical model, (ii) Th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23494;&#38598;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27979;&#35797;&#29615;&#22659;&#65292;&#21253;&#21547;&#22810;&#20010;&#20195;&#29702;&#27169;&#22411;&#24182;&#20248;&#21270;&#20854;&#32452;&#21512;&#31995;&#25968;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19275</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#24378;&#21270;&#23398;&#20064;&#30340;&#20114;&#32852;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33258;&#36866;&#24212;&#27979;&#35797;&#29615;&#22659;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23494;&#38598;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27979;&#35797;&#29615;&#22659;&#65292;&#21253;&#21547;&#22810;&#20010;&#20195;&#29702;&#27169;&#22411;&#24182;&#20248;&#21270;&#20854;&#32452;&#21512;&#31995;&#25968;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#33021;&#35780;&#20272;&#22312;&#20114;&#32852;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21457;&#23637;&#21644;&#37096;&#32626;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#23545;CAVs&#30340;&#20808;&#21069;&#30693;&#35782;&#65288;&#22914;&#20195;&#29702;&#27169;&#22411;&#65289;&#35774;&#35745;&#27979;&#35797;&#22330;&#26223;&#65292; &#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#28982;&#21518;&#35780;&#20272;CAVs&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;CAVs&#21644;&#20808;&#21069;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#35780;&#20272;&#25928;&#29575;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;CAV&#27979;&#35797;&#36807;&#31243;&#20013;&#27979;&#35797;&#22330;&#26223;&#30340;&#33258;&#36866;&#24212;&#35774;&#35745;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27979;&#35797;&#29615;&#22659;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#20195;&#29702;&#27169;&#22411;&#24182;&#20248;&#21270;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#30340;&#32452;&#21512;&#31995;&#25968;&#26469;&#22686;&#24378;&#35780;&#20272;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#35780;&#20272;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19275v1 Announce Type: cross  Abstract: The assessment of safety performance plays a pivotal role in the development and deployment of connected and automated vehicles (CAVs). A common approach involves designing testing scenarios based on prior knowledge of CAVs (e.g., surrogate models), conducting tests in these scenarios, and subsequently evaluating CAVs' safety performances. However, substantial differences between CAVs and the prior knowledge can significantly diminish the evaluation efficiency. In response to this issue, existing studies predominantly concentrate on the adaptive design of testing scenarios during the CAV testing process. Yet, these methods have limitations in their applicability to high-dimensional scenarios. To overcome this challenge, we develop an adaptive testing environment that bolsters evaluation robustness by incorporating multiple surrogate models and optimizing the combination coefficients of these surrogate models to enhance evaluation effic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#20174;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#65292;&#20197;&#25351;&#23548;&#25919;&#31574;&#36873;&#25321;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19265</link><description>&lt;p&gt;
&#22312;POMDPs&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#20197;&#25351;&#23548;&#25919;&#31574;&#65306;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#20174;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#65292;&#20197;&#25351;&#23548;&#25919;&#31574;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#35268;&#21010;&#26694;&#26550;&#65292;&#20801;&#35768;&#23558;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#20026;&#20449;&#24565;&#27010;&#29575;&#20998;&#24067;&#12290;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#30340;&#36817;&#20284;&#27714;&#35299;&#22120;&#26174;&#31034;&#20986;&#24456;&#22823;&#25104;&#21151;&#65292;&#20197;&#25918;&#23485;&#35745;&#31639;&#38656;&#27714;&#24182;&#25191;&#34892;&#22312;&#32447;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#21040;&#20855;&#26377;&#35768;&#22810;&#21160;&#20316;&#21644;&#38271;&#26399;&#35268;&#21010;&#35270;&#37326;&#30340;&#22797;&#26434;&#29616;&#23454;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#30340;&#20851;&#38190;&#28857;&#26159;&#36890;&#36807;&#23450;&#21046;&#29305;&#23450;&#24212;&#29992;&#22495;&#30340;&#39046;&#22495;&#30456;&#20851;&#31574;&#30053;&#21551;&#21457;&#26469;&#24341;&#23548;&#34892;&#21160;&#36873;&#25321;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#30001;&#20219;&#20309;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#23558;&#20449;&#24565;-&#21160;&#20316;&#23545;&#36716;&#25442;&#20026;&#36923;&#36753;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20449;&#24565;&#30340;&#31574;&#30053;&#35268;&#33539;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#22312;&#32447;&#21551;&#21457;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19265v1 Announce Type: new  Abstract: Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty. They allow to model state uncertainty as a belief probability distribution. Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning. However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain. We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver. We convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristi
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29575;&#22238;&#28335;&#65288;LRR&#65289;&#36890;&#36807;&#26089;&#26399;&#32763;&#36716;&#21442;&#25968;&#31526;&#21495;&#19988;&#23545;&#31526;&#21495;&#25200;&#21160;&#20445;&#25345;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#25513;&#27169;&#35782;&#21035;&#26041;&#38754;&#26356;&#26377;&#25928;&#65292;&#32780;&#19988;&#21487;&#20197;&#20248;&#21270;&#21508;&#31181;&#25513;&#27169;&#65292;&#21253;&#25324;&#38543;&#26426;&#25513;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.19262</link><description>&lt;p&gt;
&#21475;&#32617;&#12289;&#26631;&#24535;&#21644;&#23398;&#20064;&#29575;&#22238;&#28335;
&lt;/p&gt;
&lt;p&gt;
Masks, Signs, And Learning Rate Rewinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19262
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#22238;&#28335;&#65288;LRR&#65289;&#36890;&#36807;&#26089;&#26399;&#32763;&#36716;&#21442;&#25968;&#31526;&#21495;&#19988;&#23545;&#31526;&#21495;&#25200;&#21160;&#20445;&#25345;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#25513;&#27169;&#35782;&#21035;&#26041;&#38754;&#26356;&#26377;&#25928;&#65292;&#32780;&#19988;&#21487;&#20197;&#20248;&#21270;&#21508;&#31181;&#25513;&#27169;&#65292;&#21253;&#25324;&#38543;&#26426;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#22238;&#28335;&#65288;LRR&#65289;&#24050;&#34987;&#30830;&#23450;&#20026;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23547;&#25214;&#8220;&#20013;&#22870;&#21048;&#8221;&#30340;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#24378;&#21464;&#20307;&#12290;&#34429;&#28982;&#20004;&#31181;&#36845;&#20195;&#21098;&#26525;&#26041;&#26696;&#37117;&#23558;&#32467;&#26500;&#21644;&#21442;&#25968;&#23398;&#20064;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#20294;&#29702;&#35299;LRR&#22312;&#20004;&#20010;&#26041;&#38754;&#30340;&#20248;&#21183;&#22914;&#20309;&#21487;&#20197;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#35774;&#35745;&#26356;&#28789;&#27963;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#20248;&#21270;&#21508;&#31181;&#31232;&#30095;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35299;&#24320;&#20102;&#25513;&#27169;&#23398;&#20064;&#21644;&#21442;&#25968;&#20248;&#21270;&#30340;&#25928;&#26524;&#20197;&#21450;&#20004;&#32773;&#22914;&#20309;&#20174;&#36229;&#21442;&#25968;&#21270;&#20013;&#21463;&#30410;&#30340;&#26041;&#24335;&#12290;LRR&#26089;&#26399;&#32763;&#36716;&#21442;&#25968;&#31526;&#21495;&#24182;&#20445;&#25345;&#23545;&#31526;&#21495;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#20284;&#20046;&#20351;&#20854;&#22312;&#19981;&#20165;&#22312;&#25513;&#27169;&#35782;&#21035;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#20248;&#21270;&#21508;&#31181;&#25513;&#27169;&#65292;&#21253;&#25324;&#38543;&#26426;&#25513;&#27169;&#26041;&#38754;&#20063;&#26356;&#21152;&#26377;&#25928;&#12290;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#21333;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#35774;&#32622;&#20013;&#35777;&#26126;LRR&#25104;&#21151;&#30340;&#24773;&#20917;&#27604;IMP&#26356;&#22810;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25670;&#33073;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19262v1 Announce Type: new  Abstract: Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape i
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#27169;&#20056;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#65292;&#20026;&#22522;&#20110;&#21152;&#23494;&#31995;&#32479;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#35777;&#25454;</title><link>https://arxiv.org/abs/2402.19254</link><description>&lt;p&gt;
&#29992;&#20110;&#27169;&#20056;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for modular multiplication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19254
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#27169;&#20056;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#65292;&#20026;&#22522;&#20110;&#21152;&#23494;&#31995;&#32479;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23494;&#30721;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#20056;&#65306;&#24490;&#29615;&#22238;&#24402;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#32467;&#26524;&#20013;&#23637;&#31034;&#30340;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#38480;&#25104;&#21151;&#35777;&#26126;&#20102;&#19982;&#22522;&#20110;&#21152;&#23494;&#31995;&#32479;&#30340;&#27169;&#20056;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19254v1 Announce Type: new  Abstract: Motivated by cryptographic applications, we investigate two machine learning approaches to modular multiplication: namely circular regression and a sequence-to-sequence transformer model. The limited success of both methods demonstrated in our results gives evidence for the hardness of tasks involving modular multiplication upon which cryptosystems are based.
&lt;/p&gt;</description></item><item><title>DE-DeepONet&#36890;&#36807;&#25972;&#21512;&#23548;&#25968;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#27604;&#20256;&#32479;DeepONet&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19242</link><description>&lt;p&gt;
&#28145;&#24230;&#23548;&#25968;&#22686;&#24378;&#30340;&#25805;&#20316;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Derivative-enhanced Deep Operator Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19242
&lt;/p&gt;
&lt;p&gt;
DE-DeepONet&#36890;&#36807;&#25972;&#21512;&#23548;&#25968;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#27604;&#20256;&#32479;DeepONet&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DeepONets&#65289;&#26159;&#19968;&#31867;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#26368;&#36817;&#34987;&#24320;&#21457;&#20026;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#22686;&#24378;&#30340;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DE-DeepONet&#65289;&#65292;&#23427;&#21033;&#29992;&#23548;&#25968;&#20449;&#24687;&#22686;&#24378;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#23548;&#25968;&#36817;&#20284;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;DE-DeepONet&#23558;&#36755;&#20837;&#30340;&#32500;&#24230;&#32553;&#20943;&#21040;DeepONet&#20013;&#65292;&#24182;&#22312;&#35757;&#32451;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#21253;&#21547;&#20004;&#31181;&#31867;&#22411;&#30340;&#23548;&#25968;&#26631;&#31614;&#65292;&#21363;&#20851;&#20110;&#36755;&#20837;&#20989;&#25968;&#30340;&#36755;&#20986;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#21644;&#20851;&#20110;&#29289;&#29702;&#22495;&#21464;&#37327;&#30340;&#36755;&#20986;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24230;&#30340;&#26041;&#31243;&#19978;&#27979;&#35797;DE-DeepONet&#65292;&#20197;&#23637;&#31034;&#20854;&#30456;&#23545;&#20110;&#26222;&#36890;DeepONet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19242v1 Announce Type: new  Abstract: Deep operator networks (DeepONets), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages the derivative information to enhance the prediction accuracy, and provide a more accurate approximation of the derivatives, especially when the training data are limited. DE-DeepONet incorporates dimension reduction of input into DeepONet and includes two types of derivative labels in the loss function for training, that is, the directional derivatives of the output function with respect to the input function and the gradient of the output function with respect to the physical domain variables. We test DE-DeepONet on three different equations with increasing complexity to demonstrate its effectiveness compared to the vanilla DeepON
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.19232</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#23436;&#20840;&#25581;&#31034;&#24744;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Trained Random Forests Completely Reveal your Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19232
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#33021;&#22815;&#23436;&#20840;&#25110;&#20960;&#20046;&#23436;&#20840;&#37325;&#24314;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#26862;&#26519;&#30340;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#24120;&#29992;&#24211;&#65288;&#22914;scikit-learn&#65289;&#20013;&#38543;&#22788;&#21487;&#24471;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#20284;&#28982;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#21487;&#20197;&#21033;&#29992;&#32422;&#26463;&#32534;&#31243;&#22312;&#35268;&#27169;&#19978;&#35299;&#20915; &#8212;&#8212; &#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#20256;&#25773;&#21644;&#35299;&#22495;&#32553;&#20943;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#38543;&#26426;&#26862;&#26519;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#12290;&#21363;&#20351;&#20351;&#29992;&#23569;&#37327;&#26641;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;&#21363;&#20351;&#36890;&#36807;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#19968;&#31181;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19232v1 Announce Type: new  Abstract: We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming -- an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19226</link><description>&lt;p&gt;
&#25506;&#31350;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#24930;&#24615;&#30140;&#30171;&#20010;&#24615;&#21270;&#25252;&#29702;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#33218;&#26694;&#26550;&#65292;&#20351;&#29992;LinUCB&#31639;&#27861;&#23545;&#21253;&#21547;164&#20301;&#24739;&#32773;&#22312;&#27599;&#20010;10&#27425;&#20250;&#35805;&#20013;&#30340;&#20114;&#21160;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#21046;&#23450;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#31639;&#27861;&#21442;&#25968;&#36827;&#34892;&#35843;&#25972;&#20250;&#24433;&#21709;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#22312;&#19981;&#21516;&#24615;&#21035;&#38388;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#30340;&#30140;&#30171;&#27979;&#37327;&#20540;&#65292;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19226v1 Announce Type: new  Abstract: This study investigates gender fairness in personalized pain care recommendations using machine learning algorithms. Leveraging a contextual bandits framework, personalized recommendations are formulated and evaluated using LinUCB algorithm on a dataset comprising interactions with $164$ patients across $10$ sessions each. Results indicate that while adjustments to algorithm parameters influence the quality of pain care recommendations, this impact remains consistent across genders. However, when certain patient information, such as self-reported pain measurements, is absent, the quality of pain care recommendations for women is notably inferior to that for men.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.19212</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning: A Convex Optimization Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38598;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#27599;&#20010;&#38598;&#20013;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#25214;&#21040;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;&#20984;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#27599;&#20010;&#38598;&#21512;&#20013;&#35745;&#31639;&#30340;&#26435;&#37325;&#26159;&#26368;&#20248;&#30340;&#65292;&#20851;&#20110;&#24403;&#21069;&#38598;&#21512;&#30340;&#37319;&#26679;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#23545;&#20110;&#31283;&#23450;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#65292;&#24182;&#19988;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#19982;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26080;&#38480;&#25509;&#36817;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#27491;&#21017;&#21270;&#21442;&#25968;&#20026;$\rho$&#65292;&#26102;&#38388;&#38271;&#24230;&#20026;$T$&#65292;&#37027;&#20040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25910;&#25947;&#21040;$w$&#65292;&#20854;&#20013;$w$&#19982;&#26368;&#20248;&#21442;&#25968;$w^\star$&#20043;&#38388;&#30340;&#36317;&#31163;&#21463;&#21040;$\mathcal{O}(\rho T^{-1})$&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
&lt;/p&gt;</description></item><item><title>FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19197</link><description>&lt;p&gt;
&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19197
&lt;/p&gt;
&lt;p&gt;
FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#32032;&#23545;&#40784;&#30340;&#38544;&#24335;&#27169;&#22411;&#65292;&#22914;PIFu&#12289;PIFuHD&#21644;ICON&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#30528;&#35013;&#20154;&#20307;&#37325;&#24314;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#34180;&#34920;&#38754;&#65288;&#22914;&#32819;&#26421;&#12289;&#25163;&#25351;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#37325;&#24314;&#32593;&#26684;&#20013;&#30340;&#22122;&#22768;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#65288;FSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#35757;&#32451;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#12290;FSS&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;FSS&#26174;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#65292;FSS&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#27861;&#32447;&#21464;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.19186</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#24320;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangling representations of retinal images with generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19186
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#22312;&#26089;&#26399;&#26816;&#27979;&#30524;&#37096;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#29978;&#33267;&#34920;&#26126;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#22270;&#20687;&#36824;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24515;&#34880;&#31649;&#39118;&#38505;&#22240;&#32032;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#21463;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#21487;&#33021;&#23545;&#30524;&#31185;&#39046;&#22495;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22823;&#22411;&#24213;&#22270;&#38431;&#21015;&#24448;&#24448;&#21463;&#21040;&#30456;&#26426;&#31867;&#22411;&#12289;&#22270;&#20687;&#36136;&#37327;&#25110;&#29031;&#26126;&#27700;&#24179;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#22240;&#26524;&#20851;&#31995;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26032;&#39062;&#35299;&#24320;&#25439;&#22833;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
&lt;/p&gt;</description></item><item><title>FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.19163</link><description>&lt;p&gt;
FedStruct&#65306;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#22312;&#20114;&#32852;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
FedStruct: Federated Decoupled Learning over Interconnected Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19163
&lt;/p&gt;
&lt;p&gt;
FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#20114;&#32852;&#23376;&#22270;&#30340;&#26222;&#36941;&#24773;&#20917;&#65292;&#20854;&#20013;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#20114;&#36830;&#25509;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21517;&#20026;FedStruct&#65292;&#23427;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#32500;&#25252;&#38544;&#31169;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FedStruct&#28040;&#38500;&#20102;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#25110;&#29983;&#25104;&#25935;&#24863;&#33410;&#28857;&#29305;&#24449;&#25110;&#23884;&#20837;&#30340;&#24517;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#23427;&#21033;&#29992;&#26174;&#24335;&#20840;&#23616;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;FedStruct&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65288;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#12289;&#19981;&#21516;&#26631;&#31614;&#21487;&#29992;&#24615;&#20197;&#21450;&#23458;&#25143;&#20010;&#25968;&#30340;&#65289;&#25509;&#36817;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19163v1 Announce Type: new  Abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of cl
&lt;/p&gt;</description></item><item><title>bGPT&#27169;&#22411;&#21033;&#29992;next byte prediction&#25104;&#21151;&#27169;&#25311;&#24182;&#39044;&#27979;&#25968;&#23383;&#19990;&#30028;&#30340;&#21508;&#31181;&#25805;&#20316;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22312;&#38899;&#20048;&#25968;&#25454;&#36716;&#25442;&#21644;CPU&#34892;&#20026;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.19155</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#65306;&#23383;&#33410;&#27169;&#22411;&#26159;&#25968;&#23383;&#19990;&#30028;&#30340;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Beyond Language Models: Byte Models are Digital World Simulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19155
&lt;/p&gt;
&lt;p&gt;
bGPT&#27169;&#22411;&#21033;&#29992;next byte prediction&#25104;&#21151;&#27169;&#25311;&#24182;&#39044;&#27979;&#25968;&#23383;&#19990;&#30028;&#30340;&#21508;&#31181;&#25805;&#20316;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22312;&#38899;&#20048;&#25968;&#25454;&#36716;&#25442;&#21644;CPU&#34892;&#20026;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#24120;&#24120;&#24573;&#30053;&#23383;&#33410;&#65292;&#21363;&#25968;&#23383;&#19990;&#30028;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#25152;&#26377;&#24418;&#24335;&#30340;&#20449;&#24687;&#21644;&#25805;&#20316;&#37117;&#26159;&#20197;&#20108;&#36827;&#21046;&#26684;&#24335;&#32534;&#30721;&#21644;&#25805;&#20316;&#30340;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;bGPT&#65292;&#19968;&#20010;&#20855;&#26377;&#19979;&#19968;&#20010;&#23383;&#33410;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#25968;&#23383;&#19990;&#30028;&#12290;bGPT&#22312;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#22270;&#20687;&#65289;&#19978;&#30340;&#24615;&#33021;&#19982;&#19987;&#19994;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#20026;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#35786;&#26029;&#31639;&#27861;&#25110;&#30828;&#20214;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#20960;&#20046;&#23436;&#32654;&#22320;&#22797;&#21046;&#20102;&#23558;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#36716;&#25442;&#30340;&#36807;&#31243;&#65292;&#22312;&#23558;ABC&#35760;&#21495;&#36716;&#25442;&#20026;MIDI&#26684;&#24335;&#26102;&#65292;&#38169;&#35823;&#29575;&#20165;&#20026;0.0011&#27604;&#29305;&#27599;&#23383;&#33410;&#12290;&#27492;&#22806;&#65292;bGPT&#22312;&#27169;&#25311;CPU&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;99.99%&#12290;&#21033;&#29992;&#19979;&#19968;&#20010;&#23383;&#33410;&#39044;&#27979;&#65292;&#20687;bGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;l
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19155v1 Announce Type: new  Abstract: Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations. Leveraging next byte prediction, models like bGPT can directly l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ProtoP-OD&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19142</link><description>&lt;p&gt;
ProtoP-OD: &#20855;&#26377;&#20856;&#22411;&#37096;&#20998;&#30340;&#21487;&#35299;&#37322;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ProtoP-OD: Explainable Object Detection with Prototypical Parts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19142
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ProtoP-OD&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#26816;&#27979;&#21464;&#22411;&#22120;&#30340;&#34892;&#20026;&#24448;&#24448;&#20250;&#31361;&#20986;&#27169;&#22411;&#20851;&#27880;&#30340;&#22270;&#20687;&#20301;&#32622;&#65292;&#20294;&#23545;&#27169;&#22411;&#20851;&#27880;&#30340;\emph{&#35821;&#20041;}&#25552;&#20379;&#26377;&#38480;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#26816;&#27979;&#21464;&#21387;&#22120;&#65292;&#23427;&#26500;&#24314;&#20856;&#22411;&#30340;&#23616;&#37096;&#29305;&#24449;&#24182;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20856;&#22411;&#37096;&#20998;&#30340;&#36825;&#20123;&#33258;&#23450;&#20041;&#29305;&#24449;&#26088;&#22312;&#24444;&#27492;&#20114;&#26021;&#24182;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#19968;&#33268;&#12290;&#25152;&#25552;&#20986;&#30340;&#25193;&#23637;&#21253;&#25324;&#19968;&#20010;&#29942;&#39048;&#27169;&#22359;&#65292;&#21407;&#22411;&#39048;&#65292;&#23427;&#35745;&#31639;&#21407;&#22411;&#28608;&#27963;&#30340;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#39033;&#65292;&#23427;&#23558;&#21407;&#22411;&#19982;&#23545;&#35937;&#31867;&#21305;&#37197;&#12290;&#36825;&#19968;&#35774;&#32622;&#23548;&#33268;&#20102;&#21407;&#22411;&#39048;&#20013;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#20801;&#35768;&#23545;&#27169;&#22411;&#24863;&#30693;&#30340;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#65292;&#24182;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19142v1 Announce Type: cross  Abstract: Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \emph{semantics} that the model is focusing on. This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection. These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes. This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability. We show experimentally
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#36135;&#24066;&#22330;&#21644;&#23481;&#28798;&#39057;&#29575;&#25511;&#21046;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#36827;&#34892;&#20986;&#20215;&#30340;&#26032;&#39062;BESS&#32852;&#21512;&#31454;&#26631;&#31574;&#30053;&#65292;&#36890;&#36807;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#22120;&#26377;&#25928;&#21709;&#24212;&#22810;&#20010;&#24066;&#22330;&#30340;&#20215;&#26684;&#27874;&#21160;&#65292;&#21457;&#23637;&#20986;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;DRL&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.19110</link><description>&lt;p&gt;
&#33021;&#28304;&#23384;&#20648;&#20986;&#20215;&#20013;&#30340;&#26102;&#24207;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#21644;&#22791;&#29992;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19110
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#36135;&#24066;&#22330;&#21644;&#23481;&#28798;&#39057;&#29575;&#25511;&#21046;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#36827;&#34892;&#20986;&#20215;&#30340;&#26032;&#39062;BESS&#32852;&#21512;&#31454;&#26631;&#31574;&#30053;&#65292;&#36890;&#36807;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#22120;&#26377;&#25928;&#21709;&#24212;&#22810;&#20010;&#24066;&#22330;&#30340;&#20215;&#26684;&#27874;&#21160;&#65292;&#21457;&#23637;&#20986;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;DRL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#65288;BESS&#65289;&#36890;&#36807;&#21442;&#19982;&#30005;&#21147;&#24066;&#22330;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;BESS&#24120;&#24120;&#36890;&#36807;&#21442;&#19982;&#22810;&#20010;&#24066;&#22330;&#26469;&#23547;&#27714;&#21508;&#31181;&#25910;&#20837;&#26469;&#28304;&#65292;&#20197;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#22312;&#20215;&#26684;&#19981;&#30830;&#23450;&#24615;&#19979;&#32852;&#21512;&#21442;&#19982;&#24066;&#22330;&#30340;&#26377;&#25928;&#31639;&#27861;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#29616;&#36135;&#24066;&#22330;&#21644;&#23481;&#28798;&#39057;&#29575;&#25511;&#21046;&#36741;&#21161;&#26381;&#21153;&#65288;FCAS&#65289;&#24066;&#22330;&#20986;&#20215;&#30340;&#26032;&#39062;BESS&#32852;&#21512;&#31454;&#26631;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#26377;&#25928;&#24212;&#23545;&#19971;&#20010;&#24066;&#22330;&#21516;&#26102;&#30340;&#20215;&#26684;&#27874;&#21160;&#65292;&#24182;&#24110;&#21161;DRL&#23398;&#20064;&#26368;&#20339;&#30340;BESS&#20986;&#20215;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;DRL&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19110v1 Announce Type: cross  Abstract: The battery energy storage system (BESS) has immense potential for enhancing grid reliability and security through its participation in the electricity market. BESS often seeks various revenue streams by taking part in multiple markets to unlock its full potential, but effective algorithms for joint-market participation under price uncertainties are insufficiently explored in the existing research. To bridge this gap, we develop a novel BESS joint bidding strategy that utilizes deep reinforcement learning (DRL) to bid in the spot and contingency frequency control ancillary services (FCAS) markets. Our approach leverages a transformer-based temporal feature extractor to effectively respond to price fluctuations in seven markets simultaneously and helps DRL learn the best BESS bidding strategy in joint-market participation. Additionally, unlike conventional "black-box" DRL model, our approach is more interpretable and provides valuable i
&lt;/p&gt;</description></item><item><title>CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19105</link><description>&lt;p&gt;
CollaFuse&#65306;&#22312;&#21327;&#20316;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#23548;&#33322;&#26377;&#38480;&#36164;&#28304;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19105
&lt;/p&gt;
&lt;p&gt;
CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#25193;&#25955;&#24335;&#27169;&#22411;&#22312;&#25968;&#25454;&#38656;&#27714;&#21644;&#38544;&#31169;&#26041;&#38754;&#32473;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24102;&#26469;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#32852;&#37030;&#23398;&#20064;&#20998;&#21457;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#20250;&#32473;&#20010;&#21035;&#23458;&#25143;&#24102;&#26469;&#21387;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CollaFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#26377;&#25928;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;CollaFuse&#23454;&#29616;&#20102;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36825;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#26412;&#22320;&#20445;&#30041;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;GPU&#36827;&#31243;&#65292;&#21516;&#26102;&#23558;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#36827;&#31243;&#22806;&#21253;&#32473;&#20849;&#20139;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#23637;&#31034;&#65292;CollaFuse&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#23545;&#25935;&#24863;&#20449;&#24687;&#20849;&#20139;&#30340;&#38656;&#27714;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19105v1 Announce Type: cross  Abstract: In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabiliti
&lt;/p&gt;</description></item><item><title>FlatNAS&#26159;&#25991;&#29486;&#20013;&#39318;&#20010;&#31995;&#32479;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20002;&#22833;&#20989;&#25968;&#24179;&#22374;&#21306;&#22495;&#30340;NAS&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#20854;&#22312;&#20998;&#24067;&#25968;&#25454;&#21644;&#20998;&#24067;&#20043;&#22806;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#32422;&#26463;&#20854;&#26550;&#26500;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.19102</link><description>&lt;p&gt;
FlatNAS&#65306;&#20248;&#21270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#24179;&#22374;&#24615;&#20197;&#23454;&#29616;&#23545;&#20998;&#24067;&#20043;&#22806;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19102
&lt;/p&gt;
&lt;p&gt;
FlatNAS&#26159;&#25991;&#29486;&#20013;&#39318;&#20010;&#31995;&#32479;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20002;&#22833;&#20989;&#25968;&#24179;&#22374;&#21306;&#22495;&#30340;NAS&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#20854;&#22312;&#20998;&#24067;&#25968;&#25454;&#21644;&#20998;&#24067;&#20043;&#22806;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#32422;&#26463;&#20854;&#26550;&#26500;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20026;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#30340;&#33258;&#21160;&#23450;&#20041;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Flat Neural Architecture Search&#65288;FlatNAS&#65289;&#30340;&#26032;&#39062;NAS&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#25506;&#35752;&#20102;&#22522;&#20110;&#23545;&#26435;&#37325;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#20855;&#26377;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#21333;&#19968;NN&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#19982;&#24403;&#21069;&#20027;&#35201;&#38598;&#20013;&#20110;OOD&#31639;&#27861;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;FlatNAS&#25104;&#21151;&#22320;&#35780;&#20272;&#20102;NN&#26550;&#26500;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19102v1 Announce Type: cross  Abstract: Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19101</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#25512;&#33616;&#20869;&#23481;&#21464;&#24471;&#36234;&#26469;&#36234;&#20016;&#23500; -- &#21333;&#20010;&#29992;&#25143;&#21453;&#39304;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#23454;&#20307;&#65292;&#22914;&#38144;&#21806;&#20135;&#21697;&#12289;&#30701;&#35270;&#39057;&#21644;&#20869;&#23481;&#24086;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#23454;&#20307;&#25512;&#33616;&#38382;&#39064;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37319;&#29992;&#22522;&#20110;&#20849;&#20139;&#32593;&#32476;&#30340;&#26550;&#26500;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#36825;&#19968;&#24819;&#27861;&#26159;&#23558;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#28304;&#23454;&#20307;&#65289;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#30446;&#26631;&#23454;&#20307;&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19101v1 Announce Type: cross  Abstract: In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19095</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#23545;&#20110;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#32467;&#26500;&#20915;&#23450;&#20854;&#21151;&#33021;&#12290;&#34507;&#30333;&#30340;&#20108;&#32423;&#32467;&#26500;&#26159;&#30001;&#34507;&#30333;&#36136;&#30340;&#19968;&#32423;&#32467;&#26500;&#25240;&#21472;&#24418;&#25104;&#30340;&#65292;&#32780;&#34507;&#30333;&#30340;&#19977;&#32423;&#32467;&#26500;&#26159;&#30001;&#20108;&#32423;&#32467;&#26500;&#30340;&#24367;&#26354;&#21644;&#25240;&#21472;&#24418;&#25104;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#34507;&#30333;&#30340;&#20108;&#32423;&#32467;&#26500;&#23545;&#20110;&#25972;&#20307;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#34429;&#28982;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#34507;&#30333;&#32467;&#26500;&#39044;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#19981;&#36275;&#20197;&#28385;&#36275;&#23545;&#34507;&#30333;&#36136;&#20449;&#24687;&#30340;&#22823;&#37327;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#19968;&#20010;&#30417;&#30563;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19095v1 Announce Type: cross  Abstract: Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neural Networks (CCN) and a supervised Transformer pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#65292;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#20197;&#25509;&#36817;&#26368;&#20248;&#36895;&#24230;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#24182;&#21457;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19090</link><description>&lt;p&gt;
&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification with Resource Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#65292;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#20197;&#25509;&#36817;&#26368;&#20248;&#36895;&#24230;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#24182;&#21457;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#19981;&#21516;&#26367;&#20195;&#26041;&#26696;&#23454;&#39564;&#25104;&#26412;&#24322;&#36136;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAIwRC&#65289;&#38382;&#39064;&#12290;&#20195;&#29702;&#21830;&#26088;&#22312;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#20854;&#20013;&#36164;&#28304;&#34987;&#27599;&#27425;&#25289;&#21160;&#25163;&#33218;&#28040;&#32791;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#20998;&#26512;&#20102;&#20855;&#26377;&#36164;&#28304;&#27604;&#20363;&#30340;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#12290;SH-RR&#20197;&#36817;&#20046;&#26368;&#20248;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#23454;&#29616;&#20102;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#30340;&#27010;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#25910;&#25947;&#36895;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19090v1 Announce Type: new  Abstract: Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;3D&#24515;&#33039;&#32593;&#26684;&#37325;&#24314;&#21644;&#21518;&#32493;&#20219;&#21153;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.19062</link><description>&lt;p&gt;
&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;3D&#24515;&#33039;&#32593;&#26684;&#37325;&#24314;&#21644;&#21518;&#32493;&#20219;&#21153;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#23545;&#24515;&#33039;&#36229;&#22768;&#27874;&#65288;US&#65289;&#30340;&#35786;&#26029;&#65292;&#20020;&#24202;&#23454;&#36341;&#24050;&#32463;&#24314;&#31435;&#20102;&#24515;&#33039;&#30340;&#20960;&#31181;&#26631;&#20934;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#20316;&#20026;&#35786;&#26029;&#27979;&#37327;&#30340;&#21442;&#32771;&#28857;&#65292;&#24182;&#23450;&#20041;&#20102;&#22270;&#20687;&#33719;&#21462;&#30340;&#35270;&#21475;&#12290;&#33258;&#21160;&#35270;&#22270;&#35782;&#21035;&#28041;&#21450;&#23558;&#36825;&#20123;&#22270;&#20687;&#20998;&#32452;&#20026;&#26631;&#20934;&#35270;&#22270;&#30340;&#31867;&#21035;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#23454;&#29616;&#36825;&#19968;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#39564;&#35777;&#22270;&#20687;&#26159;&#21542;&#36866;&#21512;&#29305;&#23450;&#27979;&#37327;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#21407;&#22240;&#21253;&#25324;&#24515;&#33039;&#32467;&#26500;&#30340;&#27491;&#30830;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#21487;&#33021;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#35270;&#22270;&#20998;&#31867;&#65292;&#24182;&#34701;&#21512;&#20102;&#23545;&#24515;&#33039;&#30340;&#19977;&#32500;&#32593;&#26684;&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22810;&#21518;&#32493;&#20219;&#21153;&#65292;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22270;&#21367;&#31215;&#23398;&#20064;3D&#24515;&#33039;&#32593;&#26684;&#65292;&#20351;&#29992;&#31867;&#20284;&#30340;&#25216;&#26415;&#26469;&#23398;&#20064;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;3D&#32593;&#26684;&#65292;&#20363;&#22914;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19062v1 Announce Type: cross  Abstract: To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the 
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.19047</link><description>&lt;p&gt;
&#28145;&#24230;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations of Deep Selective State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19047
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22914;S4&#65292;&#28304;&#33258;Gu&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#20316;&#20026;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#28145;&#24230;SSM&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;transformers&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#39537;&#21160;SSM&#30340;&#32447;&#24615;&#36882;&#24402;&#20801;&#35768;&#36755;&#20837;&#21644;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#20056;&#27861;&#20132;&#20114;&#65288;&#22914;GateLoop&#65292;Mamba&#65292;GLA&#65289;&#65292;&#37027;&#20040;&#25152;&#24471;&#21040;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21442;&#25968;&#35268;&#27169;&#36798;&#21040;&#21313;&#20159;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Rough Path Theory&#30340;&#24037;&#20855;&#65292;&#20026;&#36825;&#19968;&#26368;&#36817;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65306;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#38543;&#26426;&#32447;&#24615;&#36882;&#24402;&#37197;&#22791;&#31616;&#21333;&#30340;&#36755;&#20837;&#25511;&#21046;&#36716;&#25442;&#65288;&#36873;&#25321;&#24615;&#26426;&#21046;&#65289;&#26102;&#65292;&#38544;&#34255;&#29366;&#24577;&#21487;&#34987;&#35777;&#26126;&#26159;&#20302;&#32500;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19047v1 Announce Type: new  Abstract: Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional proj
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23450;&#20301;&#30446;&#26631;&#21152;&#23494;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#65292;&#21363;&#20351;&#23384;&#22312;&#36319;&#36394;&#21464;&#24418;&#12290;</title><link>https://arxiv.org/abs/2402.19037</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23450;&#20301;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23494;&#30721;&#25805;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Deep-Learning Technique to Locate Cryptographic Operations in Side-Channel Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23450;&#20301;&#30446;&#26631;&#21152;&#23494;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#65292;&#21363;&#20351;&#23384;&#22312;&#36319;&#36394;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20391;&#20449;&#36947;&#25915;&#20987;&#36890;&#36807;&#30456;&#20851;&#37096;&#20998;&#24050;&#30693;&#35745;&#31639;&#25968;&#25454;&#21644;&#24050;&#27979;&#37327;&#30340;&#20391;&#20449;&#36947;&#20449;&#21495;&#65292;&#20801;&#35768;&#20174;&#21152;&#23494;&#21407;&#35821;&#30340;&#25191;&#34892;&#20013;&#25552;&#21462;&#31192;&#23494;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23450;&#20301;&#20391;&#20449;&#36947;&#36861;&#36394;&#20013;&#25191;&#34892;&#30340;&#30446;&#26631;&#35745;&#31639;&#23494;&#30721;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#25554;&#20837;&#25216;&#26415;&#33719;&#24471;&#30340;&#36319;&#36394;&#21464;&#24418;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19037v1 Announce Type: cross  Abstract: Side-channel attacks allow extracting secret information from the execution of cryptographic primitives by correlating the partially known computed data and the measured side-channel signal. However, to set up a successful side-channel attack, the attacker has to perform i) the challenging task of locating the time instant in which the target cryptographic primitive is executed inside a side-channel trace and then ii)the time-alignment of the measured data on that time instant. This paper presents a novel deep-learning technique to locate the time instant in which the target computed cryptographic operations are executed in the side-channel trace. In contrast to state-of-the-art solutions, the proposed methodology works even in the presence of trace deformations obtained through random delay insertion techniques. We validated our proposal through a successful attack against a variety of unprotected and protected cryptographic primitive
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#23450;&#37327;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.19025</link><description>&lt;p&gt;
&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#30340;&#32452;&#21512;&#29992;&#20110;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19025
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#23450;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
XAI&#20013;&#30340;&#31283;&#20581;&#24615;&#27010;&#24565;&#25351;&#30340;&#26159;&#35266;&#23519;&#21040;&#30340;&#20851;&#20110;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#22312;&#23545;&#23548;&#33268;&#35813;&#39044;&#27979;&#30340;&#36755;&#20837;&#21464;&#21270;&#26102;&#30340;&#21464;&#21270;&#12290;&#30452;&#35273;&#19978;&#65292;&#22914;&#26524;&#35201;&#35299;&#37322;&#30340;&#36755;&#20837;&#30053;&#24494;&#21464;&#21270;&#65292;&#20197;&#33267;&#20110;&#19981;&#20250;&#22826;&#22823;&#31243;&#24230;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#37027;&#20040;&#25105;&#20204;&#26399;&#26395;&#23545;&#20110;&#35813;&#26032;&#36755;&#20837;&#30340;&#35299;&#37322;&#20063;&#19981;&#20250;&#26377;&#22826;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36890;&#36807;&#23545;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#30340;&#32452;&#21512;&#21487;&#20197;&#25552;&#39640;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#30340;&#31283;&#20581;&#24615;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21518;&#32493;SHAP&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#20013;&#24471;&#21040;&#23454;&#26045;&#21644;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25152;&#33719;&#24471;&#30340;&#25913;&#36827;&#24050;&#32463;&#36890;&#36807;&#23450;&#37327;&#26041;&#24335;&#36827;&#34892;&#20102;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#24615;&#31283;&#20581;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19025v1 Announce Type: cross  Abstract: The notion of robustness in XAI refers to the observed variations in the explanation of the prediction of a learned model with respect to changes in the input leading to that prediction. Intuitively, if the input being explained is modified slightly subtly enough so as to not change the prediction of the model too much, then we would expect that the explanation provided for that new input does not change much either. We argue that a combination through discriminative averaging of ensembles weak learners explanations can improve the robustness of explanations in ensemble methods.This approach has been implemented and tested with post-hoc SHAP method and Random Forest ensemble with successful results. The improvements obtained have been measured quantitatively and some insights into the explicability robustness in ensemble methods are presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#31639;&#27861;SPriFed-OMP&#65292;&#36890;&#36807;&#23558;OMP&#36716;&#21270;&#20026;FL&#35774;&#32622;&#65292;&#24182;&#32467;&#21512;SMPC&#21644;DP&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22122;&#22768;&#28155;&#21152;&#37327;&#26469;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.19016</link><description>&lt;p&gt;
SPriFed-OMP&#65306;&#29992;&#20110;&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#31639;&#27861;SPriFed-OMP&#65292;&#36890;&#36807;&#23558;OMP&#36716;&#21270;&#20026;FL&#35774;&#32622;&#65292;&#24182;&#32467;&#21512;SMPC&#21644;DP&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22122;&#22768;&#28155;&#21152;&#37327;&#26469;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#26159;&#19968;&#20010;&#32463;&#20856;&#19988;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#32500;&#24230;$p$&#36828;&#22823;&#20110;&#26679;&#26412;&#25968;$n$&#26102;&#65292;&#23588;&#20854;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#20013;&#30740;&#31350;&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#30340;&#24037;&#20316;&#24456;&#23569;&#65292;&#20854;&#20013;&#24517;&#39035;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;&#29305;&#21035;&#26159;&#65292;&#29616;&#26377;&#30340;DP-FL&#31639;&#27861;&#65288;&#22914;DP-SGD&#65289;&#30340;&#24615;&#33021;&#20445;&#35777;&#22312;$p \gg n$&#26102;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#27492;&#23427;&#20204;&#23558;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#30495;&#23454;&#30340;&#24213;&#23618;&#31232;&#30095;&#27169;&#22411;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#31169;&#23494;&#31232;&#30095;&#22522;&#30784;&#24674;&#22797;&#31639;&#27861;&#29992;&#20110;FL&#35774;&#32622;&#65292;&#31216;&#20026;SPriFed-OMP&#12290;SPriFed-OMP&#23558;OMP&#65288;Orthogonal Matching Pursuit&#65289;&#36716;&#21270;&#20026;FL&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;SMPC&#65288;Secure Multi-Party Computation&#65289;&#21644;DP&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#30830;&#20445;&#21482;&#38656;&#28155;&#21152;&#23569;&#37327;&#30340;&#22122;&#22768;&#21363;&#21487;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19016v1 Announce Type: new  Abstract: Sparse basis recovery is a classical and important statistical learning problem when the number of model dimensions $p$ is much larger than the number of samples $n$. However, there has been little work that studies sparse basis recovery in the Federated Learning (FL) setting, where the client data's differential privacy (DP) must also be simultaneously protected. In particular, the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will degrade significantly when $p \gg n$, and thus, they will fail to learn the true underlying sparse model accurately. In this work, we develop a new differentially private sparse basis recovery algorithm for the FL setting, called SPriFed-OMP. SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to the FL setting. Further, it combines SMPC (secure multi-party computation) and DP to ensure that only a small amount of noise needs to be added in order to achieve differential privacy. As a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19009</link><description>&lt;p&gt;
&#29983;&#25104;&#12289;&#37325;&#24314;&#21644;&#34920;&#31034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#65306;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19009
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#22522;&#20110;&#19977;&#39033;&#26680;&#24515;&#33021;&#21147;--&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;--&#36328;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#31163;&#25955;&#25991;&#26412;/&#34507;&#30333;&#24207;&#21015;&#21644;&#36830;&#32493;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#23478;&#26063;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#29305;&#23450;&#33021;&#21147;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#23427;&#26080;&#32541;&#22320;&#38598;&#25104;&#20102;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;DiLED&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#21270;&#32534;&#30721;-&#35299;&#30721;&#26469;&#23558;&#26631;&#20934;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#21152;&#22122;-&#21435;&#22122;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;DiLED&#19982;&#25104;&#29087;&#30340;&#25193;&#25955;&#27169;&#22411;&#30446;&#26631;&#21644;&#35757;&#32451;&#26041;&#27861;&#20860;&#23481;&#65292;&#21487;&#26377;&#25928;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19009v1 Announce Type: cross  Abstract: The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21152;&#24555;&#25512;&#26029;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.18995</link><description>&lt;p&gt;
&#29992;&#20110;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21152;&#24555;&#25512;&#26029;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#33258;&#28982;&#22320;&#22312;&#29289;&#29702;&#21644;&#31038;&#20250;&#39046;&#22495;&#20013;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;Poisson gamma&#21160;&#24577;&#31995;&#32479;&#65288;PGDSs&#65289;&#26159;&#26032;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#25429;&#25417;&#35745;&#25968;&#24207;&#21015;&#32972;&#21518;&#34920;&#29616;&#20986;&#30340;&#26126;&#26174;&#30340;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#21644;&#31361;&#21457;&#21160;&#24577;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#20110;&#32463;&#20856;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PGDSs&#22312;&#25968;&#25454;&#22635;&#20805;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;PGDS&#19981;&#33021;&#25429;&#25417;&#22522;&#30784;&#21160;&#24577;&#36807;&#31243;&#30340;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#34892;&#20026;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#23427;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#20419;&#36827;&#20102;&#25512;&#26029;&#31639;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20272;&#35745;&#22240;&#23376;&#32467;&#26500;&#21644;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18995v1 Announce Type: cross  Abstract: Modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains. Poisson gamma dynamical systems (PGDSs) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences. In particular, PGDSs demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (LDS) based methods. Despite these advantages, PGDS cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes. To mitigate this defect, we propose a negative-binomial-randomized gamma Markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm. Moreover, we develop methods to estimate both factor-structured and graph
&lt;/p&gt;</description></item><item><title>Spyx&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#21363;&#26102;&#32534;&#35793;&#20248;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#24211;&#65292;&#20197;&#22312;&#35757;&#32451;&#26102;&#22686;&#24378;&#33021;&#25928;&#24182;&#20943;&#23569;&#30828;&#20214;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18994</link><description>&lt;p&gt;
Spyx&#65306;&#29992;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21363;&#26102;&#32534;&#35793;&#20248;&#21270;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18994
&lt;/p&gt;
&lt;p&gt;
Spyx&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#21363;&#26102;&#32534;&#35793;&#20248;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#24211;&#65292;&#20197;&#22312;&#35757;&#32451;&#26102;&#22686;&#24378;&#33021;&#25928;&#24182;&#20943;&#23569;&#30828;&#20214;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#30340;&#20316;&#29992;&#26085;&#30410;&#20851;&#38190;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#37096;&#32626;&#24050;&#25104;&#20026;&#20851;&#27880;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;&#26368;&#36817;&#20851;&#27880;&#24230;&#36739;&#39640;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#26550;&#26500;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;AI&#21152;&#36895;&#22120;&#30340;&#21457;&#23637;&#65292;&#20419;&#36827;&#20102;&#24222;&#22823;&#12289;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#24378;&#22823;&#30340;&#32593;&#32476;&#24456;&#26377;&#25928;&#65292;&#20294;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25191;&#34892;&#25104;&#26412;&#12290;&#21463;&#29983;&#29289;&#31070;&#32463;&#36807;&#31243;&#21551;&#21457;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#31232;&#30095;&#35745;&#31639;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36890;&#36807;&#20943;&#23569;&#21644;&#38477;&#20302;&#21151;&#32791;&#30828;&#20214;&#21344;&#29992;&#26469;&#22686;&#24378;&#33021;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24490;&#29615;&#24615;&#36136;&#65292;SNNs&#30340;&#35757;&#32451;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38590;&#20197;&#20687;&#29616;&#20195;AI&#21152;&#36895;&#22120;&#37027;&#26679;&#36731;&#26494;&#21033;&#29992;&#24040;&#22823;&#30340;&#24182;&#34892;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;S&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18994v1 Announce Type: cross  Abstract: As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#35889;&#20998;&#35299;&#21644;&#25193;&#25955;&#36807;&#31243;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;GRASP&#65292;&#33021;&#22815;&#36890;&#36807;&#25130;&#26029;&#25289;&#26222;&#25289;&#26031;&#39057;&#35889;&#24555;&#36895;&#20934;&#30830;&#22320;&#25429;&#25417;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#22788;&#29702;&#33410;&#28857;&#29305;&#24449;&#65292;&#36991;&#20813;&#20102;&#20108;&#27425;&#22797;&#26434;&#24615;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2402.18974</link><description>&lt;p&gt;
&#36890;&#36807;&#35889;&#25193;&#25955;&#29983;&#25104;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Graph Generation via Spectral Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#35889;&#20998;&#35299;&#21644;&#25193;&#25955;&#36807;&#31243;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;GRASP&#65292;&#33021;&#22815;&#36890;&#36807;&#25130;&#26029;&#25289;&#26222;&#25289;&#26031;&#39057;&#35889;&#24555;&#36895;&#20934;&#30830;&#22320;&#25429;&#25417;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#22788;&#29702;&#33410;&#28857;&#29305;&#24449;&#65292;&#36991;&#20813;&#20102;&#20108;&#27425;&#22797;&#26434;&#24615;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRASP&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#21644;&#25193;&#25955;&#36807;&#31243;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#27169;&#22411;&#20174;&#20013;&#37319;&#26679;&#29305;&#24449;&#21521;&#37327;&#21644;&#29305;&#24449;&#20540;&#65292;&#20174;&#32780;&#21487;&#20197;&#37325;&#24314;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#21644;&#37051;&#25509;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#32622;&#25442;&#19981;&#21464;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#23558;&#33410;&#28857;&#29305;&#24449;&#36830;&#25509;&#21040;&#27599;&#20010;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#26469;&#22788;&#29702;&#33410;&#28857;&#29305;&#24449;&#12290;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#39057;&#35889;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#28982;&#25429;&#33719;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#30452;&#25509;&#22312;&#33410;&#28857;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#38480;&#21046;&#20854;&#20182;&#26041;&#27861;&#36866;&#29992;&#24615;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#29942;&#39048;&#12290;&#36890;&#36807;&#25130;&#26029;&#35889;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#24555;&#20294;&#20934;&#30830;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#22270;&#24418;&#30340;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18974v1 Announce Type: new  Abstract: In this paper, we present GRASP, a novel graph generative model based on 1) the spectral decomposition of the graph Laplacian matrix and 2) a diffusion process. Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian and adjacency matrix. Our permutation invariant model can also handle node features by concatenating them to the eigenvectors of each node. Using the Laplacian spectrum allows us to naturally capture the structural characteristics of the graph and work directly in the node space while avoiding the quadratic complexity bottleneck that limits the applicability of other methods. This is achieved by truncating the spectrum, which as we show in our experiments results in a faster yet accurate generative process. An extensive set of experiments on both synthetic and real world graphs demonstrates the strengths of our model against state-of-the-art a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18949</link><description>&lt;p&gt;
&#25552;&#39640;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#32676;&#32452;&#36830;&#25509;&#24615;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Group Connectivity for Generalization of Federated Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18949
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22810;&#20010;&#24322;&#26500;&#23458;&#25143;&#31471;&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#26356;&#26032;&#21644;&#27169;&#22411;&#34701;&#21512;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30456;&#27604;&#65292;FL&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#27867;&#21270;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#36825;&#26159;&#20854;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#26412;&#30340;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#30740;&#31350;&#21644;&#25913;&#36827;FL&#30340;&#27867;&#21270;&#65292;&#21363;&#26412;&#22320;&#27169;&#22411;&#22312;&#21442;&#25968;&#21306;&#22495;&#20013;&#22914;&#20309;&#36830;&#25509;&#24182;&#34701;&#21512;&#20026;&#27867;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26415;&#35821;&#8220;&#36830;&#25509;&#24615;&#8221;&#28304;&#33258;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#65288;LMC&#65289;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#27169;&#24335;&#65289;&#30340;&#20869;&#25554;&#25439;&#22833;&#26223;&#35266;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22266;&#23450;&#30340;&#38170;&#23450;&#27169;&#22411;&#26469;&#30740;&#31350;&#36830;&#25509;&#24615;&#30340;&#20256;&#36882;&#24615;&#36136;&#65292;&#20174;&#20004;&#20010;&#27169;&#22411;&#65288;LMC&#65289;&#21040;&#19968;&#32452;&#27169;&#22411;&#65288;FL&#20013;&#30340;&#27169;&#22411;&#34701;&#21512;&#65289;&#12290;&#26681;&#25454;&#25152;&#21457;&#29616;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#30340;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21033;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#21644;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18946</link><description>&lt;p&gt;
&#22312;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#19981;&#30830;&#23450;&#27169;&#22411;&#30340;&#23454;&#26102;&#33258;&#36866;&#24212;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21033;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#21644;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#31995;&#32479;&#65292;&#20197;&#30830;&#20445;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#21021;&#22987;&#38454;&#27573;&#38598;&#20013;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26694;&#26550;&#19978;&#12290;&#25105;&#20204;&#39318;&#20808;&#38598;&#25104;&#20102;&#19968;&#20010;&#36951;&#24536;&#22240;&#23376;&#26469;&#20248;&#21270;&#21464;&#20998;&#31232;&#30095;GP&#31639;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#36866;&#24212;&#24615;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#29305;&#27530;&#22797;&#21512;&#26680;&#23545;&#39640;&#26031;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26356;&#26032;&#19968;&#20010;&#28304;&#33258;&#26032;&#26679;&#26412;&#30340;&#23396;&#29420;&#24863;&#24212;&#28857;&#26469;&#21152;&#24378;&#39640;&#26031;&#27169;&#22411;&#30340;&#22312;&#32447;&#25512;&#29702;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#19982;&#23398;&#20064;&#21040;&#30340;&#36229;&#21442;&#25968;&#30456;&#32467;&#21512;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;HOCBFs&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#19982;&#20808;&#21069;&#35757;&#32451;&#30340;&#23398;&#20064;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36890;&#36807;&#21033;&#29992;&#31532;&#19968;&#38454;&#27573;&#30340;&#22797;&#21512;&#26680;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18946v1 Announce Type: new  Abstract: This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model's online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18919</link><description>&lt;p&gt;
Decompose-and-Compose: &#19968;&#31181;&#32452;&#21512;&#26041;&#27861;&#26469;&#20943;&#36731;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#24067;&#36716;&#31227;&#26469;&#28304;&#26159;&#22270;&#20687;&#30340;&#32452;&#25104;&#24615;&#36136;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#30830;&#23450;&#26631;&#31614;&#30340;&#20027;&#35201;&#23545;&#35937;&#25110;&#32452;&#20214;&#22806;&#65292;&#36890;&#24120;&#36824;&#23384;&#22312;&#19968;&#20123;&#20854;&#20182;&#22270;&#20687;&#32452;&#20214;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#19982;&#26631;&#31614;&#20855;&#26377;&#20266;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decompose-and-Compose&#65288;DaC&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#32452;&#21512;&#22270;&#20687;&#20803;&#32032;&#30340;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20351;&#29992;ERM&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#39640;&#24230;&#20851;&#27880;&#35201;&#20040;&#26159;&#22240;&#26524;&#32452;&#20214;&#65292;&#35201;&#20040;&#26159;&#19982;&#26631;&#31614;&#20855;&#26377;&#39640;&#20266;&#30456;&#20851;&#24615;&#30340;&#32452;&#20214;&#65288;&#23588;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18919v1 Announce Type: cross  Abstract: While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especia
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#39640;&#25928;&#21448;&#20855;&#26377;&#26368;&#20248;&#21518;&#24724;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18917</link><description>&lt;p&gt;
&#19981;&#35201;&#20381;&#36182;&#26080;&#36873;&#25321;&#65292;&#19981;&#35201;&#37325;&#22797;&#31227;&#21160;&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20248;&#21270;&#30340;&#26368;&#20339;&#12289;&#39640;&#25928;&#21644;&#23454;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18917
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#39640;&#25928;&#21448;&#20855;&#26377;&#26368;&#20248;&#21518;&#24724;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#36873;&#25321;&#21644;&#23376;&#38598;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30495;&#23454;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#21253;&#25324;&#24191;&#21578;&#25918;&#32622;&#12289;&#22312;&#32447;&#38646;&#21806;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#36807;&#21435;&#24050;&#32463;&#34987;&#30740;&#31350;&#36807;&#65292;&#20294;&#32570;&#20047;&#30452;&#35266;&#21644;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25928;&#30340;&#31639;&#27861;&#21644;&#26368;&#20248;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#36873;&#25321;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#23384;&#22312;&#19968;&#20010;&#22987;&#32456;&#21253;&#21547;&#22312;&#36873;&#25321;&#38598;&#20013;&#30340;&#8220;&#24378;&#21442;&#32771;&#39033;&#8221;&#65292;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#34987;&#35774;&#35745;&#20026;&#37325;&#22797;&#25552;&#20379;&#30456;&#21516;&#30340;&#20998;&#31867;&#65292;&#30452;&#21040;&#21442;&#32771;&#39033;&#34987;&#36873;&#25321; &#8212; &#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#32780;&#35328;&#37117;&#38750;&#24120;&#19981;&#29616;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20998;&#31867;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18917v1 Announce Type: new  Abstract: We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortmen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;DIGIC&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;</title><link>https://arxiv.org/abs/2402.18910</link><description>&lt;p&gt;
DIGIC: &#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIGIC: Domain Generalizable Imitation Learning by Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18910
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;DIGIC&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#24050;&#32463;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#24378;&#22823;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36825;&#31867;&#26041;&#27861;&#38656;&#35201;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#21464;&#21270;&#26469;&#35782;&#21035;&#24341;&#36215;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#26114;&#36149;&#29978;&#33267;&#19981;&#21487;&#34892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#20998;&#24067;&#26469;&#21457;&#29616;&#39046;&#22495;&#27867;&#21270;&#31574;&#30053;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;DIGIC&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#20174;&#28436;&#31034;&#25968;&#25454;&#20998;&#24067;&#20013;&#25214;&#20986;&#19987;&#23478;&#21160;&#20316;&#30340;&#30452;&#25509;&#21407;&#22240;&#26469;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#30784;&#22240;&#26524;&#27169;&#22411;&#30340;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#20316;&#20026;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18910v1 Announce Type: cross  Abstract: Causality has been combined with machine learning to produce robust representations for domain generalization. Most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases. In this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy. We design a novel framework, called DIGIC, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery. Our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models. Our empirical study in various control 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;p&gt;
&#35770;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#30340;&#25910;&#25947;&#24615;&#65306;&#24212;&#32447;&#24615;&#25506;&#27979;&#36824;&#26159;&#23436;&#20840;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38750;&#31169;&#26377;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;DP&#20248;&#21270;&#25216;&#26415;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;DP&#35774;&#32622;&#20013;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;&#23436;&#20840;&#24494;&#35843;&#26377;&#26102;&#20505;&#24182;&#19981;&#24635;&#26159;&#20135;&#29983;&#26368;&#20339;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#65288;1&#65289;&#20998;&#26512;&#20102;DP&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25506;&#32034;&#20102;&#39034;&#24207;&#24494;&#35843;&#30340;&#29616;&#35937;&#65292;&#20174;&#32447;&#24615;&#25506;&#27979;&#24320;&#22987;&#65292;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#65292;&#20197;&#21450;&#23427;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;DP&#24494;&#35843;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30830;&#23450;&#38544;&#31169;&#39044;&#31639;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#23436;&#20840;&#24494;&#35843;&#20043;&#38388;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18905v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques. In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning. The theoretical results are supported by empirical evaluations on various benchmarks and models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.18888</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#25955;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;(FL)&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#19981;&#21516;&#23396;&#23707;&#38388;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;FL&#23548;&#20986;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#38476;&#29983;&#20998;&#24067;&#30340;&#25968;&#25454;&#23396;&#23707;&#26102;&#20250;&#34920;&#29616;&#20986;&#26126;&#26174;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#31616;&#21333;&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#32852;&#37030;&#23398;&#20064;(UEFL)&#12290;&#35813;&#26694;&#26550;&#21160;&#24577;&#22320;&#23558;&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#21487;&#35757;&#32451;&#30340;&#31163;&#25955;&#21521;&#37327;&#65292;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38024;&#23545;&#34920;&#29616;&#20986;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#23396;&#23707;&#29305;&#21035;&#22320;&#25193;&#23637;&#31163;&#25955;&#21270;&#35789;&#20856;&#25110;&#32534;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;DeepONet&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#33033;&#34880;&#21387;&#65288;ABP&#65289;&#27874;&#24418;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#36830;&#32493;&#39044;&#27979;ABP&#27874;&#24418;&#65292;&#28385;&#36275;Navier-Stokes&#26041;&#31243;&#21644;Windkessel&#36793;&#30028;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.18886</link><description>&lt;p&gt;
BP-DeepONet: &#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;DeepONet&#30340;&#26080;&#34966;&#34880;&#21387;&#20272;&#35745;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;DeepONet&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#33033;&#34880;&#21387;&#65288;ABP&#65289;&#27874;&#24418;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#36830;&#32493;&#39044;&#27979;ABP&#27874;&#24418;&#65292;&#28385;&#36275;Navier-Stokes&#26041;&#31243;&#21644;Windkessel&#36793;&#30028;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#20840;&#29699;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#34880;&#21387;&#26159;&#19968;&#20010;&#20851;&#38190;&#25351;&#26631;&#12290; &#21160;&#33033;&#34880;&#21387;&#65288;ABP&#65289;&#27874;&#24418;&#25552;&#20379;&#20102;&#25972;&#20010;&#24515;&#33039;&#21608;&#26399;&#20013;&#30340;&#36830;&#32493;&#21387;&#21147;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35786;&#26029;&#35265;&#35299;&#12290; &#22240;&#27492;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#26080;&#21019;&#30340;&#26080;&#34966;&#26041;&#27861;&#26469;&#25345;&#32493;&#27979;&#37327;ABP&#27874;&#24418;&#12290; &#20934;&#30830;&#39044;&#27979;ABP&#27874;&#24418;&#36824;&#21487;&#20197;&#25913;&#21892;&#23545;&#24179;&#22343;&#34880;&#21387;&#30340;&#20272;&#35745;&#65292;&#36825;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#24515;&#34880;&#31649;&#20581;&#24247;&#29305;&#24449;&#12290; &#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;DeepONet&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#26469;&#39044;&#27979;ABP&#27874;&#24418;&#12290; &#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35201;&#27714;&#39044;&#27979;&#30340;ABP&#27874;&#24418;&#28385;&#36275;&#24102;&#26377;&#26102;&#38388;&#21608;&#26399;&#26465;&#20214;&#21644;Windkessel&#36793;&#30028;&#26465;&#20214;&#30340;Navier-Stokes&#26041;&#31243;&#12290; &#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#31532;&#19968;&#20010;&#36830;&#32493;&#39044;&#27979;ABP&#27874;&#24418;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#20855;&#26377;&#22320;&#28857;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18886v1 Announce Type: new  Abstract: Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with blood pressure serving as a crucial indicator. Arterial blood pressure (ABP) waveforms provide continuous pressure measurements throughout the cardiac cycle and offer valuable diagnostic insights. Consequently, there is a significant demand for non-invasive and cuff-less methods to measure ABP waveforms continuously. Accurate prediction of ABP waveforms can also improve the estimation of mean blood pressure, an essential cardiovascular health characteristic.   This study proposes a novel framework based on the physics-informed DeepONet approach to predict ABP waveforms. Unlike previous methods, our approach requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with a time-periodic condition and a Windkessel boundary condition. Notably, our framework is the first to predict ABP waveforms continuously, both with location and time, within the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30740;&#31350;&#35299;&#20915;&#26041;&#26696;&#65292;&#25581;&#31034;&#20102;&#26368;&#23567;&#21270;SC&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#21807;&#19968;&#26368;&#23567;&#21270;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18884</link><description>&lt;p&gt;
&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65306;&#20855;&#26377;&#19981;&#21463;&#38480;&#21046;&#29305;&#24449;&#30340;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30740;&#31350;&#35299;&#20915;&#26041;&#26696;&#65292;&#25581;&#31034;&#20102;&#26368;&#23567;&#21270;SC&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#21807;&#19968;&#26368;&#23567;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#32463;&#36807;&#38646;&#35757;&#32451;&#35823;&#24046;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#65292;&#22312;&#26368;&#21518;&#19968;&#23618;&#21576;&#29616;&#20986;&#20005;&#26684;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#34987;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65288;NC&#65289;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#32593;&#32476;&#20013;&#65292;&#26368;&#32456;&#38544;&#34255;&#23618;&#36755;&#20986;&#22312;&#35757;&#32451;&#38598;&#19978;&#26174;&#31034;&#20986;&#26368;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30740;&#31350;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#24191;&#27867;&#25506;&#35752;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#20294;&#20851;&#20110;&#20854;&#23545;&#24212;&#30340;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;&#30417;&#30563;&#23545;&#27604;&#65288;SC&#65289;&#25439;&#22833;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;NC&#30340;&#35270;&#35282;&#65292;&#37319;&#29992;&#20998;&#26512;&#26041;&#27861;&#30740;&#31350;&#20102;&#20248;&#21270;SC&#25439;&#22833;&#25152;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21463;&#38480;&#21046;&#29305;&#24449;&#27169;&#22411;&#65288;UFM&#65289;&#20316;&#20026;&#20195;&#34920;&#24615;&#20195;&#29702;&#65292;&#25581;&#31034;&#20102;&#22312;&#20805;&#20998;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#34893;&#29983;&#30340;&#19982;NC&#30456;&#20851;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;SC&#25439;&#22833;&#26368;&#23567;&#21270;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25152;&#26377;&#23616;&#37096;&#26368;&#23567;&#20540;&#37117;&#26159;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#26368;&#23567;&#21270;&#22120;&#26159;&#21807;&#19968;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18884v1 Announce Type: new  Abstract: Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC). These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (SC) loss. Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss. We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks. We show that, despite the non-convexity of SC loss minimization, all local minima are global minima. Furthermore, the minimizer is unique (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#22312;&#25913;&#21892;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#24863;&#30693;&#30340;&#35757;&#32451;&#35745;&#21010;LTS&#65292;&#26377;&#25928;&#20943;&#23569;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#20943;&#36731;&#22024;&#26434;&#25968;&#25454;&#24433;&#21709;&#65292;&#22686;&#24378;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18875</link><description>&lt;p&gt;
Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#22312;&#25913;&#21892;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#24863;&#30693;&#30340;&#35757;&#32451;&#35745;&#21010;LTS&#65292;&#26377;&#25928;&#20943;&#23569;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#20943;&#36731;&#22024;&#26434;&#25968;&#25454;&#24433;&#21709;&#65292;&#22686;&#24378;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#26159;&#19968;&#31867;&#19987;&#20026;&#24322;&#26500;&#22270;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#22270;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#22312;&#25913;&#21892;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25439;&#22833;&#24863;&#30693;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#21629;&#21517;&#20026;LTS&#65292;&#35813;&#35745;&#21010;&#34913;&#37327;&#20102;&#25968;&#25454;&#27599;&#20010;&#33410;&#28857;&#30340;&#36136;&#37327;&#65292;&#24182;&#36880;&#27493;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#34701;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#36880;&#27493;&#22686;&#21152;&#38590;&#24230;&#12290; LTS&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#21508;&#31181;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#20943;&#23569;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#20943;&#36731;&#22024;&#26434;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#22686;&#24378;HGNN&#20998;&#26512;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18875v1 Announce Type: new  Abstract: Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning models designed specifically for heterogeneous graphs, which are graphs that contain different types of nodes and edges. This paper investigates the application of curriculum learning techniques to improve the performance and robustness of Heterogeneous Graph Neural Networks (GNNs). To better classify the quality of the data, we design a loss-aware training schedule, named LTS that measures the quality of every nodes of the data and incorporate the training dataset into the model in a progressive manner that increases difficulty step by step. LTS can be seamlessly integrated into various frameworks, effectively reducing bias and variance, mitigating the impact of noisy data, and enhancing overall accuracy. Our findings demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing complex graph-structured data. The code is public at ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent Dr. Strategy&#65292;&#37197;&#22791;&#20102;Dreaming Strategy&#65292;&#23454;&#29616;&#20102;&#22312;&#26790;&#22659;&#20013;&#23398;&#20064;&#19968;&#32452;&#28508;&#22312;&#22320;&#26631;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#22320;&#26631;&#23398;&#20064;&#22320;&#26631;&#26465;&#20214;&#30340;&#39640;&#36895;&#20844;&#36335;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.18866</link><description>&lt;p&gt;
Dr. Strategy: &#20855;&#26377;&#25112;&#30053;&#26790;&#24819;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent
&lt;/p&gt;
&lt;p&gt;
Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent Dr. Strategy&#65292;&#37197;&#22791;&#20102;Dreaming Strategy&#65292;&#23454;&#29616;&#20102;&#22312;&#26790;&#22659;&#20013;&#23398;&#20064;&#19968;&#32452;&#28508;&#22312;&#22320;&#26631;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#22320;&#26631;&#23398;&#20064;&#22320;&#26631;&#26465;&#20214;&#30340;&#39640;&#36895;&#20844;&#36335;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning&#65288;MBRL&#65289;&#19968;&#30452;&#26159;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#24182;&#21019;&#24314;&#36890;&#29992;&#22411;Agent&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22686;&#24378;&#26790;&#24819;&#31574;&#30053;&#26412;&#36523;&#30340;&#21162;&#21147;&#24182;&#19981;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#35748;&#30693;&#31185;&#23398;&#35266;&#23519;&#21040;&#20154;&#31867;&#22312;&#35268;&#21010;&#26102;&#20351;&#29992;&#31354;&#38388;&#20998;&#38548;&#19982;&#24449;&#26381;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MBRL agent&#65292;&#31216;&#20026;Dr. Strategy&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Dreaming Strategy&#12290;&#25152;&#25552;&#20986;&#30340;Agent&#22312;&#26790;&#22659;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20998;&#38548;&#19982;&#24449;&#26381;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18866v1 Announce Type: new  Abstract: Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can "dream better" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18865</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#23569;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;LLMs&#19981;&#26029;&#22312;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#23545;&#21382;&#21490;&#20219;&#21153;&#30340;&#25512;&#29702;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#36825;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#38656;&#35201;&#22312;&#23398;&#20064;&#21487;&#22609;&#24615;&#21644;&#35760;&#24518;&#31283;&#23450;&#24615;&#20043;&#38388;&#20445;&#25345;&#26435;&#34913;&#12290;&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#35832;&#22914;&#35760;&#24518;&#37325;&#25918;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#38548;&#31163;&#31561;&#31574;&#30053;&#65292;&#20294;&#22312;&#36830;&#32493;&#30340;LLMs&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#23545;&#21508;&#20010;&#30456;&#37051;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#21516;&#26497;&#23567;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#25439;&#22833;&#30340;&#23665;&#35895;&#30456;&#36830;&#25509;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24494;&#35843;&#20013;&#30340;&#27169;&#24335;&#36830;&#25509;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;Lipschitz&#24615;&#19982;&#31283;&#23450;&#31209;&#30340;&#30740;&#31350;&#20026;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35282;&#24230;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.18863</link><description>&lt;p&gt;
&#27010;&#29575;Lipschitz&#24615;&#21644;&#31283;&#23450;&#31209;&#29992;&#20110;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18863
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;Lipschitz&#24615;&#19982;&#31283;&#23450;&#31209;&#30340;&#30740;&#31350;&#20026;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35282;&#24230;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#27169;&#22411;&#22914;&#20170;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#22312;&#30340;&#38382;&#39064;&#26159;&#21738;&#31181;&#35299;&#37322;&#24615;&#27169;&#22411;&#26368;&#26377;&#25928;&#12290;&#27010;&#29575;Lipschitz&#24615;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#19982;&#20107;&#21518;&#35299;&#37322;&#30340;&#36136;&#37327;&#22522;&#26412;&#30456;&#20851;&#12290;&#26412;&#25991;&#22312;&#23545;Integrated Gradients&#12289;LIME&#21644;SmoothGrad&#30340;&#27010;&#29575;Lipschitzness&#36827;&#34892;&#29702;&#35770;&#19979;&#38480;&#35777;&#26126;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20351;&#29992;&#27010;&#29575;Lipschitz&#24615;&#21644;&#24402;&#19968;&#21270;&#30340;&#32874;&#26126;&#24230;&#26469;&#27604;&#36739;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;Lipschitz&#24120;&#25968;&#19982;&#20854;&#31283;&#23450;&#31209;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#31209;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18863v1 Announce Type: new  Abstract: Explainability models are now prevalent within machine learning to address the black-box nature of neural networks. The question now is which explainability model is most effective. Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations. In this work, we prove theoretical lower bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and SmoothGrad. We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models. Further, we prove a link between the local Lipschitz constant of a neural network and its stable rank. We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20026;&#30005;&#32593;&#20648;&#33021;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#24320;&#21457;&#20102;&#22235;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18859</link><description>&lt;p&gt;
&#21033;&#29992;&#23454;&#39564;&#65292;&#25968;&#25454;&#20998;&#26512;&#21644;&#20581;&#24247;&#35780;&#20272;&#20351;&#20108;&#27425;&#21033;&#29992;&#30005;&#27744;&#30001;&#32791;&#23613;&#21040;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20026;&#30005;&#32593;&#20648;&#33021;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#24320;&#21457;&#20102;&#22235;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#37096;&#32626;&#22312;&#30005;&#32593;&#20648;&#33021;&#24212;&#29992;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#65288;BMS$_2$&#65289;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#12290;&#22312;15&#20010;&#26376;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#32534;&#21046;&#12289;&#20998;&#26512;&#24182;&#20844;&#24320;&#20998;&#20139;&#20102;&#19968;&#20010;&#20108;&#27425;&#21033;&#29992;&#65288;SL&#65289;&#30005;&#27744;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#26045;&#20102;&#19968;&#20010;&#24490;&#29615;&#21327;&#35758;&#65292;&#27169;&#25311;&#20102;&#22312;3 V-4 V&#30005;&#21387;&#33539;&#22260;&#20869;&#30340;&#30005;&#32593;&#20648;&#33021;&#36127;&#33655;&#26354;&#32447;&#12290;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#22235;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;BMS$_2$&#29305;&#24449;&#21644;&#21021;&#22987;&#23481;&#37327;&#65292;&#25152;&#36873;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#20302;&#20110;2.3%&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#65292;&#22312;&#32447;&#37096;&#32626;&#36807;&#31243;&#20013;&#38480;&#21046;&#20102;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18859v1 Announce Type: new  Abstract: The reuse of retired electric vehicle (EV) batteries in electric grid energy storage emerges as a promising strategy to address environmental concerns and boost economic value. This study concentrates on devising health monitoring algorithms for retired batteries (BMS$_2$) deployed in grid storage applications. Over 15 months of testing, we compile, analyze, and publicly share a dataset of second-life (SL) batteries, implementing a cycling protocol simulating grid energy storage load profiles within a 3 V-4 V voltage window. Four machine learning-based health estimation models, relying on BMS$_2$ features and initial capacity, are developed and compared, with the selected model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test data. Additionally, an adaptive online health estimation algorithm is proposed by integrating a clustering-based method, limiting estimation errors during online deployment. These results constit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18853</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24102;&#26377;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#30340;&#22810;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Multi-domain Generalization with A General Learning Objective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#27867;&#21270;&#65288;mDG&#65289;&#30340;&#26222;&#36941;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#22686;&#24378;&#36793;&#38469;&#21040;&#26631;&#31614;&#20998;&#24067;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;mDG&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#24120;&#23545;&#38745;&#24577;&#30446;&#26631;&#36793;&#38469;&#20998;&#24067;&#26045;&#21152;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#19968;&#20010;$Y$-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;mDG&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#26469;&#35299;&#37322;&#21644;&#20998;&#26512;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;mDG&#26234;&#24935;&#12290;&#36825;&#20010;&#36890;&#29992;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#21327;&#21516;&#30340;&#30446;&#26631;&#65306;&#23398;&#20064;&#19982;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#19968;&#20010;&#21518;&#39564;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#36825;&#20123;&#39033;&#32467;&#21512;&#20102;&#20808;&#39564;&#20449;&#24687;&#24182;&#25233;&#21046;&#20102;&#26080;&#25928;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20943;&#36731;&#20102;&#25918;&#26494;&#32422;&#26463;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20026;&#22495;&#23545;&#40784;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18853v1 Announce Type: cross  Abstract: Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#36825;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#65292;&#32467;&#21512;&#21453;&#20107;&#23454;&#20272;&#35745;&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21487;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18851</link><description>&lt;p&gt;
&#22312;&#22788;&#26041;&#21644;&#39044;&#27979;&#20013;&#24212;&#29992;0-1&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Applications of 0-1 Neural Networks in Prescription and Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18851
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#36825;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#65292;&#32467;&#21512;&#21453;&#20107;&#23454;&#20272;&#35745;&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21487;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20915;&#31574;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#26377;&#38480;&#30340;&#35266;&#23519;&#25968;&#25454;&#19979;&#23398;&#20064;&#38024;&#23545;&#24739;&#32773;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#65292;&#36825;&#26159;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#30340;&#27973;&#23618;0-1&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#20272;&#35745;&#19968;&#36215;&#22312;&#20013;&#31561;&#25968;&#25454;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#12290;&#36825;&#20123;&#27169;&#22411;&#27604;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#27604;&#24120;&#35265;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PNNs&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#20135;&#21518;&#39640;&#34880;&#21387;&#27835;&#30103;&#20998;&#37197;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;PNNs&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#21487;&#38477;&#20302;&#39640;&#34880;&#21387;&#23792;&#20540;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18851v1 Announce Type: cross  Abstract: A key challenge in medical decision making is learning treatment policies for patients with limited observational data. This challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes. To address this, we introduce prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed integer programming that can be used with counterfactual estimation to optimize policies in medium data settings. These models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees. We show that PNNs can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension. In particular, PNNs are shown to produce policies that could reduce peak bloo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20302;&#20445;&#30495;&#24230;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#32858;&#21512;&#20934;&#30830;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18846</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20302;&#20445;&#30495;&#24230;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#32858;&#21512;&#20934;&#30830;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#26367;&#20195;&#24314;&#27169;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#27700;&#24179;&#23398;&#20064;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#24456;&#38590;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21253;&#25324;&#23545;&#24212;&#35299;&#30721;&#22120;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#20849;&#20139;&#32534;&#30721;&#34920;&#31034;&#12290;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#26102;&#65292;&#29992;&#19981;&#21516;&#21442;&#25968;&#35299;&#30721;&#34920;&#31034;&#65292;&#20351;&#20849;&#20139;&#20449;&#24687;&#22266;&#26377;&#19981;&#20934;&#30830;&#12290;&#36825;&#38480;&#21046;&#20102;&#25512;&#26029;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#22495;&#35206;&#30422;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20445;&#30495;&#24230;&#26367;&#20195;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18846v1 Announce Type: new  Abstract: Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by agg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18839</link><description>&lt;p&gt;
&#25193;&#23637;&#27969;&#21305;&#37197;&#65306;&#20855;&#26377;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#33879;&#21517;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#22522;&#20110;&#24341;&#23548;&#30340;&#26080;&#20998;&#31867;&#22120;&#26041;&#27861;&#20026;&#39318;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24341;&#23548;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19981;&#20165;&#35201;&#27714;&#29992;&#25143;&#24494;&#35843;&#8220;&#24341;&#23548;&#24378;&#24230;&#8221;&#65292;&#32780;&#19988;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#19981;&#19968;&#23450;&#23545;&#24212;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#27969;&#21305;&#37197;&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#27969;&#21305;&#37197;&#26159;&#25193;&#25955;&#26041;&#27861;&#30340;&#24403;&#21069;&#24378;&#22823;&#31454;&#20105;&#32773;&#20043;&#19968;&#12290;&#21463;&#23558;&#27010;&#29575;&#36335;&#24452;&#35299;&#37322;&#20026;&#36335;&#24452;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#19981;&#26159;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#12290;&#36825;&#19968;&#29702;&#35770;&#33258;&#28982;&#22320;&#25512;&#23548;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#30340;&#25193;&#23637;&#31574;&#30053;&#25439;&#22833;&#65292;&#36890;&#36807;&#35843;&#25972;&#26435;&#37325;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.18836</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#65292;&#21033;&#29992;&#19987;&#23478;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#30340;&#25193;&#23637;&#31574;&#30053;&#25439;&#22833;&#65292;&#36890;&#36807;&#35843;&#25972;&#26435;&#37325;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#19987;&#23478;&#35266;&#23519;&#65288;&#27809;&#26377;&#26126;&#30830;&#30340;&#19987;&#23478;&#34892;&#21160;&#20449;&#24687;&#65289;&#32435;&#20837;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24418;&#25104;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#31574;&#30053;&#25439;&#22833;&#65292;&#32467;&#21512;&#20102;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#19982;&#21033;&#29992;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#33258;&#21160;&#35843;&#25972;&#25193;&#23637;&#25439;&#22833;&#20989;&#25968;&#20013;&#27599;&#20010;&#32452;&#20214;&#30340;&#26435;&#37325;&#12290;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19987;&#23478;&#35266;&#23519;&#20987;&#36133;&#20102;&#21508;&#31181;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18836v1 Announce Type: new  Abstract: This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#20808;&#39564;&#21644;&#20808;&#36827;&#20248;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#24555;&#20116;&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.18830</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#20809;&#35889;&#25968;&#25454;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Training-set-free two-stage deep learning for Spectroscopic data de-noising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#20808;&#39564;&#21644;&#20808;&#36827;&#20248;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#24555;&#20116;&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#26159;&#20809;&#35889;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24555;&#36895;&#20294;&#22823;&#22810;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#27979;&#37327;&#20013;&#21487;&#33021;&#25104;&#26412;&#36739;&#39640;&#12290;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31639;&#27861;&#36895;&#24230;&#24930;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#27169;&#31946;&#22266;&#23450;&#36755;&#20837;&#30340;&#25913;&#36827;&#65292;&#24341;&#20837;&#33258;&#36866;&#24212;&#20808;&#39564;&#12290;&#32467;&#21512;&#26356;&#20808;&#36827;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#21069;&#20154;&#24037;&#20316;&#24555;&#20116;&#20493;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30456;&#24212;&#38750;&#20984;&#32447;&#24615;&#38382;&#39064;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#19968;&#38454;&#31639;&#27861;&#25910;&#25947;&#20855;&#26377;&#33391;&#22909;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18830v1 Announce Type: cross  Abstract: De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements. Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Adam&#30340;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#29256;&#26412;&#65292;&#36890;&#36807;&#25913;&#21464;&#35745;&#31639;&#39034;&#24207;&#23454;&#29616;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#24615;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.18824</link><description>&lt;p&gt;
Adam&#30340;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Batch size invariant Adam
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Adam&#30340;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#29256;&#26412;&#65292;&#36890;&#36807;&#25913;&#21464;&#35745;&#31639;&#39034;&#24207;&#23454;&#29616;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#24615;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Adam&#30340;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35774;&#32622;&#65292;&#20854;&#20013;&#23567;&#25209;&#37327;&#34987;&#20998;&#25104;&#24494;&#25209;&#37327;&#65292;&#28982;&#21518;&#20998;&#37197;&#32473;&#24037;&#20316;&#33410;&#28857;&#12290;&#22312;&#26631;&#20934;Adam&#20013;&#65292;&#23545;v&#39033;&#39318;&#20808;&#35745;&#31639;&#24494;&#25209;&#37327;&#26799;&#24230;&#30340;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#27714;&#24179;&#26041;&#65292;&#32780;&#22312;&#36825;&#37324;&#25552;&#20986;&#30340;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#30340;Adam&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#24494;&#25209;&#37327;&#26799;&#24230;&#27714;&#24179;&#26041;&#65292;&#28982;&#21518;&#24179;&#22343;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Malladi&#31561;&#20154;2022&#24180;&#65289;&#20351;&#29992;&#19968;&#31181;&#28041;&#21450;&#23398;&#20064;&#29575;&#30340;&#24179;&#26041;&#26681;&#32553;&#25918;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#20551;&#35774;&#25165;&#33021;&#36215;&#20316;&#29992;&#65307;&#29305;&#21035;&#26159;&#26799;&#24230;&#26041;&#24046;&#20027;&#23548;&#26399;&#26395;&#26799;&#24230;&#30340;&#24179;&#26041;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36825;&#37324;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#36825;&#31181;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22810;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#25209;&#37327;&#22823;&#23567;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18824v1 Announce Type: new  Abstract: We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#27169;&#22411;&#21516;&#26102;&#35299;&#37322;&#20102;&#20219;&#21153;&#23398;&#20064;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20998;&#26512;&#20102;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.18819</link><description>&lt;p&gt;
In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dual Operating Modes of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#27169;&#22411;&#21516;&#26102;&#35299;&#37322;&#20102;&#20219;&#21153;&#23398;&#20064;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20998;&#26512;&#20102;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL)&#23637;&#31034;&#20102;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65306;&#20219;&#21153;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#33719;&#21462;&#26032;&#25216;&#33021;&#65292;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#21363;&#26597;&#25214;&#24182;&#28608;&#27963;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#33021;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#25968;&#23398;&#27169;&#22411;&#26469;&#20998;&#26512;ICL&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#19968;&#27425;&#21482;&#33021;&#35299;&#37322;&#19968;&#31181;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#37322;ICL&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#12290;&#19987;&#27880;&#20110;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#20219;&#21153;&#32452;&#21644;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20998;&#24067;&#25193;&#23637;&#29616;&#26377;&#27169;&#22411;&#29992;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#21518;&#20998;&#26512;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#34920;&#29616;&#26368;&#20248;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21363;&#32473;&#23450;&#19978;&#19979;&#25991;&#26679;&#26412;&#26631;&#31614;&#30340;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#22120;&#12290;&#23558;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#35270;&#20026;&#20808;&#39564;&#65292;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#35270;&#20026;&#35266;&#27979;&#20540;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18819v1 Announce Type: new  Abstract: In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#32676;&#20307;&#20844;&#24179;&#35757;&#32451;&#23545;&#20849;&#20139;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#36890;&#36807;&#25512;&#23548;&#32676;&#20307;&#29305;&#23450;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#20197;&#35299;&#20915;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#38382;&#39064;&#65292;&#23588;&#20854;&#23545;&#20110;&#36739;&#23567;&#30340;&#32676;&#20307;&#35268;&#27169;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18803</link><description>&lt;p&gt;
&#26159;&#21542;&#36827;&#34892;&#25968;&#25454;&#27719;&#38598;&#65306;&#20998;&#26512;&#32676;&#20307;&#20844;&#24179;&#35757;&#32451;&#23545;&#20849;&#20139;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#32676;&#20307;&#20844;&#24179;&#35757;&#32451;&#23545;&#20849;&#20139;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#36890;&#36807;&#25512;&#23548;&#32676;&#20307;&#29305;&#23450;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#20197;&#35299;&#20915;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#38382;&#39064;&#65292;&#23588;&#20854;&#23545;&#20110;&#36739;&#23567;&#30340;&#32676;&#20307;&#35268;&#27169;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23548;&#33268;&#32676;&#20307;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#23545;&#30456;&#23545;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22260;&#32469;&#22810;&#25968;&#32676;&#20307;&#26356;&#22823;&#26679;&#26412;&#37327;&#30340;&#31119;&#21033;&#20013;&#24515;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#32676;&#20307;&#29305;&#23450;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#21463;&#38480;&#20551;&#35774;&#31867;&#21035;&#19978;&#30340;&#32676;&#20307;&#29305;&#23450;Rademacher&#24179;&#22343;&#20540;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#20551;&#35774;&#31867;&#21035;&#21253;&#21547;&#22312;&#20844;&#24179;&#23398;&#20064;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#24130;&#22343;&#20540;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#26063;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#36825;&#20123;&#30028;&#38480;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#65292;&#36825;&#31526;&#21512;&#29702;&#35770;&#39044;&#26399;&#65292;&#23588;&#20854;&#23545;&#20110;&#36739;&#23567;&#30340;&#32676;&#20307;&#35268;&#27169;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18803v1 Announce Type: new  Abstract: In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training samples. We derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger sample size of the majority group. We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean). Our simulations demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlockEcho&#30340;&#26032;&#30697;&#38453;&#22635;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30697;&#38453;&#20998;&#35299;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21019;&#36896;&#24615;&#22320;&#20445;&#30041;&#20102;&#21407;&#22987;&#30697;&#38453;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#22359;&#29366;&#32570;&#22833;&#25968;&#25454;&#23545;&#25968;&#25454;&#25554;&#20540;&#21644;&#39044;&#27979;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18800</link><description>&lt;p&gt;
BlockEcho&#65306;&#20445;&#30041;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#29992;&#20110;&#22635;&#34917;&#22359;&#29366;&#32570;&#22833;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlockEcho&#30340;&#26032;&#30697;&#38453;&#22635;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30697;&#38453;&#20998;&#35299;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21019;&#36896;&#24615;&#22320;&#20445;&#30041;&#20102;&#21407;&#22987;&#30697;&#38453;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#22359;&#29366;&#32570;&#22833;&#25968;&#25454;&#23545;&#25968;&#25454;&#25554;&#20540;&#21644;&#39044;&#27979;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18800v1 &#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22359;&#29366;&#32570;&#22833;&#25968;&#25454;&#22312;&#23454;&#38469;&#25968;&#25454;&#22635;&#34917;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#19982;&#20998;&#25955;&#30340;&#32570;&#22833;&#25968;&#25454;&#30456;&#27604;&#65292;&#22359;&#29366;&#32570;&#22833;&#25968;&#25454;&#21152;&#21095;&#20102;&#23545;&#21518;&#32493;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#22240;&#20026;&#32570;&#20047;&#23616;&#37096;&#30456;&#37051;&#20803;&#32032;&#26174;&#33879;&#38477;&#20302;&#20102;&#25554;&#20540;&#33021;&#21147;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#30001;&#20110;&#36807;&#24230;&#20381;&#36182;&#37051;&#36817;&#20803;&#32032;&#36827;&#34892;&#39044;&#27979;&#65292;&#22823;&#22810;&#25968;SOTA&#30697;&#38453;&#22635;&#20805;&#26041;&#27861;&#26174;&#31034;&#20986;&#36739;&#20302;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30697;&#38453;&#22635;&#20805;&#26041;&#27861;&#8220;BlockEcho&#8221;&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#21019;&#36896;&#24615;&#22320;&#23558;&#30697;&#38453;&#20998;&#35299;&#65288;MF&#65289;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#26126;&#30830;&#20445;&#30041;&#21407;&#22987;&#30697;&#38453;&#20013;&#30340;&#38271;&#36317;&#31163;&#20803;&#32032;&#38388;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;GAN&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#37492;&#21035;&#22120;&#65292;&#27604;&#36739;&#29983;&#25104;&#22120;&#30340;&#20013;&#38388;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18800v1 Announce Type: new  Abstract: Block-wise missing data poses significant challenges in real-world data imputation tasks. Compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power. However, this issue has not received adequate attention. Most SOTA matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions. We systematically analyze the issue and propose a novel matrix completion method ``BlockEcho" for a more comprehensive solution. This method creatively integrates Matrix Factorization (MF) within Generative Adversarial Networks (GAN) to explicitly retain long-distance inter-element relationships in the original matrix. Besides, we incorporate an additional discriminator for GAN, comparing the generator's intermediate progre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#26469;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.18792</link><description>&lt;p&gt;
MPAT: &#25239;&#20987;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18792
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#26469;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#38450;&#24481;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#20445;&#25345;&#26377;&#25928;&#30340;&#38450;&#24481;&#21516;&#26102;&#30830;&#20445;&#21407;&#22987;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;MPAT&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#25269;&#24481;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#32423;&#24694;&#24847;&#31034;&#20363;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#29983;&#25104;&#24102;&#26377;&#24694;&#24847;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#34987;&#29992;&#26469;&#20195;&#26367;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21407;&#22987;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#36798;&#21040;&#38450;&#24481;&#30446;&#26631;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#20116;&#20010;v
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18792v1 Announce Type: cross  Abstract: Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five v
&lt;/p&gt;</description></item><item><title>FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;</title><link>https://arxiv.org/abs/2402.18789</link><description>&lt;p&gt;
FlexLLM&#65306;&#19968;&#31181;&#29992;&#20110;&#20849;&#21516;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18789
&lt;/p&gt;
&lt;p&gt;
FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter-efficient finetuning&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#20250;&#20026;&#29992;&#25143;&#21019;&#24314;&#21333;&#29420;&#30340;&#31995;&#32479;&#65292;&#20197;&#25191;&#34892;PEFT&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#21253;&#21547;&#25512;&#29702;&#21644;PEFT&#24494;&#35843;&#35831;&#27714;&#28151;&#21512;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20026;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#26469;&#20849;&#21516;&#36816;&#34892;&#23427;&#20204;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#25552;&#20379;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;FlexLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#65292;&#23558;&#24207;&#21015;&#30340;&#24494;&#35843;&#35745;&#31639;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#26631;&#35760;&#32423;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20813;&#30123;&#21147;&#8221;(Immunity)&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26550;&#26500;&#65292;&#32467;&#21512;&#38543;&#26426;&#20999;&#25442;&#38376;(RSGs)&#21644;&#22522;&#20110;&#20114;&#20449;&#24687;(MI)&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#21152;&#20102;&#19987;&#23478;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.18787</link><description>&lt;p&gt;
&#25552;&#21319;&#28151;&#21512;&#19987;&#23478;&#32593;&#32476;&#30340;&#8220;&#20813;&#30123;&#21147;&#8221;&#20197;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Enhancing the "Immunity" of Mixture-of-Experts Networks for Adversarial Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20813;&#30123;&#21147;&#8221;(Immunity)&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26550;&#26500;&#65292;&#32467;&#21512;&#38543;&#26426;&#20999;&#25442;&#38376;(RSGs)&#21644;&#22522;&#20110;&#20114;&#20449;&#24687;(MI)&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#21152;&#20102;&#19987;&#23478;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#21487;&#20197;&#36731;&#26494;&#22320;&#24858;&#24324;DNNs&#65292;&#20351;&#20854;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#32570;&#38519;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#25913;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26550;&#26500;&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#31216;&#20026;&#8220;&#20813;&#30123;&#21147;&#8221;(Innovative MoE with MUtual information \&amp; positioN stabilITY)&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#25913;&#36827;&#26377;&#20004;&#26041;&#38754;&#65306;1)&#38598;&#25104;&#38543;&#26426;&#20999;&#25442;&#38376;(RSGs)&#65292;&#22312;&#35780;&#20272;&#26102;&#36890;&#36807;&#38543;&#26426;&#25490;&#21015;RSG&#21442;&#25968;&#33719;&#24471;&#22810;&#26679;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#23613;&#31649;RSGs&#22312;&#32463;&#36807;&#19968;&#27425;&#35757;&#32451;&#21518;&#30830;&#23450;&#65307;2)&#21033;&#29992;Grad-CAM&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#20114;&#20449;&#24687;(MI)&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#21152;&#19987;&#23478;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;MI&#30340;&#25439;&#22833;&#30452;&#25509;&#22312;&#28909;&#22270;&#19978;&#36816;&#34892;&#65292;&#20174;&#32780;&#35825;&#23548;&#20986;&#26356;&#24494;&#22937;&#30340;&#36127;&#21521;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18787v1 Announce Type: new  Abstract: Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called "Immunity" (Innovative MoE with MUtual information \&amp; positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.18781</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#32423;&#20449;&#24565;&#30340;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#22312;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#20986;&#29616;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#22914;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;IT&#22522;&#30784;&#35774;&#26045;&#65292;&#20449;&#24687;&#19981;&#23545;&#31216;&#20026;&#20915;&#31574;&#23454;&#20307;&#65288;&#29609;&#23478;&#65289;&#30340;&#20915;&#31574;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#65288;AISG&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#26159;&#31163;&#32447;&#30340;&#65292;&#38024;&#23545;&#29305;&#27530;&#31867;&#21035;&#30340;AISG&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#23618;&#27425;&#65292;&#24182;&#19988;&#32570;&#20047;&#36866;&#24212;&#22343;&#34913;&#20559;&#24046;&#30340;&#22312;&#32447;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#19987;&#38376;&#38024;&#23545;&#36890;&#29992;AISG&#12290;COL&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#12290;&#38024;&#23545;&#20551;&#35774;&#30340;&#23545;&#25163;&#65292;COL&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;COL&#20013;&#30340;&#20551;&#35774;&#19982;t&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#21407;&#22240;&#65292;&#25506;&#35752;&#20102;&#19981;&#31283;&#23450;&#24615;&#30340;&#26681;&#28304;&#20197;&#21450;&#22914;&#20309;&#32467;&#21512;&#20943;&#36731;&#31574;&#30053;&#20197;&#20445;&#25345;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18762</link><description>&lt;p&gt;
&#35299;&#24320;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Causes of Plasticity Loss in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#21407;&#22240;&#65292;&#25506;&#35752;&#20102;&#19981;&#31283;&#23450;&#24615;&#30340;&#26681;&#28304;&#20197;&#21450;&#22914;&#20309;&#32467;&#21512;&#20943;&#36731;&#31574;&#30053;&#20197;&#20445;&#25345;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#12289;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#26159;&#19968;&#20010;&#30475;&#20284;&#26080;&#20851;&#32039;&#35201;&#30340;&#20551;&#35774;&#65306;&#32593;&#32476;&#26159;&#22312;&#19968;&#20010;\textit{&#22266;&#23450;&#30340;}&#25968;&#25454;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#36829;&#21453;&#36825;&#19968;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#29978;&#33267;&#38543;&#26426;&#31181;&#23376;&#21464;&#24471;&#19981;&#31283;&#23450;&#21644;&#33030;&#24369;&#12290;&#23548;&#33268;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#65292;&#36825;&#24847;&#21619;&#30528;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#65292;&#26356;&#26032;&#32593;&#32476;&#30340;&#39044;&#27979;&#20197;&#24212;&#23545;&#26032;&#20449;&#24687;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#36825;&#31181;&#29616;&#35937;&#36827;&#34892;&#20102;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#22238;&#31572;&#65306;&#24050;&#30693;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#26426;&#21046;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#21472;&#65292;&#20197;&#21450;&#22914;&#20309;&#32467;&#21512;&#20943;&#36731;&#31574;&#30053;&#25165;&#33021;&#26368;&#22909;&#22320;&#32500;&#25345;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#24615;&#65311;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21487;&#22609;&#24615;&#20007;&#22833;&#21487;&#20197;&#34987;&#35299;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18762v1 Announce Type: new  Abstract: Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \textit{stationary} data distribution. In settings where this assumption is violated, e.g.\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be dec
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18759</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#29366;&#24577;&#25277;&#35937;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Language-Guided State Abstractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18759
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35774;&#35745;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#22312;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#23454;&#29616;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#23558;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#23637;&#29616;&#20986;&#26469;&#24182;&#38544;&#34255;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#26159;&#25163;&#21160;&#25351;&#23450;&#30340;&#65292;&#25110;&#32773;&#26159;&#20174;&#20854;&#20182;&#32321;&#37325;&#30340;&#26631;&#35760;&#36807;&#31243;&#20013;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;LGA&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#32467;&#21512;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;LGA&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30446;&#26631;&#20219;&#21153;&#30340;&#65288;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#65289;&#25551;&#36848;&#65307;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#36825;&#20010;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#25513;&#30422;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#29366;&#24577;&#25277;&#35937;&#20989;&#25968;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;&#23569;&#37327;&#28436;&#31034;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18759v1 Announce Type: cross  Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.18752</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#20844;&#20849;&#25968;&#25454;&#23545;&#26377;&#24046;&#24322;&#38544;&#31169;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Differentially Private Models with Limited Public Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18752
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#21331;&#36234;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#38656;&#35201;&#27491;&#24335;&#20445;&#25252;&#30340;&#25935;&#24863;&#12289;&#31169;&#20154;&#21644;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#25552;&#20379;&#30340;&#23433;&#20840;&#31243;&#24230;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#28982;&#32780;&#30001;&#20110;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#20854;&#24212;&#29992;&#36890;&#24120;&#20165;&#38480;&#20110;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#30446;&#21069;&#23578;&#19981;&#33021;&#20445;&#25252;&#21021;&#22987;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#27599;&#27425;&#36845;&#20195;&#30340;&#25439;&#22833;&#25913;&#36827;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#24341;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#36890;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18752v1 Announce Type: new  Abstract: The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, usi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32467;&#21512;&#22810;&#27169;&#24335;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#24555;&#36895;&#20998;&#31867;&#22823;&#35910;&#24178;&#26097;&#32961;&#36843;&#30151;&#29366;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24178;&#26097;&#32961;&#36843;&#30340;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18751</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#21644;&#22810;&#26102;&#38388;&#39640;&#36890;&#37327;&#34920;&#22411;&#37492;&#23450;&#25216;&#26415;&#29992;&#20110;&#30416;&#32961;&#36843;&#30417;&#27979;&#21644;&#26089;&#26399;&#26816;&#27979;&#22823;&#35910;&#27700;&#20998;&#38480;&#21046;&#24615;&#32961;&#36843;
&lt;/p&gt;
&lt;p&gt;
Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32467;&#21512;&#22810;&#27169;&#24335;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#24555;&#36895;&#20998;&#31867;&#22823;&#35910;&#24178;&#26097;&#32961;&#36843;&#30151;&#29366;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24178;&#26097;&#32961;&#36843;&#30340;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35910;&#29983;&#20135;&#23481;&#26131;&#21463;&#21040;&#29983;&#29289;&#24615;&#21644;&#38750;&#29983;&#29289;&#24615;&#32961;&#36843;&#30340;&#24433;&#21709;&#65292;&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#21152;&#21095;&#19979;&#26356;&#26159;&#22914;&#27492;&#12290;&#27700;&#20998;&#38480;&#21046;&#24615;&#32961;&#36843;&#65292;&#21363;&#24178;&#26097;&#65292;&#25104;&#20026;&#22823;&#35910;&#29983;&#20135;&#19968;&#20010;&#37325;&#35201;&#39118;&#38505;&#65292;&#24378;&#35843;&#20102;&#23545;&#20316;&#29289;&#32946;&#31181;&#21644;&#29983;&#20135;&#20013;&#32961;&#36843;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;&#26412;&#39033;&#30446;&#32467;&#21512;&#22810;&#27169;&#24335;&#20449;&#24687;&#65292;&#30830;&#23450;&#20102;&#26368;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#24178;&#26097;&#21709;&#24212;&#12290;&#25105;&#20204;&#20197;&#22810;&#20256;&#24863;&#22120;&#21644;&#26102;&#38388;&#24207;&#21015;&#39640;&#36890;&#37327;&#34920;&#22411;&#26041;&#24335;&#35843;&#26597;&#20102;&#19968;&#32452;&#19981;&#21516;&#30340;&#22823;&#35910;&#31181;&#36136;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#24320;&#21457;&#29992;&#20110;&#24555;&#36895;&#20998;&#31867;&#22823;&#35910;&#24178;&#26097;&#32961;&#36843;&#30151;&#29366;&#30340;&#27969;&#31243;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30740;&#31350;&#24178;&#26097;&#32961;&#36843;&#26089;&#26399;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#36890;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#22411;&#25216;&#26415;&#65292;&#32467;&#21512;&#26080;&#20154;&#26426;&#21644;&#20256;&#24863;&#22120;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#34920;&#22411;&#26041;&#27861;&#12290;&#32418;&#32536;&#21644;&#32511;&#27874;&#27573;&#23545;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18751v1 Announce Type: new  Abstract: Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events. Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production. This project combines multi-modal information to identify the most effective and efficient automated methods to investigate drought response. We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress. We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping. The red-edge and green bands were most effective to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#27169;&#25311;&#65292;&#21033;&#29992;&#24212;&#29992;&#31243;&#24207;&#29305;&#24449;&#21644;&#24494;&#20307;&#31995;&#32467;&#26500;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#39044;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20307;&#31995;&#32467;&#26500;&#25506;&#32034;&#30340;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#39044;&#27979;IPC&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18746</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accelerating Computer Architecture Simulation through Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18746
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#27169;&#25311;&#65292;&#21033;&#29992;&#24212;&#29992;&#31243;&#24207;&#29305;&#24449;&#21644;&#24494;&#20307;&#31995;&#32467;&#26500;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#39044;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20307;&#31995;&#32467;&#26500;&#25506;&#32034;&#30340;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#39044;&#27979;IPC&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#27169;&#25311;&#32791;&#26102;&#36739;&#38271;&#65292;&#20351;&#24471;&#39640;&#25928;&#22320;&#25506;&#32034;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#24212;&#29992;&#31243;&#24207;&#29305;&#24449;&#21644;&#24494;&#20307;&#31995;&#32467;&#26500;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#29305;&#24449;&#26469;&#28304;&#20110;&#24212;&#29992;&#31243;&#24207;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#27169;&#25311;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21644;&#35780;&#20272;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#27169;&#22411;&#22312;&#20307;&#31995;&#32467;&#26500;&#25506;&#32034;&#20013;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#39044;&#27979;IPC&#20540;&#30340;&#33021;&#21147;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#23567;&#20110;0.1&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18746v1 Announce Type: cross  Abstract: This paper presents our approach to accelerate computer architecture simulation by leveraging machine learning techniques. Traditional computer architecture simulations are time-consuming, making it challenging to explore different design choices efficiently. Our proposed model utilizes a combination of application features and micro-architectural features to predict the performance of an application. These features are derived from simulations of a small portion of the application. We demonstrate the effectiveness of our approach by building and evaluating a machine learning model that offers significant speedup in architectural exploration. This model demonstrates the ability to predict IPC values for the testing data with a root mean square error of less than 0.1.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#25353;&#29031;&#27169;&#22411;&#20449;&#24515;&#24230;&#20135;&#29983;&#21807;&#19968;&#26679;&#26412;&#65292;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26102;&#34920;&#29616;&#20248;&#20110;&#26680;&#37319;&#26679;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18734</link><description>&lt;p&gt;
&#32534;&#35793;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#20808;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Priority Sampling of Large Language Models for Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#25353;&#29031;&#27169;&#22411;&#20449;&#24515;&#24230;&#20135;&#29983;&#21807;&#19968;&#26679;&#26412;&#65292;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26102;&#34920;&#29616;&#20248;&#20110;&#26680;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#27604;&#22914;&#26680;&#37319;&#26679;&#65288;Nucleus Sampling&#65289;&#65292;&#22686;&#21152;&#20102;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#22312;&#20302;&#28201;&#24230;&#19979;&#32463;&#24120;&#20135;&#29983;&#37325;&#22797;&#30340;&#26679;&#26412;&#65292;&#22312;&#39640;&#28201;&#24230;&#19979;&#20135;&#29983;&#19981;&#36830;&#36143;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#28201;&#24230;&#31995;&#25968;&#24517;&#39035;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#37319;&#26679;&#65288;Priority Sampling&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#23427;&#20135;&#29983;&#25353;&#27169;&#22411;&#32622;&#20449;&#24230;&#25490;&#24207;&#30340;&#21807;&#19968;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#26679;&#26412;&#37117;&#20250;&#25193;&#23637;&#25193;&#23637;&#25628;&#32034;&#26641;&#20013;&#27010;&#29575;&#26368;&#39640;&#30340;&#26410;&#25193;&#23637;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#20248;&#20808;&#37319;&#26679;&#25903;&#25345;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#65292;&#25552;&#20379;&#21487;&#25511;&#21644;&#32467;&#26500;&#21270;&#30340;&#25506;&#32034;&#36807;&#31243;&#12290;&#20248;&#20808;&#37319;&#26679;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#26680;&#37319;&#26679;&#65292;&#23558;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;2.87%&#25552;&#21319;&#33267;5%&#36229;&#36807;-Oz&#12290;&#27492;&#22806;&#65292;&#23427;&#36229;&#36807;&#20102;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18734v1 Announce Type: cross  Abstract: Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the auto
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;GAIA&#65292;&#37319;&#29992;&#23618;&#27425;&#27169;&#22411;&#21644;&#21333;&#32431;&#22797;&#21512;&#20307;&#32452;&#32455;&#27169;&#22359;&#65292;&#23558;&#21442;&#25968;&#26356;&#26032;&#24314;&#27169;&#20026;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#65292;&#24182;&#37319;&#29992;&#20313;&#20195;&#25968;&#30340;&#24418;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18732</link><description>&lt;p&gt;
GAIA: &#29983;&#25104;AI&#30340;&#33539;&#30068;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
GAIA: Categorical Foundations of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;GAIA&#65292;&#37319;&#29992;&#23618;&#27425;&#27169;&#22411;&#21644;&#21333;&#32431;&#22797;&#21512;&#20307;&#32452;&#32455;&#27169;&#22359;&#65292;&#23558;&#21442;&#25968;&#26356;&#26032;&#24314;&#27169;&#20026;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#65292;&#24182;&#37319;&#29992;&#20313;&#20195;&#25968;&#30340;&#24418;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAIA&#65292;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;&#12290;GAIA&#22522;&#20110;&#19968;&#20010;&#23618;&#27425;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22359;&#34987;&#32452;&#32455;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#12290;&#27599;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#26681;&#25454;&#20174;&#20854;&#19978;&#32423;&#21333;&#32431;&#20307;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#26356;&#26032;&#20854;&#20869;&#37096;&#21442;&#25968;&#65292;&#24182;&#23558;&#26356;&#26032;&#20256;&#36882;&#32473;&#20854;&#19979;&#32423;&#23376;&#21333;&#32431;&#20307;&#12290;&#21442;&#25968;&#26356;&#26032;&#20197;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#30340;&#24418;&#24335;&#36827;&#34892;&#65292;&#20854;&#20013;&#20869;&#37096;&#21644;&#22806;&#37096;&#35282;&#25193;&#23637;&#23545;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#21453;&#21521;&#20256;&#25773;&#34987;&#24314;&#27169;&#20026;&#21442;&#25968;&#33539;&#30068;&#19978;&#30340;&#19968;&#20010;&#33258;&#20989;&#23376;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20313;&#20195;&#25968;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18732v1 Announce Type: new  Abstract: In this paper, we propose GAIA, a generative AI architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.18729</link><description>&lt;p&gt;
&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#28237;&#27969;&#23553;&#38381;&#27169;&#22411;&#36827;&#34892;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18729
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20026;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#20013;&#30340;&#23376;&#28388;&#27874;&#23610;&#24230;&#65288;SFS&#65289;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#29289;&#29702;&#30340;&#23553;&#38381;&#27169;&#22411;&#24418;&#24335;&#65292;&#20294;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#25552;&#20379;&#30340;&#22823;&#37327;&#25968;&#25454;&#20026;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#25216;&#26415;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#28789;&#27963;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20173;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#37319;&#29992;&#36825;&#31181;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#21487;&#38752;&#22320;&#20272;&#35745;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#25968;&#25454;&#30693;&#35782;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#26469;&#25429;&#25417;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#36923;&#36753;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28388;&#27874;&#36827;&#23637;&#21464;&#37327;&#26631;&#37327;&#32791;&#25955;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BNN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#39537;&#21160;&#23553;&#38381;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32467;&#26500;&#30340;&#29420;&#29305;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18729v1 Announce Type: cross  Abstract: While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.18726</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Unveiling Privacy, Memorization, and Input Curvature Links
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#25104;&#20026;&#35299;&#20915;&#35768;&#22810;&#26032;&#20852;&#38382;&#39064;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#21644;&#35760;&#24518;&#35757;&#32451;&#38598;&#12290;&#35760;&#24518;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19982;&#35832;&#22810;&#27010;&#24565;&#22914;&#27867;&#21270;&#12289;&#26377;&#22122;&#23398;&#20064;&#21644;&#38544;&#31169;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#65288;&#36890;&#36807;&#25439;&#22833;Hessian&#30697;&#38453;&#23545;&#36755;&#20837;&#30340;&#36857;&#36827;&#34892;&#27979;&#37327;&#65289;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#32463;&#39564;&#24615;&#32852;&#31995;&#12290;&#23427;&#34987;&#35777;&#26126;&#27604;&#35745;&#31639;&#35760;&#24518;&#20998;&#25968;&#35201;&#39640;&#25928;&#32422;3&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23558;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#32852;&#31995;&#36215;&#26469;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#26412;&#25991;&#19981;&#20165;&#30740;&#31350;&#20102;&#36825;&#31181;&#32852;&#31995;&#65292;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18726v1 Announce Type: cross  Abstract: Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. F
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#21644;&#35823;&#24046;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18724</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20851;&#32852;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Learning Associative Memories with Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#21644;&#35823;&#24046;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#23384;&#20648;&#26631;&#35760;&#23884;&#20837;&#30340;&#22806;&#31215;&#30340;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#30740;&#31350;&#19968;&#20010;&#31890;&#23376;&#31995;&#32479;&#65292;&#36825;&#20123;&#31890;&#23376;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#20197;&#21450;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#8220;&#20998;&#31867;&#36793;&#30028;&#8221;&#30340;&#23545;&#25968;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#26631;&#35760;&#39057;&#29575;&#30340;&#19981;&#24179;&#34913;&#21644;&#30001;&#30456;&#20851;&#23884;&#20837;&#23548;&#33268;&#30340;&#20869;&#23384;&#24178;&#25200;&#20250;&#23548;&#33268;&#25391;&#33633;&#30340;&#30636;&#24577;&#21306;&#22495;&#12290;&#25391;&#33633;&#22312;&#27493;&#38271;&#36739;&#22823;&#26102;&#26356;&#20026;&#26126;&#26174;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#33391;&#24615;&#25439;&#22833;&#23792;&#65292;&#23613;&#31649;&#36825;&#20123;&#23398;&#20064;&#29575;&#21152;&#36895;&#20102;&#21160;&#24577;&#24182;&#21152;&#36895;&#20102;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#22914;&#20309;&#23548;&#33268;&#27425;&#20248;&#30340;&#35760;&#24518;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#22312;&#23567;&#35268;&#27169;Tr&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18724v1 Announce Type: cross  Abstract: This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Tr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20026;IPF&#30340;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#32467;&#26500;&#30456;&#20851;&#30340;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.18697</link><description>&lt;p&gt;
&#20174;&#36793;&#38469;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#30340;&#26041;&#27861;&#65306;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18697
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20026;IPF&#30340;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#32467;&#26500;&#30456;&#20851;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#29616;&#23454;&#25968;&#25454;&#32422;&#26463;&#30340;&#24120;&#35265;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#26102;&#38388;&#32858;&#21512;&#30340;&#37051;&#25509;&#30697;&#38453;&#21644;&#26102;&#38388;&#21464;&#21270;&#36793;&#38469;&#65288;&#21363;&#34892;&#21521;&#37327;&#21644;&#21015;&#21521;&#37327;&#20043;&#21644;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#21033;&#29992;&#20102;&#32463;&#20856;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#36807;&#31243;&#65292;&#20063;&#31216;&#20026;Sinkhorn&#31639;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;IPF&#30340;&#32479;&#35745;&#22522;&#30784;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65306;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#65292;IPF&#25552;&#20379;&#20102;&#20174;&#36793;&#38469;&#20934;&#30830;&#20272;&#35745;&#21160;&#24577;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20272;&#35745;&#20102;&#32593;&#32476;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#32622;&#65292;&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#20854;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26032;&#30340;&#20998;&#26512;&#65292;&#22914;&#26377;&#20851;IPF&#21442;&#25968;&#20272;&#35745;&#30340;&#32467;&#26500;&#30456;&#20851;&#35823;&#24046;&#30028;&#12290;&#24403;IPF&#22833;&#36133;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18697v1 Announce Type: cross  Abstract: A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to c
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#31532;&#19977;&#32500;&#24230;&#65292;&#23558;ROC&#26354;&#32447;&#25552;&#21319;&#20026;ROC&#26354;&#38754;&#65292;&#25552;&#20986;VOROS&#20316;&#20026;2D ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#30340;3D&#27867;&#21270;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18689</link><description>&lt;p&gt;
VOROS&#65306;&#23558;ROC&#26354;&#32447;&#25552;&#21319;&#21040;3D
&lt;/p&gt;
&lt;p&gt;
The VOROS: Lifting ROC curves to 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18689
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#31532;&#19977;&#32500;&#24230;&#65292;&#23558;ROC&#26354;&#32447;&#25552;&#21319;&#20026;ROC&#26354;&#38754;&#65292;&#25552;&#20986;VOROS&#20316;&#20026;2D ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#30340;3D&#27867;&#21270;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#26159;&#19968;&#20010;&#24120;&#29992;&#30340;&#24230;&#37327;&#65292;&#36890;&#24120;&#29992;&#20110;&#25490;&#21015;&#19981;&#21516;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#20197;&#21069;&#25152;&#25351;&#20986;&#30340;&#65292;&#24403;&#30495;&#23454;&#31867;&#20540;&#25110;&#35823;&#20998;&#31867;&#25104;&#26412;&#22312;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#39640;&#24230;&#19981;&#24179;&#34913;&#26102;&#65292;&#23427;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#25928;&#30410;&#12290;&#25105;&#20204;&#24341;&#20837;&#31532;&#19977;&#32500;&#26469;&#25429;&#33719;&#36825;&#20123;&#25104;&#26412;&#65292;&#24182;&#20197;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#23558;ROC&#26354;&#32447;&#25552;&#21319;&#20026;ROC&#26354;&#38754;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#26354;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;VOROS&#65292;&#21363;ROC&#26354;&#38754;&#19978;&#26041;&#30340;&#20307;&#31215;&#65292;&#20316;&#20026;2D ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#30340;3D&#27867;&#21270;&#12290;&#23545;&#20110;&#23384;&#22312;&#39044;&#26399;&#25104;&#26412;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#36793;&#30028;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38480;&#21046;&#32771;&#34385;&#36866;&#24403;&#23376;&#21306;&#22495;&#30340;ROC&#26354;&#38754;&#30340;&#20307;&#31215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VOROS&#22914;&#20309;&#22312;&#32463;&#20856;&#21644;&#29616;&#20195;&#31034;&#20363;&#25968;&#25454;&#38598;&#19978;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18689v1 Announce Type: new  Abstract: The area under the ROC curve is a common measure that is often used to rank the relative performance of different binary classifiers. However, as has been also previously noted, it can be a measure that ill-captures the benefits of different classifiers when either the true class values or misclassification costs are highly unbalanced between the two classes. We introduce a third dimension to capture these costs, and lift the ROC curve to a ROC surface in a natural way. We study both this surface and introduce the VOROS, the volume over this ROC surface, as a 3D generalization of the 2D area under the ROC curve. For problems where there are only bounds on the expected costs or class imbalances, we restrict consideration to the volume of the appropriate subregion of the ROC surface. We show how the VOROS can better capture the costs of different classifiers on both a classical and a modern example dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.18668</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#24179;&#34913;&#20102;&#21484;&#22238;-&#21534;&#21520;&#37327;&#30340;&#25240;&#34935;
&lt;/p&gt;
&lt;p&gt;
Simple linear attention language models balance the recall-throughput tradeoff
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#21484;&#22238;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#24050;&#32463;&#30475;&#21040;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21463;&#21040;KV-cache&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#29942;&#39048;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65288;&#20363;&#22914;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65289;&#32780;&#19981;&#24433;&#21709;&#21484;&#22238;&#12290;&#36890;&#36807;&#23558;&#23454;&#39564;&#21644;&#29702;&#35770;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29366;&#24577;&#22823;&#23567;&#21644;&#21484;&#22238;&#33021;&#21147;&#20043;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65288;&#20363;&#22914;H3&#12289;Mamba&#12289;RWKV&#65289;&#20445;&#25345;&#22266;&#23450;&#22823;&#23567;&#30340;&#24490;&#29615;&#29366;&#24577;&#65292;&#20294;&#22312;&#21484;&#22238;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BASED&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32447;&#24615;&#21644;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#31616;&#21333;&#26550;&#26500;&#12290;&#36890;&#36807;&#25913;&#21464;BASED&#31383;&#21475;&#22823;&#23567;&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24449;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#29366;&#24577;&#22823;&#23567;&#65292;&#24182;&#36941;&#21382;&#21484;&#22238;-&#20869;&#23384;&#25240;&#34935;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18668v1 Announce Type: new  Abstract: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#29305;&#24449;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#20542;&#21521;&#65292;&#20026;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18651</link><description>&lt;p&gt;
&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Quantifying Human Priors over Social and Navigation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#29305;&#24449;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#20542;&#21521;&#65292;&#20026;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30693;&#35782;&#20027;&#35201;&#26159;&#38544;&#21547;&#30340;&#21644;&#20851;&#31995;&#22411;&#30340; &#8212;&#8212; &#25105;&#20204;&#26159;&#21542;&#26377;&#20849;&#21516;&#30340;&#26379;&#21451;&#65311;&#25105;&#33021;&#20174;&#36825;&#37324;&#36208;&#21040;&#37027;&#37324;&#21527;&#65311;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#36825;&#31181;&#20851;&#31995;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30528;&#37325;&#20110;&#20004;&#20010;&#22312;&#36827;&#21270;&#26102;&#38388;&#23610;&#24230;&#19978;&#25345;&#32493;&#30456;&#20851;&#30340;&#39046;&#22495;&#65306;&#31038;&#20132;&#20114;&#21160;&#21644;&#31354;&#38388;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#25512;&#26029;&#24471;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#29305;&#24449;&#38750;&#24120;&#19968;&#33268;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#20542;&#21521;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#21464;&#21270;&#12290;&#20854;&#20182;&#29305;&#24449;&#26159;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#65292;&#20363;&#22914;&#31038;&#20132;&#20114;&#21160;&#20013;&#30340;&#19977;&#20803;&#38381;&#21512;&#20542;&#21521;&#12290;&#26356;&#24191;&#27867;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38388;&#25509;&#34892;&#20026;&#23454;&#39564;&#30340;&#38750;&#32463;&#20856;&#32479;&#35745;&#20998;&#26512;&#26469;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18651v1 Announce Type: cross  Abstract: Human knowledge is largely implicit and relational -- do we have a friend in common? can I walk from here to there? In this work, we leverage the combinatorial structure of graphs to quantify human priors over such relational data. Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of graph size. Other features are domain-specific, such as the propensity for triadic closure in social interactions. More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#20256;&#32479;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22810;&#31449;&#23450;&#20301;&#36807;&#31243;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;GNSS&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.18630</link><description>&lt;p&gt;
&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#35843;&#33410;&#22810;&#31449;&#23450;&#20301;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;GNSS&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18630
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#20256;&#32479;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22810;&#31449;&#23450;&#20301;&#36807;&#31243;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;GNSS&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;GNSS&#21355;&#26143;&#30340;&#35270;&#36317;&#20449;&#21495;&#32463;&#24120;&#34987;&#39640;&#23618;&#24314;&#31569;&#29289;&#38459;&#25377;&#65292;&#23548;&#33268;GNSS&#25509;&#25910;&#26426;&#22312;&#27979;&#37327;&#21355;&#26143;&#36317;&#31163;&#26102;&#20135;&#29983;&#36739;&#22823;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#20195;&#20102;&#20256;&#32479;&#30340;&#35823;&#24046;&#20272;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#31449;&#23450;&#20301;&#36807;&#31243;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20272;&#35745;&#35823;&#24046;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#22312;&#35823;&#24046;&#20272;&#35745;&#20934;&#30830;&#24230;&#25552;&#39640;&#26102;&#65292;&#22810;&#31449;&#23450;&#20301;&#23558;&#25910;&#25947;&#21040;&#25509;&#25910;&#26426;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#22810;&#20010;&#22478;&#24066;&#12289;&#19981;&#21516;&#29305;&#24615;&#30340;10&#19975;&#22810;&#20010;GNSS&#21382;&#20803;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#27700;&#24179;&#23450;&#20301;&#35823;&#24046;&#30340;&#25913;&#36827;&#33539;&#22260;&#20026;40%&#21040;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18630v1 Announce Type: new  Abstract: In urban environments, where line-of-sight signals from GNSS satellites are frequently blocked by high-rise objects, GNSS receivers are subject to large errors in measuring satellite ranges. Heuristic methods are commonly used to estimate these errors and reduce the impact of noisy measurements on localization accuracy. In our work, we replace these error estimation heuristics with a deep learning model based on Graph Neural Networks. Additionally, by analyzing the cost function of the multilateration process, we derive an optimal method to utilize the estimated errors. Our approach guarantees that the multilateration converges to the receiver's location as the error estimation accuracy increases. We evaluate our solution on a real-world dataset containing more than 100k GNSS epochs, collected from multiple cities with diverse characteristics. The empirical results show improvements from 40% to 80% in the horizontal localization error ag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31163;&#32447;&#23398;&#20064;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18617</link><description>&lt;p&gt;
ELA&#65306;&#38646;&#21644;&#21338;&#24328;&#20013;&#34701;&#20837;&#21033;&#29992;&#27700;&#24179;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31163;&#32447;&#23398;&#20064;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#30001;&#20110;&#33021;&#22815;&#20174;&#19987;&#23478;&#31034;&#33539;&#32773;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#26377;&#25928;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#19987;&#19994;&#27700;&#24179;&#25110;&#22810;&#20010;&#31034;&#33539;&#32773;&#65289;&#26469;&#22686;&#24378;&#31163;&#32447;&#23398;&#20064;&#25928;&#29575;&#30340;&#21508;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#21644;&#21338;&#24328;&#30340;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#32467;&#26524;&#26681;&#25454;&#23545;&#25163;&#30340;&#31574;&#30053;&#32780;&#26174;&#33879;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#30001;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20272;&#35745;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#32467;&#21512;&#21040;&#31163;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#31034;&#33539;&#32773;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#21487;&#35299;&#37322;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18617v1 Announce Type: cross  Abstract: Offline learning has become widely used due to its ability to derive effective policies from offline datasets gathered by expert demonstrators without interacting with the environment directly. Recent research has explored various ways to enhance offline learning efficiency by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset. However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent. In this study, we introduce a novel approach that uses unsupervised learning techniques to estimate the exploited level of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators. Subsequently, we incorporate the estimated exploited level into the offline learning to maximize the influence of the dominant strategy. Our method enables interpretable exploited level estimation in multiple z
&lt;/p&gt;</description></item><item><title>DNN&#27169;&#22411;&#21033;&#29992;ETF&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36328;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18614</link><description>&lt;p&gt;
&#29992;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18614
&lt;/p&gt;
&lt;p&gt;
DNN&#27169;&#22411;&#21033;&#29992;ETF&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36328;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#65288;NC&#65289;&#29616;&#35937;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20250;&#25910;&#25947;&#21040;&#25152;&#35859;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#21333;&#32431;&#24418;&#12290;&#36825;&#31181;ETF&#20960;&#20309;&#24418;&#29366;&#30456;&#24403;&#20110;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#30340;&#31867;&#20869;&#21464;&#21270;&#28040;&#22833;&#12290;&#21463;NC&#23646;&#24615;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#22266;&#23450;&#20026;ETF&#30340;DNN&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#36890;&#36807;&#28040;&#38500;&#31867;&#21327;&#26041;&#24046;&#20449;&#24687;&#24378;&#21270;&#20102;&#31867;&#21035;&#20998;&#31163;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#31181;&#22266;&#23450;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#36716;&#31227;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#24191;&#27867;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#27809;&#26377;&#25191;&#34892;&#20219;&#20309;&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#30340;&#22522;&#32447;&#26041;&#27861;&#65288;&#39640;&#36798;22%&#65289;&#65292;&#20197;&#21450;&#26126;&#30830;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18614v1 Announce Type: new  Abstract: The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF geometry is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explici
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18612</link><description>&lt;p&gt;
&#29702;&#35299;&#38543;&#26426;&#26862;&#26519;&#21644;&#36807;&#25311;&#21512;&#65306;&#19968;&#39033;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding random forests and overfitting: a visualization and simulation study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18612
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#24314;&#27169;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#22312;&#19968;&#39033;&#20851;&#20110;&#39044;&#27979;&#21365;&#24034;&#24694;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#38598;&#19978;&#30340;c-&#32479;&#35745;&#20540;&#25509;&#36817;1&#12290;&#23613;&#31649;&#36825;&#34920;&#26126;&#23384;&#22312;&#36807;&#25311;&#21512;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#65288;1&#65289;&#22312;&#19977;&#20010;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#20013;&#21487;&#35270;&#21270;&#25968;&#25454;&#31354;&#38388;&#21644;&#65288;2&#65289;&#36827;&#34892;&#27169;&#25311;&#30740;&#31350;&#26469;&#29702;&#35299;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#12290;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#28909;&#21147;&#22270;&#22312;&#20108;&#32500;&#23376;&#31354;&#38388;&#20013;&#21487;&#35270;&#21270;&#39118;&#38505;&#20272;&#35745;&#12290;&#27169;&#25311;&#30740;&#31350;&#21253;&#25324;48&#20010;&#36923;&#36753;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65288;DGM&#65289;&#65292;&#21464;&#21270;&#39044;&#27979;&#21464;&#37327;&#20998;&#24067;&#12289;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#12289;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#30495;&#23454;c-&#32479;&#35745;&#20540;&#21644;&#30495;&#23454;&#39044;&#27979;&#21464;&#37327;&#30340;&#24378;&#24230;&#12290;&#23545;&#20110;&#27599;&#20010;DGM&#65292;&#27169;&#25311;&#29983;&#25104;&#22823;&#23567;&#20026;200&#25110;4000&#30340;1000&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;ranger&#21253;&#35757;&#32451;&#26368;&#23567;&#33410;&#28857;&#22823;&#23567;&#20026;2&#25110;20&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;192&#20010;&#22330;&#26223;&#12290;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18612v1 Announce Type: cross  Abstract: Random forests have become popular for clinical risk prediction modelling. In a case study on predicting ovarian malignancy, we observed training c-statistics close to 1. Although this suggests overfitting, performance was competitive on test data. We aimed to understand the behaviour of random forests by (1) visualizing data space in three real world case studies and (2) a simulation study. For the case studies, risk estimates were visualised using heatmaps in a 2-dimensional subspace. The simulation study included 48 logistic data generating mechanisms (DGM), varying the predictor distribution, the number of predictors, the correlation between predictors, the true c-statistic and the strength of true predictors. For each DGM, 1000 training datasets of size 200 or 4000 were simulated and RF models trained with minimum node size 2 or 20 using ranger package, resulting in 192 scenarios in total. The visualizations suggested that the mod
&lt;/p&gt;</description></item><item><title>HemaGraph&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#23545;&#34880;&#28082;&#23398;&#21333;&#32454;&#32990;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#24040;&#22823;&#22270;&#20197;&#26816;&#27979;&#20302;&#39057;&#32454;&#32990;&#32676;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.18611</link><description>&lt;p&gt;
HemaGraph&#65306;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#31361;&#30772;&#34880;&#28082;&#23398;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18611
&lt;/p&gt;
&lt;p&gt;
HemaGraph&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#23545;&#34880;&#28082;&#23398;&#21333;&#32454;&#32990;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#24040;&#22823;&#22270;&#20197;&#26816;&#27979;&#20302;&#39057;&#32454;&#32990;&#32676;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34880;&#28082;&#23398;&#32454;&#32990;&#32676;&#20998;&#31867;&#39046;&#22495;&#65292;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#35201;&#27714;&#20351;&#29992;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATs&#65289;&#30340;&#26032;&#26694;&#26550;&#8220;HemaGraph&#8221;&#65292;&#29992;&#20110;&#20174;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#23545;&#34880;&#28082;&#23398;&#32454;&#32990;&#36827;&#34892;&#21333;&#32454;&#32990;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#21033;&#29992;GATs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#24494;&#23567;&#30340;&#32454;&#32990;&#20851;&#31995;&#65292;&#25552;&#20379;&#39640;&#24230;&#20934;&#30830;&#30340;&#24739;&#32773;&#30011;&#20687;&#12290;&#36890;&#36807;&#23545;30&#21517;&#24739;&#32773;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;HemaGraph&#23637;&#31034;&#20102;&#36328;&#20116;&#20010;&#19981;&#21516;&#32454;&#32990;&#31867;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#32988;&#36807;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;HemaGraph&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#65292;&#23427;&#24050;&#34987;&#24212;&#29992;&#20110;&#21253;&#21547;&#39640;&#36798;&#25968;&#21313;&#19975;&#20010;&#33410;&#28857;&#21644;&#20004;&#30334;&#19975;&#26465;&#36793;&#30340;&#26497;&#22823;&#22270;&#24418;&#65292;&#20197;&#26816;&#27979;&#20302;&#39057;&#32454;&#32990;&#32676;&#20307;&#65288;&#20363;&#22914;&#19968;&#20010;&#32676;&#20307;&#30340;0.01%&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18611v1 Announce Type: cross  Abstract: In the realm of hematologic cell populations classification, the intricate patterns within flow cytometry data necessitate advanced analytical tools. This paper presents 'HemaGraph', a novel framework based on Graph Attention Networks (GATs) for single-cell multi-class classification of hematological cells from flow cytometry data. Harnessing the power of GATs, our method captures subtle cell relationships, offering highly accurate patient profiling. Based on evaluation of data from 30 patients, HemaGraph demonstrates classification performance across five different cell classes, outperforming traditional methodologies and state-of-the-art methods. Moreover, the uniqueness of this framework lies in the training and testing phase of HemaGraph, where it has been applied for extremely large graphs, containing up to hundreds of thousands of nodes and two million edges, to detect low frequency cell populations (e.g. 0.01% for one population
&lt;/p&gt;</description></item><item><title>LeukoGraph&#26159;&#19968;&#20010;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23454;&#29616;&#32454;&#32990;&#32676;&#20307;&#30340;&#23618;&#27425;&#20998;&#31867;&#30340;&#20808;&#39537;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35299;&#20915;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#32454;&#32990;&#32676;&#20307;&#30340;&#23618;&#27425;&#32467;&#26500;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18610</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20851;&#27880;&#22270;&#24418;&#23601;&#36275;&#22815;&#20102;&#65306;LeukoGraph&#30340;&#36896;&#34880;&#32454;&#32990;&#32676;&#20307;&#30340;&#23618;&#27425;&#20998;&#31867;&#20808;&#39537;
&lt;/p&gt;
&lt;p&gt;
Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18610
&lt;/p&gt;
&lt;p&gt;
LeukoGraph&#26159;&#19968;&#20010;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23454;&#29616;&#32454;&#32990;&#32676;&#20307;&#30340;&#23618;&#27425;&#20998;&#31867;&#30340;&#20808;&#39537;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35299;&#20915;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#32454;&#32990;&#32676;&#20307;&#30340;&#23618;&#27425;&#32467;&#26500;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34880;&#28082;&#23398;&#26679;&#26412;&#30340;&#22797;&#26434;&#26223;&#35266;&#20013;&#65292;&#22914;&#22806;&#21608;&#34880;&#25110;&#39592;&#39635;&#65292;&#32454;&#32990;&#20998;&#31867;&#23558;&#19981;&#21516;&#31181;&#32676;&#21010;&#20998;&#20026;&#20998;&#23618;&#32467;&#26500;&#65292;&#23545;&#27492;&#25552;&#20986;&#20102; LeukoGraph&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36825;&#19968;&#30446;&#30340;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476; (GATs) &#26469;&#24212;&#23545;&#23618;&#27425;&#20998;&#31867;&#30340;&#22797;&#26434;&#24615;&#12290;LeukoGraph&#26631;&#24535;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#24212;&#29992;&#20110;&#22270;&#30340;&#23618;&#27425;&#25512;&#29702;&#30340;&#20808;&#39537;&#21162;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;&#19968;&#30334;&#19975;&#20010;&#33410;&#28857;&#21644;&#25104;&#30334;&#19975;&#26465;&#36793;&#65292;&#20840;&#37096;&#26469;&#33258;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#12290;LeukoGraph &#20197;&#31934;&#23494;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#31867;&#33539;&#24335;&#65292;&#20363;&#22914;&#22235;&#31181;&#19981;&#21516;&#32454;&#32990;&#32676;&#20307;&#32463;&#36807;&#25153;&#24179;&#20998;&#31867;&#65292;&#32780;&#31532;&#20116;&#31181;&#21017;&#20998;&#25104;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20998;&#25903;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#24494;&#22937;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18610v1 Announce Type: new  Abstract: In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges. This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing graph attention networks (GATs) to navigate hierarchical classification (HC) complexities. Notably, LeukoGraph stands as a pioneering effort, marking the application of graph neural networks (GNNs) for hierarchical inference on graphs, accommodating up to one million nodes and millions of edges, all derived from flow cytometry data. LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets. The technique is more general 
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#30740;&#31350;&#20102;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18606</link><description>&lt;p&gt;
&#32593;&#32476;&#25299;&#25169;&#23545;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of network topology on the performance of Decentralized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18606
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#30740;&#31350;&#20102;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#29992;&#20110;&#22312;&#20114;&#32852;&#32593;&#36793;&#32536;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#35299;&#20915;&#22522;&#30784;&#35774;&#26045;&#25361;&#25112;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#65292;&#27599;&#20010;&#33410;&#28857;&#26681;&#25454;&#20854;&#21508;&#33258;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#28982;&#21518;&#36825;&#20123;&#26412;&#22320;&#27169;&#22411;&#34987;&#20849;&#20139;&#21644;&#21512;&#24182;&#65292;&#24418;&#25104;&#33021;&#22815;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#30693;&#35782;&#20256;&#25773;&#65292;&#21363;&#33410;&#28857;&#22914;&#20309;&#21560;&#25910;&#26469;&#33258;&#32593;&#32476;&#19978;&#20854;&#20182;&#33410;&#28857;&#21487;&#29992;&#25968;&#25454;&#23398;&#20064;&#27169;&#24335;&#30340;&#27934;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#25506;&#35752;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#39030;&#28857;&#23646;&#24615;&#65292;&#21253;&#25324;&#24230;&#20013;&#24515;&#24615;&#65292;&#20171;&#25968;&#20013;&#24515;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18606v1 Announce Type: cross  Abstract: Fully decentralized learning is gaining momentum for training AI models at the Internet's edge, addressing infrastructure challenges and privacy concerns. In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset. The local models are then shared and combined to form a global model capable of making accurate predictions on new data. Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network. Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods. These methods consider different vertex properties, including degree centrality, betweenness cent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181; FORML &#26041;&#27861;&#65292;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#19968;&#38454;&#23548;&#25968;&#36817;&#20284;&#65292;&#36890;&#36807;&#24341;&#20837;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20803;&#23398;&#20064;&#20013;&#23454;&#29616;&#21442;&#25968;&#27491;&#20132;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.18605</link><description>&lt;p&gt;
FORML&#65306;&#19968;&#31181;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#27969;&#24418;&#28023;&#26862;&#33258;&#30001;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18605
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181; FORML &#26041;&#27861;&#65292;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#19968;&#38454;&#23548;&#25968;&#36817;&#20284;&#65292;&#36890;&#36807;&#24341;&#20837;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20803;&#23398;&#20064;&#20013;&#23454;&#29616;&#21442;&#25968;&#27491;&#20132;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#38382;&#39064;&#36890;&#24120;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#21644;&#20803;&#21442;&#25968;&#20998;&#21035;&#22312;&#20248;&#21270;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#24490;&#29615;&#20013;&#36827;&#34892;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#21442;&#25968;&#21644;&#20803;&#21442;&#25968;&#20301;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#38750;&#24120;&#23494;&#38598;&#30340;&#12290;&#19982;&#27431;&#20960;&#37324;&#24503;&#26041;&#27861;&#19981;&#21516;&#65292;&#40654;&#26364;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#35745;&#31639;&#21253;&#25324;&#36890;&#36807;&#40654;&#26364;&#31639;&#23376;&#65288;&#22914;&#25910;&#32553;&#21644;&#27491;&#20132;&#25237;&#24433;&#65289;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#23548;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#30340;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#26031;&#33922;&#22827;&#23572;&#20840;&#36830;&#25509;&#23618;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#30340;&#26368;&#21518;&#20998;&#31867;&#23618;&#21442;&#25968;&#19978;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18605v1 Announce Type: new  Abstract: Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbon
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18599</link><description>&lt;p&gt;
Meta-Tasks: &#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Meta-Tasks: An alternative view on Meta-Learning Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#35757;&#32451;&#21644;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#22806;&#23618;&#24490;&#29615;&#20043;&#21069;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#23545;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#65292;&#23558;&#20854;&#20316;&#20026;&#8220;&#20803;&#20219;&#21153;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#39062;&#21644;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25910;&#25947;&#26356;&#24555;&#12289;&#26356;&#22909;&#65292;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#20934;&#24046;&#26356;&#20302;&#65292;&#34920;&#26126;&#20854;&#22312;FSL&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#21407;&#22411;&#32593;&#32476;&#39640;&#20986;3.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18599v1 Announce Type: cross  Abstract: Few-shot learning (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using unsupervised techniques as ``meta-tasks''. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18595</link><description>&lt;p&gt;
EncodingNet: &#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;MAC&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#36328;  &#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35832;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#25191;&#34892;&#38656;&#35201;&#22312;&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#37327;&#30340;&#20056;-&#32047;&#31215;&#65288;MAC&#65289;&#36816;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#21151;&#32791;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#12290;&#22312;&#36825;&#31181;&#26032;&#35774;&#35745;&#20013;&#65292;&#20056;&#27861;&#22120;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#25152;&#21462;&#20195;&#65292;&#29992;&#20110;&#23558;&#32467;&#26524;&#25237;&#24433;&#21040;&#23485;&#27604;&#29305;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;&#27604;&#29305;&#25658;&#24102;&#21508;&#33258;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;&#25512;&#26029;&#31934;&#24230;&#12290;&#26032;&#20056;&#27861;&#22120;&#30340;&#36755;&#20986;&#36890;&#36807;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#36827;&#34892;&#30456;&#21152;&#65292;&#24182;&#19988;&#32047;&#31215;&#32467;&#26524;&#19982;&#29616;&#26377;&#35745;&#31639;&#24179;&#21488;&#20860;&#23481;&#65292;&#21487;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25110;&#38750;&#32479;&#19968;&#37327;&#21270;&#12290;&#30001;&#20110;&#20056;&#27861;&#20989;&#25968;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#25237;&#24433;&#25152;&#21462;&#20195;&#65292;&#23548;&#33268;&#33021;&#37327;&#25928;&#29575;&#21644;&#35745;&#31639;&#25928;&#26524;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to project the results onto a wide bit representation. These bits carry individual position weights, which can be trained for specific neural networks to enhance inference accuracy. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks with either uniform or non-uniform quantization. Since the multiplication function is replaced by simple logic projection, the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#23398;&#20064;&#26497;&#38480;&#30340;&#22270;&#35770;&#37327; $\beta_M(G)$&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.18591</link><description>&lt;p&gt;
&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#19978;&#19979;&#25991;&#36172;&#21338;&#65306;&#20174;&#29420;&#31435;&#25968;&#21040;MAS&#25968;
&lt;/p&gt;
&lt;p&gt;
Stochastic contextual bandits with graph feedback: from independence number to MAS number
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#23398;&#20064;&#26497;&#38480;&#30340;&#22270;&#35770;&#37327; $\beta_M(G)$&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#65292;&#22312;&#36825;&#31867;&#20114;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#27604;&#26222;&#36890;&#19978;&#19979;&#25991;&#36172;&#21338;&#26356;&#20016;&#23500;&#32467;&#26500;&#65292;&#20854;&#20013;&#37319;&#21462;&#19968;&#20010;&#34892;&#21160;&#23558;&#22312;&#25152;&#26377;&#24773;&#22659;&#19979;&#25581;&#31034;&#25152;&#26377;&#30456;&#37051;&#34892;&#21160;&#30340;&#22870;&#21169;&#12290;&#19982;&#22810;&#33218;&#36172;&#21338;&#35774;&#32622;&#19981;&#21516;&#65292;&#22810;&#25991;&#29486;&#24050;&#32463;&#23545;&#22270;&#21453;&#39304;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#35752;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#36172;&#21338;&#23545;&#24212;&#37096;&#20998;&#20173;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#35752;&#30340;&#22320;&#26041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36951;&#25022;&#19979;&#38480; $\Omega(\sqrt{\beta_M(G) T})$ &#25506;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013; $M$ &#26159;&#24773;&#22659;&#25968;&#65292;$G$ &#26159;&#21453;&#39304;&#22270;&#65292;$\beta_M(G)$ &#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#24449;&#35813;&#38382;&#39064;&#31867;&#30340;&#22522;&#30784;&#23398;&#20064;&#38480;&#21046;&#30340;&#22270;&#35770;&#37327;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;$\beta_M(G)$ &#22312; $\alpha(G)$ (&#22270;&#30340;&#29420;&#31435;&#25968;) &#21644; $\mathsf{m}(G)$ (&#22270;&#30340;&#26368;&#22823;&#26080;&#29615;&#23376;&#22270;&#65288;MAS&#65289;&#25968;) &#20043;&#38388;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18591v1 Announce Type: new  Abstract: We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. Unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. In this paper, we make inroads into this inquiry by establishing a regret lower bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$ is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretical quantity that characterizes the fundamental learning limit for this class of problems. Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic subgraph (MAS) number of the graph) as 
&lt;/p&gt;</description></item><item><title>Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.18589</link><description>&lt;p&gt;
Verif.ai: &#19968;&#31181;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18589
&lt;/p&gt;
&lt;p&gt;
Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39033;&#30446;Verif.ai&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#35821;&#20041;&#21644;&#35789;&#27719;&#25628;&#32034;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#65288;PubMed&#65289;&#36827;&#34892;&#26816;&#32034;&#65292;&#65288;2&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;Mistral 7B&#65289;&#65292;&#33719;&#21462;&#21069;&#20960;&#20010;&#31572;&#26696;&#24182;&#29983;&#25104;&#38468;&#26377;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#35770;&#25991;&#24341;&#29992;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#39564;&#35777;&#24341;&#25806;&#65292;&#29992;&#20110;&#20132;&#21449;&#26816;&#26597;&#29983;&#25104;&#30340;&#20027;&#24352;&#21644;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#25688;&#35201;&#25110;&#35770;&#25991;&#65292;&#39564;&#35777;&#29983;&#25104;&#20027;&#24352;&#26102;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#38169;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19978;&#19979;&#25991;&#20013;&#30340;&#25688;&#35201;&#21152;&#24378;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#27492;&#22806;&#65292;&#19968;&#20010;&#29420;&#31435;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#38598;&#27491;&#22312;&#39564;&#35777;&#31572;&#26696;&#24182;&#26816;&#26597;&#26159;&#21542;&#23384;&#22312;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#31185;&#23398;&#23478;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
&lt;/p&gt;</description></item><item><title>GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#26159;&#20851;&#38190;&#36164;&#20135;&#65292;&#33021;&#22815;&#22788;&#29702;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#38590;&#20197;&#33719;&#21462;&#21644;&#29702;&#35299;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#26412;&#25991;&#27719;&#24635;&#20102;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#39046;&#22495;&#30340;&#26032;&#21069;&#27839;&#12290;</title><link>https://arxiv.org/abs/2402.18587</link><description>&lt;p&gt;
&#22788;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20043;&#21021;&#65306;&#20851;&#20110;6G&#26080;&#32447;&#26234;&#33021;&#26032;&#39046;&#22495;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18587
&lt;/p&gt;
&lt;p&gt;
GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#26159;&#20851;&#38190;&#36164;&#20135;&#65292;&#33021;&#22815;&#22788;&#29702;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#38590;&#20197;&#33719;&#21462;&#21644;&#29702;&#35299;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#26412;&#25991;&#27719;&#24635;&#20102;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#39046;&#22495;&#30340;&#26032;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#32447;&#30740;&#31350;&#20005;&#37325;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;DAI&#65289;&#12290;&#19982;DAI&#19981;&#21516;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#28041;&#21450;&#33021;&#22815;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;GMs&#65289;&#12290;&#36825;&#20351;&#24471;GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#36164;&#20135;&#65292;&#20854;&#20013;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36890;&#24120;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#33719;&#21462;&#25104;&#26412;&#39640;&#65292;&#38590;&#20197;&#24314;&#27169;&#25110;&#29702;&#35299;&#12290;&#26377;&#20102;&#36825;&#20123;&#21560;&#24341;&#20154;&#30340;&#29305;&#24449;&#65292;GenAI&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;DAI&#26041;&#27861;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#31687;&#32467;&#21512;&#20102;&#25945;&#31243;&#21644;&#35843;&#30740;&#30340;&#35770;&#25991;&#20174;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#30340;&#22522;&#30784;&#24320;&#22987;&#65292;&#36890;&#36807;&#27010;&#36848;&#20505;&#36873;6G&#24212;&#29992;&#21644;&#26381;&#21153;&#12289;&#25552;&#20986;&#29616;&#20195;DAI&#27169;&#22411;&#30340;&#20998;&#31867;&#27861;&#12289;&#20030;&#20363;&#35828;&#26126;&#33879;&#21517;&#30340;DAI&#29992;&#20363;&#65292;&#24182;&#38416;&#26126;GenAI&#22914;&#20309;&#22686;&#24378;DAI&#30340;&#22810;&#26041;&#38754;&#26041;&#24335;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#28857;&#20171;&#32461;&#24320;&#21019;&#24615;&#30340;GMs&#26469;&#21576;&#29616;&#19968;&#20010;GMs&#30340;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18587v1 Announce Type: cross  Abstract: The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#32467;&#21512;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#65288;BindDM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25552;&#21462;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;SE(3)-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#23558;&#32467;&#21512;&#20449;&#24687;&#20256;&#22238;&#27599;&#20010;&#21407;&#23376;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;3D&#20998;&#23376;&#25193;&#25955;&#29983;&#25104;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18583</link><description>&lt;p&gt;
&#32467;&#21512;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Binding-Adaptive Diffusion Models for Structure-Based Drug Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#32467;&#21512;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#65288;BindDM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25552;&#21462;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;SE(3)-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#23558;&#32467;&#21512;&#20449;&#24687;&#20256;&#22238;&#27599;&#20010;&#21407;&#23376;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;3D&#20998;&#23376;&#25193;&#25955;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#19982;&#29305;&#23450;&#34507;&#30333;&#38774;&#21521;&#32467;&#21512;&#30340;&#19977;&#32500;&#37197;&#20307;&#20998;&#23376;&#12290;&#29616;&#26377;&#30340;3D&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#20013;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#31934;&#30830;&#25429;&#33719;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#26159;&#22797;&#26434;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#32467;&#21512;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#65288;BindDM&#65289;&#12290;&#22312;BindDM&#20013;&#65292;&#25105;&#20204;&#33258;&#36866;&#24212;&#22320;&#25552;&#21462;&#20122;&#22797;&#26434;&#20307;&#65292;&#21363;&#36127;&#36131;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#21512;&#20301;&#28857;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#21518;&#65292;&#25152;&#36873;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#20122;&#22797;&#26434;&#20307;&#36890;&#36807;SE(3)-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#20256;&#22238;&#21040;&#22797;&#26434;&#20307;&#30340;&#27599;&#20010;&#21407;&#23376;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;&#30446;&#26631;&#30340;3D&#20998;&#23376;&#25193;&#25955;&#29983;&#25104;&#65292;&#24182;&#24102;&#26377;&#32467;&#21512;&#30456;&#20114;&#20316;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#36328;&#23618;&#27425;&#20132;&#20114;&#33410;&#28857;&#36845;&#20195;&#36827;&#34892;&#36825;&#31181;&#20998;&#23618;&#22797;&#26434;&#20307;-&#20122;&#22797;&#26434;&#20307;&#36807;&#31243;&#65292;&#20197;&#20805;&#20998;&#34701;&#21512;&#20840;&#23616;&#32467;&#21512;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18583v1 Announce Type: cross  Abstract: Structure-based drug design (SBDD) aims to generate 3D ligand molecules that bind to specific protein targets. Existing 3D deep generative models including diffusion models have shown great promise for SBDD. However, it is complex to capture the essential protein-ligand interactions exactly in 3D space for molecular generation. To address this problem, we propose a novel framework, namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively extract subcomplex, the essential part of binding sites responsible for protein-ligand interactions. Then the selected protein-ligand subcomplex is processed with SE(3)-equivariant neural networks, and transmitted back to each atom of the complex for augmenting the target-aware 3D molecule diffusion generation with binding interaction information. We iterate this hierarchical complex-subcomplex process with cross-hierarchy interaction node for adequately fusing global binding context
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#36827;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#25552;&#39640;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18576</link><description>&lt;p&gt;
&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#21892;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Forecasting Using a PSO-RDV Framework to Enhance Artificial Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#36827;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#25552;&#39640;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#21644;&#35268;&#21010;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#12290;&#25919;&#24220;&#21644;&#20844;&#20247;&#22312;&#28508;&#22312;&#26410;&#26469;&#20844;&#20849;&#21355;&#29983;&#19981;&#30830;&#23450;&#24615;&#38754;&#20020;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#21033;&#30410;&#26368;&#22823;&#21270;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25913;&#36827;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#26469;&#25552;&#39640;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290; &#21463;&#39640;&#23572;&#22827;&#29699;&#36816;&#21160;&#21551;&#21457;&#65292;IW&#25216;&#26415;&#20462;&#25913;&#20102;&#31890;&#23376;&#25509;&#36817;&#35299;&#20915;&#26041;&#26696;&#28857;&#26102;&#30340;&#36895;&#24230;&#65292;&#20351;&#20854;&#21576;&#29616;&#25243;&#29289;&#32447;&#19979;&#38477;&#32467;&#26500;&#12290; &#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24314;&#35758;&#30340;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;[0.4, 0.9]&#30340;alpha&#21644;alpha_dump&#32452;&#21512;&#65292;&#30456;&#23545;&#20110;&#26087;&#27169;&#22411;&#22312;&#20301;&#32622;&#35823;&#24046;&#19978;&#26377;6.36&#65285;&#30340;&#25913;&#36827;&#65292;&#35745;&#31639;&#26102;&#38388;&#19978;&#26377;11.75&#65285;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290; &#23427;&#36798;&#21040;&#20102;&#26368;&#20248;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18576v1 Announce Type: cross  Abstract: Decision making and planning have long relied heavily on AI-driven forecasts. The government and the general public are working to minimize the risks while maximizing benefits in the face of potential future public health uncertainties. This study used an improved method of forecasting utilizing the Random Descending Velocity Inertia Weight (RDV IW) technique to improve the convergence of Particle Swarm Optimization (PSO) and the accuracy of Artificial Neural Network (ANN). The IW technique, inspired by the motions of a golf ball, modified the particles' velocities as they approached the solution point to a parabolically descending structure. Simulation results revealed that the proposed forecasting model with [0.4, 0.9] combination of alpha and alpha_dump exhibits a 6.36% improvement in position error and 11.75% improvement in computational time compared to the old model, thus, improving its convergence. It reached the optimum level a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DiffuseRAW&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#20302;&#20809;&#29031;&#22270;&#20687;&#22788;&#29702;&#20013;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18575</link><description>&lt;p&gt;
DiffuseRAW&#65306;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#29992;&#20110;&#20302;&#20809;&#29031;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DiffuseRAW: End-to-End Generative RAW Image Processing for Low-Light Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DiffuseRAW&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#20302;&#20809;&#29031;&#22270;&#20687;&#22788;&#29702;&#20013;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26497;&#20302;&#20809;&#26465;&#20214;&#19979;&#25104;&#20687;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#65292;&#30001;&#20110;&#26368;&#23567;&#20809;&#23376;&#25429;&#33719;&#24341;&#36215;&#30340;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36870;&#38382;&#39064;&#12290;&#20197;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#29992;&#20110;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#26159;&#22312;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#26497;&#20302;&#20809;&#20219;&#21153;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#25110;&#22270;&#20687;&#21040;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;RAW&#22270;&#20687;&#21040;&#22788;&#29702;&#21518;&#22270;&#20687;&#30340;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#36890;&#24120;&#30001;&#22810;&#20010;&#19987;&#38376;&#21270;&#37096;&#20998;&#32452;&#25104;&#65292;&#36807;&#24230;&#20381;&#36182;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#36825;&#20123;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;ISP&#65292;&#20381;&#36182;&#20110;&#24494;&#35843;&#28508;&#22312;&#30340;&#25193;&#25955;&#27169;&#22411;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18575v1 Announce Type: cross  Abstract: Imaging under extremely low-light conditions presents a significant challenge and is an ill-posed problem due to the low signal-to-noise ratio (SNR) caused by minimal photon capture. Previously, diffusion models have been used for multiple kinds of generative tasks and image-to-image tasks, however, these models work as a post-processing step. These diffusion models are trained on processed images and learn on processed images. However, such approaches are often not well-suited for extremely low-light tasks. Unlike the task of low-light image enhancement or image-to-image enhancement, we tackle the task of learning the entire image-processing pipeline, from the RAW image to a processed image. For this task, a traditional image processing pipeline often consists of multiple specialized parts that are overly reliant on the downstream tasks. Unlike these, we develop a new generative ISP that relies on fine-tuning latent diffusion models o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18546</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#25925;&#38556;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;Tokenization + Transformers &#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18546
&lt;/p&gt;
&lt;p&gt;
TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#21457;&#29616;&#33021;&#22815;&#27867;&#21270;&#30340;&#31070;&#32463;&#25968;&#25454;&#34920;&#31034;&#12290;&#36825;&#19968;&#30446;&#26631;&#21463;&#21040;&#35760;&#24405;&#20250;&#35805;&#65288;&#20363;&#22914;&#29615;&#22659;&#65289;&#12289;&#21463;&#35797;&#32773;&#65288;&#20363;&#22914;&#21464;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#65289;&#21644;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#22122;&#22768;&#65289;&#31561;&#22240;&#32032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36328;&#20250;&#35805;&#21644;&#21463;&#35797;&#32773;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#38024;&#23545;&#22312;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27867;&#21270;&#24615;&#32500;&#24230;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20250;&#35805;&#12289;&#21463;&#35797;&#32773;&#21644;&#20256;&#24863;&#22120;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65306;EEGNet&#65288;Lawhern&#31561;&#20154;&#65292;2018&#65289;&#21644;TOTEM&#65288;Talukder&#31561;&#20154;&#65292;2024&#65289;&#12290;EEGNet &#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780; TOTEM &#26159;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#26631;&#35760;&#22120;&#21644; Transformer &#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#27867;&#21270;&#26696;&#20363;&#20013;&#65292;TOTEM &#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982; EEGNet &#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512; TOTEM &#30340;&#28508;&#22312;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18546v1 Announce Type: new  Abstract: A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18495</link><description>&lt;p&gt;
ROG$_{PL}$: &#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#24050;&#30693;&#20998;&#31867;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#35782;&#21035;&#20026;&#26410;&#30693;&#12290;&#20256;&#32479;&#30340;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#22914;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#21644;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#22122;&#22768;&#12290;OOD&#25968;&#25454;&#26159;&#19981;&#23646;&#20110;&#20219;&#20309;&#24050;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#22914;&#26524;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#20986;&#29616;&#65288;OOD&#22122;&#22768;&#65289;&#65292;&#22312;&#27979;&#35797;&#20013;&#20986;&#29616;&#21017;&#20026;&#24320;&#25918;&#38598;&#26679;&#26412;&#12290;IND&#22122;&#22768;&#26159;&#34987;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;IND&#22122;&#22768;&#21644;OOD&#22122;&#22768;&#30340;&#23384;&#22312;&#26159;&#26222;&#36941;&#30340;&#65292;&#36890;&#24120;&#20250;&#24341;&#36215;&#27169;&#31946;&#38382;&#39064;&#65292;&#21253;&#25324;&#31867;&#20869;&#21464;&#21270;&#38382;&#39064;&#21644;&#31867;&#38388;&#28151;&#28102;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#26041;&#27861;&#26159;&#24517;&#35201;&#19988;&#22256;&#38590;&#30340;&#65292;&#23545;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#26469;&#35828;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18495v1 Announce Type: new  Abstract: Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data.To this end, we propose a unified framework named ROG$_
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.17886</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#38646;&#38454;&#37319;&#26679;&#26041;&#27861;&#65306;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#32531;&#35299;&#20122;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#20854;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#26410;&#24402;&#19968;&#21270;&#23494;&#24230;&#26597;&#35810;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#21363;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;DMC&#65289;&#65292;&#20854;&#24471;&#20998;&#20989;&#25968;&#36890;&#36807;&#36890;&#29992;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#36924;&#36817;&#12290;DMC&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#35861;&#30340;&#20803;&#31639;&#27861;&#65292;&#20854;&#20013;&#31070;&#35861;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#30340;&#35775;&#38382;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#36825;&#20010;&#31070;&#35861;&#30340;&#23454;&#29616;&#65292;&#36825;&#23558;DMC&#36716;&#21270;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;ZOD-MC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;DMC&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#32780;&#19981;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#20026;&#23545;&#25968;&#20985;&#25110;&#28385;&#36275;&#20219;&#20309;&#31561;&#21608;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;ZOD-MC&#23545;&#25152;&#38656;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#65292;&#23613;&#31649;&#20173;&#28982;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;</title><link>https://arxiv.org/abs/2402.17002</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21547;&#27491;&#20132;&#20559;&#32622;&#21457;&#29616;&#23545;&#31216;&#32676;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Symmetry Group Structures via Implicit Orthogonality Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17002
&lt;/p&gt;
&lt;p&gt;
HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HyperCube&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#20013;&#23545;&#31216;&#32676;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21521;&#23398;&#20064;&#27491;&#20132;&#34920;&#31034;&#28748;&#36755;&#20102;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#21033;&#29992;&#20102;&#34920;&#31034;&#29702;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#23450;&#29702;&#65292;&#21363;&#25152;&#26377;&#32039;&#33268;/&#26377;&#38480;&#32676;&#37117;&#21487;&#20197;&#30001;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#12290;HyperCube&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#32676;&#25805;&#20316;&#65292;&#25104;&#21151;&#24674;&#22797;&#23436;&#25972;&#30340;&#25805;&#20316;&#34920;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#23398;&#20064;&#20986;&#30340;&#22240;&#32032;&#30452;&#25509;&#23545;&#24212;&#20110;&#24213;&#23618;&#32676;&#30340;&#31934;&#30830;&#30697;&#38453;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22240;&#32032;&#25429;&#25417;&#21040;&#20102;&#32676;&#30340;&#23436;&#25972;&#19981;&#21487;&#32422;&#34920;&#31034;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#25191;&#34892;&#32676;&#21367;&#31215;&#30340;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#12290;&#22312;&#23545;&#32676;&#21644;&#38750;&#32676;&#31526;&#21495;&#25805;&#20316;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;HyperCube&#23637;&#31034;&#20102;10
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16326</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#35777;&#23454;&#20934;&#30830;&#24615;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Provably Accurate Randomized Sampling Algorithm for Logistic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36923;&#36753;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#24403;&#35266;&#27979;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#31639;&#27861;&#65292;&#20445;&#35777;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#32467;&#26500;&#26465;&#20214;&#22522;&#30784;&#19978;&#65292;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#24402;&#32467;&#20026;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#65292;&#26159;&#38543;&#26426;&#21270;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#22522;&#26412;&#19988;&#28145;&#20837;&#29702;&#35299;&#30340;&#22522;&#20803;&#12290;&#24403;&#21033;&#29992;&#26464;&#26438;&#20998;&#25968;&#23545;&#35266;&#27979;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36923;&#36753;&#22238;&#24402;&#30340;&#20272;&#35745;&#27010;&#29575;&#23646;&#24615;&#65292;&#24182;&#35777;&#26126;&#20934;&#30830;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#36828;&#23567;&#20110;&#24635;&#35266;&#27979;&#25968;&#30340;&#26679;&#26412;&#23454;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16041</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16041
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#22914;&#25220;&#34989;&#38382;&#39064;&#12289;&#35823;&#23548;&#24615;&#20449;&#24687;&#25110;&#24187;&#35273;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#38750;&#24120;&#32039;&#36843;&#21644;&#37325;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#24120;&#24120;&#38750;&#24120;&#24494;&#22937;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;\textit{&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;}&#65288;MMD&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;MMD&#21487;&#20197;&#24456;&#22909;&#22320;&#35782;&#21035;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;MMD&#36827;&#34892;&#35757;&#32451;&#23558;&#23548;&#33268;MMD&#30340;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#65292;&#22240;&#20026;&#19981;&#21516;LLMs&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#21253;&#21547;\textit{&#22810;&#20010;&#25991;&#26412;&#32676;&#20307;}&#12290;&#36825;&#23558;&#20005;&#37325;&#25439;&#23475;MMD&#27979;&#37327;&#20998;&#24067;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15893</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#24182;&#34892;&#23398;&#20064;&#31574;&#30053;&#21644;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31574;&#30053;&#38754;&#20020;&#30528;&#30830;&#20445;&#23433;&#20840;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#39044;&#23450;&#20041;&#30340;&#23433;&#20840;&#32422;&#26463;&#32435;&#20837;&#21040;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#23545;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#30340;&#20381;&#36182;&#22312;&#23433;&#20840;&#25511;&#21046;RL&#20219;&#21153;&#20013;&#20855;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#25110;&#19981;&#22815;&#36866;&#24212;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#30340;RL&#25511;&#21046;&#31574;&#30053;&#24182;&#30830;&#23450;&#32473;&#23450;&#29615;&#22659;&#30340;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;pSTL&#65289;&#23433;&#20840;&#35268;&#33539;&#21644;&#19968;&#20010;&#23567;&#30340;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20219;&#21153;&#65292;&#24039;&#22937;&#22320;&#23558;&#21463;&#38480;&#31574;&#30053;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15893v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19982;&#25705;&#25830;&#65306;&#28369;&#21160;&#12289;&#20445;&#25345;&#12289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks and Friction: Slide, Hold, Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14148
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#26550;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20855;&#26377;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#23450;&#24459;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;&#29992;&#20110;&#35757;&#32451;&#32593;&#32476;&#30340;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#26041;&#31243;&#32467;&#21512;&#29366;&#24577;&#28436;&#21270;&#32769;&#21270;&#23450;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#21046;&#23450;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26126;&#30830;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#30452;&#25509;&#25928;&#24212;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;GRU&#26550;&#26500;&#30340;RNN&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#39044;&#27979;&#25705;&#25830;&#31995;&#25968;&#30001;&#20110;&#36895;&#24230;&#36339;&#36291;&#32780;&#20135;&#29983;&#30340;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26159;&#20851;&#20110;&#23545;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#23384;&#20648;&#21487;&#21387;&#32553;&#24615;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#20195;&#25968;&#25454;&#32553;&#20943;&#24037;&#20855;&#22312;&#22788;&#29702;PTM&#25968;&#25454;&#38598;&#26102;&#24182;&#19981;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.13429</link><description>&lt;p&gt;
&#26377;&#20851;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23384;&#20648;&#21487;&#21387;&#32553;&#24615;&#30340;&#19968;&#20999;&#20320;&#24819;&#30693;&#36947;&#20294;&#21364;&#19981;&#25954;&#35810;&#38382;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13429
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26159;&#20851;&#20110;&#23545;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#23384;&#20648;&#21487;&#21387;&#32553;&#24615;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#20195;&#25968;&#25454;&#32553;&#20943;&#24037;&#20855;&#22312;&#22788;&#29702;PTM&#25968;&#25454;&#38598;&#26102;&#24182;&#19981;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#25968;&#25454;&#32553;&#20943;&#24037;&#20855;&#21364;&#26410;&#33021;&#36319;&#19978;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#32553;&#20943;&#25216;&#26415;&#24182;&#38750;&#19987;&#38376;&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#25968;&#25454;&#38598;&#25991;&#20214;&#32780;&#35774;&#35745;&#12290;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#29702;&#35299;&#19981;&#36275;&#65292;&#23588;&#20854;&#26159;&#19982;&#25968;&#25454;&#32553;&#20943;&#21644;&#21487;&#21387;&#32553;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13429v1 Announce Type: cross  Abstract: As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility.   This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters. We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26126;&#30830;&#23450;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35813;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10401</link><description>&lt;p&gt;
ManiFPT: &#23450;&#20041;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
ManiFPT: Defining and Analyzing Fingerprints of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26126;&#30830;&#23450;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35813;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#30041;&#19979;&#20102;&#23427;&#20204;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#30165;&#36857;&#65292;&#24191;&#27867;&#31216;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#26816;&#27979;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#32441;&#33021;&#22815;&#21306;&#20998;&#21508;&#31181;&#31867;&#22411;&#30340;&#21512;&#25104;&#22270;&#20687;&#20197;&#21450;&#24110;&#21161;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#31243;&#24230;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#23588;&#20854;&#26159;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25351;&#32441;&#30340;&#23450;&#20041;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#23450;&#20041;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#26368;&#32456;&#30740;&#31350;&#20102;&#23427;&#22312;&#21306;&#20998;&#22823;&#37327;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20174;&#26679;&#26412;&#20013;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10401v1 Announce Type: new  Abstract: Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09164</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#26159;&#32654;&#65306;&#36890;&#36807;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#20943;&#23569;&#21487;&#35299;&#37322;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Less is More: Fewer Interpretable Region via Submodular Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24402;&#23646;&#31639;&#27861;&#26088;&#22312;&#30830;&#23450;&#19982;&#27169;&#22411;&#20915;&#31574;&#39640;&#24230;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30446;&#26631;&#20803;&#32032;&#20998;&#37197;&#37325;&#35201;&#24615;&#65292;&#20294;&#20173;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;1&#65289;&#29616;&#26377;&#30340;&#24402;&#23646;&#26041;&#27861;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#23567;&#21306;&#22495;&#65292;&#20174;&#32780;&#35823;&#23548;&#27491;&#30830;&#24402;&#23646;&#30340;&#26041;&#21521;&#65307;2&#65289;&#27169;&#22411;&#26080;&#27861;&#20026;&#39044;&#27979;&#38169;&#35823;&#30340;&#26679;&#26412;&#20135;&#29983;&#33391;&#22909;&#30340;&#24402;&#23646;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#19978;&#36848;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#26088;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#23616;&#37096;&#21306;&#22495;&#30340;&#20851;&#27880;&#19981;&#36275;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#27425;&#27169;&#20989;&#25968;&#26469;&#21457;&#29616;&#26356;&#20934;&#30830;&#30340;&#31934;&#32454;&#35299;&#37322;&#21306;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;&#25152;&#26377;&#26679;&#26412;&#30340;&#24402;&#23646;&#25928;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#23376;&#21306;&#22495;&#36873;&#25321;&#26045;&#21152;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32422;&#26463;&#65292;&#21363;&#32622;&#20449;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09164v1 Announce Type: cross Abstract: Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISAHP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2402.03726</link><description>&lt;p&gt;
&#20174;&#23454;&#20363;&#32423;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;Hawkes&#36807;&#31243;&#20013;&#23398;&#20064;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISAHP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23545;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#23454;&#20363;&#32423;&#30340;&#22240;&#26524;&#32467;&#26500;&#24863;&#20852;&#36259;&#12290;&#23454;&#20363;&#32423;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#21333;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#35201;&#20040;&#38656;&#35201;&#24378;&#21152;&#19968;&#20123;&#20551;&#35774;&#65292;&#27604;&#22914;&#24378;&#21152;&#21040;&#24378;&#24230;&#20989;&#25968;&#20013;&#30340;&#32447;&#24615;&#20551;&#35774;&#65292;&#35201;&#20040;&#21551;&#21457;&#24335;&#22320;&#23450;&#20041;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#20123;&#19981;&#19968;&#23450;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#23454;&#20363;&#32423;&#33258;&#25105;&#27880;&#24847;&#21147;Hawkes&#36807;&#31243;&#65288;ISAHP&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#25512;&#26029;&#20107;&#20214;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;ISAHP&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#20102;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#21407;&#29702;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;ISAHP&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ICME 2024 Grand Challenge&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25506;&#32034;&#20102;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#21319;&#22768;&#22330;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02694</link><description>&lt;p&gt;
IEEE ICME 2024&#22823;&#25361;&#25112;&#36187;: &#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ICME 2024 Grand Challenge&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25506;&#32034;&#20102;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#21319;&#22768;&#22330;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#22330;&#20998;&#31867;&#26159;&#35745;&#31639;&#22768;&#22330;&#20998;&#26512;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#29615;&#22659;&#30340;&#29420;&#29305;&#22768;&#23398;&#29305;&#24449;&#12290;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#24341;&#36215;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#30340;&#22768;&#22330;&#20998;&#31867;&#25361;&#25112;&#24050;&#32463;&#22312;&#35774;&#22791;&#36890;&#29992;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20294;&#28041;&#21450;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#25991;&#21270;&#21644;&#35821;&#35328;&#31561;&#29305;&#24449;&#30340;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#26410;&#26631;&#35760;&#30340;&#22768;&#22330;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#21033;&#29992;&#36825;&#20123;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#33021;&#26041;&#27861;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;ICME 2024&#22823;&#25361;&#25112;&#36187;&#20013;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment. One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data. Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices. Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present. In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We encourage par
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#22914;&#20309;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#36873;&#25321;&#26631;&#20934;&#21644;&#33719;&#21462;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>https://arxiv.org/abs/2401.15721</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33719;&#21462;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Acquisition Functions for Medical Imaging Deep Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#22914;&#20309;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#36873;&#25321;&#26631;&#20934;&#21644;&#33719;&#21462;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#38761;&#21629;&#24050;&#32463;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290; &#20174;&#20083;&#33146;&#30284;&#26816;&#27979;&#21040;&#34507;&#30333;&#36136;&#25240;&#21472;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19968;&#30452;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#36827;&#27493;&#30340;&#26680;&#24515;&#12290; &#20294;&#26159;&#65292;&#36825;&#20123;&#29616;&#20195;&#36827;&#27493;&#36234;&#26469;&#36234;&#38656;&#35201;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26631;&#35760;&#25968;&#25454;&#65292;&#20854;&#21487;&#29992;&#24615;&#31232;&#32570;&#65306;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#38750;&#24120;&#26377;&#25928;&#65292;&#20854;&#20013;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#65288;&#25110;&#27880;&#37322;&#39044;&#31639;&#38750;&#24120;&#26377;&#38480;&#65289;&#12290; &#25105;&#20204;&#22312;ISIC 2016&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#36873;&#25321;&#26631;&#20934;&#65288;BALD&#65292;MeanSTD&#21644;MaxEntropy&#65289;&#12290; &#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#33719;&#21462;&#30340;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#35777;&#23454;&#20102;&#20316;&#32773;&#30340;&#29468;&#27979;&#65292;&#21363;\textit {bald} &#24179;&#22343;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#22909;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15721v2 Announce Type: replace-cross  Abstract: The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where obtaining labeled data (or annotation budget is very limited). We compare several selection criteria (BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the effect of acquired pool size on the model's performance. Our results suggest that uncertainty is useful to the Melanoma detection task, and confirms the hypotheses of the author of the paper of interest, that \textit{bald} performs on average better than other a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;OK-Robot&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#23548;&#33322;&#21407;&#35821;&#21644;&#25235;&#21462;&#21407;&#35821;&#65292;&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.12202</link><description>&lt;p&gt;
OK-Robot: &#25972;&#21512;&#24320;&#25918;&#30693;&#35782;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;OK-Robot&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#23548;&#33322;&#21407;&#35821;&#21644;&#25235;&#21462;&#21407;&#35821;&#65292;&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#29616;&#22312;&#25317;&#26377;&#33021;&#22815;&#26681;&#25454;&#35821;&#35328;&#26597;&#35810;&#35782;&#21035;&#29289;&#20307;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#25511;&#21046;&#31227;&#21160;&#31995;&#32479;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20197;&#21450;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#29289;&#20307;&#30340;&#25235;&#21462;&#27169;&#22411;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#27493;&#65292;&#20294;&#26426;&#22120;&#20154;&#30340;&#36890;&#29992;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#23613;&#31649;&#23427;&#20204;&#20381;&#36182;&#20110;&#35782;&#21035;&#12289;&#23548;&#33322;&#21644;&#25235;&#21462;&#31561;&#22522;&#26412;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31995;&#32479;&#20248;&#20808;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OK-Robot&#30340;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29992;&#20110;&#23545;&#35937;&#26816;&#27979;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12289;&#29992;&#20110;&#31227;&#21160;&#30340;&#23548;&#33322;&#21407;&#35821;&#21644;&#29992;&#20110;&#29289;&#20307;&#25805;&#20316;&#30340;&#25235;&#21462;&#21407;&#35821;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;OK-Robot&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;10&#20010;&#30495;&#23454;&#23478;&#24237;&#29615;&#22659;&#20013;&#36816;&#34892;&#20102;OK-Robot&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12202v2 Announce Type: replace-cross  Abstract: Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;Bayesian&#31070;&#32463;&#32593;&#32476;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#30456;&#27604;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#19988;&#33021;&#22815;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#25214;&#21040;&#23433;&#20840;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2401.11627</link><description>&lt;p&gt;
Bayesian&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Tight Verification of Probabilistic Robustness in Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11627
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;Bayesian&#31070;&#32463;&#32593;&#32476;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#30456;&#27604;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#19988;&#33021;&#22815;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#25214;&#21040;&#23433;&#20840;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#29992;&#20110;&#35745;&#31639;Bayesian&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#19978;&#20005;&#26684;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#35745;&#31639;BNNs&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#35201;&#27604;&#39564;&#35777;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#40065;&#26834;&#24615;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#25628;&#32034;&#23433;&#20840;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;NNs&#39564;&#35777;&#30340;&#32039;&#23494;&#21644;&#23436;&#25972;&#26041;&#27861;&#65292;&#20363;&#22914;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#26041;&#27861;&#65292;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;BNNs&#30340;&#39564;&#35777;&#65292;&#22240;&#20026;&#30001;&#20110;&#32534;&#30721;&#26435;&#37325;&#30340;&#21464;&#37327;&#36830;&#32493;&#30456;&#20056;&#32780;&#20135;&#29983;&#30340;&#22810;&#39033;&#24335;&#39033;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#25193;&#23637;&#21644;&#32593;&#32476;&#30340;&#26799;&#24230;&#26377;&#25928;&#22320;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#23547;&#25214;&#23433;&#20840;&#26435;&#37325;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;BNNs&#30340;&#20219;&#20309;&#39564;&#35777;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11627v2 Announce Type: replace-cross  Abstract: We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the So
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Infinite dSprites&#24037;&#20855;&#65292;&#29992;&#20110;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#21487;&#20197;&#20840;&#38754;&#25511;&#21046;&#29983;&#25104;&#22240;&#32032;&#65292;&#26377;&#26395;&#32553;&#23567;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19982;&#20154;&#31867;&#23398;&#20064;&#22312;&#21160;&#24577;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2312.16731</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#35299;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#38480;dSprites&#65306;&#23558;&#35760;&#24518;&#32534;&#36753;&#19982;&#27867;&#21270;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16731
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Infinite dSprites&#24037;&#20855;&#65292;&#29992;&#20110;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#21487;&#20197;&#20840;&#38754;&#25511;&#21046;&#29983;&#25104;&#22240;&#32032;&#65292;&#26377;&#26395;&#32553;&#23567;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19982;&#20154;&#31867;&#23398;&#20064;&#22312;&#21160;&#24577;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#21463;&#21040;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38459;&#30861;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#35206;&#30422;&#29616;&#26377;&#30693;&#35782;&#12290;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#12289;&#21442;&#25968;&#38548;&#31163;&#25110;&#25490;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#20165;&#21253;&#21547;&#23569;&#25968;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#20197;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Infinite dSprites&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#27905;&#30340;&#24037;&#20855;&#65292;&#21487;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#24182;&#23545;&#29983;&#25104;&#22240;&#32032;&#25317;&#26377;&#23436;&#20840;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36275;&#22815;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16731v2 Announce Type: replace  Abstract: The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#23545;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#20013;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;&#36827;&#34892;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2312.16624</link><description>&lt;p&gt;
&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#29992;&#20110;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dual-stage optimizer for systematic overestimation adjustment applied to multi-objective genetic algorithms for biomarker selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#23545;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#20013;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;&#36827;&#34892;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#25361;&#25112;&#22312;&#20110;&#20998;&#23376;&#29305;&#24449;&#30340;&#20016;&#23500;&#24615;&#20294;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#12290;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#38656;&#35201;&#35780;&#20272;&#21508;&#31181;&#29305;&#24449;&#38598;&#65288;&#27169;&#22411;&#65289;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#20351;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#28041;&#21450;&#27979;&#35797;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#20197;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#23384;&#22312;&#24615;&#33021;&#20272;&#35745;&#35823;&#24046;&#65292;&#24403;&#36873;&#25321;&#28041;&#21450;&#22810;&#20010;&#27169;&#22411;&#26102;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#20960;&#20046;&#32943;&#23450;&#34987;&#36807;&#20272;&#12290;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#21487;&#20197;&#35270;&#20026;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#21644;&#29305;&#24449;&#25968;&#37327;&#20013;&#30340;&#31616;&#32422;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#12290;&#36951;&#20256;&#31639;&#27861;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#36827;&#21270;&#20986;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#36807;&#20272;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22312;&#36873;&#25321;&#20102;&#27169;&#22411;&#21518;&#20943;&#23569;&#36807;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in biomarker discovery using machine learning from omics data lies in the abundance of molecular features but scarcity of samples. Most feature selection methods in machine learning require evaluating various sets of features (models) to determine the most effective combination. This process, typically conducted using a validation dataset, involves testing different feature sets to optimize the model's performance. Evaluations have performance estimation error and when the selection involves many models the best ones are almost certainly overestimated. Biomarker identification with feature selection methods can be addressed as a multi-objective problem with trade-offs between predictive ability and parsimony in the number of features. Genetic algorithms are a popular tool for multi-objective optimization but they evolve numerous solutions thus are prone to overestimation. Methods have been proposed to reduce the overestimation after a model has already been selected in si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Poincar&#233;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;PoinDP&#65292;&#29992;&#20110;&#20445;&#25252;&#22522;&#20110;&#23618;&#27425;&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#27425;&#24615;&#24102;&#26469;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.12183</link><description>&lt;p&gt;
&#22522;&#20110;Poincar&#233;&#24046;&#20998;&#38544;&#31169;&#30340;&#23618;&#27425;&#24863;&#30693;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Poincar\'e Differential Privacy for Hierarchy-Aware Graph Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Poincar&#233;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;PoinDP&#65292;&#29992;&#20110;&#20445;&#25252;&#22522;&#20110;&#23618;&#27425;&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#27425;&#24615;&#24102;&#26469;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#26159;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#24120;&#35265;&#30340;&#25299;&#25169;&#24615;&#36136;&#65292;&#23427;&#34920;&#26126;&#20102;&#20027;&#31649;&#21644;&#19979;&#23646;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#20154;&#31867;&#32676;&#20307;&#30340;&#32452;&#32455;&#34892;&#20026;&#12290;&#20276;&#38543;&#30528;&#23618;&#27425;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#24046;&#34987;&#24341;&#20837;&#21040;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#36825;&#26263;&#31034;&#20102;&#25915;&#20987;&#32773;&#25913;&#36827;&#20854;&#25512;&#26029;&#25915;&#20987;&#24615;&#33021;&#30340;&#28508;&#22312;&#25299;&#25169;&#20851;&#31995;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#20445;&#25252;&#26694;&#26550;&#22312;&#23618;&#27425;&#20256;&#25773;&#20013;&#26080;&#27861;&#27491;&#30830;&#20272;&#35745;&#23618;&#27425;&#24615;&#25200;&#21160;&#36793;&#30028;&#30340;&#33258;&#36866;&#24212;&#19978;&#30028;&#65292;&#23548;&#33268;&#20854;&#22312;&#23618;&#27425;&#20256;&#25773;&#20013;&#20445;&#25252;&#33021;&#21147;&#38477;&#20302;&#65292;&#24613;&#38656;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#23618;&#27425;&#29305;&#24615;&#20197;&#30830;&#20445;&#38544;&#31169;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Poincar&#233;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#30340;PoinDP&#65292;&#29992;&#20110;&#20445;&#25252;&#22522;&#20110;&#23618;&#27425;&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12183v3 Announce Type: replace  Abstract: Hierarchy is an important and commonly observed topological property in real-world graphs that indicate the relationships between supervisors and subordinates or the organizational behavior of human groups. As hierarchy is introduced as a new inductive bias into the Graph Neural Networks (GNNs) in various tasks, it implies latent topological relations for attackers to improve their inference attack performance, leading to serious privacy leakage issues. In addition, existing privacy-preserving frameworks suffer from reduced protection ability in hierarchical propagation due to the deficiency of adaptive upper-bound estimation of the hierarchical perturbation boundary. It is of great urgency to effectively leverage the hierarchical property of data while satisfying privacy guarantees. To solve the problem, we propose the Poincar\'e Differential Privacy framework, named PoinDP, to protect the hierarchy-aware graph embedding based on hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Panda&#26426;&#22120;&#20154;&#33218;&#19978;&#21019;&#24314;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2312.09468</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#33218;&#20013;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning in a Simulated Robotic Arm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Panda&#26426;&#22120;&#20154;&#33218;&#19978;&#21019;&#24314;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#65292;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#25311;&#22120;&#30340;&#24191;&#27867;&#20351;&#29992;&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;&#23433;&#20840;&#25506;&#32034;&#65292;&#24403;RL&#31995;&#32479;&#38656;&#35201;&#30452;&#25509;&#22312;&#29289;&#29702;&#29615;&#22659;&#65288;&#20363;&#22914;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#65289;&#20013;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23433;&#20840;&#25506;&#32034;&#23558;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#27969;&#34892;&#30340;Safety Gym&#24211;&#25552;&#20379;&#20102;&#19977;&#31181;&#31227;&#21160;&#20195;&#29702;&#31867;&#22411;&#65292;&#21487;&#20197;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#21516;&#26102;&#32771;&#34385;&#21508;&#31181;&#23433;&#20840;&#32422;&#26463;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24102;&#26377;Panda&#26426;&#22120;&#20154;&#33218;&#30340;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#27979;&#35797;Safety Gym&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#27969;&#34892;&#30340;PPO&#31639;&#27861;&#36827;&#34892;&#20102;&#35797;&#28857;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#32447;&#19982;&#21463;&#38480;&#29256;&#26412;&#65292;&#24182;&#34920;&#26126;&#21463;&#38480;&#29256;&#26412;&#33021;&#22815;&#23398;&#20064;&#20986;&#21516;&#26679;&#20248;&#31168;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#31526;&#21512;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09468v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2312.05248</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#30340;&#21435;&#37325;&#24314;&#38450;&#25252;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Topology-Based Reconstruction Prevention for Decentralised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#33719;&#24471;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#21327;&#35843;&#37117;&#20998;&#24067;&#22312;&#29992;&#25143;&#20043;&#38388;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20381;&#36182;&#20110;&#24046;&#20998;&#38544;&#31169;&#12289;&#22810;&#26041;&#35745;&#31639;&#65292;&#25110;&#32773;&#20108;&#32773;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#36816;&#34892;&#22810;&#20010;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21487;&#33021;&#20250;&#20351;&#23545;&#25163;&#36827;&#34892;&#37325;&#24314;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;&#37325;&#24314;&#23545;&#31574;&#35201;&#20040;&#26080;&#27861;&#31616;&#21333;&#22320;&#36866;&#24212;&#20998;&#24067;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#20250;&#28155;&#21152;&#36807;&#22810;&#30340;&#22122;&#38899;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#20043;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#25299;&#25169;&#20013;&#26377;18&#20010;&#29992;&#25143;&#30340;&#23376;&#22270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#19977;&#20010;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#25104;&#21151;&#37325;&#24314;&#31169;&#20154;&#25968;&#25454;&#30340;&#27010;&#29575;&#20026;11.0%&#65292;&#24179;&#22343;&#27599;&#20010;&#23545;&#25163;&#38656;&#35201;8.8&#27425;&#27714;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05248v2 Announce Type: replace-cross  Abstract: Decentralised learning has recently gained traction as an alternative to federated learning in which both data and coordination are distributed over its users. To preserve data confidentiality, decentralised learning relies on differential privacy, multi-party computation, or a combination thereof. However, running multiple privacy-preserving summations in sequence may allow adversaries to perform reconstruction attacks. Unfortunately, current reconstruction countermeasures either cannot trivially be adapted to the distributed setting, or add excessive amounts of noise.   In this work, we first show that passive honest-but-curious adversaries can infer other users' private data after several privacy-preserving summations. For example, in subgraphs with 18 users, we show that only three passive honest-but-curious adversaries succeed at reconstructing private data 11.0% of the time, requiring an average of 8.8 summations per adve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#26368;&#31967;&#31957;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20316;&#20026;&#35299;&#20915;&#32452;&#20869;&#20844;&#24179;&#25361;&#25112;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.03151</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#26368;&#24046;&#32676;&#20307;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning Can Improve Worst-Group Outcomes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#26368;&#31967;&#31957;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20316;&#20026;&#35299;&#20915;&#32452;&#20869;&#20844;&#24179;&#25361;&#25112;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21019;&#24314;&#33021;&#22815;&#20026;&#21508;&#31181;&#29992;&#25143;&#25552;&#20379;&#33391;&#22909;&#26381;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#19981;&#20165;&#38656;&#35201;&#23454;&#29616;&#39640;&#24179;&#22343;&#24615;&#33021;&#65292;&#36824;&#38656;&#35201;&#30830;&#20445;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#23454;&#29616;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25913;&#21892;&#27169;&#22411;&#22312;&#36873;&#25321;&#30340;&#26368;&#32456;&#20219;&#21153;&#19978;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#23545;&#26368;&#24046;&#32676;&#20307;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#19981;&#20165;&#29702;&#35299;MTL&#23545;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#25506;&#35752;&#20854;&#20316;&#20026;&#35299;&#20915;&#32452;&#20869;&#20844;&#24179;&#25361;&#25112;&#30340;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20027;&#35201;&#32771;&#34385;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26631;&#20934;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#20219;&#21153;&#19982;&#26469;&#33258;&#26368;&#32456;&#20219;&#21153;&#25968;&#25454;&#26412;&#36523;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#22810;&#20219;&#21153;&#22788;&#29702;&#12290;&#22312;&#23569;&#37327;&#25110;&#27809;&#26377;&#32676;&#20307;&#27880;&#37322;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#20219;&#21153;&#22788;&#29702;&#36890;&#24120;&#65292;&#20294;&#19981;&#24635;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03151v2 Announce Type: replace  Abstract: In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work \citep{gururangan2020don, dery2023aang}, we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.02959</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#26816;&#27979;&#31639;&#27861;&#20559;&#20506;
&lt;/p&gt;
&lt;p&gt;
Detecting algorithmic bias in medical AI-models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26085;&#30410;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#20197;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;&#26041;&#24335;&#25552;&#20379;&#24739;&#32773;&#32467;&#26524;&#21464;&#24471;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;CART&#65289;&#31639;&#27861;&#65292;&#22312;&#33043;&#27602;&#30151;&#39044;&#27979;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#20559;&#20506;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20351;&#29992;&#20122;&#29305;&#20848;&#22823;&#20052;&#27835;&#20122;&#24030;&#26684;&#38647;&#36842;&#32426;&#24565;&#21307;&#38498;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#23454;&#39564;&#36827;&#19968;&#27493;&#24471;&#21040;&#39564;&#35777;&#12290;&#36825;&#20123;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#31574;&#30053;&#22312;&#20020;&#24202;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02959v3 Announce Type: replace-cross  Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clini
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#23398;&#20064;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#36890;&#36807;Riemannian&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20855;&#26377;&#19968;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2311.14658</link><description>&lt;p&gt;
&#23398;&#20064;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#23398;&#20064;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#36890;&#36807;Riemannian&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20855;&#26377;&#19968;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#26435;&#37325;&#30697;&#38453;&#20855;&#26377;&#27491;&#20132;&#25110;&#31561;&#36317;&#24615;&#36136;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#20943;&#36731;&#26799;&#24230;&#29190;&#28856;/&#28040;&#22833;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#24182;&#22686;&#21152;&#23398;&#21040;&#30340;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#23454;&#38469;&#24615;&#33021;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#20013;&#27491;&#20132;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#65307;&#20363;&#22914;&#65292;&#27491;&#20132;&#24615;&#22914;&#20309;&#24433;&#21709;&#35757;&#32451;&#36807;&#31243;&#30340;&#25910;&#25947;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20026;&#35757;&#32451;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#25910;&#25947;&#20998;&#26512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;Riemannian&#26799;&#24230;&#19979;&#38477;&#22312;&#19968;&#31867;&#25439;&#22833;&#20989;&#25968;&#19979;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#35757;&#32451;&#27491;&#20132;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#30340;&#36890;&#36807;&#27491;&#20132;&#21270;&#25152;&#26377;&#23618;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25490;&#38500;&#20102;&#23545;&#19968;&#20010;&#23618;&#30340;&#27491;&#20132;&#26435;&#37325;&#30697;&#38453;&#30340;&#35201;&#27714;&#65292;&#36825;&#23545;&#24314;&#31435;&#25910;&#25947;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14658v2 Announce Type: replace  Abstract: Enforcing orthonormal or isometric property for the weight matrices has been shown to enhance the training of deep neural networks by mitigating gradient exploding/vanishing and increasing the robustness of the learned networks. However, despite its practical performance, the theoretical analysis of orthonormality in neural networks is still lacking; for example, how orthonormality affects the convergence of the training process. In this letter, we aim to bridge this gap by providing convergence analysis for training orthonormal deep linear neural networks. Specifically, we show that Riemannian gradient descent with an appropriate initialization converges at a linear rate for training orthonormal deep linear neural networks with a class of loss functions. Unlike existing works that enforce orthonormal weight matrices for all the layers, our approach excludes this requirement for one layer, which is crucial to establish the convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.13817</link><description>&lt;p&gt;
&#20998;&#23376;&#37492;&#23450;&#19982;&#23792;&#24402;&#23646;&#65306;&#22312;NMR&#19978;&#21033;&#29992;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#35299;&#35835;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#24577;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#22522;&#20110;AI&#30340;NMR&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20855;&#26377;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#40784;&#30340;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20004;&#31181;&#24322;&#36136;&#27169;&#24577;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65306;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#12290;K-M3AID&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#21327;&#35843;&#23545;&#27604;&#23398;&#20064;&#26550;&#26500;&#65292;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#22270;&#32423;&#23545;&#40784;&#27169;&#22359;&#12289;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#21644;&#36890;&#20449;&#36890;&#36947;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#20013;&#65292;K-M3AID&#24341;&#20837;&#20102;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;K-M3AID&#34920;&#26126;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 Announce Type: replace  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays an essential role in deciphering molecular structure and dynamic behaviors. While AI-enhanced NMR prediction models hold promise, challenges still persist in tasks such as molecular retrieval, isomer recognition, and peak assignment. In response, this paper introduces a novel solution, Multi-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination (K-M3AID), which establishes correspondences between two heterogeneous modalities: molecular graphs and NMR spectra. K-M3AID employs a dual-coordinated contrastive learning architecture with three key modules: a graph-level alignment module, a node-level alignment module, and a communication channel. Notably, K-M3AID introduces knowledge-guided instance-wise discrimination into contrastive learning within the node-level alignment module. In addition, K-M3AID demonstrates that skills acquired during node-level alignment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FedHCA$^2$&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#24322;&#26500;&#23458;&#25143;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#27169;&#22411;&#32852;&#37030;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.13250</link><description>&lt;p&gt;
FedHCA$^2$: &#38754;&#21521;&#24322;&#26500;&#23458;&#25143;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedHCA$^2$&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#24322;&#26500;&#23458;&#25143;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#27169;&#22411;&#32852;&#37030;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20351;&#29992;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#24314;&#31435;&#22312;FL&#22522;&#30784;&#19978;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#20551;&#35774;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#37096;&#32626;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#20026;&#20102;&#25918;&#23485;&#36825;&#19968;&#20551;&#35774;&#65292;&#20174;&#32780;&#25193;&#23637;&#29616;&#23454;&#19990;&#30028;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#38754;&#21521;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;HC-FMTL&#65289;&#65292;&#20197;&#36866;&#24212;&#22810;&#26679;&#30340;&#20219;&#21153;&#35774;&#32622;&#12290;HC-FMTL&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36825;&#20351;&#20256;&#32479;&#32858;&#21512;&#26041;&#27861;&#22833;&#25928;&#12290;&#36825;&#20063;&#20351;&#24471;&#20934;&#30830;&#27169;&#22411;&#32858;&#21512;&#20197;&#22788;&#29702;FMTL&#20013;&#22266;&#26377;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#24322;&#36136;&#24615;&#30340;&#22256;&#38590;&#21152;&#21095;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCA$^2$&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#23545;&#24322;&#26500;&#23458;&#25143;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#32852;&#37030;&#35757;&#32451;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#23458;&#25143;&#31471;&#19981;&#21516;&#20043;&#22788;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13250v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA$^2$ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference be
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#22312;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#19979;&#30340;&#26368;&#23567;&#26680;&#24515;&#38598;&#22823;&#23567;&#31934;&#21270;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20248;&#21270;&#26680;&#24515;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.08675</link><description>&lt;p&gt;
&#32463;&#36807;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#30340;&#26368;&#23567;&#26680;&#24515;&#38598;&#22823;&#23567;&#31934;&#21270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08675
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#22312;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#19979;&#30340;&#26368;&#23567;&#26680;&#24515;&#38598;&#22823;&#23567;&#31934;&#21270;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20248;&#21270;&#26680;&#24515;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#24515;&#38598;&#36873;&#25321;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12289;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#20316;&#29992;&#12290;&#23427;&#33268;&#21147;&#20110;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#35782;&#21035;&#19968;&#20010;&#23567;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20165;&#22312;&#23376;&#38598;&#19978;&#35757;&#32451;&#23601;&#33021;&#23454;&#38469;&#19978;&#19982;&#23436;&#25972;&#25968;&#25454;&#34920;&#29616;&#30456;&#24403;&#12290;&#23454;&#36341;&#32773;&#32463;&#24120;&#24076;&#26395;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#35782;&#21035;&#21487;&#33021;&#30340;&#26368;&#23567;&#26680;&#24515;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#26368;&#23567;&#21270;&#25104;&#26412;&#21644;&#26368;&#22823;&#21270;&#21152;&#36895;&#12290;&#21463;&#27492;&#24895;&#26223;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#32463;&#36807;&#31934;&#21270;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#19979;&#30340;&#26368;&#23567;&#26680;&#24515;&#38598;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26680;&#24515;&#38598;&#36873;&#25321;&#36807;&#31243;&#20013;&#20445;&#25345;&#20248;&#21270;&#20248;&#20808;&#39034;&#24207;&#65292;&#20248;&#20808;&#32771;&#34385;&#27169;&#22411;&#24615;&#33021;&#21644;&#26680;&#24515;&#38598;&#22823;&#23567;&#65292;&#24182;&#26377;&#25928;&#22320;&#20248;&#21270;&#23427;&#20204;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08675v2 Announce Type: replace  Abstract: Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. Practitioners regularly desire to identify the smallest possible coreset in realistic scenes while maintaining comparable model performance, to minimize costs and maximize acceleration. Motivated by this desideratum, for the first time, we pose the problem of refined coreset selection, in which the minimal coreset size under model performance constraints is explored. Moreover, to address this problem, we propose an innovative method, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20316;&#20026;&#29992;&#25143;&#32423;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;2022&#24180;&#27861;&#22269;&#24635;&#32479;&#36873;&#20030;&#30456;&#20851;&#30340;Twitter&#26631;&#31614;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#20998;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#20855;&#26377;&#26368;&#21463;&#27426;&#36814;&#30340;&#21704;&#24076;&#26631;&#31614;&#30340;&#26368;&#22823;&#29983;&#25104;&#26641;</title><link>https://arxiv.org/abs/2310.07576</link><description>&lt;p&gt;
&#20998;&#26512;2022&#24180;&#27861;&#22269;&#36873;&#20030;&#20013;&#27969;&#34892;&#30340;Twitter&#35805;&#39064;
&lt;/p&gt;
&lt;p&gt;
Analyzing Trendy Twitter Hashtags in the 2022 French Election
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07576
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20316;&#20026;&#29992;&#25143;&#32423;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;2022&#24180;&#27861;&#22269;&#24635;&#32479;&#36873;&#20030;&#30456;&#20851;&#30340;Twitter&#26631;&#31614;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#20998;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#20855;&#26377;&#26368;&#21463;&#27426;&#36814;&#30340;&#21704;&#24076;&#26631;&#31614;&#30340;&#26368;&#22823;&#29983;&#25104;&#26641;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26410;&#26469;&#27963;&#21160;&#30340;&#22238;&#24402;&#27169;&#22411;&#38656;&#35201;&#20016;&#23500;&#30340;&#29305;&#24449;&#25165;&#33021;&#24471;&#21040;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#35768;&#22810;&#20808;&#36827;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#36825;&#20123;&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#36890;&#24120;&#26159;&#31105;&#38178;&#30340;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#35821;&#20041;&#32593;&#32476;&#29305;&#24449;&#21487;&#33021;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#29992;&#20110;&#22238;&#24402;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20316;&#20026;&#29992;&#25143;&#32423;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;370&#19975;&#26465;&#19982;2022&#24180;&#27861;&#22269;&#24635;&#32479;&#36873;&#20030;&#26377;&#20851;&#30340;&#25512;&#25991;&#30340;1037&#20010;Twitter&#26631;&#31614;&#30340;&#35821;&#20041;&#32593;&#32476;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#20998;&#22270;&#65292;&#20854;&#20013;&#26631;&#31614;&#26159;&#33410;&#28857;&#65292;&#24102;&#26435;&#36793;&#36830;&#25509;&#26631;&#31614;&#65292;&#21453;&#26144;&#20102;&#19982;&#36825;&#20123;&#26631;&#31614;&#20114;&#21160;&#30340;Twitter&#29992;&#25143;&#25968;&#37327;&#12290;&#28982;&#21518;&#23558;&#22270;&#36716;&#25442;&#20026;&#20855;&#26377;&#26368;&#21463;&#27426;&#36814;&#30340;&#21704;&#24076;&#26631;&#31614;&#30340;&#26368;&#22823;&#29983;&#25104;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07576v2 Announce Type: replace-cross  Abstract: Regressions trained to predict the future activity of social media users need rich features for accurate predictions. Many advanced models exist to generate such features; however, the time complexities of their computations are often prohibitive when they run on enormous data-sets. Some studies have shown that simple semantic network features can be rich enough to use for regressions without requiring complex computations. We propose a method for using semantic networks as user-level features for machine learning tasks. We conducted an experiment using a semantic network of 1037 Twitter hashtags from a corpus of 3.7 million tweets related to the 2022 French presidential election. A bipartite graph is formed where hashtags are nodes and weighted edges connect the hashtags reflecting the number of Twitter users that interacted with both hashtags. The graph is then transformed into a maximum-spanning tree with the most popular ha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#40481;&#23614;&#37202;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#28151;&#21512;&#21040;&#19968;&#20010;&#23884;&#20837;&#20013;&#65292;&#32467;&#21512;&#36890;&#29992;&#25511;&#21046;&#32593;&#32476;&#12289;&#21487;&#25511;&#24402;&#19968;&#21270;&#21644;&#31354;&#38388;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26412;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#31354;&#38388;&#31934;&#32454;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2306.00964</link><description>&lt;p&gt;
&#40481;&#23614;&#37202;&#65306;&#28151;&#21512;&#22810;&#27169;&#24577;&#25511;&#21046;&#20197;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#40481;&#23614;&#37202;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#28151;&#21512;&#21040;&#19968;&#20010;&#23884;&#20837;&#20013;&#65292;&#32467;&#21512;&#36890;&#29992;&#25511;&#21046;&#32593;&#32476;&#12289;&#21487;&#25511;&#24402;&#19968;&#21270;&#21644;&#31354;&#38388;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26412;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#31354;&#38388;&#31934;&#32454;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00964v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25991;&#26412;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#20869;&#23481;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#34920;&#31034;&#32463;&#24120;&#23637;&#31034;&#23545;&#25152;&#35774;&#24819;&#30446;&#26631;&#22270;&#20687;&#30340;&#27169;&#26865;&#20004;&#21487;&#25551;&#36848;&#65292;&#38656;&#35201;&#24341;&#20837;&#39069;&#22806;&#30340;&#25511;&#21046;&#20449;&#21495;&#26469;&#22686;&#24378;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40481;&#23614;&#37202;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21508;&#31181;&#27169;&#24577;&#28151;&#21512;&#20026;&#19968;&#20010;&#23884;&#20837;&#30340;&#27969;&#27700;&#32447;&#65292;&#19982;&#36890;&#29992;&#25511;&#21046;&#32593;&#32476;&#65288;gControlNet&#65289;&#12289;&#21487;&#25511;&#24402;&#19968;&#21270;&#65288;ControlNorm&#65289;&#21644;&#31354;&#38388;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25991;&#26412;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#31354;&#38388;&#31934;&#32454;&#25511;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36229;&#32593;&#32476;gControlNet&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#25511;&#21046;&#20449;&#21495;&#19982;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;gControlNet&#33021;&#22815;&#25509;&#21463;&#28789;&#27963;&#30340;&#27169;&#24577;&#20449;&#21495;&#65292;&#21253;&#25324;&#21516;&#26102;&#25509;&#25910;&#20219;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00964v1 Announce Type: cross  Abstract: Text-conditional diffusion models are able to generate high-fidelity images with diverse contents. However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#23545;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#36827;&#34892;&#23450;&#21046;&#21270;&#65292;&#20026;&#25193;&#25955;&#27169;&#22411;&#22686;&#21152;&#20102;&#31890;&#24230;&#25511;&#21046;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2306.00950</link><description>&lt;p&gt;
&#24046;&#20998;&#25193;&#25955;&#65306;&#36171;&#20104;&#27599;&#20010;&#20687;&#32032;&#20197;&#20854;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Differential Diffusion: Giving Each Pixel Its Strength
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#23545;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#36827;&#34892;&#23450;&#21046;&#21270;&#65292;&#20026;&#25193;&#25955;&#27169;&#22411;&#22686;&#21152;&#20102;&#31890;&#24230;&#25511;&#21046;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#21487;&#20197;&#36827;&#34892;&#23450;&#21046;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20026;&#20854;&#22686;&#21152;&#36825;&#31181;&#21151;&#33021;&#12290;&#23545;&#21464;&#21270;&#37327;&#30340;&#31890;&#24230;&#25511;&#21046;&#25171;&#24320;&#20102;&#21508;&#31181;&#26032;&#30340;&#32534;&#36753;&#33021;&#21147;&#65292;&#22914;&#25511;&#21046;&#21333;&#20010;&#23545;&#35937;&#34987;&#20462;&#25913;&#30340;&#31243;&#24230;&#65292;&#25110;&#32773;&#24341;&#20837;&#36880;&#28176;&#30340;&#31354;&#38388;&#21464;&#21270;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#36719;&#20462;&#22797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#22312;&#23436;&#25104;&#22270;&#20687;&#37096;&#20998;&#30340;&#21516;&#26102;&#65292;&#24494;&#35843;&#21608;&#22260;&#21306;&#22495;&#20197;&#30830;&#20445;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00950v2 Announce Type: replace-cross  Abstract: Diffusion models have revolutionized image generation and editing, producing state-of-the-art results in conditioned and unconditioned image synthesis. While current techniques enable user control over the degree of change in an image edit, the controllability is limited to global changes over an entire edited region. This paper introduces a novel framework that enables customization of the amount of change per pixel or per image region. Our framework can be integrated into any existing diffusion model, enhancing it with this capability. Such granular control on the quantity of change opens up a diverse array of new editing capabilities, such as control of the extent to which individual objects are modified, or the ability to introduce gradual spatial changes. Furthermore, we showcase the framework's effectiveness in soft-inpainting -- the completion of portions of an image while subtly adjusting the surrounding areas to ensure
&lt;/p&gt;</description></item><item><title>&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2304.00933</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#19982;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00933
&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#8220;&#36755;&#20986;&#32423;&#21035;&#8221;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#26377;&#20105;&#35758;&#30340;&#26159;&#26159;&#21542;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#32423;&#21035;&#20063;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#22810;&#20010;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#34920;&#31034;&#24402;&#22240;&#20026;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#22266;&#26377;&#25239;&#36951;&#24536;&#24615; - &#20165;&#20250;&#26368;&#23567;&#31243;&#24230;&#24536;&#35760;&#19988;&#19981;&#20250;&#36951;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#25581;&#31034;&#36825;&#31181;&#36951;&#24536;&#24046;&#24322;&#30340;&#23454;&#39564;&#65292;&#35828;&#26126;&#20102;&#24433;&#21709;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#30340;&#20004;&#31181;&#29616;&#35937;&#20849;&#23384;&#65306;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#12290;&#25105;&#20204;&#35880;&#24910;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#34920;&#26126;&#23613;&#31649;&#32477;&#23545;&#20540;&#19978;&#29305;&#24449;&#36951;&#24536;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#19982;&#36755;&#20986;&#23618;&#38754;&#19968;&#26679;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00933v3 Announce Type: replace  Abstract: Continual learning research has shown that neural networks suffer from catastrophic forgetting "at the output level", but it is debated whether this is also the case at the level of learned representations. Multiple recent studies ascribe representations a certain level of innate robustness against forgetting - that they only forget minimally and no critical information. We revisit and expand upon the experiments that revealed this difference in forgetting and illustrate the coexistence of two phenomena that affect the quality of continually learned representations: knowledge accumulation and feature forgetting. Carefully taking both aspects into account, we show that, even though it is true that feature forgetting can be small in absolute terms, newly learned information tends to be forgotten just as catastrophically at the level of the representation as it is at the output level. Next we show that this feature forgetting is problem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSForest&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Conformalized Semi-Supervised Random Forest&#25216;&#26415;&#26469;&#35299;&#20915;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#26631;&#35782;&#26410;&#35265;&#30340;&#31163;&#32676;&#20540;&#12290;</title><link>https://arxiv.org/abs/2302.02237</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#38543;&#26426;&#26862;&#26519;&#30340;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#19968;&#33268;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformalized Semi-supervised Random Forest for Classification and Abnormality Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSForest&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Conformalized Semi-Supervised Random Forest&#25216;&#26415;&#26469;&#35299;&#20915;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#26631;&#35782;&#26410;&#35265;&#30340;&#31163;&#32676;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#20998;&#31867;&#24037;&#20855;&#65292;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#19982;&#20854;&#20182;&#26631;&#20934;&#20998;&#31867;&#22120;&#26469;&#33258;&#30456;&#21516;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#31561;&#20851;&#38190;&#23433;&#20840;&#22330;&#26223;&#20013;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#28508;&#22312;&#20986;&#29616;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#30340;&#26032;&#31163;&#32676;&#26679;&#26412;&#65292;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Conformalized Semi-Supervised Random Forest (CSForest)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#19968;&#33268;&#21270;&#25216;&#26415;Jackknife+aB&#19982;&#21322;&#30417;&#30563;&#26641;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#19968;&#20010;&#38598;&#20540;&#39044;&#27979; C(x)&#12290;CSForest&#19981;&#20877;&#20248;&#21270;&#35757;&#32451;&#20998;&#24067;&#65292;&#32780;&#26159;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#26679;&#26412;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#31354;&#38598;&#26469;&#26631;&#35782;&#26410;&#35265;&#30340;&#31163;&#32676;&#20540;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;CSForest&#35206;&#30422;&#20197;&#21069;&#35266;&#23519;&#21040;&#30340;&#20869;&#32676;&#31867;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02237v2 Announce Type: replace  Abstract: The Random Forests classifier, a widely utilized off-the-shelf classification tool, assumes training and test samples come from the same distribution as other standard classifiers. However, in safety-critical scenarios like medical diagnosis and network attack detection, discrepancies between the training and test sets, including the potential presence of novel outlier samples not appearing during training, can pose significant challenges. To address this problem, we introduce the Conformalized Semi-Supervised Random Forest (CSForest), which couples the conformalization technique Jackknife+aB with semi-supervised tree ensembles to construct a set-valued prediction $C(x)$. Instead of optimizing over the training distribution, CSForest employs unlabeled test samples to enhance accuracy and flag unseen outliers by generating an empty set. Theoretically, we establish CSForest to cover true labels for previously observed inlier classes un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#22270;&#20687;&#20013;&#39044;&#35757;&#32451;&#33719;&#24471;&#30340;&#34920;&#31034;&#36866;&#24212;&#20110;IMU&#24863;&#30693;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;IMG2IMU&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;IMU&#24863;&#30693;&#24212;&#29992;&#30340;&#30693;&#35782;&#36716;&#21270;&#12290;</title><link>https://arxiv.org/abs/2209.00945</link><description>&lt;p&gt;
IMG2IMU&#65306;&#20174;&#22823;&#35268;&#27169;&#22270;&#20687;&#21521;IMU&#24863;&#30693;&#24212;&#29992;&#30340;&#30693;&#35782;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
IMG2IMU: Translating Knowledge from Large-Scale Images to IMU Sensing Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#22270;&#20687;&#20013;&#39044;&#35757;&#32451;&#33719;&#24471;&#30340;&#34920;&#31034;&#36866;&#24212;&#20110;IMU&#24863;&#30693;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;IMG2IMU&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;IMU&#24863;&#30693;&#24212;&#29992;&#30340;&#30693;&#35782;&#36716;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#33719;&#21462;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#22312;&#21363;&#20351;&#26159;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#20219;&#21153;&#19978;&#20063;&#33021;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#19981;&#21516;&#65292;IMU&#24863;&#30693;&#24212;&#29992;&#30340;&#39044;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#36275;&#22815;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#29992;&#20197;&#23398;&#20064;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IMG2IMU&#65292;&#23558;&#20174;&#22823;&#35268;&#27169;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#36866;&#24212;&#20110;&#21508;&#31181;IMU&#24863;&#30693;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#36716;&#25442;&#20026;&#21487;&#35270;&#21270;&#30340;&#35889;&#22270;&#65292;&#20379;&#27169;&#22411;&#21033;&#29992;&#20174;&#35270;&#35273;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22270;&#20687;&#30340;&#20256;&#24863;&#22120;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33719;&#21462;&#23545;IMU&#24863;&#30693;&#24212;&#29992;&#20855;&#26377;&#29305;&#21035;&#24433;&#21709;&#30340;&#30693;&#35782;&#12290;&#36825;&#28041;&#21450;&#22312;&#25105;&#20204;&#20026;&#20256;&#24863;&#22120;&#25968;&#25454;&#23646;&#24615;&#23450;&#21046;&#30340;&#22686;&#24378;&#38598;&#19978;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;IMU&#24863;&#30693;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00945v2 Announce Type: replace  Abstract: Pre-training representations acquired via self-supervised learning could achieve high accuracy on even tasks with small training data. Unlike in vision and natural language processing domains, pre-training for IMU-based applications is challenging, as there are few public datasets with sufficient size and diversity to learn generalizable representations. To overcome this problem, we propose IMG2IMU that adapts pre-trained representation from large-scale images to diverse IMU sensing tasks. We convert the sensor data into visually interpretable spectrograms for the model to utilize the knowledge gained from vision. We further present a sensor-aware pre-training method for images that enables models to acquire particularly impactful knowledge for IMU sensing applications. This involves using contrastive learning on our augmentation set customized for the properties of sensor data. Our evaluation with four different IMU sensing tasks sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE &#21644; JC-OTCE&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2207.05510</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20256;&#36882;&#24615;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Transferability-Guided Cross-Domain Cross-Task Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.05510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE &#21644; JC-OTCE&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE&#65288;&#22522;&#20110;&#24555;&#36895;&#26368;&#20248;&#20256;&#36755;&#30340;&#26465;&#20214;&#29109;&#65289;&#21644; JC-OTCE&#65288;&#32852;&#21512;&#23545;&#24212; OTCE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#65288;&#20219;&#21153;&#65289;&#23545;&#30446;&#26631;&#20219;&#21153;&#23398;&#20064;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;&#19982;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35780;&#20272;&#32463;&#39564;&#20256;&#36882;&#24615;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#26159;&#26080;&#38656;&#36741;&#21161;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;F-OTCE&#39318;&#20808;&#36890;&#36807;&#22312;&#28304;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#26469;&#20272;&#35745;&#20256;&#36882;&#24615;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#20248;&#32806;&#21512;&#26469;&#35745;&#31639;&#28304;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#36127;&#26465;&#20214;&#29109;&#12290;&#23427;&#36824;&#21487;&#20197;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#26368;&#22823;&#21270;&#28304;&#27169;&#22411;&#30340;&#20256;&#36882;&#24615;&#12290;&#21516;&#26102;&#65292;JC-OTCE&#25913;&#21892;&#20102;&#20256;&#36882;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.05510v2 Announce Type: replace-cross  Abstract: We propose two novel transferability metrics F-OTCE (Fast Optimal Transport based Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate how much the source model (task) can benefit the learning of the target task and to learn more transferable representations for cross-domain cross-task transfer learning. Unlike the existing metric that requires evaluating the empirical transferability on auxiliary tasks, our metrics are auxiliary-free such that they can be computed much more efficiently. Specifically, F-OTCE estimates transferability by first solving an Optimal Transport (OT) problem between source and target distributions, and then uses the optimal coupling to compute the Negative Conditional Entropy between source and target labels. It can also serve as a loss function to maximize the transferability of the source model before finetuning on the target task. Meanwhile, JC-OTCE improves the transferability r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;Galerkin&#26041;&#26696;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65292;&#33021;&#22815;&#33258;&#20027;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#27714;&#35299;</title><link>https://arxiv.org/abs/2203.01360</link><description>&lt;p&gt;
&#20855;&#26377;&#20027;&#21160;&#23398;&#20064;&#30340;&#31070;&#32463;Galerkin&#26041;&#26696;&#29992;&#20110;&#39640;&#32500;&#28436;&#21270;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Galerkin Schemes with Active Learning for High-Dimensional Evolution Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.01360
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;Galerkin&#26041;&#26696;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65292;&#33021;&#22815;&#33258;&#20027;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#20013;&#25552;&#20379;&#20934;&#30830;&#30340;&#20989;&#25968;&#36924;&#36817;&#12290;&#28982;&#32780;&#65292;&#25311;&#21512;&#32593;&#32476;&#21442;&#25968;&#38656;&#35201;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#24448;&#24448;&#38590;&#20197;&#25910;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;&#25968;&#20540;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#31070;&#32463;Galerkin&#26041;&#26696;&#22522;&#20110;Dirac-Frenkel&#21464;&#20998;&#21407;&#29702;&#65292;&#36890;&#36807;&#38543;&#26102;&#38388;&#39034;&#24207;&#26368;&#23567;&#21270;&#27531;&#24046;&#26469;&#35757;&#32451;&#32593;&#32476;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#20197;&#33258;&#20027;&#12289;&#21160;&#24577;&#25551;&#36848;&#30340;&#26041;&#24335;&#25910;&#38598;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25351;&#23548;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#21160;&#24577;&#12290;&#36825;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#20854;&#20182;&#26041;&#27861;&#26088;&#22312;&#20840;&#23616;&#26102;&#38388;&#20869;&#25311;&#21512;&#32593;&#32476;&#21442;&#25968;&#65292;&#32780;&#19981;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#33719;&#21462;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#20027;&#21160;&#24418;&#24335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.01360v4 Announce Type: replace-cross  Abstract: Deep neural networks have been shown to provide accurate function approximations in high dimensions. However, fitting network parameters requires informative training data that are often challenging to collect in science and engineering applications. This work proposes Neural Galerkin schemes based on deep learning that generate training data with active learning for numerically solving high-dimensional partial differential equations. Neural Galerkin schemes build on the Dirac-Frenkel variational principle to train networks by minimizing the residual sequentially over time, which enables adaptively collecting new training data in a self-informed manner that is guided by the dynamics described by the partial differential equations. This is in contrast to other machine learning methods that aim to fit network parameters globally in time without taking into account training data acquisition. Our finding is that the active form of 
&lt;/p&gt;</description></item><item><title>CAREER&#27169;&#22411;&#32467;&#21512;&#22823;&#35268;&#27169;&#22312;&#32447;&#31616;&#21382;&#25968;&#25454;&#21644;&#23567;&#22411;&#32437;&#21521;&#35843;&#26597;&#25968;&#25454;&#65292;&#26377;&#25928;&#39044;&#27979;&#24037;&#20316;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2202.08370</link><description>&lt;p&gt;
CAREER&#65306;&#21171;&#21160;&#24207;&#21015;&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAREER: A Foundation Model for Labor Sequence Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.08370
&lt;/p&gt;
&lt;p&gt;
CAREER&#27169;&#22411;&#32467;&#21512;&#22823;&#35268;&#27169;&#22312;&#32447;&#31616;&#21382;&#25968;&#25454;&#21644;&#23567;&#22411;&#32437;&#21521;&#35843;&#26597;&#25968;&#25454;&#65292;&#26377;&#25928;&#39044;&#27979;&#24037;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21160;&#32463;&#27982;&#23398;&#23478;&#32463;&#24120;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#25311;&#21512;&#21040;&#23567;&#22411;&#12289;&#31934;&#24515;&#26500;&#24314;&#30340;&#32437;&#21521;&#35843;&#26597;&#25968;&#25454;&#38598;&#20013;&#26469;&#20998;&#26512;&#23601;&#19994;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#22312;&#32447;&#31616;&#21382;&#25968;&#25454;&#38598;&#20063;&#21464;&#24471;&#21487;&#29992;&#65292;&#25552;&#20379;&#20102;&#25968;&#30334;&#19975;&#20154;&#30340;&#32844;&#19994;&#36712;&#36857;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CAREER&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#24037;&#20316;&#24207;&#21015;&#12290;&#39318;&#20808;&#65292;&#23558;CAREER&#25311;&#21512;&#21040;&#22823;&#35268;&#27169;&#34987;&#21160;&#25910;&#38598;&#30340;&#31616;&#21382;&#25968;&#25454;&#19978;&#65292;&#28982;&#21518;&#24494;&#35843;&#21040;&#35268;&#27169;&#36739;&#23567;&#12289;&#26356;&#22909;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#32463;&#27982;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;CAREER&#25311;&#21512;&#21040;&#20102;&#21253;&#21547;&#26469;&#33258;&#31616;&#21382;&#30340;2400&#19975;&#20010;&#24037;&#20316;&#24207;&#21015;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#24182;&#22312;&#23567;&#22411;&#32437;&#21521;&#35843;&#26597;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;CAREER&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#24037;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.08370v4 Announce Type: replace  Abstract: Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, standard econometric models cannot take advantage of their scale or incorporate them into the analysis of survey data. To this end we develop CAREER, a foundation model for job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned to smaller, better-curated datasets for economic inferences. We fit CAREER to a dataset of 24 million job sequences from resumes, and adjust it on small longitudinal survey datasets. We find that CAREER forms accurate predictions of job sequences
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;VQ-VAE&#30340;&#20869;&#23481;&#20016;&#23500;&#30340;&#31163;&#25955;&#35270;&#35273;&#30721;&#20070;&#65292;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20840;&#23616;&#32972;&#26223;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2112.01799</link><description>&lt;p&gt;
&#22312;&#30690;&#37327;&#37327;&#21270;&#24314;&#27169;&#20013;&#24212;&#29992;&#31163;&#25955;&#25193;&#25955;&#30340;&#20840;&#23616;&#32972;&#26223;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Global Context with Discrete Diffusion in Vector Quantised Modelling for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.01799
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;VQ-VAE&#30340;&#20869;&#23481;&#20016;&#23500;&#30340;&#31163;&#25955;&#35270;&#35273;&#30721;&#20070;&#65292;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20840;&#23616;&#32972;&#26223;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#25972;&#21512;&#20316;&#20026;&#29983;&#25104;&#37096;&#20998;&#22312;&#22270;&#20687;&#29983;&#25104;&#19978;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#20250;&#20005;&#26684;&#36981;&#24490;&#36880;&#28176;&#25195;&#25551;&#39034;&#24207;&#65292;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;VQ&#31995;&#21015;&#27169;&#22411;&#24456;&#38590;&#25670;&#33073;&#32570;&#20047;&#20840;&#23616;&#20449;&#24687;&#30340;&#22256;&#22659;&#12290;&#22312;&#36830;&#32493;&#22495;&#20013;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#25191;&#34892;&#25991;&#26412;&#29983;&#25104;&#21644;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#20174;VQ-VAE&#33719;&#24471;&#20869;&#23481;&#20016;&#23500;&#30340;&#31163;&#25955;&#35270;&#35273;&#30721;&#20070;&#30340;&#24110;&#21161;&#65292;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20063;&#33021;&#29983;&#25104;&#20855;&#26377;&#20840;&#23616;&#32972;&#26223;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#32463;&#20856;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#20687;&#32032;&#31354;&#38388;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.01799v1 Announce Type: cross  Abstract: The integration of Vector Quantised Variational AutoEncoder (VQ-VAE) with autoregressive models as generation part has yielded high-quality results on image generation. However, the autoregressive models will strictly follow the progressive scanning order during the sampling phase. This leads the existing VQ series models to hardly escape the trap of lacking global information. Denoising Diffusion Probabilistic Models (DDPM) in the continuous domain have shown a capability to capture the global context, while generating high-quality images. In the discrete state space, some works have demonstrated the potential to perform text generation and low resolution image generation. We show that with the help of a content-rich discrete visual codebook from VQ-VAE, the discrete diffusion model can also generate high fidelity images with global context, which compensates for the deficiency of the classical autoregressive model along pixel space. 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#20027;&#12289;&#33258;&#25105;&#28608;&#21169;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#36880;&#27493;&#23398;&#20064;&#21644;&#25552;&#21319;&#30693;&#35782;&#19982;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2110.11385</link><description>&lt;p&gt;
&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Initiated Open World Learning for Autonomous AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.11385
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#20027;&#12289;&#33258;&#25105;&#28608;&#21169;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#36880;&#27493;&#23398;&#20064;&#21644;&#25552;&#21319;&#30693;&#35782;&#19982;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#23454;&#38469;&#24212;&#29992;&#65292;&#26159;&#26102;&#20505;&#32771;&#34385;&#22914;&#20309;&#20351;&#36825;&#20123;&#20195;&#29702;&#23436;&#20840;&#33258;&#20027;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#23450;&#26399;&#30001;&#20154;&#31867;&#24037;&#31243;&#24072;&#21551;&#21160;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#26159;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#30340;&#24320;&#25918;&#29615;&#22659;&#65292;&#26816;&#27979;&#26032;&#39062;&#24615;&#25110;&#26410;&#30693;&#24615;&#65292;&#25551;&#36848;&#23427;&#20204;&#65292;&#36866;&#24212;&#25110;&#36866;&#24212;&#23427;&#20204;&#65292;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36880;&#27493;&#23398;&#20064;&#26410;&#30693;&#21644;&#26032;&#39062;&#24615;&#23545;&#20110;&#20351;&#20195;&#29702;&#36234;&#26469;&#36234;&#26377;&#30693;&#35782;&#21644;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20351;&#20854;&#30001;&#20195;&#29702;&#20027;&#21160;&#36827;&#34892;&#24182;&#36890;&#36807;&#20854;&#19982;&#20154;&#31867;&#21644;&#29615;&#22659;&#30340;&#20114;&#21160;&#36827;&#34892;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#24120;&#26377;&#19968;&#20010;&#24615;&#33021;&#20219;&#21153;&#65292;&#22240;&#27492;&#23545;&#27599;&#31181;&#26032;&#39062;&#24615;&#36827;&#34892;&#29305;&#24449;&#21270;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#35201;&#65292;&#20197;&#20415;&#20195;&#29702;&#21487;&#20197;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.11385v3 Announce Type: replace  Abstract: As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can learn by themselves in a self-motivated and self-supervised manner rather than being retrained periodically on the initiation of human engineers using expanded training data. As the real-world is an open environment with unknowns or novelties, detecting novelties or unknowns, characterizing them, accommodating or adapting to them, gathering ground-truth training data, and incrementally learning the unknowns/novelties are critical to making the agent more and more knowledgeable and powerful over time. The key challenge is how to automate the process so that it is carried out on the agent's own initiative and through its own interactions with humans and the environment. Since an AI agent usually has a performance task, characterizing each novelty becomes critical and necessary so that the agent can formu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;JC-NCE&#20998;&#25968;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2106.10479</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#29992;&#21487;&#36716;&#31227;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Practical Transferability Estimation for Image Classification Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.10479
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;JC-NCE&#20998;&#25968;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36716;&#31227;&#24615;&#20272;&#35745;&#26159;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#29992;&#20110;&#39044;&#27979;&#23558;&#28304;&#27169;&#22411;&#65288;&#25110;&#28304;&#20219;&#21153;&#65289;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#26102;&#24615;&#33021;&#26377;&#22810;&#22909;&#12290;&#26368;&#36817;&#65292;&#20998;&#26512;&#24615;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#28304;&#27169;&#22411;&#36873;&#25321;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#30340;&#35774;&#32622;&#19979;&#20351;&#36716;&#31227;&#24615;&#20272;&#35745;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;OTCE&#20998;&#25968;&#36890;&#36807;&#32771;&#34385;&#22495;&#21644;&#20219;&#21153;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20511;&#21161;&#36741;&#21161;&#20219;&#21153;&#30340;&#36716;&#31227;&#32463;&#39564;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#25928;&#29575;&#24320;&#38144;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;JC-NCE&#20998;&#25968;&#30340;&#23454;&#29992;&#36716;&#31227;&#24615;&#24230;&#37327;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#24314;&#31435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#30340;&#32852;&#21512;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.10479v3 Announce Type: replace-cross  Abstract: Transferability estimation is an essential problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. Recent analytical transferability metrics have been widely used for source model selection and multi-task learning. A major challenge is how to make transfereability estimation robust under the cross-domain cross-task settings. The recently proposed OTCE score solves this problem by considering both domain and task differences, with the help of transfer experiences on auxiliary tasks, which causes an efficiency overhead. In this work, we propose a practical transferability metric called JC-NCE score that dramatically improves the robustness of the task difference estimation in OTCE, thus removing the need for auxiliary tasks. Specifically, we build the joint correspondences between source and target data via solving an optimal transport problem with a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22270;&#20449;&#21495;&#20013;&#22343;&#20540;&#21464;&#21270;&#28857;&#65292;&#36890;&#36807;&#22312;&#39057;&#35889;&#22495;&#35299;&#20915;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#31232;&#30095;&#24615;&#65292;&#37319;&#29992;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#33258;&#21160;&#30830;&#23450;&#21464;&#28857;&#30340;&#25968;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#38750;&#28176;&#36817;oracle&#19981;&#31561;&#24335;&#30340;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2006.10628</link><description>&lt;p&gt;
&#31163;&#32447;&#26816;&#27979;&#24179;&#31283;&#22270;&#20449;&#21495;&#22343;&#20540;&#21464;&#21270;&#28857;
&lt;/p&gt;
&lt;p&gt;
Offline detection of change-points in the mean for stationary graph signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.10628
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22270;&#20449;&#21495;&#20013;&#22343;&#20540;&#21464;&#21270;&#28857;&#65292;&#36890;&#36807;&#22312;&#39057;&#35889;&#22495;&#35299;&#20915;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#31232;&#30095;&#24615;&#65292;&#37319;&#29992;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#33258;&#21160;&#30830;&#23450;&#21464;&#28857;&#30340;&#25968;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#38750;&#28176;&#36817;oracle&#19981;&#31561;&#24335;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#22270;&#20449;&#21495;&#27969;&#20998;&#21106;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#26088;&#22312;&#26816;&#27979;&#24050;&#30693;&#22270;&#19978;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#22343;&#20540;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#22270;&#20449;&#21495;&#24179;&#31283;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20801;&#35768;&#23558;&#38382;&#39064;&#20174;&#21407;&#22987;&#39030;&#28857;&#22495;&#36716;&#25442;&#21040;&#39057;&#35889;&#22495;&#65288;&#22270;&#20613;&#37324;&#21494;&#21464;&#25442;&#65289;&#65292;&#22312;&#37027;&#37324;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#34429;&#28982;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33719;&#24471;&#30340;&#39057;&#35889;&#34920;&#31034;&#26159;&#31232;&#30095;&#30340;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#29305;&#24615;&#22312;&#29616;&#26377;&#30456;&#20851;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#39057;&#35889;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#33258;&#21160;&#30830;&#23450;&#21464;&#28857;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26816;&#27979;&#22120;&#20276;&#38543;&#30528;&#38750;&#28176;&#36817;&#20248;&#31561;&#24615;&#30340;&#35777;&#26126;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.10628v2 Announce Type: replace  Abstract: This paper addresses the problem of segmenting a stream of graph signals: we aim to detect changes in the mean of a multivariate signal defined over the nodes of a known graph. We propose an offline method that relies on the concept of graph signal stationarity and allows the convenient translation of the problem from the original vertex domain to the spectral domain (Graph Fourier Transform), where it is much easier to solve. Although the obtained spectral representation is sparse in real applications, to the best of our knowledge this property has not been sufficiently exploited in the existing related literature. Our change-point detection method adopts a model selection approach that takes into account the sparsity of the spectral representation and determines automatically the number of change-points. Our detector comes with a proof of a non-asymptotic oracle inequality. Numerical experiments demonstrate the performance of the p
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22996;&#21592;&#20250;&#26426;&#22120;&#30340;&#20005;&#26684;&#29702;&#35770;&#22522;&#30784;&#21644;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#35745;&#31639;&#21040;&#32479;&#35745;&#23398;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/1806.05451</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#26426;&#22120;&#65306;&#23398;&#20064;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#35745;&#31639;&#21040;&#32479;&#35745;&#23398;&#24046;&#36317;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The committee machine: Computational to statistical gaps in learning a two-layers neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1806.05451
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22996;&#21592;&#20250;&#26426;&#22120;&#30340;&#20005;&#26684;&#29702;&#35770;&#22522;&#30784;&#21644;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#35745;&#31639;&#21040;&#32479;&#35745;&#23398;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#65292;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#21551;&#21457;&#24335;&#24037;&#20855;&#34987;&#29992;&#26469;&#23450;&#20301;&#30456;&#21464;&#24182;&#35745;&#31639;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#25945;&#24072;-&#23398;&#29983;&#22330;&#26223;&#20013;&#30340;&#26368;&#20248;&#23398;&#20064;&#21644;&#27867;&#21270;&#38169;&#35823;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19968;&#20010;&#21517;&#20026;&#22996;&#21592;&#20250;&#26426;&#22120;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20005;&#26684;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22996;&#21592;&#20250;&#26426;&#22120;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#31639;&#27861;&#29256;&#26412;&#65292;&#20801;&#35768;&#22312;&#22810;&#31181;&#21442;&#25968;&#19979;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#25191;&#34892;&#26368;&#20339;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#34429;&#28982;AMP&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#65292;&#20294;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#21487;&#20197;&#23454;&#29616;&#20302;&#27867;&#21270;&#38169;&#35823;&#29575;&#65292;&#36825;&#24378;&#28872;&#26263;&#31034;&#23545;&#20110;&#36825;&#20123;&#24773;&#20917;&#19981;&#23384;&#22312;&#26377;&#25928;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24040;&#22823;&#30340;&#35745;&#31639;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1806.05451v3 Announce Type: replace  Abstract: Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09181</link><description>&lt;p&gt;
&#36229;&#36234;&#21453;&#36951;&#24536;: &#24102;&#26377;&#27491;&#21521;&#20256;&#36882;&#30340;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. (arXiv:2401.09181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#65288;MCIT&#65289;&#20351;&#24471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#28385;&#36275;&#19981;&#26029;&#20986;&#29616;&#30340;&#38656;&#27714;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;MCIT&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65306;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;&#26087;&#30693;&#35782;&#34987;&#36951;&#24536;&#65289;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#65288;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65289;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22823;&#22823;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#28982;&#36973;&#21463;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#36755;&#20837;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#19982;&#26087;&#30340;&#21644;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fwd-Prompt&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#26799;&#24230;&#25237;&#24433;&#21040;&#27531;&#24046;&#31354;&#38388;&#20013;&#65292;&#20197;&#20943;&#23567;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#24182;&#25237;&#24433;&#21040;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#20197;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.02013</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#29992;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#12290;&#31163;&#32447;GCRL&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#22810;&#26679;&#21270;&#21644;&#21487;&#22797;&#29992;&#25216;&#33021;&#30340;&#36890;&#29992;&#22411;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#20195;GCRL&#26041;&#27861;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;GCRL&#30340;&#21478;&#19968;&#31181;&#35266;&#28857;&#26159;&#20248;&#21270;&#21344;&#26377;&#21305;&#37197;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#38543;&#21518;&#35813;&#37492;&#21035;&#22120;&#20316;&#20026;&#19979;&#28216;&#24378;&#21270;&#23398;&#20064;&#30340;&#20266;&#22870;&#21169;&#12290;&#23398;&#20064;&#21040;&#30340;&#37492;&#21035;&#22120;&#30340;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#29983;&#25104;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GCRL&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#35270;&#35282;&#65292;&#37319;&#29992;&#26080;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#65306;SMORe&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;GCRL&#30340;&#21344;&#26377;&#21305;&#37197;&#35270;&#35282;&#19982;&#19968;&#20010;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26080;&#37492;&#21035;&#22120;&#26041;&#27861;&#65306;SMORe&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a conve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;SDE&#21644;ODE&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SDE&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.01410</link><description>&lt;p&gt;
&#38543;&#26426;&#24615;&#30340;&#31119;&#38899;&#65306;SDE&#22312;&#19968;&#33324;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#20013;&#36229;&#36807;ODE
&lt;/p&gt;
&lt;p&gt;
The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;SDE&#21644;ODE&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SDE&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#38544;&#21464;&#37327;&#20197;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#21407;&#22987;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24341;&#21457;&#30340;&#36793;&#32536;&#20998;&#24067;&#26377;&#25152;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#23427;&#20026;&#32534;&#36753;&#23450;&#20041;&#20102;&#30456;&#24212;&#30340;SDE&#25110;ODE&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;SDE&#30340;&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#23545;&#20110;ODE&#26469;&#35828;&#65292;&#38543;&#30528;&#26102;&#38388;&#36235;&#36817;&#20110;&#38646;&#65292;&#25955;&#24230;&#20445;&#25345;&#19981;&#21464;&#65292;&#36825;&#26174;&#31034;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;ODE&#22522;&#32447;&#30340;SDE&#23545;&#24212;&#29289;&#65292;&#21253;&#25324;&#20462;&#22797;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;SDE&#26174;&#31034;&#20102;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;SDE-Drag - &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;SDE&#20844;&#24335;&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#65288;&#31216;&#20026;DragBench&#65289; &#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDAIn&#30340;&#25193;&#23637;&#28145;&#24230;&#33258;&#36866;&#24212;&#36755;&#20837;&#35268;&#33539;&#21270;&#23618;&#65292;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36866;&#24403;&#22320;&#35268;&#33539;&#21270;&#26102;&#24207;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#35268;&#33539;&#21270;&#26041;&#26696;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#24207;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14720</link><description>&lt;p&gt;
&#25193;&#23637;&#28145;&#24230;&#33258;&#36866;&#24212;&#36755;&#20837;&#35268;&#33539;&#21270;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#24207;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks. (arXiv:2310.14720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDAIn&#30340;&#25193;&#23637;&#28145;&#24230;&#33258;&#36866;&#24212;&#36755;&#20837;&#35268;&#33539;&#21270;&#23618;&#65292;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36866;&#24403;&#22320;&#35268;&#33539;&#21270;&#26102;&#24207;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#35268;&#33539;&#21270;&#26041;&#26696;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#24207;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#23545;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#24403;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#24207;&#39044;&#27979;&#21644;&#20998;&#31867;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#26126;&#26174;&#65306;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#24207;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#12289;&#20559;&#26012;&#21644;&#24322;&#24120;&#20540;&#31561;&#19981;&#35268;&#21017;&#29305;&#24449;&#65292;&#22914;&#26524;&#19981;&#20805;&#20998;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;&#27169;&#22411;&#24615;&#33021;&#24456;&#24555;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDAIN&#65288;&#25193;&#23637;&#28145;&#24230;&#33258;&#36866;&#24212;&#36755;&#20837;&#35268;&#33539;&#21270;&#65289;&#23618;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#31070;&#32463;&#23618;&#65292;&#23427;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36866;&#24403;&#22320;&#35268;&#33539;&#21270;&#19981;&#35268;&#21017;&#30340;&#26102;&#24207;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#35268;&#33539;&#21270;&#26041;&#26696;&#12290;&#36825;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#20854;&#26410;&#30693;&#21442;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12289;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#38480;&#20215;&#21333;&#31807;&#22522;&#20934;&#25968;&#25454;&#38598;&#26102;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data preprocessing is a crucial part of any machine learning pipeline, and it can have a significant impact on both performance and training efficiency. This is especially evident when using deep neural networks for time series prediction and classification: real-world time series data often exhibit irregularities such as multi-modality, skewness and outliers, and the model performance can degrade rapidly if these characteristics are not adequately addressed. In this work, we propose the EDAIN (Extended Deep Adaptive Input Normalization) layer, a novel adaptive neural layer that learns how to appropriately normalize irregular time series data for a given task in an end-to-end fashion, instead of using a fixed normalization scheme. This is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation. Our experiments, conducted using synthetic data, a credit default prediction dataset, and a large-scale limit order book benchmark datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#30340;&#24503;&#22269;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#27681;&#27668;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#27681;&#27668;&#26159;&#19968;&#31181;&#33268;&#30284;&#30340;&#25918;&#23556;&#24615;&#27668;&#20307;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;&#31215;&#32047;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#23460;&#20869;&#27681;&#26292;&#38706;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#20272;&#35745;&#24471;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29305;&#24449;&#24448;&#24448;&#19982;&#20154;&#21475;&#29305;&#24449;&#19981;&#21516;&#65292;&#36825;&#26159;&#30001;&#20110;&#35768;&#22810;&#30456;&#20851;&#22240;&#32032;&#65292;&#22914;&#22320;&#36136;&#28304;&#27681;&#27668;&#30340;&#21487;&#29992;&#24615;&#25110;&#27004;&#23618;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#22823;&#23567;&#36890;&#24120;&#19981;&#20801;&#35768;&#20197;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#26292;&#38706;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#32431;&#25968;&#25454;&#26041;&#27861;&#26356;&#21152;&#29616;&#23454;&#22320;&#20272;&#35745;&#23460;&#20869;&#27681;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#24314;&#27169;&#26041;&#27861;&#65306;1&#65289;&#24212;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65292;&#20351;&#29992;&#29615;&#22659;&#21644;&#24314;&#31569;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#65292;&#20272;&#35745;&#20102;&#24503;&#22269;&#27599;&#20010;&#20303;&#23429;&#27004;&#30340;&#27599;&#20010;&#27004;&#23618;&#30340;&#23460;&#20869;&#27681;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#65307;2&#65289;&#20351;&#29992;&#27010;&#29575;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#25216;&#26415;&#20351;&#23427;&#20204;&#32452;&#21512;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01678</link><description>&lt;p&gt;
Score dynamics: &#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#30382;&#31186;&#26102;&#38388;&#27493;&#25552;&#39640;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Score dynamics (SD) &#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#28436;&#21270;&#31639;&#23376;&#65292;&#29992;&#20110;&#21407;&#23376;&#32423;&#21644;&#31895;&#31890;&#21270;&#21160;&#21147;&#23398;&#12290;SD&#20197;&#20998;&#25968;&#20026;&#20013;&#24515;&#65292;&#21363;&#19982;&#21160;&#24577;&#33258;&#30001;&#24230;&#30340;&#36716;&#25442;&#23545;&#25968;&#27010;&#29575;&#23548;&#25968;&#30456;&#20851;&#30340;&#37327;&#12290;&#21518;&#32773;&#22312;&#20998;&#25968;&#26102;&#38388;&#27493;&#20013;&#36215;&#21040;&#19982;MD&#20013;&#21147;&#22330;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#21464;&#37327;&#30340;&#31163;&#25955;&#36716;&#21464;&#12290;&#36825;&#31181;&#26102;&#38388;&#27493;&#38271;&#21487;&#20197;&#27604;&#20856;&#22411;&#30340;MD&#26102;&#38388;&#27493;&#38271;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#65292;&#29992;&#20110;&#28436;&#21270;&#20197;1~ps&#26102;&#38388;&#27493;&#38271;&#30340;&#29616;&#23454;&#20998;&#23376;&#20307;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#27700;&#28342;&#28082;&#20013;&#30340;&#30701;&#38142;&#28919;&#28867;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;Score dynamics&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#20174;&#26465;&#20214;&#27010;&#29575;&#30340;&#24179;&#31283;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#30340;&#24179;&#34913;&#39044;&#27979;&#21644;&#23545;&#36716;&#25442;&#36895;&#29575;&#21644;&#36716;&#25442;&#30340;&#21160;&#21147;&#23398;&#39044;&#27979;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;MDM&#65289;&#65292;&#21487;&#20197;&#22312;&#21463;&#38480;&#21046;&#38598;&#21512;&#19978;&#29983;&#25104;&#25968;&#25454;&#32780;&#19981;&#20007;&#22833;&#21487;&#36861;&#28335;&#24615;&#12290;&#36825;&#36890;&#36807;&#22312;&#19968;&#20010;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23398;&#20064;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#38236;&#20687;&#26144;&#23556;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01236</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;&#21644;&#24102;&#27700;&#21360;&#29983;&#25104;&#30340;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mirror Diffusion Models for Constrained and Watermarked Generation. (arXiv:2310.01236v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;MDM&#65289;&#65292;&#21487;&#20197;&#22312;&#21463;&#38480;&#21046;&#38598;&#21512;&#19978;&#29983;&#25104;&#25968;&#25454;&#32780;&#19981;&#20007;&#22833;&#21487;&#36861;&#28335;&#24615;&#12290;&#36825;&#36890;&#36807;&#22312;&#19968;&#20010;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23398;&#20064;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#38236;&#20687;&#26144;&#23556;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#37096;&#20998;&#24402;&#21151;&#20110;&#20854;&#33021;&#22815;&#26500;&#24314;&#20855;&#26377;&#35299;&#26512;&#36716;&#31227;&#26680;&#20989;&#25968;&#21644;&#35780;&#20998;&#20989;&#25968;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#31181;&#21487;&#36861;&#28335;&#24615;&#32467;&#26524;&#22312;&#19981;&#38656;&#35201;&#27169;&#25311;&#30340;&#26694;&#26550;&#20013;&#20855;&#26377;&#31283;&#23450;&#30340;&#22238;&#24402;&#25439;&#22833;&#65292;&#20174;&#32780;&#21487;&#20197;&#23398;&#20064;&#21040;&#21487;&#20197;&#25193;&#23637;&#30340;&#36870;&#21521;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#34987;&#38480;&#21046;&#22312;&#21463;&#38480;&#21046;&#38598;&#21512;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26102;&#65292;&#26681;&#25454;&#20043;&#21069;&#30340;&#23581;&#35797;&#65292;&#36825;&#20123;&#29702;&#24819;&#30340;&#29305;&#24615;&#20284;&#20046;&#20007;&#22833;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;MDM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#31867;&#65292;&#21487;&#20197;&#22312;&#20984;&#32422;&#26463;&#38598;&#21512;&#19978;&#29983;&#25104;&#25968;&#25454;&#32780;&#19981;&#20007;&#22833;&#20219;&#20309;&#21487;&#36861;&#28335;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#20174;&#38236;&#20687;&#26144;&#23556;&#26500;&#24314;&#30340;&#23545;&#20598;&#31354;&#38388;&#20013;&#23398;&#20064;&#25193;&#25955;&#36807;&#31243;&#26469;&#23454;&#29616;&#30340;&#65292;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#26159;&#19968;&#20010;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27969;&#34892;&#30340;&#32422;&#26463;&#38598;&#21512;&#65288;&#22914;&#21333;&#32431;&#24418;&#21644;$\ell_2$-&#29699;&#65289;&#30340;&#38236;&#20687;&#26144;&#23556;&#30340;&#26377;&#25928;&#35745;&#31639;&#65292;&#26174;&#31034;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and $\ell_2$-balls, showing significantly im
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16598</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#28982;&#32780;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#32463;&#24120;&#38656;&#35201;&#32321;&#29712;&#30340;&#20154;&#24037;&#26631;&#27880;&#25110;&#32773;&#32531;&#24930;&#26114;&#36149;&#30340;&#31185;&#23398;&#27979;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#31934;&#23494;&#30340;&#39044;&#27979;&#25216;&#26415;&#21487;&#20197;&#24555;&#36895;&#12289;&#24265;&#20215;&#22320;&#20135;&#29983;&#22823;&#37327;&#39044;&#27979;&#26631;&#31614;&#65307;&#20363;&#22914;&#65292;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34987;&#29992;&#26469;&#34917;&#20805;&#23454;&#39564;&#24471;&#21040;&#30340;&#32467;&#26500;&#65292;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#30340;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#34987;&#29992;&#26469;&#34917;&#20805;&#20934;&#30830;&#30340;&#35843;&#26597;&#25968;&#25454;&#31561;&#12290;&#30001;&#20110;&#39044;&#27979;&#20855;&#26377;&#19981;&#23436;&#32654;&#21644;&#28508;&#22312;&#20559;&#24046;&#30340;&#29305;&#28857;&#65292;&#36825;&#31181;&#20570;&#27861;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#19968;&#20010;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20132;&#21449;&#39044;&#27979;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;</title><link>http://arxiv.org/abs/2309.13192</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32511;&#33394;AI&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. (arXiv:2309.13192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#21040;&#19979;&#28216;&#24212;&#29992;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;LLM&#39537;&#21160;&#30340;AI&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#20197;&#21450;&#24320;&#28304;LLM&#30340;&#27665;&#20027;&#21270;&#65292;&#38750;&#19987;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36827;&#34892;&#32454;&#35843;&#65292;&#20294;&#26159;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;LLM&#30340;&#22823;&#35268;&#27169;&#32454;&#35843;&#21487;&#33021;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#26174;&#33879;&#22686;&#21152;&#65292;&#20174;&#32780;&#23545;&#29615;&#22659;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#23454;&#29616;&#32511;&#33394;AI&#20197;&#20943;&#23569;&#32454;&#35843;&#30340;FLOPs&#30452;&#25509;&#30456;&#20851;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#39640;&#25928;LLM&#32454;&#35843;&#25216;&#26415;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;FLOPs&#38477;&#20302;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#32454;&#35843;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#24352;&#37327;&#26469;&#26368;&#23567;&#21270;&#32454;&#35843;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#21270;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#20419;&#36827;&#31232;&#30095;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10834</link><description>&lt;p&gt;
&#23384;&#22312;&#31232;&#30095;&#30340;&#38543;&#26426;&#32593;&#32476;&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#21270;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#20419;&#36827;&#31232;&#30095;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38543;&#26426;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20248;&#21270;&#30340;&#26159;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#35813;&#25513;&#30721;&#34920;&#24449;&#20102;&#19968;&#20010;&#33021;&#22815;&#21644;&#36739;&#23567;&#30340;&#30446;&#26631;&#32593;&#32476;&#19968;&#26679;&#22909;&#22320;&#27867;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20256;&#32479;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20132;&#25442;&#30340;&#26159;&#31232;&#30095;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#32780;&#19981;&#26159;&#28014;&#28857;&#26435;&#37325;&#65292;&#36825;&#26679;&#21487;&#20197;&#23558;&#36890;&#20449;&#25104;&#26412;&#38477;&#20302;&#21040;&#27599;&#20010;&#21442;&#25968;&#26368;&#22810;1&#20010;&#27604;&#29305;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;&#26041;&#27861;&#26080;&#27861;&#25214;&#21040;&#33021;&#22815;&#36890;&#36807;&#19968;&#33268;&#30340;&#25439;&#22833;&#30446;&#26631;&#20943;&#23569;&#36890;&#20449;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#65292;&#36890;&#36807;&#28040;&#38500;&#23376;&#32593;&#32476;&#20043;&#38388;&#30340;&#20887;&#20313;&#29305;&#24449;&#26469;&#20419;&#36827;&#26356;&#31232;&#30095;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39640;&#36798;&#20116;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.16089</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#28909;&#28809;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces. (arXiv:2308.16089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#30784;&#20135;&#19994;&#30340;&#32463;&#27982;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#20854;&#29983;&#20135;&#38142;&#20013;&#30340;&#19968;&#20123;&#32452;&#20214;&#65292;&#22914;&#21152;&#28909;&#28809;&#65292;&#33021;&#32791;&#36739;&#39640;&#12290;&#36890;&#36807;&#20943;&#23569;&#21152;&#28909;&#28809;&#20013;&#30340;&#25972;&#20307;&#21152;&#28909;&#26102;&#38388;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#12290;&#22312;&#22522;&#30784;&#20135;&#19994;&#21487;&#25345;&#32493;&#21046;&#36896;&#20013;&#65292;&#35745;&#31639;&#26426;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25511;&#21046;&#31995;&#32479;&#21487;&#33021;&#26159;&#23454;&#29616;&#8220;&#38646;&#20928;&#25490;&#25918;&#8221;&#30446;&#26631;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20013;&#65292;&#30001;&#20110;&#22312;&#21152;&#28909;&#28809;&#31561;&#22330;&#26223;&#20013;&#26080;&#27861;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;ML&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#30340;&#22238;&#24402;&#35757;&#32451;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21306;&#22495;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#36752;&#23556;&#20256;&#28909;&#65288;RHT&#65289;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#36825;&#26159;&#21152;&#28909;&#28809;&#20869;&#39640;&#28201;&#36807;&#31243;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20256;&#28909;&#26426;&#21046;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the high economic relevance of Foundation Industries, certain components like Reheating furnaces within their manufacturing chain are energy-intensive. Notable energy consumption reduction could be obtained by reducing the overall heating time in furnaces. Computer-integrated Machine Learning (ML) and Artificial Intelligence (AI) powered control systems in furnaces could be enablers in achieving the Net-Zero goals in Foundation Industries for sustainable manufacturing.  In this work, due to the infeasibility of achieving good quality data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for ML and Deep Learning (DL) based model training via regression. It should be noted that the zone method provides an elegant way to model the physical phenomenon of Radiative Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature processes inside heating furnaces. Using this data, an extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#65292;&#22810;&#26679;&#21270;&#29615;&#22659;&#21644;&#24314;&#27169;&#34892;&#20154;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14947</link><description>&lt;p&gt;
&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning Training Regimes for Social Robot Navigation. (arXiv:2308.14947v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#65292;&#22810;&#26679;&#21270;&#29615;&#22659;&#21644;&#24314;&#27169;&#34892;&#20154;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#20154;&#31867;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#23427;&#20204;&#24517;&#39035;&#36981;&#23432;&#25105;&#20204;&#30340;&#31038;&#20132;&#35268;&#33539;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20351;&#20854;&#33021;&#22815;&#36981;&#23432;&#36825;&#20123;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20013;&#29616;&#26377;&#24037;&#20316;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#22312;&#31616;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;RL&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#32467;&#26524;&#30340;&#23454;&#36136;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;RL&#31038;&#20132;&#23548;&#33322;&#26041;&#27861;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#29615;&#22659;&#31867;&#22411;&#21644;&#22810;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#24314;&#27169;&#34892;&#20154;&#65292;&#25105;&#20204;&#33021;&#22815;&#36880;&#27493;&#22686;&#21152;&#35757;&#32451;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#30340;&#35757;&#32451;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#32479;&#35745;&#32467;&#26524;&#30340;&#32467;&#26524;&#20043;&#19968;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for autonomous mobile robots to navigate in human spaces, they must abide by our social norms. Reinforcement learning (RL) has emerged as an effective method to train robot navigation policies that are able to respect these norms. However, a large portion of existing work in the field conducts both RL training and testing in simplistic environments. This limits the generalization potential of these models to unseen environments, and the meaningfulness of their reported results. We propose a method to improve the generalization performance of RL social navigation methods using curriculum learning. By employing multiple environment types and by modeling pedestrians using multiple dynamics models, we are able to progressively diversify and escalate difficulty in training. Our results show that the use of curriculum learning in training can be used to achieve better generalization performance than previous training methods. We also show that results presented in many existing stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08705</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#19982;&#65288;&#20934;&#65289;&#25928;&#29575;&#65306;&#20449;&#24687;&#20849;&#20139;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing. (arXiv:2308.08705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;POSGs&#65289;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#20026;&#20102;&#35268;&#36991;&#24050;&#30693;&#30340;&#38590;&#24230;&#38382;&#39064;&#21644;&#20351;&#29992;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#20513;&#23548;&#21033;&#29992;Agent&#20043;&#38388;&#30340;&#28508;&#22312;&#8220;&#20449;&#24687;&#20849;&#20139;&#8221;&#65292;&#36825;&#26159;&#23454;&#35777;MARL&#20013;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20063;&#26159;&#20855;&#22791;&#36890;&#20449;&#21151;&#33021;&#30340;&#22810;Agent&#25511;&#21046;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33509;&#24178;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#26469;&#35777;&#26126;&#20449;&#24687;&#20849;&#20139;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#21450;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#20026;&#20102;&#27714;&#35299;POSGs&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#20351;&#24471;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20934;&#25928;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#36827;&#19968;&#27493;&#8220;&#36817;&#20284;&#8221;&#20849;&#20139;&#30340;&#20844;&#20849;&#20449;&#24687;&#26500;&#24314;POSG&#30340;&#8220;&#36817;&#20284;&#27169;&#22411;&#8221;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#35745;&#21010;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65288;&#20174;&#35299;&#20915;&#21407;&#22987;POSG&#30340;&#35282;&#24230;&#65289;&#21487;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#65292;&#21363;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21069;&#25552;&#26159;&#19978;&#36848;&#20551;&#35774;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermo
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65288;CMISR&#65289;&#65292;&#37319;&#29992;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#27424;&#20998;&#36776;&#29575;&#21644;&#36229;&#20998;&#36776;&#29575;&#20803;&#32032;&#12290;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08567</link><description>&lt;p&gt;
CMISR&#65306;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
CMISR: Circular Medical Image Super-Resolution. (arXiv:2308.08567v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08567
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65288;CMISR&#65289;&#65292;&#37319;&#29992;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#27424;&#20998;&#36776;&#29575;&#21644;&#36229;&#20998;&#36776;&#29575;&#20803;&#32032;&#12290;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;MISR&#65289;&#26041;&#27861;&#37319;&#29992;&#38544;&#24335;&#30340;&#27424;&#20998;&#36776;&#29575;&#65288;UR&#65289;&#21333;&#20803;&#21644;&#26126;&#30830;&#30340;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#21333;&#20803;&#30340;&#24320;&#29615;&#26550;&#26500;&#12290;UR&#21333;&#20803;&#21487;&#20197;&#22987;&#32456;&#32473;&#20986;&#12289;&#20551;&#35774;&#25110;&#20272;&#35745;&#65292;&#32780;SR&#21333;&#20803;&#26681;&#25454;&#21508;&#31181;SR&#31639;&#27861;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#38381;&#29615;&#21453;&#39304;&#26426;&#21046;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#21069;&#30340;MISR&#26041;&#27861;&#65292;&#24182;&#33021;&#26377;&#25928;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#23616;&#37096;&#21453;&#39304;&#21644;&#20840;&#23616;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;&#24490;&#29615;MISR&#65288;CMISR&#65289;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;UR&#21644;SR&#20803;&#32032;&#12290;&#24314;&#31435;CMISR&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#38381;&#29615;&#26041;&#31243;&#12290;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#36924;&#36817;&#30340;&#25968;&#23398;&#35777;&#26126;&#34920;&#26126;&#65292;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;CMISR&#20855;&#26377;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#24615;&#65292;&#21487;&#20197;&#24314;&#31435;&#22312;&#20219;&#20309;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#19978;&#12290;&#20998;&#21035;&#25552;&#20986;&#20102;&#20116;&#31181;CMISR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical methods of medical image super-resolution (MISR) utilize open-loop architecture with implicit under-resolution (UR) unit and explicit super-resolution (SR) unit. The UR unit can always be given, assumed, or estimated, while the SR unit is elaborately designed according to various SR algorithms. The closed-loop feedback mechanism is widely employed in current MISR approaches and can efficiently improve their performance. The feedback mechanism may be divided into two categories: local and global feedback. Therefore, this paper proposes a global feedback-based closed-cycle framework, circular MISR (CMISR), with unambiguous UR and SR elements. Mathematical model and closed-loop equation of CMISR are built. Mathematical proof with Taylor-series approximation indicates that CMISR has zero recovery error in steady-state. In addition, CMISR holds plug-and-play characteristic which can be established on any existing MISR algorithms. Five CMISR algorithms are respectively proposed bas
&lt;/p&gt;</description></item><item><title>Symphony&#26159;&#19968;&#20010;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#65292;&#21487;&#20197;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26381;&#21153;&#65292;&#22312;&#28385;&#36275;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#21644;&#24310;&#36831;SLO&#30340;&#21516;&#26102;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#12290;&#36890;&#36807;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#21644;&#27169;&#22411;&#20998;&#37197;&#31639;&#27861;&#65292;Symphony&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#21151;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#25552;&#20379;&#39640;&#36798;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.07470</link><description>&lt;p&gt;
Symphony: &#20351;&#29992;&#38598;&#20013;&#24335;&#21327;&#35843;&#26469;&#20248;&#21270;&#27169;&#22411;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Symphony: Optimized Model Serving using Centralized Orchestration. (arXiv:2308.07470v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07470
&lt;/p&gt;
&lt;p&gt;
Symphony&#26159;&#19968;&#20010;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#65292;&#21487;&#20197;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26381;&#21153;&#65292;&#22312;&#28385;&#36275;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#21644;&#24310;&#36831;SLO&#30340;&#21516;&#26102;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#12290;&#36890;&#36807;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#21644;&#27169;&#22411;&#20998;&#37197;&#31639;&#27861;&#65292;Symphony&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#21151;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#25552;&#20379;&#39640;&#36798;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPU&#38598;&#32676;&#19978;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#25512;&#29702;&#30340;&#21327;&#35843;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#22312;&#28385;&#36275;&#25209;&#22788;&#29702;&#23646;&#24615;&#30340;&#27169;&#22411;&#25512;&#29702;&#21516;&#26102;&#23454;&#29616;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#65292;&#24182;&#28385;&#36275;&#24310;&#36831;&#26381;&#21153;&#32423;&#21035;&#30446;&#26631;(SLO)&#65292;&#20197;&#21450;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#30340;&#21464;&#21270;&#65292;&#26080;&#35770;&#26159;&#30701;&#26399;&#27874;&#21160;&#36824;&#26159;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Symphony&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#25193;&#23637;&#21040;&#27599;&#31186;&#25968;&#30334;&#19975;&#20010;&#35831;&#27714;&#24182;&#21327;&#35843;&#25968;&#19975;&#20010;GPU&#30340;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20063;&#33021;&#23454;&#29616;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20998;&#37197;&#23376;&#38598;&#32676;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;Symphony&#30340;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#31995;&#32479;&#65292;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08924</link><description>&lt;p&gt;
&#23398;&#20064;&#37319;&#26679;&#20219;&#21153;&#29992;&#20110;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21508;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#20219;&#21153;&#37319;&#26679;&#22120;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#37319;&#26679;&#22120;&#65288;ASr&#65289;&#12290;ASr&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#12290;&#20026;&#20102;&#20248;&#21270;ASr&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2307.07357</link><description>&lt;p&gt;
&#21453;&#21521;&#26368;&#20248;&#21270;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;IO&#26694;&#26550;&#23646;&#20110;&#30417;&#30563;&#23398;&#20064;&#31867;&#21035;&#65292;&#24182;&#24314;&#31435;&#22312;&#30446;&#26631;&#34892;&#20026;&#26159;&#26410;&#30693;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#22120;&#30340;&#21069;&#25552;&#19979;&#12290;&#36825;&#20010;&#25104;&#26412;&#20989;&#25968;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#36335;&#30001;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20915;&#31574;&#32773;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;&#30340;IO&#26041;&#27861;&#65292;&#21253;&#25324;&#20551;&#35774;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;IO&#26041;&#27861;&#65292;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#25104;&#21315;&#19978;&#19975;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36335;&#30001;&#26696;&#20363;&#23398;&#20064;&#27169;&#22411;&#20197;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#25105;&#20204;&#26368;&#32456;&#23398;&#20064;&#21040;&#30340;IO&#36335;&#30001;&#27169;&#22411;&#22312;48&#20010;&#26187;&#32423;&#21040;&#20915;&#36187;&#30340;&#27169;&#22411;&#20013;&#25490;&#21517;&#31532;2&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning decision-makers' behavior in routing problems using Inverse Optimization (IO). The IO framework falls into the supervised learning category and builds on the premise that the target behavior is an optimizer of an unknown cost function. This cost function is to be learned through historical data, and in the context of routing problems, can be interpreted as the routing preferences of the decision-makers. In this view, the main contributions of this study are to propose an IO methodology with a hypothesis function, loss function, and stochastic first-order algorithm tailored to routing problems. We further test our IO approach in the Amazon Last Mile Routing Research Challenge, where the goal is to learn models that replicate the routing preferences of human drivers, using thousands of real-world routing examples. Our final IO-learned routing model achieves a score that ranks 2nd compared with the 48 models that qualified for the final round of the challe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#26080;&#27169;&#22411;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#31639;&#27861;&#65292;&#20801;&#35768;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.03997</link><description>&lt;p&gt;
&#20302;&#31209;MDP&#20013;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Model-Free Exploration in Low-Rank MDPs. (arXiv:2307.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#26080;&#27169;&#22411;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#31639;&#27861;&#65292;&#20801;&#35768;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#38656;&#35201;&#27867;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#30340;&#39640;&#32500;&#39046;&#22495;&#20013;&#24320;&#21457;&#20986;&#23454;&#29992;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#12290;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#8212;&#8212;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#21487;&#20197;&#22522;&#20110;&#26410;&#30693;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#8212;&#8212;&#20026;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26694;&#26550;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38480;&#21046;&#24615;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#22914;&#28508;&#21464;&#37327;&#32467;&#26500;&#12289;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20989;&#25968;&#36924;&#36817;&#30340;&#35775;&#38382;&#24615;&#25110;&#21487;&#36798;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26082;&#35745;&#31639;&#39640;&#25928;&#21448;&#26080;&#27169;&#22411;&#65292;&#20801;&#35768;&#36827;&#34892;&#36890;&#29992;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;VoX&#20351;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#20248;&#21270;&#35774;&#35745;&#30340;&#29305;&#24449;&#23884;&#20837;&#27010;&#24565;&#20316;&#20026;&#25506;&#32034;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. Low-Rank Markov Decision Processes -- where transition probabilities admit a low-rank factorization based on an unknown feature embedding -- offer a simple, yet expressive framework for RL with function approximation, but existing algorithms are either (1) computationally intractable, or (2) reliant upon restrictive statistical assumptions such as latent variable structure, access to model-based function approximation, or reachability. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation and requiring no additional structural assumptions. Our algorithm, VoX, uses the notion of a generalized optimal design for the feature embedding as an eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02140</link><description>&lt;p&gt;
&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65306;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#30340;&#32508;&#36848;&#21644;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36981;&#24490;&#26381;&#21153;&#22120;&#20027;&#23548;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#38480;&#21046;&#20102;FL&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#38477;&#20302;&#20102;&#25968;&#25454;&#25345;&#26377;&#32773;&#21442;&#19982;&#30340;&#28909;&#24773;&#12290;&#20026;&#20102;&#20805;&#20998;&#37322;&#25918;FL&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20027;&#24352;&#37325;&#26032;&#24605;&#32771;&#24403;&#21069;FL&#26694;&#26550;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#27010;&#24565;&#65306;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#21512;&#20316;&#30340;FL&#26694;&#26550;&#65306;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;FL&#12290;&#22312;&#36825;&#20010;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;FL&#30340;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#32806;&#21512;&#12289;&#27169;&#22411;&#21487;&#37325;&#29992;&#24615;&#20302;&#21644;&#38750;&#20844;&#24320;&#24615;&#12290;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#24179;&#21488;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#36171;&#33021;&#30340;&#24320;&#25918;&#27169;&#22411;&#20849;&#20139;&#21644;&#37325;&#29992;&#24179;&#21488;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#26377;&#20215;&#20540;&#30340;&#20027;&#39064;&#65292;&#21253;&#25324;&#20840;&#29699;&#26368;&#26032;&#21487;&#29992;&#27169;&#22411;&#21644;&#27169;&#22411;&#30340;&#26597;&#35810;&#12289;&#26381;&#21153;&#36136;&#37327;&#20445;&#35777;&#21644;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15012</link><description>&lt;p&gt;
&#29992;&#20110;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#30446;&#26631;&#20449;&#21495;&#24674;&#22797;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#23545;&#32473;&#23450;&#20449;&#21495;&#30340;&#29305;&#23450;&#23646;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#20174;&#19968;&#20010;&#21152;&#24615;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#20449;&#21495;&#21487;&#33021;&#26159;&#19968;&#20010;&#19981;&#24517;&#35201;&#22320;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26356;&#31616;&#21333;&#30340;&#8220;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#8221;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#19987;&#27880;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#39044;&#23450;&#20041;&#32479;&#35745;&#25551;&#36848;&#37327;&#12290;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#22122;&#22768;&#36807;&#31243;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#20351;&#21463;&#22122;&#22768;&#26679;&#26412;&#27745;&#26579;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#30340;&#32479;&#35745;&#29305;&#24615;&#19982;&#35266;&#27979;&#30340;&#28151;&#21512;&#29289;&#30340;&#32479;&#35745;&#29305;&#24615;&#21305;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#35299;&#26512;&#21487;&#36861;&#36394;&#35745;&#31639;&#30340;&#31616;&#21333;&#31034;&#20363;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#38477;&#22122;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#20102;1&#65289;&#22522;&#20110;&#23567;&#27874;&#30340;&#25551;&#36848;&#31526;&#65292;2&#65289;&#38024;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;ImageNet&#25968;&#25454;&#30340;ConvNet-based&#25551;&#36848;&#31526;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#27604;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#26356;&#22909;&#22320;&#24674;&#22797;&#20102;&#30446;&#26631;&#25968;&#25454;&#30340;&#25551;&#36848;&#31526;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#19981;&#26159;&#20026;&#27492;&#30446;&#30340;&#26500;&#24314;&#30340;&#65292;&#23427;&#20063;&#34920;&#29616;&#20986;&#23545;&#30446;&#26631;&#20449;&#21495;&#25551;&#36848;&#31526;&#24674;&#22797;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10398</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#22312;&#22810;&#31867;&#20998;&#31867;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#23637;&#31034;&#65292;&#20294;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#26377;&#22810;&#20010;&#26631;&#31614;&#65292;&#19968;&#30452;&#20197;&#26469;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36827;&#34892;&#20851;&#20110;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#30340;&#37325;&#28857;&#30740;&#31350;&#30340;&#39318;&#35201;&#25361;&#25112;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#26631;&#31614;&#22270;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#22270;&#29983;&#25104;&#22120;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#35843;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#39640;&#26631;&#31614;&#30456;&#20284;&#24615;&#65288;&#39640;&#21516;&#31867;&#20559;&#22909;&#65289;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;GNN&#30340;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#24182;&#19981;&#36981;&#24490;&#30446;&#21069;&#20026;&#22810;&#31867;&#22330;&#26223;&#23450;&#20041;&#30340;&#21516;&#31867;&#20559;&#22909;&#21644;&#24322;&#31867;&#20559;&#22909;&#30340;&#24120;&#35268;&#35821;&#20041;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#38500;&#20102;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#23450;&#20041;&#21516;&#31867;&#20559;&#22909;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;MLGCN&#65288;&#22810;&#26631;&#31614;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#26469;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.17674</link><description>&lt;p&gt;
&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#12290;&#21487;&#36798;&#38598;&#22312;&#25511;&#21046;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#35745;&#31639;&#36215;&#26469;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#36807;&#36924;&#36817;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#65292;&#23558;&#20854;&#34920;&#31034;&#25104;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#36825;&#20010;&#26377;&#38480;&#32500;&#30340;&#21051;&#30011;&#24320;&#21551;&#20102;&#19968;&#31181;&#32039;&#23494;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#65292;&#19988;&#25104;&#26412;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20302;&#12289;&#26356;&#31934;&#20934;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#31070;&#32463;&#21453;&#39304;&#29615;&#20998;&#26512;&#21644;&#40065;&#26834;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07457</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;&#26469;&#29702;&#35299;&#19987;&#19994;&#25216;&#33021;&#65306;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning. (arXiv:2302.07457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;Offline IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#20195;&#29702;&#30340;&#22266;&#23450;&#26377;&#38480;&#28436;&#31034;&#20013;&#24674;&#22797;&#25903;&#25745;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#30340;&#22870;&#21169;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#32467;&#26500;&#12290;&#20934;&#30830;&#30340;&#19987;&#19994;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#20020;&#24202;&#20915;&#31574;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#21916;&#22909;&#38544;&#21547;&#22312;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#20013;&#30340;&#32467;&#26500;&#19982;&#19987;&#23478;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#27169;&#22411;&#65288;&#21363;&#8220;&#19990;&#30028;&#8221;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35206;&#30422;&#33539;&#22260;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#19981;&#20934;&#30830;&#19990;&#30028;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20272;&#35745;&#30340;&#22870;&#21169;&#30340;&#19981;&#20934;&#30830;&#24615;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#20854;&#20013;&#19978;&#23618;&#26159;&#22522;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#20445;&#23432;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;&#19979;&#23618;&#65289;&#12290;&#31574;&#30053;&#27169;&#22411;&#26159;&#20445;&#23432;&#30340;&#65292;&#22240;&#20026;&#23427;&#22312;&#24809;&#32602;&#65288;&#24809;&#32602;&#20250;&#38543;&#30528;&#19987;&#23478;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#22686;&#21152;&#65289;&#19979;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#31163;&#32447;IRL&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncerta
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06089</link><description>&lt;p&gt;
&#38754;&#21521;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated contrastive learning models for prostate cancer diagnosis and Gleason grading. (arXiv:2302.06089v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#31283;&#20581;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#25968;&#25454;&#25910;&#38598;&#38754;&#20020;&#27807;&#36890;&#12289;&#20262;&#29702;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#38480;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#22810;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#21442;&#25968;&#20256;&#36755;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#24182;&#39564;&#35777;FCL&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#28155;&#21152;&#22122;&#38899;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;19,635&#20010;&#26469;&#33258;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#21069;&#21015;&#33146;&#30284;WSI&#19978;&#35780;&#20272;&#20102;FCL&#22312;&#30284;&#30151;&#35786;&#26029;&#20219;&#21153;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#35786;&#26029;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21306;&#20998;&#30284;&#24615;&#21644;&#38750;&#30284;&#24615;WSI&#30340;0.99&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;&#22312;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;FCL&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20026;0.143&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;21.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application effect of artificial intelligence (AI) in the field of medical imaging is remarkable. Robust AI model training requires large datasets, but data collection faces communication, ethics, and privacy protection constraints. Fortunately, federated learning can solve the above problems by coordinating multiple clients to train the model without sharing the original data. In this study, we design a federated contrastive learning framework (FCL) for large-scale pathology images and the heterogeneity challenges. It enhances the model's generalization ability by maximizing the attention consistency between the local client and server models. To alleviate the privacy leakage problem when transferring parameters and verify the robustness of FCL, we use differential privacy to further protect the model by adding noise. We evaluate the effectiveness of FCL on the cancer diagnosis task and Gleason grading task on 19,635 prostate cancer WSIs from multiple clients. In the diagnosis tas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;AdaFGDA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#22312;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.07303</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#21450;&#20854;&#36739;&#20302;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Minimax Optimization with Lower complexities. (arXiv:2211.07303v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;AdaFGDA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#22312;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#65292;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#31639;&#27861;&#19987;&#27880;&#20110;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#21152;&#36895;&#31639;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;&#21363;AdaFGDA&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;AdaFGDA&#24314;&#31435;&#22312;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#20943;&#23569;&#21644;&#23616;&#37096;SGD&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#33258;&#36866;&#24212;&#30697;&#38453;&#21487;&#20197;&#28789;&#27963;&#22320;&#32467;&#21512;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22362;&#23454;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a popular distributed and privacy-preserving machine learning paradigm. Meanwhile, minimax optimization, as an effective hierarchical optimization, is widely applied in machine learning. Recently, some federated optimization methods have been proposed to solve the distributed minimax problems. However, these federated minimax methods still suffer from high gradient and communication complexities. Meanwhile, few algorithm focuses on using adaptive learning rate to accelerate algorithms. To fill this gap, in the paper, we study a class of nonconvex minimax optimization, and propose an efficient adaptive federated minimax optimization algorithm (i.e., AdaFGDA) to solve these distributed minimax problems. Specifically, our AdaFGDA builds on the momentum-based variance reduced and local-SGD techniques, and it can flexibly incorporate various adaptive learning rates by using the unified adaptive matrix. Theoretically, we provide a solid convergence analysis framework fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14484</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#25554;&#34917;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imputation of missing values in multi-view data. (arXiv:2210.14484v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#26159;&#25351;&#30001;&#22810;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#25551;&#36848;&#30340;&#25968;&#25454;&#12290;&#22312;&#22788;&#29702;&#22810;&#35270;&#35282;&#25968;&#25454;&#26102;&#65292;&#33509;&#20986;&#29616;&#32570;&#22833;&#20540;&#65292;&#21017;&#19968;&#20010;&#35270;&#35282;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#26497;&#26377;&#21487;&#33021;&#21516;&#26102;&#32570;&#22833;&#65292;&#22240;&#32780;&#23548;&#33268;&#38750;&#24120;&#22823;&#37327;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#25554;&#34917;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#22534;&#21472;&#24809;&#32602;&#36923;&#36753;&#22238;&#24402;(StaPLR)&#31639;&#27861;&#65292;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#22810;&#35270;&#35282;&#35745;&#31639;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#25554;&#34917;&#31639;&#27861;&#65292;&#20363;&#22914;missForest&#12290;
&lt;/p&gt;
&lt;p&gt;
Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This leads to very large quantities of missing data which, especially when combined with high-dimensionality, makes the application of conditional imputation methods computationally infeasible. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.01834</link><description>&lt;p&gt;
&#38450;&#24481;&#32852;&#37030;&#32972;&#21518;&#25915;&#20987;&#30340;&#19981;&#21464;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#25928;&#27169;&#22411;&#32780;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32852;&#37030;&#35774;&#32622;&#20351;&#24471;&#27169;&#22411;&#22312;&#23384;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25932;&#23545;&#25915;&#20987;&#12290;&#23613;&#31649;&#23545;&#20110;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#30340;&#25915;&#20987;&#30340;&#38450;&#24481;&#24050;&#32463;&#21462;&#24471;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#38450;&#24481;&#20165;&#25552;&#39640;&#32972;&#21518;&#26679;&#26412;&#19978;&#27169;&#22411;&#20934;&#30830;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26679;&#26412;&#25928;&#29992;&#30340;&#32972;&#21518;&#25915;&#20987;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#23545;&#32972;&#21518;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#31181;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#24120;&#35265;&#20110;&#35774;&#35745;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;Resnet [He et al., 2015]&#65292;&#20294;&#24448;&#24448;&#34987;&#20808;&#21069;&#30340;&#24037;&#20316;&#25152;&#24573;&#35270;&#12290;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#65292;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20197;&#20165;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#32972;&#21518;&#26679;&#26412;&#26377;&#21033;&#65292;&#24182;&#19981;&#38656;&#35201;&#24694;&#24847;&#21644;&#33391;&#24615;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is gaining popularity as it enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the vulnerability of federated learning to backdoor attacks over a flat loss landscape which is common for well-designed neural networks such as Resnet [He et al., 2015] but is often overlooked by previous works. Over a flat loss landscape, misleading federated learning models to exclusively benefit malicious clients with backdoor samples do not require a significant difference between malicious and benign cli
&lt;/p&gt;</description></item></channel></rss>