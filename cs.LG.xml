<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#65292;&#26041;&#27861;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#38754;&#24191;&#12290;</title><link>http://arxiv.org/abs/2305.03712</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference for Fairness Auditing. (arXiv:2305.03712v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#65292;&#26041;&#27861;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#38754;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#40657;&#30418;&#27169;&#22411;&#29992;&#20110;&#39640;&#39118;&#38505;&#38382;&#39064;&#20043;&#21069;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20844;&#24179;&#24615;&#23457;&#35745;&#8221;&#30340;&#20219;&#21153;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#23558;&#20854;&#34920;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#20197;&#32479;&#35745;&#20445;&#35777;&#30340;&#26041;&#24335;&#21516;&#26102;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20960;&#20046;&#20219;&#20309;&#24615;&#33021;&#24230;&#37327;&#25110;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#20016;&#23500;&#30340;&#12289;&#29978;&#33267;&#26159;&#26080;&#38480;&#30340;&#23376;&#32676;&#20307;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#22810;&#20010;&#28508;&#22312;&#37325;&#21472;&#26631;&#20934;&#19979;&#30340;&#27169;&#22411;&#34920;&#29616;&#23457;&#35745;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as "fairness auditing," in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpo
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#19968;&#31181;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03711</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#30340;&#38134;&#24377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is dataset condensation a silver bullet for healthcare data sharing?. (arXiv:2305.03711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03711
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#19968;&#31181;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#19968;&#20010;&#19975;&#26080;&#19968;&#22833;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26032;&#25216;&#26415;&#8212;&#8212;&#25968;&#25454;&#38598;&#21387;&#32553;&#65292;&#20197;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20849;&#20139;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#21069;&#26223;&#24191;&#38420;&#12290;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#25688;&#35201;&#20102;&#21407;&#22987;&#35760;&#24405;&#65292;&#19981;&#21487;&#36870;&#22320;&#38544;&#34255;&#20102;&#20010;&#20307;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#30495;&#27491;&#30340;&#21435;&#35782;&#21035;&#21270;&#65292;&#20801;&#35768;&#33258;&#30001;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#21387;&#32553;&#25968;&#25454;&#20013;&#20445;&#30041;&#20102;&#21407;&#22987;&#28145;&#24230;&#23398;&#20064;&#23454;&#29992;&#31243;&#24207;&#65292;&#19988;&#25968;&#25454;&#37327;&#36739;&#23567;&#19988;&#27169;&#22411;&#25910;&#25947;&#21152;&#36895;&#12290;&#22312;PhysioNet-2012&#20013;&#65292;20&#20010;&#26679;&#26412;&#30340;&#21387;&#32553;&#25968;&#25454;&#33021;&#22815;&#20351;&#28145;&#24230;&#27169;&#22411;&#36798;&#21040;&#20102;80.3%&#30340;&#27515;&#20129;&#39044;&#27979;&#27979;&#35797;AUC&#65288;&#32780;&#21407;&#22987;&#35760;&#24405;&#20026;5120&#20010;&#26679;&#26412;&#30340;85.8%&#65289;&#65292;&#36825;&#19968;&#21457;&#29616;&#20063;&#36866;&#29992;&#20110;MIMIC-III&#21644;Coswara&#31561;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#32463;&#39564;&#35777;&#26126;&#20102;DC&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#20849;&#20139;&#21307;&#30103;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding personal information is paramount for healthcare data sharing, a challenging issue without any silver bullet thus far. We study the prospect of a recent deep-learning advent, dataset condensation (DC), in sharing healthcare data for AI research, and the results are promising. The condensed data abstracts original records and irreversibly conceals individual-level knowledge to achieve a bona fide de-identification, which permits free sharing. Moreover, the original deep-learning utilities are well preserved in the condensed data with compressed volume and accelerated model convergences. In PhysioNet-2012, a condensed dataset of 20 samples can orient deep models attaining 80.3% test AUC of mortality prediction (versus 85.8% of 5120 original records), an inspiring discovery generalised to MIMIC-III and Coswara datasets. We also interpret the inhere privacy protections of DC through theoretical analysis and empirical evidence. Dataset condensation opens a new gate to sharing h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21487;&#36870;&#25968;&#25454;&#32534;&#30721;&#23454;&#29616;&#21307;&#30103;&#25968;&#25454;&#27665;&#20027;&#21270;&#21644;&#20449;&#24687;&#27844;&#38706;&#39044;&#38450;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#38543;&#26426;&#37327;&#23376;&#32534;&#30721;&#23454;&#29616;&#23494;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32534;&#30721;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#35821;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.03710</link><description>&lt;p&gt;
&#20581;&#24247;&#25968;&#25454;&#27665;&#20027;&#21270;&#21644;&#20449;&#24687;&#27844;&#38706;&#39044;&#38450;&#30340;&#25968;&#25454;&#32534;&#30721;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention. (arXiv:2305.03710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21487;&#36870;&#25968;&#25454;&#32534;&#30721;&#23454;&#29616;&#21307;&#30103;&#25968;&#25454;&#27665;&#20027;&#21270;&#21644;&#20449;&#24687;&#27844;&#38706;&#39044;&#38450;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#38543;&#26426;&#37327;&#23376;&#32534;&#30721;&#23454;&#29616;&#23494;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32534;&#30721;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#35821;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27665;&#20027;&#21270;&#19981;&#36275;&#21644;&#24050;&#35757;&#32451;&#27169;&#22411;&#30340;&#20449;&#24687;&#27844;&#38706;&#22952;&#30861;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20581;&#20840;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19981;&#21487;&#36870;&#25968;&#25454;&#32534;&#30721;&#21487;&#20197;&#25552;&#20379;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#25968;&#25454;&#27665;&#20027;&#21270;&#65292;&#21516;&#26102;&#21448;&#19981;&#36829;&#21453;&#23545;&#21307;&#30103;&#25968;&#25454;&#21644;&#20020;&#24202;&#27169;&#22411;&#30340;&#38544;&#31169;&#32422;&#26463;&#12290;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#25152;&#38656;&#32534;&#30721;&#26694;&#26550;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#38543;&#26426;&#37327;&#23376;&#32534;&#30721;&#23454;&#29616;&#23494;&#38598;&#21644;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32534;&#30721;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#20351;&#29992;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#30340;&#29942;&#39048;&#21407;&#21017;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of data democratization and information leakage from trained models hinder the development and acceptance of robust deep learning-based healthcare solutions. This paper argues that irreversible data encoding can provide an effective solution to achieve data democratization without violating the privacy constraints imposed on healthcare data and clinical models. An ideal encoding framework transforms the data into a new space where it is imperceptible to a manual or computational inspection. However, encoded data should preserve the semantics of the original data such that deep learning models can be trained effectively. This paper hypothesizes the characteristics of the desired encoding framework and then exploits random projections and random quantum encoding to realize this framework for dense and longitudinal or time-series data. Experimental evaluation highlights that models trained on encoded time-series data effectively uphold the information bottleneck principle and hen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#32454;&#31890;&#24230;&#20135;&#21697;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#25991;&#26412;&#32467;&#21512;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#20110;&#32454;&#33410;&#30456;&#20284;&#30340;&#20135;&#21697;&#20998;&#31867;&#31934;&#30830;&#24230;&#65292;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#29575;&#20026;96.4%&#12290;</title><link>http://arxiv.org/abs/2305.03706</link><description>&lt;p&gt;
&#20256;&#21333;&#24191;&#21578;&#19978;&#30340;&#32454;&#31890;&#24230;&#20135;&#21697;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Product Classification on Leaflet Advertisements. (arXiv:2305.03706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#32454;&#31890;&#24230;&#20135;&#21697;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#25991;&#26412;&#32467;&#21512;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#20110;&#32454;&#33410;&#30456;&#20284;&#30340;&#20135;&#21697;&#20998;&#31867;&#31934;&#30830;&#24230;&#65292;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#29575;&#20026;96.4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#20256;&#21333;&#22270;&#29255;&#30340;&#39318;&#20010;&#20844;&#24320;&#32454;&#31890;&#24230;&#20135;&#21697;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20174;&#19981;&#21516;&#27431;&#27954;&#38646;&#21806;&#21830;&#25910;&#38598;&#24191;&#21578;&#20256;&#21333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;832&#20010;&#31867;&#21035;&#20013;&#30340;41600&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#20135;&#21697;&#22270;&#20687;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#32454;&#31890;&#24230;&#20135;&#21697;&#20998;&#31867;&#20219;&#21153;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#22270;&#20687;&#12289;&#22522;&#20110;&#25991;&#26412;&#20197;&#21450;&#22522;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;"&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;"&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;&#20174;&#20256;&#21333;&#20013;&#25552;&#21462;&#30340;&#21830;&#21697;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#30340;&#32467;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#20110;&#22806;&#35266;&#30456;&#20284;&#20294;&#38590;&#20197;&#21306;&#20998;&#30340;&#20135;&#21697;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;96.4%&#65292;Top-3&#24471;&#20998;&#20026;99.2%&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#20844;&#24320;&#22312; https://github.com/ladwigd/Leaflet-Product-Classification &#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe a first publicly available fine-grained product recognition dataset based on leaflet images. Using advertisement leaflets, collected over several years from different European retailers, we provide a total of 41.6k manually annotated product images in 832 classes. Further, we investigate three different approaches for this fine-grained product classification task, Classification by Image, by Text, as well as by Image and Text. The approach "Classification by Text" uses the text extracted directly from the leaflet product images. We show, that the combination of image and text as input improves the classification of visual difficult to distinguish products. The final model leads to an accuracy of 96.4% with a Top-3 score of 99.2%. We release our code at https://github.com/ladwigd/Leaflet-Product-Classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#30446;&#26631;&#31867;&#21035;&#19978;&#37327;&#21270;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.03691</link><description>&lt;p&gt;
&#20174;&#27888;&#26862;&#22810;&#36793;&#24418;&#20013;&#25366;&#25496;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mining bias-target Alignment from Voronoi Cells. (arXiv:2305.03691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#30446;&#26631;&#31867;&#21035;&#19978;&#37327;&#21270;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#20559;&#24046;:&#36825;&#24341;&#36215;&#20102;&#20182;&#20204;&#20844;&#27491;&#24615;&#30340;&#25285;&#24551;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#25351;&#26631;&#26469;&#37327;&#21270;&#30446;&#26631;&#31867;&#21035;&#19978;&#30340;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#26469;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#21435;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#21644;&#20559;&#35265;&#29305;&#23450;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#22810;&#20010;&#26679;&#26412;&#20013;&#30340;&#20559;&#35265;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03686</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#36817;&#20284;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20027;&#35201;&#20851;&#27880;&#23616;&#37096;&#40065;&#26834;&#24615;&#65292;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#32473;&#23450;&#23646;&#24615;&#26159;&#21542;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20869;&#20840;&#23616;&#25104;&#31435;&#65292;&#22914;&#26524;&#19981;&#25104;&#31435;&#65292;&#21017;&#38656;&#35201;&#30693;&#36947;&#23646;&#24615;&#25104;&#31435;&#30340;&#36755;&#20837;&#27604;&#20363;&#26159;&#22810;&#23569;&#12290;&#23613;&#31649;&#31934;&#30830;&#30340;&#21407;&#20687;&#29983;&#25104;&#21487;&#20197;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#20215;&#34920;&#31034;&#65292;&#20294;&#22312;&#35268;&#27169;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21306;&#22495;&#21010;&#20998;&#20026;&#23376;&#21306;&#22495;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#26494;&#24347;&#36793;&#30028;&#21464;&#24471;&#26356;&#32039;&#65292;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20307;&#31215;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#37319;&#26679;&#21644;&#21487;&#24494;&#20307;&#31215;&#36924;&#36817;&#26469;&#20248;&#20808;&#21010;&#20998;&#21306;&#22495;&#65292;&#24182;&#20248;&#21270;&#26494;&#24347;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03660</link><description>&lt;p&gt;
&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval Augmented Generation (RAG) &#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20687;OpenAI text-davinci-003&#12289;gpt-3.5-turbo&#21644;gpt-4&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#24182;&#25552;&#20379;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#20197;&#25105;&#20204;&#25152;&#38656;&#30340;&#26684;&#24335;&#29983;&#25104;&#25253;&#21578;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#65292;BERTScore&#20026;0.2865&#65288;&#916;+25.88%&#65289;&#65292;Semb Score&#20026;0.4026&#65288;&#916;+6.31%&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20020;&#24202;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36807;&#31243;&#65292;&#21516;&#26102;&#20855;&#22791;&#36866;&#21512;&#35813;&#35774;&#32622;&#30340;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03655</link><description>&lt;p&gt;
&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#22312;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#29983;&#25104;&#31995;&#32479;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#22312;&#20256;&#32479;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;DG&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22909;&#22855;&#24515;&#12290; &#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25915;&#20987;DG&#27169;&#22411;&#30340;&#26102;&#20505;&#65292;&#23545;&#24403;&#21069;&#21477;&#23376;&#30340;&#25200;&#21160;&#20960;&#20046;&#19981;&#20250;&#38477;&#20302;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#26410;&#25913;&#21464;&#30340;&#32842;&#22825;&#35760;&#24405;&#20063;&#20250;&#34987;&#32771;&#34385;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#26469;&#36843;&#20351;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#65292;&#26377;&#21033;&#20110;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615; - &#29983;&#25104;&#30340;&#21709;&#24212;&#36890;&#24120;&#26159;&#19981;&#30456;&#20851;&#12289;&#20887;&#38271;&#21644;&#37325;&#22797;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGSlow&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#25915;&#20987;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DGSlow&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631; - &#29983;&#25104;&#20934;&#30830;&#24230;&#21644;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLER&#30340;OCL&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31561;&#21464;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#65292;&#36991;&#20813;&#20102;CSSL&#22312;OCL&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#19982;&#36830;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.03648</link><description>&lt;p&gt;
&#20851;&#20110;&#31561;&#21464;&#27491;&#21017;&#21270;&#23545;&#20110;&#40065;&#26834;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning. (arXiv:2305.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLER&#30340;OCL&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31561;&#21464;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#65292;&#36991;&#20813;&#20102;CSSL&#22312;OCL&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#19982;&#36830;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#22686;&#37327;&#23398;&#20064;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20250;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#28798;&#38590;&#24615;&#22320;&#24536;&#35760;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#30693;&#35782;&#21521;&#26087;&#20219;&#21153;&#65288;&#21521;&#21518;&#36716;&#31227;&#65289;&#21644;&#26410;&#26469;&#20219;&#21153;&#65288;&#21521;&#21069;&#36716;&#31227;&#65289;&#30340;&#20256;&#36882;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#33021;&#22815;&#20135;&#29983;&#22810;&#25165;&#22810;&#33402;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CSSL&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#25216;&#26415;&#65292;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;OCL&#21482;&#20801;&#35768;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#19968;&#27425;&#36845;&#20195;&#65292;&#32780;CSSL&#30340;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#23427;&#22312;&#36755;&#20837;&#25968;&#25454;&#27969;&#19978;&#30340;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31561;&#21464;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#65288;CLER&#65289;&#30340;OCL&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31561;&#21464;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#65292;&#36991;&#20813;&#20102;CSSL&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#23558;&#31561;&#21464;&#30693;&#35782;&#19982;CL&#30456;&#32467;&#21512;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;OCL&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can learn incrementally, whereas neural networks forget previously acquired information catastrophically. Continual Learning (CL) approaches seek to bridge this gap by facilitating the transfer of knowledge to both previous tasks (backward transfer) and future ones (forward transfer) during training.  Recent research has shown that self-supervision can produce versatile models that can generalize well to diverse downstream tasks. However, contrastive self-supervised learning (CSSL), a popular self-supervision technique, has limited effectiveness in online CL (OCL). OCL only permits one iteration of the input dataset, and CSSL's low sample efficiency hinders its use on the input data-stream.  In this work, we propose Continual Learning via Equivariant Regularization (CLER), an OCL approach that leverages equivariant tasks for self-supervision, avoiding CSSL's limitations. Our method represents the first attempt at combining equivariant knowledge with CL and can be easily integrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03626</link><description>&lt;p&gt;
&#40065;&#26834;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#21487;&#39564;&#35777;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#65292;&#23545;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159; NP-hard &#65292;&#22240;&#27492;&#23545;&#20110;&#29305;&#23450;&#30340;&#36755;&#20837;&#26469;&#35828;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#21463;&#38480;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#31216;&#20026; large-spread &#38598;&#25104;&#65292;&#20854;&#20801;&#35768;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#23433;&#20840;&#39564;&#35777;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39564;&#35777;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20513;&#23548;&#35757;&#32451;&#36825;&#31181;&#26131;&#20110;&#39564;&#35777;&#30340;&#21463;&#38480;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064; large-spread &#20915;&#31574;&#26641;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#30410;&#22788;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#12290;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#35757;&#32451;&#30340; large-spread &#38598;&#25104;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20351;&#29992;&#26631;&#20934;&#21322;&#23450;&#32534;&#31243;&#27714;&#35299;&#22120;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21512;&#26381;&#37327;&#21270;&#22238;&#24402;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#30456;&#27604;&#39640;&#26031;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#23545;&#35266;&#27979;&#22122;&#22768;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#65292;&#26356;&#30495;&#23454;&#40065;&#26834;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#22312;&#22810;&#20445;&#30495;&#24230;&#35774;&#32622;&#20013;&#32858;&#21512;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03623</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#26381;&#37327;&#21270;&#22238;&#24402;&#20248;&#21270;&#36229;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Optimizing Hyperparameters with Conformal Quantile Regression. (arXiv:2305.03623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21512;&#26381;&#37327;&#21270;&#22238;&#24402;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#30456;&#27604;&#39640;&#26031;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#23545;&#35266;&#27979;&#22122;&#22768;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#65292;&#26356;&#30495;&#23454;&#40065;&#26834;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#22312;&#22810;&#20445;&#30495;&#24230;&#35774;&#32622;&#20013;&#32858;&#21512;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#29616;&#26377;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#24037;&#20855;&#65292;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#26469;&#25351;&#23548;&#25628;&#32034;&#12290;&#39640;&#26031;&#36807;&#31243;&#26159;&#40664;&#35748;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#35266;&#27979;&#22122;&#22768;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21512;&#26381;&#37327;&#21270;&#22238;&#24402;&#65292;&#35813;&#26041;&#27861;&#23545;&#35266;&#27979;&#22122;&#22768;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#65292;&#22240;&#27492;&#26356;&#30495;&#23454;&#21644;&#40065;&#26834;&#22320;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#22312;&#23454;&#35777;&#22522;&#20934;&#19978;&#24555;&#36895;&#23454;&#29616;&#36229;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#12290;&#20026;&#20102;&#22312;&#22810;&#20445;&#30495;&#24230;&#35774;&#32622;&#20013;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36164;&#28304;&#27700;&#24179;&#19978;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty but they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03617</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#20998;&#21106;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#34880;&#31649;&#23545;&#20110;&#26089;&#26399;&#31579;&#26597;&#12289;&#35786;&#26029;&#21644;&#35780;&#20272;&#26576;&#20123;&#30524;&#37096;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#21464;&#24471;&#38750;&#24120;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#27880;&#24847;&#34701;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#32467;&#21512;&#20102;Transformer&#26500;&#24314;&#30340;&#36890;&#36947;&#27880;&#24847;&#21644;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#28040;&#38500;&#32534;&#30721;&#22120;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#22312;&#36339;&#36291;&#36830;&#25509;&#20013;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Dropout&#23618;&#38543;&#26426;&#33293;&#24323;&#19968;&#20123;&#31070;&#32463;&#20803;&#65292;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#24182;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;DERIVE&#12289;STARE&#21644;CHASEDB1&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#19968;&#20123;&#26368;&#36817;&#30340;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#65292;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#35745;&#31639;&#25345;&#20037;&#22270;&#24182;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#20445;&#25252;&#38544;&#31169;&#65292;&#25104;&#21151;&#23454;&#29616;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.03609</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Topological Data Analysis. (arXiv:2305.03609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#65292;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#35745;&#31639;&#25345;&#20037;&#22270;&#24182;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#20445;&#25252;&#38544;&#31169;&#65292;&#25104;&#21151;&#23454;&#29616;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#39318;&#31687;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#29942;&#39048;&#36317;&#31163;&#20998;&#26512;&#25345;&#20037;&#22270;&#30340;&#28789;&#25935;&#24230;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340; \v{C}ech &#22797;&#24418;&#30340;&#28789;&#25935;&#24230;&#24182;&#19981;&#20250;&#38543;&#30528;&#26679;&#26412;&#37327; $n$ &#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#65292;&#36825;&#20351;&#24471; \v{C}ech &#22797;&#24418;&#25345;&#20037;&#22270;&#38590;&#20197;&#38544;&#31169;&#21270;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#26469;&#35745;&#31639;&#25345;&#20037;&#22270;&#65292;&#21457;&#29616;&#20854;&#28789;&#25935;&#24230;&#20026; $O(1/n)$&#12290;&#22522;&#20110;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#65292;&#20854;&#25928;&#29992;&#20989;&#25968;&#23450;&#20041;&#20026; $L^1$-DTM &#25345;&#20037;&#22270;&#30340;&#29942;&#39048;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#38544;&#31169;&#26426;&#21046;&#30340;&#31934;&#24230;&#19978;&#19979;&#30028;&#65307;&#24471;&#21040;&#30340;&#30028;&#38480;&#34920;&#26126;&#25105;&#20204;&#30340;&#26426;&#21046;&#38544;&#31169;&#35823;&#24046;&#25509;&#36817;&#26368;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is the first to attempt differentially private (DP) topological data analysis (TDA), producing near-optimal private persistence diagrams. We analyze the sensitivity of persistence diagrams in terms of the bottleneck distance, and we show that the commonly used \v{C}ech complex has sensitivity that does not decrease as the sample size $n$ increases. This makes it challenging for the persistence diagrams of \v{C}ech complexes to be privatized. As an alternative, we show that the persistence diagram obtained by the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the sensitivity analysis, we propose using the exponential mechanism whose utility function is defined in terms of the bottleneck distance of the $L^1$-DTM persistence diagrams. We also derive upper and lower bounds of the accuracy of our privacy mechanism; the obtained bounds indicate that the privacy error of our mechanism is near-optimal. We demonstrate the performance of our privatized persistence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27493;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(AM-CBF)&#26469;&#35299;&#20915;&#24403;&#21069;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBF)&#30340;&#22522;&#26412;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#31867;-$\mathcal{K}$&#20989;&#25968;&#24182;&#19982;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19968;&#36215;&#35757;&#32451;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#22810;&#27493;&#35757;&#32451;&#21644;&#21333;&#27493;&#25191;&#34892;&#8221;&#33539;&#24335;&#26469;&#20943;&#36731;CBF&#30340;&#30701;&#35270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03608</link><description>&lt;p&gt;
&#20851;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#26368;&#20248;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#21487;&#34892;&#24615;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
On the Optimality, Stability, and Feasibility of Control Barrier Functions: An Adaptive Learning-Based Approach. (arXiv:2305.03608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27493;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(AM-CBF)&#26469;&#35299;&#20915;&#24403;&#21069;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBF)&#30340;&#22522;&#26412;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#31867;-$\mathcal{K}$&#20989;&#25968;&#24182;&#19982;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19968;&#36215;&#35757;&#32451;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#22810;&#27493;&#35757;&#32451;&#21644;&#21333;&#27493;&#25191;&#34892;&#8221;&#33539;&#24335;&#26469;&#20943;&#36731;CBF&#30340;&#30701;&#35270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#19968;&#30452;&#26159;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#37096;&#32626;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBF)&#21450;&#20854;&#21464;&#20307;&#24050;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#29992;&#20110;&#20851;&#38190;&#23433;&#20840;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CBF&#30340;&#30701;&#35270;&#21333;&#27493;&#24615;&#36136;&#20197;&#21450;&#32570;&#20047;&#35774;&#35745;&#31867;-$\mathcal{K}$&#20989;&#25968;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#30446;&#21069;&#30340;CBF&#20173;&#23384;&#22312;&#22522;&#26412;&#23616;&#38480;&#24615;&#65306;&#26368;&#20248;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21644;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#33258;&#36866;&#24212;&#22810;&#27493;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(AM-CBF)&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#31867;-$\mathcal{K}$&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#19982;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19968;&#36215;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#30701;&#35270;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#8220;&#22810;&#27493;&#35757;&#32451;&#21644;&#21333;&#27493;&#25191;&#34892;&#8221;&#33539;&#24335;&#65292;&#20351;CBF&#20855;&#26377;&#36828;&#35265;&#24615;&#65292;&#32780;&#25191;&#34892;&#20173;&#28982;&#35299;&#20915;&#21333;&#27493;&#20984;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety has been a critical issue for the deployment of learning-based approaches in real-world applications. To address this issue, control barrier function (CBF) and its variants have attracted extensive attention for safety-critical control. However, due to the myopic one-step nature of CBF and the lack of principled methods to design the class-$\mathcal{K}$ functions, there are still fundamental limitations of current CBFs: optimality, stability, and feasibility. In this paper, we proposed a novel and unified approach to address these limitations with Adaptive Multi-step Control Barrier Function (AM-CBF), where we parameterize the class-$\mathcal{K}$ function by a neural network and train it together with the reinforcement learning policy. Moreover, to mitigate the myopic nature, we propose a novel \textit{multi-step training and single-step execution} paradigm to make CBF farsighted while the execution remains solving a single-step convex quadratic program. Our method is evaluated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03582</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#28508;&#22312;&#31354;&#38388;&#34987;&#26500;&#36896;&#20026;&#23558;&#22312;&#21508;&#20010;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#30340;&#28508;&#22312;&#21160;&#24577;&#22240;&#32032;&#19982;&#27599;&#20010;&#27169;&#24577;&#29305;&#23450;&#30340;&#22240;&#32032;&#21306;&#20998;&#24320;&#26469;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#20010;&#38745;&#24577;&#28508;&#21464;&#37327;&#26469;&#32534;&#30721;&#38899;&#35270;&#39057;&#35821;&#38899;&#24207;&#21015;&#20013;&#38543;&#26102;&#38388;&#24658;&#23450;&#30340;&#20449;&#24687;&#12290;&#27169;&#22411;&#22312;&#19968;&#20010;&#38899;&#35270;&#39057;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#65292;&#39318;&#20808;&#29420;&#31435;&#23398;&#20064;&#19968;&#20010;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#65292;&#32780;&#27809;&#26377;&#26102;&#38388;&#24314;&#27169;&#12290;&#31532;&#20108;&#38454;&#27573;&#21017;&#22312;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAEs&#65289;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#23398;&#20064;MDVAE&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38480;&#21046;&#37325;&#26032;&#35843;&#24230;&#33539;&#22260;&#30340;&#26680;&#24515;&#38382;&#39064;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#38382;&#39064;&#65292;&#21021;&#27493;&#20351;&#29992;Flatland&#27169;&#25311;&#29615;&#22659;&#24471;&#21040;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03574</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#23454;&#26102;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#30340;&#33539;&#22260;&#38480;&#21046;&#65306;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scope Restriction for Scalable Real-Time Railway Rescheduling: An Exploratory Study. (arXiv:2305.03574v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38480;&#21046;&#37325;&#26032;&#35843;&#24230;&#33539;&#22260;&#30340;&#26680;&#24515;&#38382;&#39064;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#38382;&#39064;&#65292;&#21021;&#27493;&#20351;&#29992;Flatland&#27169;&#25311;&#29615;&#22659;&#24471;&#21040;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#38382;&#39064;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#26088;&#22312;&#21050;&#28608;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26680;&#24515;&#38382;&#39064;&#30340;&#23450;&#20041;&#65292;&#21363;&#22312;&#21709;&#24212;&#25200;&#21160;&#26102;&#20165;&#38480;&#21046;&#38656;&#35201;&#37325;&#26032;&#35843;&#24230;&#30340;&#21015;&#36710;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#38480;&#21046;&#33539;&#22260;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#22320;&#29702;&#33539;&#22260;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22256;&#38590;&#22312;&#20110;&#23450;&#20041;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#25200;&#21160;&#20250;&#24433;&#21709;&#21738;&#20123;&#21015;&#36710;&#26381;&#21153;&#23376;&#38598;&#30340;&#33539;&#22260;&#35268;&#23450;&#22120;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;Flatland&#27169;&#25311;&#29615;&#22659;&#24471;&#20986;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#36825;&#20010;&#24819;&#27861;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#28216;&#20048;&#22330;&#24320;&#28304;&#23454;&#29616;&#65292;&#22522;&#20110;Flatland&#38081;&#36335;&#29615;&#22659;&#21644;Answer-Set&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to stimulate future research, we describe an exploratory study of a railway rescheduling problem. A widely used approach in practice and state of the art is to decompose these complex problems by geographical scope. Instead, we propose defining a core problem that restricts a rescheduling problem in response to a disturbance to only trains that need to be rescheduled, hence restricting the scope in both time and space. In this context, the difficulty resides in defining a scoper that can predict a subset of train services that will be affected by a given disturbance. We report preliminary results using the Flatland simulation environment that highlights the potential and challenges of this idea. We provide an extensible playground open-source implementation based on the Flatland railway environment and Answer-Set Programming.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#24378;&#21270;&#23398;&#20064;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#36890;&#36947;&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#20256;&#36755;&#24847;&#20041;&#32780;&#38750;&#31934;&#30830;&#29256;&#26412;&#65292;&#36798;&#21040;&#20102;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.03571</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#27169;&#22411;&#26080;&#20851;&#35821;&#20041;&#36890;&#20449;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient. (arXiv:2305.03571v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#24378;&#21270;&#23398;&#20064;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#36890;&#36947;&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#20256;&#36755;&#24847;&#20041;&#32780;&#38750;&#31934;&#30830;&#29256;&#26412;&#65292;&#36798;&#21040;&#20102;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#26080;&#32447;&#36890;&#20449;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#38886;&#24343;&#65288;Weaver&#65289;&#20110;1949&#24180;&#25552;&#20986;&#30340;&#35821;&#20041;&#36890;&#20449;&#27010;&#24565;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#23427;&#25171;&#30772;&#20102;&#39321;&#20892;&#32463;&#20856;&#30340;&#35774;&#35745;&#33539;&#20363;&#65292;&#26088;&#22312;&#20256;&#36755;&#28040;&#24687;&#30340;&#24847;&#20041;&#65292;&#21363;&#35821;&#20041;&#65292;&#32780;&#19981;&#26159;&#31934;&#30830;&#29256;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#26469;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#19981;&#38656;&#35201;&#24050;&#30693;&#25110;&#21487;&#24494;&#20998;&#36890;&#36947;&#27169;&#22411;&#65292;&#36825;&#26159;&#23454;&#38469;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#26368;&#22823;&#21270;&#25509;&#25910;&#21644;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20986;&#21457;&#65292;&#28608;&#21457;&#20102;&#23558;SPG&#29992;&#20110;&#32463;&#20856;&#21644;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#22522;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#25910;&#25947;&#36895;&#24230;&#26377;&#25152;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent success of Machine Learning tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning, i.e., semantics, of a message instead of its exact version, allowing for information rate savings. In this work, we apply the Stochastic Policy Gradient (SPG) to design a semantic communication system by reinforcement learning, not requiring a known or differentiable channel model a crucial step towards deployment in practice. Further, we motivate the use of SPG for both classic and semantic communication from the maximization of the mutual information between received and target variables. Numerical results show that our approach achieves comparable performance to a model-aware approach based on the reparametrization trick, albeit with a decreased convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03568</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#30340;&#30690;&#37327;&#37327;&#21270;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#38754;&#30417;&#30563;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#65292;&#24050;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65288;VQ-MAE-AV&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#21407;&#22987;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;MAEs&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;VoxCeleb2&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#38899;&#35270;&#39057;SER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;Wasserstein&#32858;&#31867;&#65292;&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#26426;&#26500;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#30417;&#31649;&#32773;&#30340;&#30417;&#31649;&#24037;&#20316;&#65292;&#24182;&#22312;&#20854;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03565</link><description>&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;&#30340;&#20960;&#20309;&#24418;&#24577;--&#37329;&#34701;&#25968;&#25454;&#30340;Wasserstein&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
The geometry of financial institutions -- Wasserstein clustering of financial data. (arXiv:2305.03565v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;Wasserstein&#32858;&#31867;&#65292;&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#26426;&#26500;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#30417;&#31649;&#32773;&#30340;&#30417;&#31649;&#24037;&#20316;&#65292;&#24182;&#22312;&#20854;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#21152;&#30340;&#21508;&#31181;&#26377;&#36259;&#23545;&#35937;&#30340;&#32454;&#33410;&#21644;&#22823;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#26377;&#24517;&#35201;&#24320;&#21457;&#23558;&#36825;&#20123;&#20449;&#24687;&#21387;&#32553;&#25104;&#20195;&#34920;&#24615;&#21644;&#21487;&#29702;&#35299;&#30340;&#22320;&#22270;&#30340;&#26041;&#27861;&#12290;&#37329;&#34701;&#30417;&#31649;&#26159;&#19968;&#20010;&#23637;&#31034;&#36825;&#31181;&#38656;&#27714;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#30417;&#31649;&#26426;&#26500;&#38656;&#35201;&#20174;&#37329;&#34701;&#26426;&#26500;&#33719;&#21462;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#26377;&#26102;&#26159;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#65292;&#20197;&#30417;&#30563;&#21644;&#35780;&#20272;&#20182;&#20204;&#30340;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#22788;&#29702;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;Lloyd&#31639;&#27861;&#21464;&#20307;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;Wasserstein&#37325;&#24515;&#26500;&#24314;&#34920;&#31034;&#19981;&#21516;&#23545;&#35937;&#19978;&#30340;&#32473;&#23450;&#25968;&#25454;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#24212;&#23545;&#37329;&#34701;&#30417;&#31649;&#32972;&#26223;&#19979;&#30417;&#31649;&#32773;&#38754;&#20020;&#30340;&#20855;&#20307;&#25361;&#25112;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#22312;&#37329;&#34701;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of granular and big data on various objects of interest has made it necessary to develop methods for condensing this information into a representative and intelligible map. Financial regulation is a field that exemplifies this need, as regulators require diverse and often highly granular data from financial institutions to monitor and assess their activities. However, processing and analyzing such data can be a daunting task, especially given the challenges of dealing with missing values and identifying clusters based on specific features.  To address these challenges, we propose a variant of Lloyd's algorithm that applies to probability distributions and uses generalized Wasserstein barycenters to construct a metric space which represents given data on various objects in condensed form. By applying our method to the financial regulation context, we demonstrate its usefulness in dealing with the specific challenges faced by regulators in this domain. We beli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#26354;&#29575;&#31354;&#38388;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#26041;&#27861;CONGREGATE&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03555</link><description>&lt;p&gt;
&#26354;&#29575;&#31354;&#38388;&#20013;&#23545;&#27604;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Clustering in Curvature Spaces. (arXiv:2305.03555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#26354;&#29575;&#31354;&#38388;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#26041;&#27861;CONGREGATE&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30740;&#31350;&#30340;&#35805;&#39064;&#65292;&#22312;&#36817;&#24180;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#19968;&#26041;&#38754;&#65292;&#20174;&#20960;&#20309;&#35282;&#24230;&#36827;&#34892;&#22270;&#32858;&#31867;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#24456;&#23569;&#28041;&#21450;&#21040;&#23427;&#30340;&#20960;&#20309;&#32858;&#31867;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#22270;&#32858;&#31867;&#30340;&#25928;&#26524;&#65292;&#20294;&#36890;&#24120;&#20250;&#22312;&#22270;&#22686;&#24378;&#25110;&#38590;&#20363;&#25366;&#25496;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#23581;&#35797;&#39318;&#27425;&#24341;&#20837;&#24322;&#26500;&#26354;&#29575;&#31354;&#38388;&#21040;&#22270;&#32858;&#31867;&#38382;&#39064;&#20013;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CONGREGATE&#30340;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#23545;&#27604;&#22270;&#32858;&#31867;&#27169;&#22411;&#65292;&#29992;Ricci&#26354;&#29575;&#35299;&#20915;&#20960;&#20309;&#22270;&#32858;&#31867;&#12290;&#20026;&#20102;&#25903;&#25345;&#20960;&#20309;&#32858;&#31867;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#25903;&#25745;&#30340;&#24322;&#26500;&#26354;&#29575;&#31354;&#38388;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22270;&#30340;&#21508;&#31181;&#26354;&#29575;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;&#26354;&#29575;&#30340;&#20960;&#20309;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a longstanding research topic, and has achieved remarkable success with the deep learning methods in recent years. Nevertheless, we observe that several important issues largely remain open. On the one hand, graph clustering from the geometric perspective is appealing but has rarely been touched before, as it lacks a promising space for geometric clustering. On the other hand, contrastive learning boosts the deep graph clustering but usually struggles in either graph augmentation or hard sample mining. To bridge this gap, we rethink the problem of graph clustering from geometric perspective and, to the best of our knowledge, make the first attempt to introduce a heterogeneous curvature space to graph clustering problem. Correspondingly, we present a novel end-to-end contrastive graph clustering model named CONGREGATE, addressing geometric graph clustering with Ricci curvatures. To support geometric clustering, we construct a theoretically grounded Heterogeneous Curv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#38480;&#21151;&#32791;&#21644;&#38544;&#31169;&#39044;&#31639;&#30340;&#24046;&#20998;&#38544;&#31169;OTA&#32852;&#37030;&#24179;&#22343;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#40784;&#31995;&#25968;&#23545;&#26799;&#24230;&#36827;&#34892;&#32858;&#21512;&#24182;&#20351;&#29992;&#20449;&#36947;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03547</link><description>&lt;p&gt;
&#26377;&#38480;&#21151;&#32791;&#21644;&#38544;&#31169;&#39044;&#31639;&#30340;OTA&#32852;&#37030;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Federated Averaging with Limited Power and Privacy Budgets. (arXiv:2305.03547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#38480;&#21151;&#32791;&#21644;&#38544;&#31169;&#39044;&#31639;&#30340;&#24046;&#20998;&#38544;&#31169;OTA&#32852;&#37030;&#24179;&#22343;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#40784;&#31995;&#25968;&#23545;&#26799;&#24230;&#36827;&#34892;&#32858;&#21512;&#24182;&#20351;&#29992;&#20449;&#36947;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20849;&#21516;&#20811;&#26381;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;(FL)&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#38480;&#24635;&#21151;&#29575;&#39044;&#31639;&#30340;&#24046;&#20998;&#38544;&#31169;OTA&#32852;&#37030;&#24179;&#22343;(DP-OTA-FedAvg)&#31995;&#32479;&#12290;&#20351;&#29992;DP-OTA-FedAvg&#65292;&#26799;&#24230;&#36890;&#36807;&#19968;&#20010;&#23545;&#40784;&#31995;&#25968;&#36827;&#34892;&#23545;&#40784;&#24182;&#22312;&#31354;&#20013;&#32858;&#21512;&#65292;&#21516;&#26102;&#21033;&#29992;&#20449;&#36947;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#28385;&#36275;&#24635;&#21151;&#29575;&#21644;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#32852;&#21512;&#35774;&#35745;&#35774;&#22791;&#35843;&#24230;&#12289;&#23545;&#40784;&#31995;&#25968;&#21644;&#32852;&#37030;&#24179;&#22343;(FedAvg)&#30340;&#32858;&#21512;&#36718;&#25968;&#65292;&#20197;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;(DP)&#25552;&#20379;&#20102;&#38544;&#31169;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30740;&#31350;&#35774;&#22791;&#35843;&#24230;&#12289;&#23545;&#40784;&#31995;&#25968;&#21644;&#20840;&#23616;&#32858;&#21512;&#25968;&#37327;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23545;DP-OTA-FedAvg&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
To jointly overcome the communication bottleneck and privacy leakage of wireless federated learning (FL), this paper studies a differentially private over-the-air federated averaging (DP-OTA-FedAvg) system with a limited sum power budget. With DP-OTA-FedAvg, the gradients are aligned by an alignment coefficient and aggregated over the air, and channel noise is employed to protect privacy. We aim to improve the learning performance by jointly designing the device scheduling, alignment coefficient, and the number of aggregation rounds of federated averaging (FedAvg) subject to sum power and privacy constraints. We first present the privacy analysis based on differential privacy (DP) to quantify the impact of the alignment coefficient on privacy preservation in each communication round. Furthermore, to study how the device scheduling, alignment coefficient, and the number of the global aggregation affect the learning process, we conduct the convergence analysis of DP-OTA-FedAvg in the cas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856;Sobolev&#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03531</link><description>&lt;p&gt;
&#26680;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Random Smoothing Regularization in Kernel Gradient Descent Learning. (arXiv:2305.03531v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856;Sobolev&#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#36755;&#20837;&#25968;&#25454;&#24341;&#20837;&#22122;&#22768;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#29305;&#24449;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#26426;&#24179;&#28369;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#32570;&#20047;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856; Sobolev &#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22522;&#30784;&#30340;&#20989;&#25968;&#31354;&#38388;&#65306;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340; Sobolev &#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#25324; $D$ &#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#25110;&#20302;&#32500;&#23376;&#27969;&#24418;&#20316;&#20026;&#29305;&#20363;&#65292;&#20197;&#21450;&#20855;&#26377;&#24352;&#37327;&#32467;&#26500;&#30340;&#28151;&#21512;&#24179;&#28369; Sobolev &#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#20316;&#20026;&#26032;&#22411;&#21367;&#31215;&#24179;&#28369;&#26680;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random smoothing data augmentation is a unique form of regularization that can prevent overfitting by introducing noise to the input data, encouraging the model to learn more generalized features. Despite its success in various applications, there has been a lack of systematic study on the regularization ability of random smoothing. In this paper, we aim to bridge this gap by presenting a framework for random smoothing regularization that can adaptively and effectively learn a wide range of ground truth functions belonging to the classical Sobolev spaces. Specifically, we investigate two underlying function spaces: the Sobolev space of low intrinsic dimension, which includes the Sobolev space in $D$-dimensional Euclidean space or low-dimensional sub-manifolds as special cases, and the mixed smooth Sobolev space with a tensor structure. By using random smoothing regularization as novel convolution-based smoothing kernels, we can attain optimal convergence rates in these cases using a ke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#25104;&#21151;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.03530</link><description>&lt;p&gt;
&#25506;&#32034;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#21487;&#25511;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation. (arXiv:2305.03530v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#25104;&#21151;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#65288;SMLM&#65289;&#24212;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#26089;&#26399;&#25506;&#32034;&#12290;SMLM&#21487;&#35270;&#20026;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#20854;&#20013;&#36755;&#20837;&#38598;&#21512;&#30340;&#27599;&#20010;&#20803;&#32032;&#21487;&#20197;&#26159;&#37096;&#20998;&#24050;&#30693;&#30340;&#65292;&#32780;&#19981;&#26159;&#24050;&#30693;&#25110;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#23637;&#31034;&#20102;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#21487;&#22312;https://erl-j.github.io/smlm-web-supplement/&#19978;&#25214;&#21040;&#33509;&#24178;&#38899;&#39057;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents some early explorations of applying Softly Masked Language Modelling (SMLM) to symbolic music generation. SMLM can be seen as a generalisation of masked language modelling (MLM), where instead of each element of the input set being either known or unknown, elements can be partly known. We demonstrate some results of applying SMLM to constrained symbolic music generation using a transformer encoder architecture. Several audio examples are available at https://erl-j.github.io/smlm-web-supplement/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;</title><link>http://arxiv.org/abs/2305.03514</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#35768;&#22810;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot&#25805;&#20316;&#65288;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#22914;&#26524;&#36825;&#31181;&#33021;&#21147;&#20063;&#36866;&#29992;&#20110;&#23545;&#35828;&#26381;&#21147;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#31561;&#31038;&#20250;&#29616;&#35937;&#30340;&#32534;&#30721;&#65292;&#37027;&#20040;LLMs&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;(CSS)&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;CSS&#24037;&#20855;&#30340;&#36335;&#32447;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20248;&#31168;&#30340;&#25552;&#31034;&#23454;&#36341;&#20197;&#21450;&#19968;&#20010;&#24191;&#27867;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20197;&#27979;&#37327;13&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;24&#20010;&#20195;&#34920;&#24615;&#30340;CSS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;LLMs&#26080;&#27861;&#36229;&#36234;&#26368;&#20339;&#24494;&#35843;&#27169;&#22411;&#65292;&#20294;&#20173;&#28982;&#19982;&#20154;&#31867;&#36798;&#25104;&#20102;&#20844;&#24179;&#30340;&#21327;&#35758;&#27700;&#24179;&#12290;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#32534;&#30721;&#20219;&#21153;&#65288;&#29983;&#25104;&#65289;&#19978;&#65292;LLMs&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#36229;&#36807;&#20102;&#24037;&#20316;&#32773;&#30340;&#40644;&#37329;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20170;&#22825;&#30340;LLMs&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20174;&#26681;&#26412;&#19978;&#22686;&#24378;CSS&#30740;&#31350;&#27969;&#31243;&#65306;(1)&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#26080;&#32541;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
&lt;/p&gt;</description></item><item><title>ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2305.03513</link><description>&lt;p&gt;
ChatGraph: &#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03513
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26368;&#36817;&#25512;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#65306;&#65288;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#19981;&#28789;&#27963;&#24615;&#21644;&#65288;2&#65289;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27861;&#24459;&#20889;&#20316;&#20013;&#24341;&#29992;&#20540;&#30340;&#37492;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#27979;&#35797;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03508</link><description>&lt;p&gt;
CiteCaseLAW: &#29992;&#20110;&#27861;&#24459;&#36741;&#21161;&#20889;&#20316;&#30340;&#21028;&#20363;&#27861;&#24341;&#29992;&#20540;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing. (arXiv:2305.03508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27861;&#24459;&#20889;&#20316;&#20013;&#24341;&#29992;&#20540;&#30340;&#37492;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#27979;&#35797;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#25991;&#20214;&#25776;&#20889;&#20013;&#65292;&#27491;&#30830;&#24341;&#29992;&#26696;&#20363;&#27861;&#21644;&#20854;&#20182;&#26469;&#28304;&#20197;&#35777;&#26126;&#22768;&#26126;&#21644;&#35770;&#28857;&#26159;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#12290;&#29702;&#35299;&#27861;&#24459;&#39046;&#22495;&#24182;&#35782;&#21035;&#36866;&#24403;&#30340;&#24341;&#29992;&#19978;&#19979;&#25991;&#25110;&#20540;&#24471;&#24341;&#29992;&#30340;&#21477;&#23376;&#26159;&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27861;&#24459;&#26415;&#35821;&#12289;&#35821;&#20041;&#21644;&#39640;&#24230;&#29305;&#24322;&#24615;&#30340;&#23384;&#22312;&#20351;&#24471;&#27861;&#24459;&#35821;&#35328;&#21464;&#24471;&#22797;&#26434;&#65292;&#20174;&#32780;&#20351;&#20219;&#20309;&#30456;&#20851;&#30340;&#27861;&#24459;&#20219;&#21153;&#38590;&#20197;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#24341;&#29992;&#20540;&#37492;&#21035;&#30340;&#38382;&#39064;&#12290;&#23427;&#26088;&#22312;&#25104;&#20026;&#24403;&#20170;&#24341;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#20197;&#20943;&#36731;&#25552;&#21462;&#36275;&#22815;&#30340;&#24341;&#25991;&#19978;&#19979;&#25991;&#30340;&#36127;&#25285;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174; Caselaw &#35775;&#38382;&#39033;&#30446; (CAP) &#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#24341;&#25991;&#20540;&#26816;&#27979;&#12290;&#22312;&#36825;&#20010;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In legal document writing, one of the key elements is properly citing the case laws and other sources to substantiate claims and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences are challenging tasks that demand expensive manual annotation. The presence of jargon, language semantics, and high domain specificity makes legal language complex, making any associated legal task hard for automation. The current work focuses on the problem of citation-worthiness identification. It is designed as the initial step in today's citation recommendation systems to lighten the burden of extracting an adequate set of citation contexts. To accomplish this, we introduce a labeled dataset of 178M sentences for citation-worthiness detection in the legal domain from the Caselaw Access Project (CAP). The performance of various deep learning models was examined on this novel dataset. The domain-specific pre-trained model tends to outperform other
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#32508;&#36848;&#20102;&#32593;&#32476;&#23884;&#20837;&#25991;&#29486;&#21644;&#24403;&#21069;&#36235;&#21183;&#30340;&#29992;&#25143;&#21451;&#22909;&#25351;&#21335;&#65292;&#20026;&#32593;&#32476;&#25512;&#29702;&#30340;&#22522;&#26412;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.03474</link><description>&lt;p&gt;
Zoo Guide to Network Embedding.&#65288;&#32593;&#32476;&#23884;&#20837;&#21160;&#29289;&#22253;&#25351;&#21335;&#65289;
&lt;/p&gt;
&lt;p&gt;
Zoo Guide to Network Embedding. (arXiv:2305.03474v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#32508;&#36848;&#20102;&#32593;&#32476;&#23884;&#20837;&#25991;&#29486;&#21644;&#24403;&#21069;&#36235;&#21183;&#30340;&#29992;&#25143;&#21451;&#22909;&#25351;&#21335;&#65292;&#20026;&#32593;&#32476;&#25512;&#29702;&#30340;&#22522;&#26412;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#20102;&#25968;&#25454;&#21644;&#22797;&#26434;&#31995;&#32479;&#30340;&#26497;&#20026;&#25104;&#21151;&#30340;&#24314;&#27169;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32452;&#21512;&#23545;&#35937;&#65292;&#32593;&#32476;&#36890;&#24120;&#27809;&#26377;&#22266;&#26377;&#30340;&#22352;&#26631;&#65292;&#20063;&#19981;&#20856;&#22411;&#22320;&#20301;&#20110;&#19968;&#20010;&#29615;&#22659;&#31354;&#38388;&#20013;&#12290;&#20026;&#32593;&#32476;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#30340;&#36807;&#31243;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#24050;&#34987;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#32593;&#32476;&#25512;&#29702;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#32593;&#32476;&#23884;&#20837;&#25991;&#29486;&#21644;&#24403;&#21069;&#36235;&#21183;&#30340;&#29992;&#25143;&#21451;&#22909;&#25351;&#21335;&#65292;&#20351;&#35835;&#32773;&#33021;&#22815;&#22312;&#36825;&#20123;&#20027;&#39064;&#30340;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#27963;&#21160;&#20013;&#27983;&#35272;&#26041;&#27861;&#21644;&#26041;&#27861;&#30340;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks have provided extremely successful models of data and complex systems. Yet, as combinatorial objects, networks do not have in general intrinsic coordinates and do not typically lie in an ambient space. The process of assigning an embedding space to a network has attracted lots of interest in the past few decades, and has been efficiently applied to fundamental problems in network inference, such as link prediction, node classification, and community detection. In this review, we provide a user-friendly guide to the network embedding literature and current trends in this field which will allow the reader to navigate through the complex landscape of methods and approaches emerging from the vibrant research activity on these subjects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#27604;&#26631;&#20934;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#26131;&#20110;&#20998;&#26512;&#12289;&#21516;&#26102;&#34920;&#29616;&#26356;&#22909;&#30340;&#21452;&#32447;&#24615;&#23618;&#65292;&#36825;&#20026;&#28145;&#20837;&#30340;&#23433;&#20840;&#27934;&#23519;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03452</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#21452;&#32447;&#24615;&#23618;&#25216;&#26415;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A technical note on bilinear layers for interpretability. (arXiv:2305.03452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#27604;&#26631;&#20934;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#26131;&#20110;&#20998;&#26512;&#12289;&#21516;&#26102;&#34920;&#29616;&#26356;&#22909;&#30340;&#21452;&#32447;&#24615;&#23618;&#65292;&#36825;&#20026;&#28145;&#20837;&#30340;&#23433;&#20840;&#27934;&#23519;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36229;&#36234;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#21472;&#21152;&#25928;&#24212;&#65292;&#24050;&#32463;&#28608;&#21457;&#20102;&#23547;&#25214;&#27604;&#20855;&#26377;&#20803;&#32032;&#32423;&#28608;&#27963;&#20989;&#25968;&#30340;&#26631;&#20934;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26356;&#26131;&#20110;&#35299;&#37322;&#30340;&#26550;&#26500;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#20010;&#27880;&#37322;&#20013;&#65292;&#25105;&#30740;&#31350;&#20102;&#21452;&#32447;&#24615;&#23618;&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#23398;&#19978;&#26356;&#23481;&#26131;&#20998;&#26512;&#32780;&#19988;&#21516;&#26102;&#34920;&#29616;&#27604;&#26631;&#20934; MLP &#26356;&#22909;&#30340; MLP &#23618;&#30340;&#31867;&#22411;&#12290;&#34429;&#28982;&#23427;&#20204;&#26159;&#20854;&#36755;&#20837;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#20294;&#25105;&#35777;&#26126;&#21452;&#32447;&#24615;&#23618;&#21487;&#20197;&#20165;&#20351;&#29992;&#32447;&#24615;&#25805;&#20316;&#21644;&#19977;&#38454;&#24352;&#37327;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#21452;&#32447;&#24615;&#23618;&#30340;&#36825;&#31181;&#34920;&#36798;&#24335;&#38598;&#25104;&#21040;&#36716;&#25442;&#22120;&#30005;&#36335;&#30340;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#35813;&#25968;&#23398;&#26694;&#26550;&#20197;&#21069;&#20165;&#36866;&#29992;&#20110;&#20165;&#20855;&#26377;&#27880;&#24847;&#21147;&#30340;&#36716;&#25442;&#22120;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#30340;&#26550;&#26500;&#65292;&#21452;&#32447;&#24615;&#23618;&#25968;&#23398;&#19978;&#26356;&#26131;&#20110;&#20998;&#26512;&#65292;&#22240;&#27492;&#21487;&#33021;&#36890;&#36807;&#20801;&#35768;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#35752;&#35770;&#26469;&#20026;&#26356;&#28145;&#20837;&#30340;&#23433;&#20840;&#27934;&#23519;&#21147;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of neural networks to represent more features than neurons makes interpreting them challenging. This phenomenon, known as superposition, has spurred efforts to find architectures that are more interpretable than standard multilayer perceptrons (MLPs) with elementwise activation functions. In this note, I examine bilinear layers, which are a type of MLP layer that are mathematically much easier to analyze while simultaneously performing better than standard MLPs. Although they are nonlinear functions of their input, I demonstrate that bilinear layers can be expressed using only linear operations and third order tensors. We can integrate this expression for bilinear layers into a mathematical framework for transformer circuits, which was previously limited to attention-only transformers. These results suggest that bilinear layers are easier to analyze mathematically than current architectures and thus may lend themselves to deeper safety insights by allowing us to talk more f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;AGCSC&#65289;&#65292;&#23558;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#30495;&#23454;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#25581;&#31034;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03414</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Graph Convolutional Subspace Clustering. (arXiv:2305.03414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;AGCSC&#65289;&#65292;&#23558;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#30495;&#23454;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#25581;&#31034;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#22411;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#22312;&#35768;&#22810;&#23376;&#31354;&#38388;&#32858;&#31867;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#20809;&#35889;&#22411;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#35201;&#20040;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25214;&#21040;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26412;&#25991;&#21463;&#21040;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#22270;&#21367;&#31215;&#25216;&#26415;&#21516;&#26102;&#24320;&#21457;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#36845;&#20195;&#21644;&#33258;&#36866;&#24212;&#22320;&#26356;&#26032;&#22270;&#21367;&#31215;&#31639;&#23376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;AGCSC&#65289;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#36890;&#36807;&#20351;&#29992;AGCSC&#65292;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#32858;&#21512;&#29305;&#24449;&#34920;&#31034;&#36866;&#21512;&#20110;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#31995;&#25968;&#30697;&#38453;&#21487;&#20197;&#26356;&#30495;&#23454;&#22320;&#25581;&#31034;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23376;&#31354;&#38388;&#32858;&#31867;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;AGCSC&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#20998;&#36776;&#29575;&#30340;T1&#21644;&#25193;&#25955;&#25968;&#25454;&#20013;&#20998;&#21106;&#20986;&#19992;&#33041;&#26680;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#19992;&#33041;&#26680;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03413</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21644;&#25193;&#25955;MRI&#30340;&#36890;&#29992;&#39046;&#22495;&#19979;&#19992;&#33041;&#26680;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion MRI. (arXiv:2305.03413v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#20998;&#36776;&#29575;&#30340;T1&#21644;&#25193;&#25955;&#25968;&#25454;&#20013;&#20998;&#21106;&#20986;&#19992;&#33041;&#26680;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#19992;&#33041;&#26680;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19992;&#33041;&#26159;&#22823;&#33041;&#20869;&#39640;&#24230;&#36830;&#25509;&#30340;&#30382;&#23618;&#19979;&#28784;&#36136;&#32467;&#26500;&#65292;&#21253;&#25324;&#25968;&#21313;&#20010;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#21644;&#36830;&#25509;&#24615;&#30340;&#26680;&#12290;&#36825;&#20123;&#26680;&#22240;&#30142;&#30149;&#32780;&#21463;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20854;&#36827;&#34892;MRI&#30740;&#31350;&#12290;&#34429;&#28982;&#26377;&#24037;&#20855;&#21487;&#20197;&#20174;1mm T1&#25195;&#25551;&#20013;&#20998;&#21106;&#20986;&#19992;&#33041;&#65292;&#20294;&#26159;&#36793;&#30028;&#30340;&#23545;&#27604;&#24230;&#22826;&#20302;&#32780;&#26080;&#27861;&#29983;&#25104;&#21487;&#38752;&#30340;&#20998;&#21106;&#12290;&#19968;&#20123;&#24037;&#20855;&#23581;&#35797;&#23558;&#25193;&#25955;MRI&#20013;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#20998;&#21106;&#20013;&#20197;&#20248;&#21270;&#36825;&#20123;&#36793;&#30028;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25193;&#25955;MRI&#33719;&#21462;&#26102;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#20174;&#20219;&#20309;&#20998;&#36776;&#29575;&#30340;T1&#21644;&#25193;&#25955;&#25968;&#25454;&#20013;&#20998;&#21106;&#19992;&#33041;&#26680;&#30340;CNN&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20844;&#20849;&#32452;&#32455;&#23398;&#30340;&#19992;&#33041;&#26680;&#22270;&#35889;&#21644;&#20855;&#26377;&#39640;&#36136;&#37327;&#25193;&#25955;&#25968;&#25454;&#30340;&#38134;&#26631;&#20934;&#20998;&#21106;&#30340;&#26368;&#26032;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#20998;&#21106;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human thalamus is a highly connected subcortical grey-matter structure within the brain. It comprises dozens of nuclei with different function and connectivity, which are affected differently by disease. For this reason, there is growing interest in studying the thalamic nuclei in vivo with MRI. Tools are available to segment the thalamus from 1 mm T1 scans, but the contrast of the lateral and internal boundaries is too faint to produce reliable segmentations. Some tools have attempted to incorporate information from diffusion MRI in the segmentation to refine these boundaries, but do not generalise well across diffusion MRI acquisitions. Here we present the first CNN that can segment thalamic nuclei from T1 and diffusion data of any resolution without retraining or fine tuning. Our method builds on a public histological atlas of the thalamic nuclei and silver standard segmentations on high-quality diffusion data obtained with a recent Bayesian adaptive segmentation tool. We combin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#31232;&#30095;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#28508;&#22312;&#20108;&#36827;&#21046;&#21464;&#37327;&#21644;&#24402;&#19968;&#21270;&#27969;&#65292;&#23454;&#29616;&#20102;&#32593;&#32476;&#22312;&#27979;&#35797;&#26102;&#30340;&#33258;&#21160;&#31232;&#30095;&#21270;&#65292;&#32780;&#19988;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.03395</link><description>&lt;p&gt;
&#29992;&#28508;&#22312;&#20108;&#36827;&#21046;&#21464;&#37327;&#21644;&#24402;&#19968;&#21270;&#27969;&#26469;&#31232;&#30095;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sparsifying Bayesian neural networks with latent binary variables and normalizing flows. (arXiv:2305.03395v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#31232;&#30095;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#28508;&#22312;&#20108;&#36827;&#21046;&#21464;&#37327;&#21644;&#24402;&#19968;&#21270;&#27969;&#65292;&#23454;&#29616;&#20102;&#32593;&#32476;&#22312;&#27979;&#35797;&#26102;&#30340;&#33258;&#21160;&#31232;&#30095;&#21270;&#65292;&#32780;&#19988;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26159;&#29616;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38754;&#37096;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30284;&#30151;&#35786;&#26029;&#12290;ANN&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#25968;&#30334;&#19975;&#25110;&#25968;&#21313;&#20159;&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#22240;&#27492;&#20542;&#21521;&#20110;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#22312;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24212;&#29992;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#21487;&#20197;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#28508;&#22312;&#20108;&#36827;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;LBBNN&#65289;&#36890;&#36807;&#20801;&#35768;&#23558;&#26435;&#37325;&#25171;&#24320;&#25110;&#20851;&#38381;&#65292;&#20174;&#32780;&#22312;&#26435;&#37325;&#21644;&#32467;&#26500;&#30340;&#32852;&#21512;&#31354;&#38388;&#20013;&#21551;&#29992;&#25512;&#26029;&#65292;&#20063;&#32771;&#34385;&#20102;&#32467;&#26500;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#23558;&#32771;&#34385;LBBNN&#26041;&#27861;&#30340;&#20004;&#20010;&#25193;&#23637;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#37325;&#21442;&#25968;&#21270;&#25216;&#24039;&#65288;LRT&#65289;&#30452;&#25509;&#37319;&#26679;&#38544;&#34255;&#21333;&#20803;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26356;&#21152;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#65292;&#25105;&#20204;&#21487;&#20197;&#36817;&#20284;&#28508;&#22312;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#26102;&#23454;&#29616;&#32593;&#32476;&#30340;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#33021;&#22815;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) are powerful machine learning methods used in many modern applications such as facial recognition, machine translation, and cancer diagnostics. A common issue with ANNs is that they usually have millions or billions of trainable parameters, and therefore tend to overfit to the training data. This is especially problematic in applications where it is important to have reliable uncertainty estimates. Bayesian neural networks (BNN) can improve on this, since they incorporate parameter uncertainty. In addition, latent binary Bayesian neural networks (LBBNN) also take into account structural uncertainty by allowing the weights to be turned on or off, enabling inference in the joint space of weights and structures. In this paper, we will consider two extensions to the LBBNN method: Firstly, by using the local reparametrization trick (LRT) to sample the hidden units directly, we get a more computationally efficient algorithm. More importantly, by using normal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#33976;&#39311;&#25439;&#22833;&#26469;&#22788;&#29702;&#38271;&#23614;&#20998;&#24067;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03378</link><description>&lt;p&gt;
&#38754;&#21521;&#38271;&#23614;&#35782;&#21035;&#30340;&#26377;&#25928;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Collaborative Learning in Long-Tailed Recognition. (arXiv:2305.03378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#33976;&#39311;&#25439;&#22833;&#26469;&#22788;&#29702;&#38271;&#23614;&#20998;&#24067;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#38271;&#23614;&#20998;&#24067;&#65292;&#20854;&#20013;&#23569;&#25968;&#31867;&#19982;&#22810;&#25968;&#31867;&#30456;&#27604;&#26174;&#30528;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#21033;&#29992;&#22810;&#19987;&#23478;&#20307;&#31995;&#32467;&#26500;&#26469;&#20943;&#36731;&#22312;&#23569;&#25968;&#31867;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#37319;&#29992;&#21327;&#20316;&#23398;&#20064;&#26469;&#27719;&#24635;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#21363;&#22312;&#32447;&#33976;&#39311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#22312;&#31867;&#21035;&#20998;&#24067;&#26041;&#38754;&#26159;&#19981;&#24179;&#34913;&#30340;&#65292;&#36825;&#23548;&#33268;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#33976;&#39311;&#25439;&#22833;&#65292;&#36890;&#36807;&#27604;&#36739;&#22312;&#32447;&#33976;&#39311;&#21644;&#26631;&#31614;&#27880;&#37322;&#20998;&#21035;&#30417;&#30563;&#30340;&#20004;&#20010;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#65292;&#29305;&#24449;&#32423;&#33976;&#39311;&#23558;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#22686;&#21152;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#23398;&#20064;&#65288;ECL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#33976;&#39311;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this paper, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers' predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastiv
&lt;/p&gt;</description></item><item><title>MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#22312;&#21508;&#33258;&#30340;&#23376;&#25361;&#25112;&#20013;&#39044;&#27979;&#24773;&#24863;&#30446;&#26631;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#30340;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.03369</link><description>&lt;p&gt;
MuSe 2023&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation. (arXiv:2305.03369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03369
&lt;/p&gt;
&lt;p&gt;
MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#22312;&#21508;&#33258;&#30340;&#23376;&#25361;&#25112;&#20013;&#39044;&#27979;&#24773;&#24863;&#30446;&#26631;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#22312;&#27169;&#25311;&#24773;&#24863;&#23376;&#25361;&#25112;&#65288;MuSe-Mimic&#65289;&#20013;&#65292;&#21442;&#19982;&#32773;&#39044;&#27979;&#19977;&#20010;&#36830;&#32493;&#24773;&#24863;&#30446;&#26631;&#12290;&#36825;&#20010;&#23376;&#25361;&#25112;&#21033;&#29992;&#20102;Hume-Vidmimic&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#29983;&#25104;&#30340;&#35270;&#39057;&#12290;&#23545;&#20110;&#36328;&#25991;&#21270;&#24189;&#40664;&#26816;&#27979;&#23376;&#25361;&#25112;&#65288;MuSe-Humour&#65289;&#65292;&#25552;&#20379;&#20102;Passau Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#39044;&#27979;&#36328;&#25991;&#21270;&#29615;&#22659;&#20013;&#33258;&#21457;&#24189;&#40664;&#30340;&#20986;&#29616;&#12290;&#20010;&#24615;&#21270;&#23376;&#25361;&#25112;&#65288;MuSe-Personalisation&#65289;&#22522;&#20110;Ulm-Trier Social Stress Test&#65288;Ulm-TSST&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22788;&#20110;&#32039;&#24352;&#29366;&#24577;&#19979;&#30340;&#34987;&#35797;&#24405;&#38899;&#12290;&#22312;&#36825;&#37324;&#65292;&#38656;&#35201;&#39044;&#27979;&#21796;&#37266;&#21644;&#20215;&#20540;&#20449;&#21495;&#65292;&#32780;&#37096;&#20998;&#27979;&#35797;&#26631;&#31614;&#21017;&#21487;&#29992;&#20110;&#20419;&#36827;&#20010;&#24615;&#21270;&#12290;MuSe 2023&#26088;&#22312;&#23558;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#21463;&#20247;&#27719;&#32858;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MuSe 2023 is a set of shared tasks addressing three different contemporary multimodal affect and sentiment analysis problems: In the Mimicked Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset comprising of user-generated videos. For the Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football Coach Humour (Passau-SFCH) dataset is provided. Participants predict the presence of spontaneous humour in a cross-cultural setting. The Personalisation Sub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed situation. Here, arousal and valence signals are to be predicted, whereas parts of the test labels are made available in order to facilitate personalisation. MuSe 2023 seeks to bring together a broad audience from different research communities such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#24615;&#25991;&#31456;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;&#25991;&#29486;&#32508;&#36848;&#20171;&#32461;&#20102;&#24403;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2305.03360</link><description>&lt;p&gt;
&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#24615;&#25991;&#31456;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;&#25991;&#29486;&#32508;&#36848;&#20171;&#32461;&#20102;&#24403;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#30001;&#20110;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20013;&#22823;&#35268;&#27169;&#21382;&#21490;&#25968;&#25454;&#38598;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#23545;&#30446;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#32508;&#36848;&#31616;&#35201;&#20171;&#32461;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#20132;&#21449;&#28857;&#12290;&#25509;&#30528;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#24403;&#21069;&#25152;&#26377;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03356</link><description>&lt;p&gt;
&#20174;&#35299;&#26512;-&#25191;&#34892;&#21040;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65306;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#31572;&#26696;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#38382;&#39064;&#35299;&#26512;&#25104;&#21487;&#25191;&#34892;&#30340;&#36923;&#36753;&#24418;&#24335;&#23545;&#20110;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26377;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KoPL&#30340;&#26032;&#22411;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#26088;&#22312;&#26174;&#24335;&#22320;&#27169;&#25311;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#22797;&#26434;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#33539;&#24335;&#26469;&#24320;&#21457;&#35821;&#20041;&#35299;&#26512;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#23436;&#21892;&#21644;&#25913;&#36827;KoPL&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#26679;&#31616;&#21333;&#30340;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#35299;&#26512;&#38454;&#27573;&#65292;&#25191;&#34892;&#38454;&#27573;&#21644;&#20248;&#21270;&#38454;&#27573;&#65292;&#20197;&#22686;&#24378;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35299;&#26512;&#22120;&#20351;&#29992;KoPL&#29983;&#25104;&#36879;&#26126;&#30340;&#36923;&#36753;&#24418;&#24335;&#12290;&#28982;&#21518;&#65292;&#25191;&#34892;&#38454;&#27573;&#23545;&#40784;&#21644;&#25191;&#34892;&#36825;&#20123;&#36923;&#36753;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and execute
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22810;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#27604;&#20197;&#24448;&#30340;&#20108;&#20803;&#20998;&#31867;&#24773;&#20917;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#24182;&#19988;&#21457;&#29616;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#20250;&#22686;&#21152;&#26679;&#26412;&#37325;&#24314;&#30340;&#26131;&#21463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03350</link><description>&lt;p&gt;
&#20174;&#22810;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Training Data from Multiclass Neural Networks. (arXiv:2305.03350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22810;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#27604;&#20197;&#24448;&#30340;&#20108;&#20803;&#20998;&#31867;&#24773;&#20917;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#24182;&#19988;&#21457;&#29616;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#20250;&#22686;&#21152;&#26679;&#26412;&#37325;&#24314;&#30340;&#26131;&#21463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38598;&#20013;&#37325;&#24314;&#26679;&#26412;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;Haim&#31561;&#20154;&#26368;&#36817;&#34920;&#26126;&#65292;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20108;&#20803;&#20998;&#31867;&#22120;&#20013;&#37325;&#24314;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#39033;&#20808;&#21069;&#24037;&#20316;&#36827;&#34892;&#20102;&#22810;&#39033;&#25913;&#36827;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#25913;&#36827;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#24314;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#19988;&#37325;&#24314;&#36136;&#37327;&#29978;&#33267;&#27604;&#20108;&#20803;&#20998;&#31867;&#24773;&#20917;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#20250;&#22686;&#21152;&#26679;&#26412;&#37325;&#24314;&#30340;&#26131;&#21463;&#24615;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26368;&#22810;&#21482;&#26377;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;1000&#20010;&#26679;&#26412;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#33021;&#22815;&#20174;&#20174;100&#20010;&#31867;&#21035;&#30340;5000&#20010;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#40065;&#26834;&#30340;PSqueeze&#22810;&#32500;&#25968;&#25454;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24191;&#20041;&#20018;&#25200;&#25928;&#24212;&#21644;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;&#21644;&#40065;&#26834;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;5400&#20010;&#25925;&#38556;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.03331</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#26381;&#21153;&#31995;&#32479;&#20013;&#38024;&#23545;&#22810;&#32500;&#25968;&#25454;&#30340;&#36890;&#29992;&#19988;&#40065;&#26834;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Generic and Robust Root Cause Localization for Multi-Dimensional Data in Online Service Systems. (arXiv:2305.03331v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#40065;&#26834;&#30340;PSqueeze&#22810;&#32500;&#25968;&#25454;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24191;&#20041;&#20018;&#25200;&#25928;&#24212;&#21644;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;&#21644;&#40065;&#26834;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;5400&#20010;&#25925;&#38556;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20301;&#22810;&#32500;&#25968;&#25454;&#30340;&#26681;&#26412;&#21407;&#22240;&#23545;&#20110;&#30830;&#20445;&#22312;&#32447;&#26381;&#21153;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20986;&#29616;&#25925;&#38556;&#26102;&#65292;&#21482;&#26377;&#29305;&#23450;&#23646;&#24615;&#32452;&#21512;&#20869;&#30340;&#27979;&#37327;&#20540;&#24322;&#24120;&#12290;&#36825;&#20123;&#23646;&#24615;&#32452;&#21512;&#26159;&#19979;&#23618;&#26681;&#26412;&#21407;&#22240;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#22810;&#32500;&#25968;&#25454;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#30340;&#36890;&#29992;&#19988;&#40065;&#26834;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#26041;&#27861;PSqueeze&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#26681;&#26412;&#21407;&#22240;&#30340;&#36890;&#29992;&#29305;&#24615;&#8212;&#8212;&#24191;&#20041;&#20018;&#25200;&#25928;&#24212;(GRE)&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;&#21644;&#40065;&#26834;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#23450;&#22806;&#37096;&#26681;&#26412;&#21407;&#22240;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#39318;&#27425;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28041;&#21450;5400&#20010;&#25925;&#38556;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;PSqueeze&#30340;F1&#24471;&#20998;&#20248;&#20110;&#22522;&#32447;32.89&#65285;&#65292;&#23450;&#20301;&#26102;&#38388;&#32422;&#20026;10&#31186;&#12290;&#22806;&#37096;&#26681;&#26412;&#21407;&#22240;&#30340;F1&#24471;&#20998;&#27604;&#22522;&#32447;&#39640;39.86&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localizing root causes for multi-dimensional data is critical to ensure online service systems' reliability. When a fault occurs, only the measure values within specific attribute combinations are abnormal. Such attribute combinations are substantial clues to the underlying root causes and thus are called root causes of multidimensional data. This paper proposes a generic and robust root cause localization approach for multi-dimensional data, PSqueeze. We propose a generic property of root cause for multi-dimensional data, generalized ripple effect (GRE). Based on it, we propose a novel probabilistic cluster method and a robust heuristic search method. Moreover, we identify the importance of determining external root causes and propose an effective method for the first time in literature. Our experiments on two real-world datasets with 5400 faults show that the F1-score of PSqueeze outperforms baselines by 32.89%, while the localization time is around 10 seconds across all cases. The F
&lt;/p&gt;</description></item><item><title>Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.03308</link><description>&lt;p&gt;
Tiny-PPG: &#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03308
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#20013;&#36827;&#34892;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;PPG&#20449;&#21495;&#24456;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#24433;&#30340;&#27745;&#26579;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tiny-PPG&#8221;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#19978;&#20934;&#30830;&#23454;&#26102;&#22320;&#20998;&#21106;PPG&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;PPG DaLiA&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#25163;&#34920;&#24335;&#35774;&#22791;&#65288;Empatica E4&#65289;&#23545;15&#21517;&#21463;&#35797;&#32773;&#22312;&#21508;&#31181;&#26085;&#24120;&#27963;&#21160;&#20013;&#30340;PPG&#20449;&#21495;&#65292;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#21644;&#24418;&#24577;&#30340;&#22797;&#26434;&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#32467;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#25439;&#22833;&#20989;&#25968;&#29305;&#21035;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;&#20026;&#20102;&#20248;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36328;&#36890;&#36947;&#23398;&#20064;&#21306;&#20998;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;PPG&#20449;&#21495;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Tiny-PPG&#22312;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#38750;&#21442;&#25968;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#36991;&#20813;&#25968;&#25454;&#20132;&#25442;&#30340;&#20998;&#25955;&#25193;&#25955;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03295</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#30340;&#20998;&#25955;&#25193;&#25955;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized diffusion-based learning under non-parametric limited prior knowledge. (arXiv:2305.03295v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#38750;&#21442;&#25968;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#36991;&#20813;&#25968;&#25454;&#20132;&#25442;&#30340;&#20998;&#25955;&#25193;&#25955;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#20174;&#23616;&#37096;&#20195;&#29702;&#30340;&#27979;&#37327;&#32467;&#26524;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#29616;&#35937; m &#30340;&#25193;&#25955;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#12290;&#23545;&#20110;&#20998;&#25955;&#30340;&#32593;&#32476;&#65292;&#20165;&#22312;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#21407;&#22987;&#25968;&#25454;&#20132;&#25442;&#65292;&#20165;&#38656;&#35201;&#23545; m &#26377;&#36731;&#24494;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#30028;&#30340;&#23548;&#20986;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35828;&#26126;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of diffusion-based network learning of a nonlinear phenomenon, $m$, from local agents' measurements collected in a noisy environment. For a decentralized network and information spreading merely between directly neighboring nodes, we propose a non-parametric learning algorithm, that avoids raw data exchange and requires only mild \textit{a priori} knowledge about $m$. Non-asymptotic estimation error bounds are derived for the proposed method. Its potential applications are illustrated through simulation experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03292</link><description>&lt;p&gt;
FedNC&#65306;&#22522;&#20110;&#32593;&#32476;&#32534;&#30721;&#21551;&#21457;&#30340;&#23433;&#20840;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#21046;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#38544;&#31169;&#27844;&#28431;&#21644;&#31995;&#32479;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#26500;&#24605;&#20102;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#21407;&#21019;&#30340;&#32852;&#21512;&#23398;&#20064;&#36890;&#20449;&#26694;&#26550;FedNC&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#32593;&#32476;&#32534;&#30721;&#30340;&#21551;&#21457;&#12290; FedNC&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#21253;&#36827;&#34892;&#38543;&#26426;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#26412;&#22320;&#27169;&#22411;&#30340;&#20449;&#24687;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#28982;&#21518;&#20877;&#19978;&#20256;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#20351;FL&#31995;&#32479;&#26356;&#21152;&#23433;&#20840;&#65292;&#21534;&#21520;&#37327;&#26356;&#39640;&#65292;&#40065;&#26834;&#24615;&#26356;&#22909;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;NC&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#38543;&#30528;FL&#22312;&#23454;&#38469;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#21487;&#20197;&#22522;&#20110;FedNC&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#24212;&#29992;&#21644;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;&#20102;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#38376;&#25511;&#19982;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#19987;&#23478;&#20989;&#25968;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#19968;&#20010;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03288</link><description>&lt;p&gt;
&#35299;&#23494;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Demystifying Softmax Gating in Gaussian Mixture of Experts. (arXiv:2305.03288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;&#20102;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#38376;&#25511;&#19982;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#19987;&#23478;&#20989;&#25968;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#19968;&#20010;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;Softmax&#38376;&#25511;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#38271;&#26399;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#19977;&#20010;&#22522;&#26412;&#29702;&#35770;&#25361;&#25112;&#19982;Softmax&#38376;&#25511;&#30456;&#20851;&#65306;&#65288;i&#65289;&#21482;&#33021;&#35782;&#21035;&#21442;&#25968;&#30340;&#24179;&#31227;&#65307;&#65288;ii&#65289;Softmax&#38376;&#25511;&#21644;&#39640;&#26031;&#20998;&#24067;&#20013;&#19987;&#23478;&#20989;&#25968;&#20043;&#38388;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20869;&#22312;&#30456;&#20114;&#20316;&#29992;&#65307;&#65288;iii&#65289;Softmax&#38376;&#25511;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26465;&#20214;&#23494;&#24230;&#30340;&#20998;&#23376;&#21644;&#20998;&#27597;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#24403;&#19987;&#23478;&#25968;&#37327;&#26410;&#30693;&#19988;&#36229;&#39069;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;MLE&#30340;&#36895;&#29575;&#19982;&#19968;&#32452;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#21487;&#35299;&#24615;&#38382;&#39064;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating: (i) the identifiability only up to the translation of the parameters; (ii) the intrinsic interaction via partial differential equation between the softmax gating and the expert functions in Gaussian distribution; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Vononoi loss functions among parameters and establishing the convergence rates of the maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the number of experts is unknown and over-specified, our findings show a connection between the rate of MLE and a solvability problem of a system of polynomial equations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#30452;&#25509;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.03286</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#25511;&#21046;&#30340;&#22797;&#21512;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Composite Motion Learning with Task Control. (arXiv:2305.03286v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#30452;&#25509;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#35282;&#33394;&#30340;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#12290;&#19982;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#20223;&#20840;&#36523;&#21160;&#20316;&#30340;&#29616;&#26377;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#30452;&#25509;&#21516;&#26102;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#19981;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#29992;&#20110;&#23398;&#20064;&#30340;&#22797;&#21512;&#21442;&#32771;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25511;&#21046;&#31574;&#30053;&#33258;&#34892;&#25506;&#32034;&#22914;&#20309;&#33258;&#21160;&#22320;&#32452;&#21512;&#22797;&#21512;&#21160;&#20316;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#21644;&#22810;&#20010;&#30446;&#26631;&#23450;&#21521;&#25511;&#21046;&#30446;&#26631;&#30340;&#19981;&#21516;&#36816;&#21160;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22797;&#21512;&#21160;&#20316;&#36890;&#24120;&#26159;&#26356;&#31616;&#21333;&#34892;&#20026;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behavio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#35752;&#35770;&#20102;ViT&#22312;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;ViT&#26550;&#26500;&#30340;&#23835;&#36215;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26367;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36235;&#21183;&#19981;&#26029;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.03273</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#20998;&#21106;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Semantic Segmentation using Vision Transformers: A survey. (arXiv:2305.03273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#35752;&#35770;&#20102;ViT&#22312;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;ViT&#26550;&#26500;&#30340;&#23835;&#36215;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26367;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36235;&#21183;&#19981;&#26029;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22303;&#22320;&#35206;&#30422;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#20026;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#20102;&#26550;&#26500;&#27169;&#22411;&#12290;&#23613;&#31649;ViT&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20687;&#22270;&#20687;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#36825;&#26679;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#27492;ViT&#19981;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20027;&#24178;&#26550;&#26500;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#19968;&#20123;&#21487;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;ViT&#26550;&#26500;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;ViT&#30340;&#23835;&#36215;&#20197;&#21450;&#20854;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#65292;&#20419;&#20351;&#31038;&#21306;&#36880;&#28176;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22238;&#39038;&#21644;&#27604;&#36739;&#35774;&#35745;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation has a broad range of applications in a variety of domains including land coverage analysis, autonomous driving, and medical image analysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs) provide the architecture models for semantic segmentation. Even though ViTs have proven success in image classification, they cannot be directly applied to dense prediction tasks such as image segmentation and object detection since ViT is not a general purpose backbone due to its patch partitioning scheme. In this survey, we discuss some of the different ViT architectures that can be used for semantic segmentation and how their evolution managed the above-stated challenge. The rise of ViT and its performance with a high success rate motivated the community to slowly replace the traditional convolutional neural networks in various computer vision tasks. This survey aims to review and compare the performances of ViT architectures designed for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03263</link><description>&lt;p&gt;
&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reinforcement Learning with Limited Cognitive Load. (arXiv:2305.03263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#30340;&#29983;&#29289;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37117;&#24517;&#39035;&#22312;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19978;&#26377;&#38480;&#21046;&#19979;&#36827;&#34892;&#23398;&#20064;&#21644;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#19968;&#33324;&#30340;&#33258;&#36866;&#24212;&#34892;&#20026;&#29702;&#35770;&#24212;&#35813;&#33021;&#22815;&#35828;&#26126;&#20195;&#29702;&#30340;&#23398;&#20064;&#21382;&#21490;&#12289;&#20915;&#31574;&#21644;&#33021;&#21147;&#38480;&#21046;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#24320;&#22987;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20915;&#31574;&#21644;&#36895;&#29575;-&#22833;&#30495;&#29702;&#35770;&#30340;&#24605;&#24819;&#34701;&#21512;&#65292;&#28548;&#28165;&#22609;&#36896;&#36825;&#20123;&#21160;&#24577;&#30340;&#21407;&#21017;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20250;&#32771;&#34385;&#22788;&#29702;&#38480;&#21046;&#23545;&#23398;&#20064;&#21644;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#30340;&#32479;&#19968;&#35268;&#33539;&#26694;&#26550;&#65292;&#21363;&#23481;&#37327;&#26377;&#38480;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31687;&#26131;&#20110;&#29702;&#35299;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
All biological and artificial agents must learn and make decisions given limits on their ability to process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#21644;&#29289;&#29702;&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#20013;&#22269;&#20179;&#40736;&#21365;&#24034;&#65288;CHO&#65289;&#32454;&#32990;&#29983;&#29289;&#21453;&#24212;&#22120;&#30340;&#21160;&#24577;&#28436;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03257</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#21644;&#29289;&#29702;&#27169;&#22411;&#30340;&#20013;&#22269;&#20179;&#40736;&#21365;&#24034;&#32454;&#32990;&#29983;&#29289;&#21453;&#24212;&#22120;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven and Physics Informed Modelling of Chinese Hamster Ovary Cell Bioreactors. (arXiv:2305.03257v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#21644;&#29289;&#29702;&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#20013;&#22269;&#20179;&#40736;&#21365;&#24034;&#65288;CHO&#65289;&#32454;&#32990;&#29983;&#29289;&#21453;&#24212;&#22120;&#30340;&#21160;&#24577;&#28436;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28404;&#23450;&#22521;&#20859;&#26159;&#21033;&#29992;&#21754;&#20083;&#21160;&#29289;&#32454;&#32990;&#22521;&#20859;&#29983;&#20135;&#29983;&#29289;&#21046;&#21697;&#30340;&#19968;&#31181;&#24050;&#32463;&#24314;&#31435;&#30340;&#25805;&#20316;&#27169;&#24335;&#12290;&#37327;&#21270;&#24314;&#27169;&#25972;&#21512;&#20102;&#19968;&#20123;&#20851;&#38190;&#21453;&#24212;&#27493;&#39588;&#30340;&#21160;&#21147;&#23398;&#20197;&#21450;&#20248;&#21270;&#39537;&#21160;&#30340;&#20195;&#35874;&#36890;&#37327;&#20998;&#37197;&#65292;&#20351;&#29992;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#12290;&#36825;&#24050;&#30693;&#20250;&#23548;&#33268;&#26576;&#20123;&#25968;&#23398;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#28151;&#21512;&#27169;&#22411;&#65288;&#8220;&#28784;&#30418;&#8221;&#65289;&#26469;&#20174;&#36807;&#31243;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#22269;&#20179;&#40736;&#21365;&#24034;&#65288;CHO&#65289;&#32454;&#32990;&#29983;&#29289;&#21453;&#24212;&#22120;&#30340;&#21160;&#24577;&#28436;&#21270;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#29289;&#29702;&#23450;&#24459;&#65288;&#20363;&#22914;&#29289;&#36136;&#24179;&#34913;&#65289;&#20197;&#21450;&#20195;&#35874;&#36890;&#37327;&#30340;&#21160;&#21147;&#23398;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#30452;&#25509;&#23398;&#20064;&#28436;&#21270;&#26041;&#31243;&#65288;&#40657;&#30418;&#24314;&#27169;&#65289;&#65307;&#24674;&#22797;&#26410;&#30693;&#30340;&#29289;&#29702;&#21442;&#25968;&#65288;&#8220;&#30333;&#30418;&#8221;&#21442;&#25968;&#25311;&#21512;&#65289;&#25110;&#32773;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23398;&#20064;&#37096;&#20998;&#26410;&#30693;&#21160;&#21147;&#23398;&#34920;&#36798;&#24335;&#65288;&#28784;&#30418;&#24314;&#27169;&#65289;&#12290;&#25105;&#20204;&#23558;&#36807;&#24230;&#20915;&#23450;&#30340;&#20195;&#35874;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#30340;&#20984;&#20248;&#21270;&#27493;&#39588;&#32534;&#30721;&#20026;&#24046;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fed-batch culture is an established operation mode for the production of biologics using mammalian cell cultures. Quantitative modeling integrates both kinetics for some key reaction steps and optimization-driven metabolic flux allocation, using flux balance analysis; this is known to lead to certain mathematical inconsistencies. Here, we propose a physically-informed data-driven hybrid model (a "gray box") to learn models of the dynamical evolution of Chinese Hamster Ovary (CHO) cell bioreactors from process data. The approach incorporates physical laws (e.g. mass balances) as well as kinetic expressions for metabolic fluxes. Machine learning (ML) is then used to (a) directly learn evolution equations (black-box modelling); (b) recover unknown physical parameters ("white-box" parameter fitting) or -- importantly -- (c) learn partially unknown kinetic expressions (gray-box modelling). We encode the convex optimization step of the overdetermined metabolic biophysical system as a differe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#20998;&#37096;&#36816;&#21160;&#20808;&#39564;(PMP)&#21160;&#30011;&#21270;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#36896;&#20986;&#19981;&#21516;&#30340;&#21160;&#20316;&#38598;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#37319;&#29992;&#24191;&#27867;&#30340;&#20998;&#37096;&#20808;&#39564;&#35757;&#32451;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03249</link><description>&lt;p&gt;
PMP&#65306;&#20351;&#29992;&#20998;&#37096;&#36816;&#21160;&#20808;&#39564;&#23398;&#20064;&#19982;&#29615;&#22659;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors. (arXiv:2305.03249v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#20998;&#37096;&#36816;&#21160;&#20808;&#39564;(PMP)&#21160;&#30011;&#21270;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#36896;&#20986;&#19981;&#21516;&#30340;&#21160;&#20316;&#38598;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#37319;&#29992;&#24191;&#27867;&#30340;&#20998;&#37096;&#20808;&#39564;&#35757;&#32451;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#20998;&#37096;&#36816;&#21160;&#20808;&#39564;&#65288;PMP&#65289;&#26469;&#21160;&#30011;&#21270;&#35282;&#33394;&#30340;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20801;&#35768;&#26681;&#25454;&#21442;&#32771;&#25968;&#25454;&#21019;&#24314;&#36924;&#30495;&#30340;&#20851;&#33410;&#36816;&#21160;&#65292;&#20294;&#36816;&#21160;&#33539;&#22260;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#21487;&#29992;&#26679;&#26412;&#30340;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20805;&#28385;&#20132;&#20114;&#30340;&#22330;&#26223;&#65292;&#35797;&#22270;&#33719;&#21462;&#27599;&#31181;&#21487;&#33021;&#30340;&#20132;&#20114;&#36816;&#21160;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#20026;&#29289;&#29702;&#21442;&#25968;&#30340;&#32452;&#21512;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25152;&#25552;&#20986;&#30340;PMP&#20801;&#35768;&#25105;&#20204;&#32452;&#35013;&#22810;&#20010;&#20998;&#37096;&#25216;&#33021;&#20197;&#21160;&#30011;&#21270;&#35282;&#33394;&#65292;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#29616;&#26377;&#25968;&#25454;&#32452;&#21512;&#30340;&#22810;&#31181;&#21160;&#20316;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#31649;&#36947;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#24191;&#27867;&#30340;&#20998;&#37096;&#20808;&#39564;&#35757;&#32451;&#20195;&#29702;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#36523;&#20307;&#37096;&#20301;&#21487;&#20197;&#20174;&#36816;&#21160;&#25429;&#25417;&#20013;&#33719;&#24471;&#19968;&#31181;&#21160;&#21147;&#23398;&#35270;&#35282;&#30340;&#39118;&#26684;&#65292;&#25110;&#32773;&#21516;&#26102;&#20174;&#38468;&#21152;&#30340;&#37096;&#20998;&#29305;&#23450;&#27169;&#25311;&#20013;&#25552;&#21462;&#19982;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#20808;&#20165;&#20026;&#22120;&#26800;&#25163;&#37096;&#20998;&#35757;&#32451;&#19968;&#33324;&#20132;&#20114;&#25216;&#33021;&#65288;&#20363;&#22914;&#25235;&#25569;&#65289;&#65292;&#28982;&#21518;&#20877;&#32467;&#21512;&#20854;&#20182;&#37096;&#20998;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to animate a character incorporating multiple part-wise motion priors (PMP). While previous works allow creating realistic articulated motions from reference data, the range of motion is largely limited by the available samples. Especially for the interaction-rich scenarios, it is impractical to attempt acquiring every possible interacting motion, as the combination of physical parameters increases exponentially. The proposed PMP allows us to assemble multiple part skills to animate a character, creating a diverse set of motions with different combinations of existing data. In our pipeline, we can train an agent with a wide range of part-wise priors. Therefore, each body part can obtain a kinematic insight of the style from the motion captures, or at the same time extract dynamics-related information from the additional part-specific simulation. For example, we can first train a general interaction skill, e.g. grasping, only for the dexterous part, and then combine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#20998;&#20301;&#22238;&#24402;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#26041;&#27861;&#36827;&#34892;&#28845;&#20215;&#39044;&#27979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03224</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#20301;&#22238;&#24402;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#30899;&#20215;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Carbon Price Forecasting with Quantile Regression and Feature Selection. (arXiv:2305.03224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03224
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#20998;&#20301;&#22238;&#24402;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#26041;&#27861;&#36827;&#34892;&#28845;&#20215;&#39044;&#27979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28845;&#20215;&#20197;&#26399;&#36135;&#24418;&#24335;&#36817;&#26399;&#25104;&#20026;&#27431;&#30431;&#21644;&#20013;&#22269;&#31561;&#20132;&#26131;&#24066;&#22330;&#20013;&#26032;&#20852;&#30340;&#37329;&#34701;&#36164;&#20135;&#65292;&#20854;&#27874;&#21160;&#24615;&#21644;&#38750;&#32447;&#24615;&#20351;&#24471;&#31934;&#30830;&#39044;&#27979;&#30899;&#20215;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#20998;&#20301;&#22238;&#24402;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#26041;&#27861;&#26469;&#36827;&#34892;&#30899;&#20215;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carbon futures has recently emerged as a novel financial asset in the trading markets such as the European Union and China. Monitoring the trend of the carbon price has become critical for both national policy-making as well as industrial manufacturing planning. However, various geopolitical, social, and economic factors can impose substantial influence on the carbon price. Due to its volatility and non-linearity, predicting accurate carbon prices is generally a difficult task. In this study, we propose to improve carbon price forecasting with several novel practices. First, we collect various influencing factors, including commodity prices, export volumes such as oil and natural gas, and prosperity indices. Then we select the most significant factors and disclose their optimal grouping for explainability. Finally, we use the Sparse Quantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust price predictions. We demonstrate through extensive experimental studies that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03223</link><description>&lt;p&gt;
&#31038;&#20250;&#27491;&#20041;&#31639;&#27861;&#65306;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24179;&#26435;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Social Justice: Affirmative Action in Social Networks. (arXiv:2305.03223v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#23545;&#20110;&#19990;&#30028;&#21508;&#22320;&#25968;&#21313;&#20159;&#29992;&#25143;&#30340;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#36890;&#24120;&#24314;&#35758;&#36830;&#25509;&#30456;&#20114;&#30456;&#20284;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#34987;&#21457;&#29616;&#20250;&#20135;&#29983;&#20449;&#24687;&#23396;&#23707;&#65292;&#21152;&#21095;&#24369;&#21183;&#31361;&#20986;&#32676;&#20307;&#25152;&#36973;&#21463;&#30340;&#23396;&#31435;&#65292;&#24182;&#24310;&#32493;&#31038;&#20250;&#25104;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#23454;&#29616;&#20844;&#24179;&#30340;&#38142;&#25509;&#25512;&#33616;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#19981;&#36136;&#30097;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#30340;&#26368;&#32456;&#30446;&#26631;&#65292;&#21363;&#25968;&#25454;&#20132;&#26131;&#30340;&#22797;&#26434;&#21830;&#19994;&#27169;&#22411;&#20013;&#29992;&#25143;&#21442;&#19982;&#30340;&#36135;&#24065;&#21270;&#12290;&#26412;&#25991;&#20027;&#24352;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#29609;&#23478;&#21644;&#30446;&#30340;&#30340;&#22810;&#26679;&#21270;&#65292;&#20197;&#23454;&#29616;&#31038;&#20250;&#27491;&#20041;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#27010;&#24565;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ERA-Link&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#26032;&#22411;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#25269;&#28040;&#31995;&#32479;&#24615;&#30340;&#31038;&#20250;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link recommendation algorithms contribute to shaping human relations of billions of users worldwide in social networks. To maximize relevance, they typically propose connecting users that are similar to each other. This has been found to create information silos, exacerbating the isolation suffered by vulnerable salient groups and perpetuating societal stereotypes. To mitigate these limitations, a significant body of work has been devoted to the implementation of fair link recommendation methods. However, most approaches do not question the ultimate goal of link recommendation algorithms, namely the monetization of users' engagement in intricate business models of data trade. This paper advocates for a diversification of players and purposes of social network platforms, aligned with the pursue of social justice. To illustrate this conceptual goal, we present ERA-Link, a novel link recommendation algorithm based on spectral graph theory that counteracts the systemic societal discriminat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;&#22806;&#37096;&#39564;&#35777;&#26080;&#27861;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#30340;MLOps&#21551;&#21457;&#24335;&#33539;&#24335;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#23545;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#21644;&#26356;&#26032;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03219</link><description>&lt;p&gt;
&#25152;&#26377;&#30340;&#27169;&#22411;&#37117;&#26159;&#23616;&#37096;&#30340;: &#29992;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#21462;&#20195;&#22806;&#37096;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
All models are local: time to replace external validation with recurrent local validation. (arXiv:2305.03219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#22806;&#37096;&#39564;&#35777;&#26080;&#27861;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#30340;MLOps&#21551;&#21457;&#24335;&#33539;&#24335;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#23545;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#21644;&#26356;&#26032;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#39564;&#35777;&#32463;&#24120;&#34987;&#25512;&#33616;&#29992;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#26082;&#19981;&#33021;&#20445;&#35777;&#27867;&#21270;&#33021;&#21147;&#65292;&#20063;&#19981;&#33021;&#31561;&#20215;&#20110;&#27169;&#22411;&#30340;&#20020;&#24202;&#23454;&#29992;&#24615;&#65288;&#20219;&#20309;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#26368;&#32456;&#30446;&#26631;&#65289;&#12290;&#22806;&#37096;&#39564;&#35777;&#19982;&#24403;&#21069;&#21307;&#30103;&#20445;&#20581;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#35201;&#19981;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#24403;&#21069;&#30340;&#24066;&#22330;&#21147;&#37327;&#21644;&#26356;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#27491;&#22312;&#20419;&#36827;&#23545;&#20010;&#20307;&#37096;&#32626;&#30340;&#27169;&#22411;&#23454;&#20363;&#30340;&#39057;&#32321;&#26356;&#26032;&#21644;&#30417;&#25511;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22806;&#37096;&#39564;&#35777;&#19981;&#36275;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#12290;&#20462;&#22797;&#22806;&#37096;&#39564;&#35777;&#33539;&#24335;&#30340;&#24314;&#35758;&#19981;&#22815;&#24443;&#24213;&#12290;&#32487;&#32493;&#20381;&#36182;&#23427;&#20316;&#20026;&#26368;&#32456;&#27979;&#35797;&#24456;&#21487;&#33021;&#20250;&#20351;&#25105;&#20204;&#36208;&#19978;&#38169;&#35823;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; MLOps &#21551;&#21457;&#24335;&#33539;&#24335;&#30340;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#30417;&#27979;&#21644;&#26356;&#26032;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#36825;&#31181;&#33539;&#24335;&#23558;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
External validation is often recommended to ensure the generalizability of ML models. However, it neither guarantees generalizability nor equates to a model's clinical usefulness (the ultimate goal of any clinical decision-support tool). External validation is misaligned with current healthcare ML needs. First, patient data changes across time, geography, and facilities. These changes create significant volatility in the performance of a single fixed model (especially for deep learning models, which dominate clinical ML). Second, newer ML techniques, current market forces, and updated regulatory frameworks are enabling frequent updating and monitoring of individual deployed model instances. We submit that external validation is insufficient to establish ML models' safety or utility. Proposals to fix the external validation paradigm do not go far enough. Continued reliance on it as the ultimate test is likely to lead us astray. We propose the MLOps-inspired paradigm of recurring local v
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.03210</link><description>&lt;p&gt;
AttentionViz&#65306;Transformer Attention&#30340;&#20840;&#23616;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#27491;&#22312;&#38761;&#26032;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#37096;&#36816;&#20316;&#20173;&#28982;&#31070;&#31192;&#33707;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24207;&#21015;&#20013;&#20803;&#32032;&#20043;&#38388;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21487;&#35270;&#21270;Transformer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21644;&#38190;&#21521;&#37327;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#19982;&#20197;&#21069;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32852;&#21512;&#26597;&#35810;-&#38190;&#23884;&#20837;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;AttentionViz&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#20960;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#25552;&#20379;&#26377;&#20851;&#26597;&#35810;-&#38190;&#20132;&#20114;&#30340;&#26032;&#35265;&#35299;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#65292;&#26368;&#32456;&#36890;&#36807;&#24212;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;94&#65285;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03201</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#26222;&#20160;&#22270;&#35821;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis. (arXiv:2305.03201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03201
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#65292;&#26368;&#32456;&#36890;&#36807;&#24212;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;94&#65285;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24320;&#21457;&#33258;&#21160;&#21270;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20197;&#25903;&#25345;&#22269;&#20869;&#22806;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#26469;&#22788;&#29702;&#26412;&#22320;&#35821;&#35328;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26723;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#32479;&#35745;&#21644;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;DistilBERT-base-multilingual-cased&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;K&#26368;&#36817;&#37051;&#12289;&#20915;&#31574;&#26641;&#12289;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#26469;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#21363;&#35789;&#34955;&#21644;&#35789;&#39057;&#36870;&#21521;&#25991;&#26723;&#39057;&#29575;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#24179;&#22343;&#20540;&#36798;&#21040;&#20102;94&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification has become a crucial task in various fields, leading to a significant amount of research on developing automated text classification systems for national and international languages. However, there is a growing need for automated text classification systems that can handle local languages. This study aims to establish an automated classification system for Pashto text. To achieve this goal, we constructed a dataset of Pashto documents and applied various models, including statistical and neural machine learning models such as DistilBERT-base-multilingual-cased, Multilayer Perceptron, Support Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression, to identify the most effective approach. We also evaluated two different feature extraction methods, bag of words and Term Frequency Inverse Document Frequency. The study achieved an average testing accuracy rate of 94% using the MLP class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#31070;&#32463;&#27169;&#20223;&#27169;&#24335;&#30340;&#36890;&#29992;&#20223;&#30495;&#38382;&#39064;&#65292;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#27493;&#39588;&#30340;&#25972;&#25968;&#35268;&#21010;&#32452;&#21512;&#22797;&#26434;&#24615;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#32593;&#32476;&#31639;&#27861;&#26469;&#23398;&#20064;&#36712;&#36857;&#21644;&#23545;&#20449;&#36947;&#20013;&#26029;&#20855;&#26377;&#20248;&#21183;&#65307;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26144;&#23556;&#30340;&#36801;&#31227;&#23398;&#20064;&#27861;&#26469;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#20854;&#20182;&#20223;&#30495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03196</link><description>&lt;p&gt;
&#27169;&#20223;&#31070;&#32463;&#31995;&#32479;&#30340;&#20223;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Emulation Learning for Neuromimetic Systems. (arXiv:2305.03196v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#31070;&#32463;&#27169;&#20223;&#27169;&#24335;&#30340;&#36890;&#29992;&#20223;&#30495;&#38382;&#39064;&#65292;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#27493;&#39588;&#30340;&#25972;&#25968;&#35268;&#21010;&#32452;&#21512;&#22797;&#26434;&#24615;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#32593;&#32476;&#31639;&#27861;&#26469;&#23398;&#20064;&#36712;&#36857;&#21644;&#23545;&#20449;&#36947;&#20013;&#26029;&#20855;&#26377;&#20248;&#21183;&#65307;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26144;&#23556;&#30340;&#36801;&#31227;&#23398;&#20064;&#27861;&#26469;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#20854;&#20182;&#20223;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25105;&#20204;&#26368;&#36817;&#22312;&#31070;&#32463;&#21551;&#21457;&#24335;&#37327;&#21270;&#31995;&#32479;&#19978;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23398;&#20064;&#37327;&#21270;&#36816;&#21160;&#21644;&#23545;&#20449;&#36947;&#20013;&#26029;&#30340;&#24377;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#31070;&#32463;&#27169;&#20223;&#27169;&#24335;&#30340;&#36890;&#29992;&#20223;&#30495;&#38382;&#39064;&#12290;&#36825;&#20010;&#26368;&#20248;&#37327;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26469;&#35299;&#20915;&#65292;&#20294;&#30001;&#20110;&#20248;&#21270;&#27493;&#39588;&#28041;&#21450;&#25972;&#25968;&#35268;&#21010;&#65292;&#24403;&#36755;&#20837;&#36890;&#36947;&#25968;&#37327;&#21464;&#22823;&#26102;&#65292;&#26041;&#27861;&#20250;&#21463;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#21363;&#20351;&#25105;&#20204;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#28857;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#26412;&#36523;&#20173;&#28982;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#31639;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#36712;&#36857;&#65292;&#36824;&#21487;&#20197;&#23637;&#31034;&#23545;&#20449;&#36947;&#20013;&#26029;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#20854;&#20182;&#20223;&#30495;&#38382;&#39064;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#24403;&#21069;&#27169;&#22411;&#19978;&#20351;&#29992;&#22522;&#20110;&#26144;&#23556;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#21462;&#26368;&#20248;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on our recent research on neural heuristic quantization systems, results on learning quantized motions and resilience to channel dropouts are reported. We propose a general emulation problem consistent with the neuromimetic paradigm. This optimal quantization problem can be solved by model predictive control (MPC), but because the optimization step involves integer programming, the approach suffers from combinatorial complexity when the number of input channels becomes large. Even if we collect data points to train a neural network simultaneously, collection of training data and the training itself are still time-consuming. Therefore, we propose a general Deep Q Network (DQN) algorithm that can not only learn the trajectory but also exhibit the advantages of resilience to channel dropout. Furthermore, to transfer the model to other emulation problems, a mapping-based transfer learning approach can be used directly on the current model to obtain the optimal direction for the ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#26029;&#26412;&#26500;&#20851;&#31995;&#26063;&#32676;&#12290;</title><link>http://arxiv.org/abs/2305.03184</link><description>&lt;p&gt;
&#19968;&#31181;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#25512;&#26029;&#22312;&#25968;&#25454;&#31232;&#23569;&#24773;&#20917;&#19979;&#30340;&#29983;&#29289;&#21147;&#23398;&#26412;&#26500;&#23450;&#24459;&#26063;&#32676;
&lt;/p&gt;
&lt;p&gt;
A Generative Modeling Framework for Inferring Families of Biomechanical Constitutive Laws in Data-Sparse Regimes. (arXiv:2305.03184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#26029;&#26412;&#26500;&#20851;&#31995;&#26063;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#20154;&#20307;&#34880;&#31649;&#32452;&#32455;&#30340;&#29983;&#29289;&#21147;&#23398;&#24615;&#36136;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#24515;&#34880;&#31649;&#30142;&#30149;&#12290;&#24120;&#35268;&#30340;&#26412;&#26500;&#24314;&#27169;&#38750;&#32447;&#24615;&#22238;&#24402;&#35201;&#27714;&#20805;&#36275;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#19968;&#20010;&#26412;&#26500;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#25512;&#26029;&#26412;&#26500;&#20851;&#31995;&#26063;&#32676;&#12290;&#21463;&#20989;&#25968;&#20808;&#39564;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21028;&#21035;&#22120;&#12290;&#29983;&#25104;&#22120;&#23558;&#20197;&#27979;&#37327;&#25968;&#25454;&#20026;&#26465;&#20214;&#30340;&#22122;&#22768;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#39044;&#27979;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#25509;&#19979;&#26469;&#21028;&#21035;&#22120;&#20250;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#26412;&#26500;&#20851;&#31995;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying biomechanical properties of the human vasculature could deepen our understanding of cardiovascular diseases. Standard nonlinear regression in constitutive modeling requires considerable high-quality data and an explicit form of the constitutive model as prior knowledge. By contrast, we propose a novel approach that combines generative deep learning with Bayesian inference to efficiently infer families of constitutive relationships in data-sparse regimes. Inspired by the concept of functional priors, we develop a generative adversarial network (GAN) that incorporates a neural operator as the generator and a fully-connected neural network as the discriminator. The generator takes a vector of noise conditioned on measurement data as input and yields the predicted constitutive relationship, which is scrutinized by the discriminator in the following step. We demonstrate that this framework can accurately estimate means and standard deviations of the constitutive relationships of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20027;&#35266;&#38388;&#30456;&#20851;&#24615;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(MViTime)&#65292;&#29992;&#20110;&#30561;&#30496;&#20998;&#26399;&#20013;&#30340;&#36328;&#20010;&#20307;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03178</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#35266;&#38388;&#30456;&#20851;&#24615;&#30340;&#23545;&#27604;&#23398;&#20064;&#22312;&#30561;&#30496;&#20998;&#26399;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Sleep Staging based on Inter Subject Correlation. (arXiv:2305.03178v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20027;&#35266;&#38388;&#30456;&#20851;&#24615;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(MViTime)&#65292;&#29992;&#20110;&#30561;&#30496;&#20998;&#26399;&#20013;&#30340;&#36328;&#20010;&#20307;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#30561;&#30496;&#20998;&#26399;&#30340;&#36328;&#20010;&#20307;&#38382;&#39064;&#19978;&#20851;&#27880;&#36739;&#23569;&#12290;&#21516;&#26102;&#65292;&#26032;&#20852;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#20027;&#35266;&#38388;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;&#21487;&#20197;&#20026;&#36328;&#20027;&#20307;&#20998;&#26512;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MViTime&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20102;&#36328;&#20027;&#20307;&#30456;&#20851;&#24615;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#30561;&#30496;&#20998;&#26399;&#20013;&#36328;&#20010;&#20307;&#38382;&#39064;&#30340;&#21487;&#34892;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#21644;&#32467;&#35770;&#34920;&#26126;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#22312;&#30561;&#30496;&#20998;&#26399;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21078;&#26512;&#23454;&#39564;&#30340;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#36328;&#20027;&#20307;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multitudes of researches have applied deep learning to automatic sleep stage classification. Whereas actually, these works have paid less attention to the issue of cross-subject in sleep staging. At the same time, emerging neuroscience theories on inter-subject correlations can provide new insights for cross-subject analysis. This paper presents the MViTime model that have been used in sleep staging study. And we implement the inter-subject correlation theory through contrastive learning, providing a feasible solution to address the cross-subject problem in sleep stage classification. Finally, experimental results and conclusions are presented, demonstrating that the developed method has achieved state-of-the-art performance on sleep staging. The results of the ablation experiment also demonstrate the effectiveness of the cross-subject approach based on contrastive learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#37325;&#24314;&#36229;&#26448;&#26009;&#30446;&#26631;&#20132;&#20114;&#31995;&#32479;&#20013;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#33021;&#21516;&#26102;&#24863;&#30693;&#30446;&#26631;&#30340;&#25968;&#37327;&#21644;&#20171;&#30005;&#24120;&#25968;&#65292;&#29983;&#25104;&#39640;&#31934;&#24230;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.03177</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#21516;&#26102;&#30446;&#26631;&#24863;&#30693;&#21644;&#36229;&#20998;&#36776;&#29575;&#25104;&#20687;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Assisted Simultaneous Targets Sensing and Super-Resolution Imaging. (arXiv:2305.03177v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#37325;&#24314;&#36229;&#26448;&#26009;&#30446;&#26631;&#20132;&#20114;&#31995;&#32479;&#20013;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#33021;&#21516;&#26102;&#24863;&#30693;&#30446;&#26631;&#30340;&#25968;&#37327;&#21644;&#20171;&#30005;&#24120;&#25968;&#65292;&#29983;&#25104;&#39640;&#31934;&#24230;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20122;&#27874;&#38271;&#30005;&#30913;&#27874;&#35843;&#25511;&#33021;&#21147;&#30340;&#24341;&#20837;&#65292;&#36229;&#26448;&#26009;&#22312;&#24863;&#30693;&#21644;&#36229;&#20998;&#36776;&#29575;&#25104;&#20687;&#39046;&#22495;&#26377;&#20102;&#38761;&#21629;&#24615;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36229;&#26448;&#26009;&#30340;&#28155;&#21152;&#20351;&#24471;&#20174;&#26816;&#27979;&#22330;&#20013;&#33719;&#21462;&#30446;&#26631;&#20449;&#24687;&#30340;&#22797;&#26434;&#24230;&#21464;&#24471;&#38750;&#24120;&#39640;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#19968;&#31995;&#21015;&#30005;&#30913;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24179;&#21488;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#21333;&#19968;&#21151;&#33021;&#65292;&#24182;&#38480;&#21046;&#20102;&#30740;&#31350;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#37325;&#24314;&#36229;&#26448;&#26009;&#30446;&#26631;&#20132;&#20114;&#31995;&#32479;&#20013;&#30340;&#30446;&#26631;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#22312;&#21021;&#27493;&#39564;&#35777;&#23454;&#39564;&#20013;&#30830;&#35748;&#20102;&#20132;&#20114;&#22330;&#26223;&#21487;&#20197;&#23481;&#24525;&#31995;&#32479;&#22122;&#22768;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#30005;&#22330;&#20998;&#24067;&#36755;&#20837;&#65292;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#20165;&#21487;&#20197;&#24863;&#30693;&#30446;&#26631;&#30340;&#25968;&#37327;&#21644;&#20171;&#30005;&#24120;&#25968;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29983;&#25104;&#39640;&#31934;&#24230;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, metasurfaces have experienced revolutionary growth in the sensing and superresolution imaging field, due to their enabling of subwavelength manipulation of electromagnetic waves. However, the addition of metasurfaces multiplies the complexity of retrieving target information from the detected fields. Besides, although the deep learning method affords a compelling platform for a series of electromagnetic problems, many studies mainly concentrate on resolving one single function and limit the research's versatility. In this study, a multifunctional deep neural network is demonstrated to reconstruct target information in a metasurface targets interactive system. Firstly, the interactive scenario is confirmed to tolerate the system noises in a primary verification experiment. Then, fed with the electric field distributions, the multitask deep neural network can not only sense the quantity and permittivity of targets but also generate superresolution images with high precision. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#24182;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#35782;&#21035;&#26368;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.03173</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New Adversarial Image Detection Based on Sentiment Analysis. (arXiv:2305.03173v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#24182;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#35782;&#21035;&#26368;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#32780;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65288;&#20363;&#22914;DeepFool&#65289;&#27491;&#22312;&#23835;&#36215;&#24182;&#36229;&#36234;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#35782;&#21035;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36825;&#26159;&#36890;&#36807;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#26368;&#23569;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#27169;&#22359;&#21270;&#23884;&#20837;&#23618;&#65292;&#23558;&#38544;&#34255;&#23618;&#29305;&#24449;&#26144;&#23556;&#21040;&#35789;&#21521;&#37327;&#20013;&#65292;&#32452;&#25104;&#21487;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#21477;&#23376;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#22411;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#38024;&#23545;CIFAR-10&#12289;SVHN&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;ResNet&#21644;Inception&#20013;&#24615;&#32593;&#32476;&#21457;&#36215;&#30340;&#26368;&#26032;&#25915;&#20987;&#26041;&#38754;&#22343;&#33021;&#19968;&#33268;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This paper presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-1
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;CSI&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;80MHz Wi-fi&#20449;&#36947;&#19978;&#19982;&#29615;&#22659;&#12289;&#20154;&#21592;&#21644;Wi-Fi&#30828;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36825;&#20026;&#24320;&#21457;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.03170</link><description>&lt;p&gt;
80 MHz Wi-Fi&#20449;&#36947;&#19978;&#30340;&#26080;&#32447;&#20154;&#20307;&#24863;&#24212;CSI&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A CSI Dataset for Wireless Human Sensing on 80 MHz Wi-Fi Channels. (arXiv:2305.03170v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03170
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;CSI&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;80MHz Wi-fi&#20449;&#36947;&#19978;&#19982;&#29615;&#22659;&#12289;&#20154;&#21592;&#21644;Wi-Fi&#30828;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36825;&#20026;&#24320;&#21457;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25216;&#26415;&#26469;&#30417;&#27979;&#20174;Wi-Fi&#20449;&#36947;&#35835;&#25968;&#30340;&#20154;&#31867;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#31283;&#20581;&#22320;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#24037;&#20316;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#22823;&#30340;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#29615;&#22659;&#12289;&#20154;&#21592;&#21644;Wi-Fi&#30828;&#20214;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23569;&#25968;&#20844;&#20849;&#25968;&#25454;&#38598;&#22823;&#22810;&#24050;&#32463;&#36807;&#26102; - &#22240;&#20026;&#26159;&#36890;&#36807;20&#25110;40 MHz&#39057;&#27573;&#36816;&#20316;&#30340;Wi-Fi&#35774;&#22791;&#24471;&#21040;&#30340; - &#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#20256;&#24863;&#31639;&#27861;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;IEEE 802.11ac&#20449;&#36947;&#27979;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;80 MHz&#24102;&#23485;&#20449;&#36947;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#28041;&#21450;&#19981;&#21516;&#29615;&#22659;&#12289;&#26085;&#23376;&#21644;&#19981;&#21516;&#30828;&#20214;&#30340;&#21313;&#19977;&#20010;&#20027;&#39064;&#30340;&#27979;&#37327;&#27963;&#21160;&#25552;&#20379;&#26032;&#30340;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last years, several machine learning-based techniques have been proposed to monitor human movements from Wi-Fi channel readings. However, the development of domain-adaptive algorithms that robustly work across different environments is still an open problem, whose solution requires large datasets characterized by strong domain diversity, in terms of environments, persons and Wi-Fi hardware. To date, the few public datasets available are mostly obsolete - as obtained via Wi-Fi devices operating on 20 or 40 MHz bands - and contain little or no domain diversity, thus dramatically limiting the advancements in the design of sensing algorithms. The present contribution aims to fill this gap by providing a dataset of IEEE 802.11ac channel measurements over an 80 MHz bandwidth channel featuring notable domain diversity, through measurement campaigns that involved thirteen subjects across different environments, days, and with different hardware. Novel experimental data is provided by bl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03169</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#12289;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20998;&#20139;&#25968;&#25454;&#24182;&#21512;&#20316;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12289;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#25512;&#36827;&#30740;&#31350;&#12290;1996&#24180;&#12298;&#20581;&#24247;&#20445;&#38505;&#27969;&#36890;&#19982;&#36131;&#20219;&#27861;&#26696;&#12299;(HIPAA)&#26159;&#19968;&#39033;&#32852;&#37030;&#27861;&#24459;&#65292;&#26088;&#22312;&#36890;&#36807;&#21046;&#23450;&#20445;&#25252;&#20581;&#24247;&#20449;&#24687;&#30340;&#35268;&#23450;&#26469;&#20445;&#25252;&#25935;&#24863;&#20581;&#24247;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20043;&#21069;&#65292;HIPAA&#27809;&#26377;&#25552;&#20379;&#26377;&#25928;&#30340;&#26816;&#27979;&#25110;&#21024;&#38500;PHI&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20174;&#32780;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there is an increasing need for healthcare providers, communities, and researchers to share data and collaborate to improve health outcomes, generate valuable insights, and advance research. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law designed to protect sensitive health information by defining regulations for protected health information (PHI). However, it does not provide efficient tools for detecting or removing PHI before data sharing. One of the challenges in this area of research is the heterogeneous nature of PHI fields in data across different parties. This variability makes rule-based sensitive variable identification systems that work on one database fail on another. To address this issue, our paper explores the use of machine learning algorithms to identify sensitive variables in structured data, thus facilitating the de-identification process. We made a key observation that the distributions of metadata of
&lt;/p&gt;</description></item><item><title>G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03153</link><description>&lt;p&gt;
G-MATT: &#20998;&#23376;&#35821;&#27861;&#26641;&#21464;&#25442;&#22120;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03153
&lt;/p&gt;
&lt;p&gt;
G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25253;&#36947;&#20102;&#20960;&#31181;&#22522;&#20110;&#21453;&#24212;&#27169;&#26495;&#21644;&#22522;&#20110;&#33258;&#30001;&#27169;&#26495;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#35768;&#22810;&#22312;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#25903;&#37197;&#21453;&#21521;&#21512;&#25104;&#30340;&#24213;&#23618;&#21270;&#23398;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#33073;&#33410;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#23558;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#30340;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#32431;SMILES&#34920;&#31034;&#27861;&#30340;&#27169;&#22411;&#24573;&#30053;&#30340;&#24213;&#23618;&#21270;&#23398;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#35821;&#27861;&#30340;&#20998;&#23376;&#27880;&#24847;&#21147;&#26641;&#21464;&#25442;&#22120;&#65288;G-MATT&#65289;&#65292;&#19982;&#22522;&#32447;&#22238;&#28335;&#21512;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290; G-MATT&#30340;&#20934;&#30830;&#29575;&#25490;&#21517;&#21069;1&#20026;51&#65285;&#65288;&#21069;10&#20026;79.1&#65285;&#65289;&#65292;&#26080;&#25928;&#29575;&#20026;1.5&#65285;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, several reaction templates-based and template-free approaches have been reported for single-step retrosynthesis prediction. Even though many of these approaches perform well from traditional data-driven metrics standpoint, there is a disconnect between model architectures used and underlying chemistry principles governing retrosynthesis. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with chemistry knowledge. We report a tree-to-sequence transformer architecture based on hierarchical SMILES grammar trees as input containing underlying chemistry information that is otherwise ignored by models based on purely SMILES-based representations. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;GNN&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#25512;&#26029;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#23384;&#31574;&#30053;&#65292;&#22522;&#20110;&#39030;&#28857;&#30340;&#22810;&#36339;&#37051;&#22495;&#25277;&#26679;&#20013;&#30340;&#28857;&#21253;&#21547;&#27010;&#29575;&#20998;&#26512;&#65288;VIP&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#37327;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03152</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#27010;&#29575;&#37051;&#22495;&#25193;&#23637;&#20998;&#26512;&#19982;&#32531;&#23384;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching. (arXiv:2305.03152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;GNN&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#25512;&#26029;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#23384;&#31574;&#30053;&#65292;&#22522;&#20110;&#39030;&#28857;&#30340;&#22810;&#36339;&#37051;&#22495;&#25277;&#26679;&#20013;&#30340;&#28857;&#21253;&#21547;&#27010;&#29575;&#20998;&#26512;&#65288;VIP&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#37327;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#35806;&#29983;&#20197;&#26469;&#65292;&#22914;&#20309;&#22312;&#35268;&#27169;&#24222;&#22823;&#30340;&#22270;&#19978;&#35757;&#32451;&#21644;&#25512;&#26029;&#19968;&#30452;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;GNN&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#37329;&#34701;&#21462;&#35777;&#31561;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#19988;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#20351;&#29992;&#33410;&#28857;&#25277;&#26679;&#30340;GNN&#36827;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#20854;&#20013;&#36328;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#39030;&#28857;&#29305;&#24449;&#30340;&#24517;&#35201;&#20998;&#21306;&#20250;&#23548;&#33268;&#29305;&#24449;&#36890;&#20449;&#25104;&#20026;&#21046;&#32422;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#37327;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#32531;&#23384;&#36828;&#31243;&#20998;&#21306;&#20013;&#19982;&#35775;&#38382;&#39057;&#29575;&#39640;&#30340;&#39030;&#28857;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22522;&#20110;&#39030;&#28857;&#30340;&#22810;&#36339;&#37051;&#22495;&#25277;&#26679;&#20013;&#30340;&#28857;&#21253;&#21547;&#27010;&#29575;&#65288;VIP&#65289;&#20998;&#26512;&#65292;&#36825;&#21487;&#33021;&#20250;&#23558;&#37051;&#22495;&#25193;&#23637;&#21040;&#22270;&#30340;&#20998;&#21306;&#36793;&#30028;&#20043;&#22806;&#12290;VIP&#20998;&#26512;&#19981;&#20165;&#33021;&#22815;&#28040;&#38500;&#31163;&#32447;&#35745;&#31639;&#65292;&#32780;&#19988;&#33021;&#22815;&#23558;&#36890;&#20449;&#24320;&#38144;&#38477;&#21040;&#36817;&#20284;&#32447;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24230;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and inference with graph neural networks (GNNs) on massive graphs has been actively studied since the inception of GNNs, owing to the widespread use and success of GNNs in applications such as recommendation systems and financial forensics. This paper is concerned with minibatch training and inference with GNNs that employ node-wise sampling in distributed settings, where the necessary partitioning of vertex features across distributed storage causes feature communication to become a major bottleneck that hampers scalability. To significantly reduce the communication volume without compromising prediction accuracy, we propose a policy for caching data associated with frequently accessed vertices in remote partitions. The proposed policy is based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which may expand the neighborhood far beyond the partition boundaries of the graph. VIP analysis not only enables the elimination of th
&lt;/p&gt;</description></item><item><title>CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.03148</link><description>&lt;p&gt;
CAMEL&#65306;&#38754;&#21521;&#39640;&#25928;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;AI&#27169;&#22411;&#21644;&#23884;&#20837;&#24335;DRAM&#30340;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03148
&lt;/p&gt;
&lt;p&gt;
CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#20852;&#36215;&#23548;&#33268;&#36793;&#32536;&#35774;&#22791;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#35774;&#22791;&#31471;&#23398;&#20064;&#20351;&#36793;&#32536;&#24179;&#21488;&#33021;&#22815;&#19981;&#26029;&#22320;&#26681;&#25454;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#35843;&#25972;AI&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;AI&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20250;&#24102;&#26469;&#23494;&#38598;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#21344;&#29992;&#22823;&#37327;&#33455;&#29255;&#20869;&#23384;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23884;&#20837;&#24335;&#21160;&#24577;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#65288;eDRAM&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#12290;&#19982;&#38745;&#24577;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;SRAM&#65289;&#30456;&#27604;&#65292;eDRAM&#22312;&#23384;&#20648;&#23494;&#24230;&#19978;&#24341;&#20837;&#20102;&#36229;&#36807;2&#20493;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#23384;&#20648;&#30340;&#25968;&#25454;&#23436;&#25972;&#65292;eDRAM&#38656;&#35201;&#25191;&#34892;&#32791;&#30005;&#30340;&#25968;&#25454;&#21047;&#26032;&#25805;&#20316;&#12290;&#22914;&#26524;&#25968;&#25454;&#23384;&#20648;&#19968;&#27573;&#26102;&#38388;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;eDRAM&#21047;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the Internet of Things (IoT) has resulted in a remarkable amount of data generated on edge devices, which are often processed using AI algorithms. On-device learning enables edge platforms to continually adapt the AI models to user personal data and further allows for a better service quality. However, AI training on resource-limited devices is extremely difficult because of the intensive computing workload and the significant amount of on-chip memory consumption exacted by deep neural networks (DNNs). To mitigate this, we propose to use embedded dynamic random-access memory (eDRAM) as the main storage medium of training data. Compared with static random-access memory (SRAM), eDRAM introduces more than $2\times$ improvement on storage density, enabling reduced off-chip memory traffic. However, to keep the stored data intact, eDRAM is required to perform the power-hungry data refresh operations.  eDRAM refresh can be eliminated if the data is stored for a period of time
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03144</link><description>&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#23545;NLP&#32858;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03144
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#20986;&#29616;&#65292;&#35780;&#35770;&#23545;&#20110;&#39038;&#23458;&#35780;&#20272;&#20135;&#21697;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#26143;&#32423;&#35780;&#20998;&#24182;&#19981;&#24635;&#26159;&#19982;&#39038;&#23458;&#32534;&#20889;&#30340;&#35780;&#35770;&#25991;&#26412;&#30456;&#21305;&#37197;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36873;&#25321;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#36825;&#20123;&#35780;&#35770;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#31350;&#20102;&#23884;&#20837;&#36873;&#25321;&#23545;&#21508;&#31181;&#31867;&#22411;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#21644;&#38750;&#19978;&#19979;&#25991;&#65288;Word2Vec&#65289;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#25991;&#26412;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#23545;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;&#22522;&#20110;&#20998;&#21306;&#30340;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#21644;&#23494;&#24230;&#22522;&#30784;&#30340;DBSCAN&#21644;HDBSCAN&#65289;&#22312;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of e-commerce platforms, reviews are crucial for customers to assess the credibility of a product. The star ratings do not always match the review text written by the customer. For example, a three star rating (out of five) may be incongruous with the review text, which may be more suitable for a five star review. A clustering approach can be used to relabel the correct star ratings by grouping the text reviews into individual groups. In this work, we explore the task of choosing different text embeddings to represent these reviews and also explore the impact the embedding choice has on the performance of various classes of clustering algorithms. We use contextual (BERT) and non-contextual (Word2Vec) text embeddings to represent the text and measure their impact of three classes on clustering algorithms - partitioning based (KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN and HDBSCAN), each with various experimental settings. We use the sil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;ISPE&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#12290;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03143</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#36870;&#30340;&#35821;&#20041;&#20445;&#25345;&#30340;&#36923;&#36753;&#20844;&#24335;&#23884;&#20837;&#65306;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;ISPE&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#12290;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#26159;&#33258;&#21160;&#25512;&#29702;&#30340;&#20027;&#35201;&#24418;&#24335;&#35821;&#35328;&#65292;&#21516;&#26102;&#20063;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#12290;&#23398;&#20064;&#21644;&#20248;&#21270;&#36923;&#36753;&#35268;&#21017;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26159;&#22522;&#20110;&#36830;&#32493;&#31354;&#38388;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#65292;&#32780;&#23398;&#20064;&#36923;&#36753;&#21017;&#22788;&#20110;&#31163;&#25955;&#30340;&#35821;&#27861;&#31354;&#38388;&#20013;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#25552;&#20986;&#19968;&#31181;&#33021;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#34429;&#28982;&#33021;&#20445;&#25345;&#35821;&#20041;&#65292;&#20294;&#26159;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;&#21487;&#36870;&#35821;&#20041;&#20445;&#25345;&#23884;&#20837;&#65288;ISPE&#65289;&#65292;&#21033;&#29992;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#32467;&#26500;&#23454;&#29616;&#20102;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03136</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#30340;&#24191;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03136
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#20989;&#25968;&#23558;&#29983;&#29289;&#24207;&#21015;&#30340;&#22823;&#32452;&#21512;&#31354;&#38388;&#26144;&#23556;&#21040;&#25152;&#20851;&#27880;&#30340;&#29305;&#24615;&#19978;&#12290;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22810;&#27169;&#24577;&#20989;&#25968;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#25928;&#19988;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#31232;&#30095;&#30340;&#28508;&#22312;&#20989;&#25968;&#36890;&#36807;&#21333;&#35843;&#38750;&#32447;&#24615;&#21464;&#25442;&#20197;&#21457;&#23556;&#21487;&#27979;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914; Bradley-Terry &#25439;&#22833;&#65289;&#26159;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#25152;&#38544;&#31034;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#30340;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#36866;&#24212;&#24615;-&#19978;&#20301;&#32852;&#31995;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20105;&#36777;&#65292;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#21487;&#20197;&#20135;&#29983;&#19981;&#20855;&#22791;&#31232;&#30095;&#34920;&#31034;&#30340;&#35266;&#23519;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#36866;&#21512;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#65289;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#25439;&#22833;&#21487;&#29992;&#20110;&#25512;&#26029;&#19981;&#36866;&#21512; MSE &#25439;&#22833;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03100</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#21516;&#21151;&#33021;&#65306;&#32479;&#19968;&#21338;&#24328;&#35770;&#20132;&#20114;&#26041;&#27861;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#36825;&#20123;&#39640;&#24615;&#33021;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#12290;&#35299;&#37322;&#27492;&#31867;&#27169;&#22411;&#23558;&#25552;&#39640;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#65292;&#24182;&#19988;&#23545;&#20110;&#29702;&#35299;&#20854;&#20182;&#23454;&#38469;&#38656;&#27714;&#65288;&#22914;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#65289;&#26159;&#24517;&#35201;&#30340;&#12290;&#22686;&#24378;&#27169;&#22411;&#36879;&#26126;&#24230;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#65288;&#31216;&#20026;&#24402;&#22240;&#65289;&#20197;&#21450;&#32676;&#32452;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#36825;&#20123;&#26041;&#27861;&#23548;&#20837;&#21338;&#24328;&#35770;&#30340;&#27010;&#24565;&#21644;&#32467;&#26524;&#26469;&#20135;&#29983;&#24402;&#22240;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21338;&#24328;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#65292;&#20551;&#35774;&#36866;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#29305;&#24449;&#20043;&#38388;&#20132;&#20114;&#30340;&#21807;&#19968;&#20840;&#38754;&#35828;&#26126;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;Bootstrap&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#21161;&#27861;&#12289;&#37325;&#25277;&#26679;&#21644;&#32447;&#24615;&#22238;&#24402;&#26469;&#26356;&#26032;&#38544;&#34255;&#23618;&#30340;&#21152;&#26435;&#36830;&#25509;&#65292;&#20174;&#32780;&#36798;&#21040;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03099</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;Bootstrap&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bootstrap Algorithm for Fast Supervised Learning. (arXiv:2305.03099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;Bootstrap&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#21161;&#27861;&#12289;&#37325;&#25277;&#26679;&#21644;&#32447;&#24615;&#22238;&#24402;&#26469;&#26356;&#26032;&#38544;&#34255;&#23618;&#30340;&#21152;&#26435;&#36830;&#25509;&#65292;&#20174;&#32780;&#36798;&#21040;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36890;&#24120;&#20381;&#36182;&#26576;&#31181;&#31867;&#22411;&#30340;&#26354;&#32447;&#36319;&#38543;&#26041;&#27861;&#65292;&#20363;&#22914;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#65288;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65289;&#65292;ADADELTA&#65292;ADAM&#25110;&#26377;&#38480;&#20869;&#23384;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#36890;&#24120;&#20381;&#36182;&#20110;&#35775;&#38382;&#22823;&#37327;&#30340;&#35266;&#27979;&#20540;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#26576;&#20123;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;epoch&#30340;&#25968;&#25454;&#28857;&#25165;&#33021;&#36827;&#34892;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#27973;&#23618;&#30340;&#32593;&#32476;&#32780;&#35328;&#12290;&#23427;&#19981;&#26159;&#26354;&#32447;&#36319;&#38543;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#8220;&#20998;&#31163;&#8221;&#38544;&#34255;&#23618;&#24182;&#36890;&#36807;&#33258;&#21161;&#27861;&#12289;&#37325;&#25277;&#26679;&#21644;&#32447;&#24615;&#22238;&#24402;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#21152;&#26435;&#36830;&#25509;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#25277;&#26679;&#30340;&#35266;&#27979;&#20540;&#65292;&#26412;&#26041;&#27861;&#30340;&#25910;&#25947;&#34987;&#23454;&#35777;&#22320;&#26174;&#31034;&#20986;&#24555;&#36895;&#21644;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#65306;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a neural network (NN) typically relies on some type of curve-following method, such as gradient descent (GD) (and stochastic gradient descent (SGD)), ADADELTA, ADAM or limited memory algorithms. Convergence for these algorithms usually relies on having access to a large quantity of observations in order to achieve a high level of accuracy and, with certain classes of functions, these algorithms could take multiple epochs of data points to catch on. Herein, a different technique with the potential of achieving dramatically better speeds of convergence, especially for shallow networks, is explored: it does not curve-follow but rather relies on 'decoupling' hidden layers and on updating their weighted connections through bootstrapping, resampling and linear regression. By utilizing resampled observations, the convergence of this process is empirically shown to be remarkably fast and to require a lower amount of data points: in particular, our experiments show that one needs a fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03098</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#20083;&#33146;&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#25668;&#24433;&#20013;&#30340;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#32959;&#30244;&#24456;&#23569;&#20986;&#29616;&#65292;&#20083;&#25151;&#32452;&#32455;&#21464;&#24322;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#37492;&#20110;&#24322;&#24120;&#22270;&#20687;&#30340;&#31232;&#32570;&#24615;&#21644;&#27491;&#24120;&#22270;&#20687;&#30340;&#20016;&#23500;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;/&#23450;&#20301;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#36866;&#21512;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#24322;&#24120;&#23450;&#20301;&#30740;&#31350;&#38598;&#20013;&#22312;&#38750;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#23436;&#25104;&#35270;&#35282;&#19979;&#30340;&#20219;&#21153;&#24471;&#21040;&#32531;&#35299;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#23384;&#22312;&#21487;&#20197;&#36890;&#36807;&#21407;&#22987;&#22806;&#35266;&#19982;&#20854;&#29615;&#22659;&#26465;&#20214;&#19979;&#33258;&#21160;&#23436;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25351;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;DBT&#25968;&#25454;&#38598;&#20013;&#65292;&#24448;&#24448;&#26377;&#24456;&#22810;&#30456;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#30340;&#27491;&#24120;&#23436;&#25104;&#65292;&#20351;&#36825;&#20010;&#35780;&#20272;&#26631;&#20934;&#19981;&#22826;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#36827;&#34892;&#22270;&#20687;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.03097</link><description>&lt;p&gt;
&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#22330;&#26223;&#19979;&#65292;&#20998;&#24067;&#24335;&#30340;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#20165;&#20351;&#29992;&#30001;&#19981;&#21516;&#30340;&#26410;&#30693;&#30340;&#34892;&#20026;&#31574;&#30053;&#29983;&#25104;&#30340;&#23567;&#22411;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21327;&#20316;&#23398;&#20064;&#20986;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31528;&#25305;&#22320;&#23558;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22240;&#27492;&#35774;&#35745;&#20102;Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FEDORA&#20195;&#30721;&#24211;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#19978;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FEDORA&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FEDORA&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#65292;&#32593;&#32476;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#36825;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03077</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26263;&#29289;&#36136;&#26197;&#30340;&#23494;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Explaining dark matter halo density profiles with neural networks. (arXiv:2305.03077v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03077
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#65292;&#32593;&#32476;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#36825;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#12290;&#35813;&#32593;&#32476;&#25429;&#33719;&#23494;&#24230;&#20998;&#24067;&#20013;&#29420;&#31435;&#30340;&#21464;&#21270;&#22240;&#32032;&#65292;&#22312;&#20302;&#32500;&#34920;&#31034;&#20013;&#29289;&#29702;&#22320;&#35299;&#37322;&#20102;&#23427;&#20204;&#65292;&#20351;&#29992;&#20102;&#20114;&#20449;&#24687;&#12290;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#32593;&#32476;&#24674;&#22797;&#20102;&#26089;&#26399;&#32452;&#35013;&#19982;&#20869;&#37096;&#36718;&#24275;&#20043;&#38388;&#30340;&#24050;&#30693;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#35813;&#21442;&#25968;&#34920;&#31034;&#26368;&#36817;&#30340;&#36136;&#37327;&#21560;&#31215;&#29575;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use explainable neural networks to connect the evolutionary history of dark matter halos with their density profiles. The network captures independent factors of variation in the density profiles within a low-dimensional representation, which we physically interpret using mutual information. Without any prior knowledge of the halos' evolution, the network recovers the known relation between the early time assembly and the inner profile, and discovers that the profile beyond the virial radius is described by a single parameter capturing the most recent mass accretion rate. The results illustrate the potential for machine-assisted scientific discovery in complicated astrophysical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2305.03063</link><description>&lt;p&gt;
&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25439;&#20260;&#26816;&#27979;&#26041;&#27861;&#36805;&#36895;&#20174;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#12289;&#38750;&#20405;&#20837;&#24615;&#22320;&#20272;&#35745;&#26753;&#32467;&#26500;&#29366;&#24577;&#12290;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#24005;&#23792;&#34920;&#29616;&#65292;&#20154;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#23427;&#20204;&#36866;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#32570;&#22833;&#65292;&#30001;&#20110;&#30693;&#35782;&#32534;&#30721;&#22312;&#24352;&#37327;&#20540;&#20013;&#32780;&#27809;&#26377;&#21253;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#22522;&#20110;&#26032;&#39062;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#30452;&#25509;&#23558;&#23454;&#38469;&#36923;&#36753;&#21253;&#21547;&#21040;&#27169;&#22411;&#20013;&#30340;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#12290;&#35813;&#28151;&#21512;&#21028;&#21035;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#31934;&#30830;&#22320;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#25439;&#20260;&#30340;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
&lt;/p&gt;</description></item><item><title>PLiX&#26159;&#19968;&#31181;&#36328;&#35821;&#31181;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#21475;&#35821;&#21333;&#35789;&#30340;&#35782;&#21035;&#65292;&#21487;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.03058</link><description>&lt;p&gt;
&#36328;&#35821;&#31181;&#21363;&#25554;&#21363;&#29992;&#23569;&#26679;&#26412;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Multilingual Few-shot Spoken Words Recognition. (arXiv:2305.03058v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03058
&lt;/p&gt;
&lt;p&gt;
PLiX&#26159;&#19968;&#31181;&#36328;&#35821;&#31181;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#21475;&#35821;&#21333;&#35789;&#30340;&#35782;&#21035;&#65292;&#21487;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#25968;&#23383;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#20154;&#26426;&#26080;&#32541;&#36890;&#35759;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#12290;&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#20854;&#20182;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24191;&#27867;&#37319;&#29992;&#24050;&#32463;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#36825;&#20123;&#26234;&#33021;&#35774;&#22791;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#20351;&#24471;&#20934;&#30830;&#30340;&#35821;&#38899;&#35782;&#21035;&#25104;&#20026;&#26377;&#25928;&#20114;&#21160;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#26032;&#20851;&#38190;&#23383;&#30340;&#24378;&#22823;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;PLiX&#65292;&#19968;&#31181;&#36328;&#35821;&#31181;&#30340;&#12289;&#21363;&#25554;&#21363;&#29992;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#26469;&#21033;&#29992;&#28023;&#37327;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#22312;&#27979;&#35797;&#26102;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#21475;&#35821;&#21333;&#35789;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;20&#31181;&#35821;&#35328;&#30340;&#25968;&#30334;&#19975;&#20010;&#19968;&#31186;&#38899;&#39057;&#21098;&#36753;&#26469;&#23398;&#20064;&#23569;&#26679;&#26412;&#28145;&#24230;&#27169;&#22411;&#65292;&#22312;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;PLiX&#33021;&#22815;&#36866;&#24212;&#26032;&#30340;&#21475;&#35821;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
As technology advances and digital devices become prevalent, seamless human-machine communication is increasingly gaining significance. The growing adoption of mobile, wearable, and other Internet of Things (IoT) devices has changed how we interact with these smart devices, making accurate spoken words recognition a crucial component for effective interaction. However, building robust spoken words detection system that can handle novel keywords remains challenging, especially for low-resource languages with limited training data. Here, we propose PLiX, a multilingual and plug-and-play keyword spotting system that leverages few-shot learning to harness massive real-world data and enable the recognition of unseen spoken words at test-time. Our few-shot deep models are learned with millions of one-second audio clips across 20 languages, achieving state-of-the-art performance while being highly efficient. Extensive evaluations show that PLiX can generalize to novel spoken words given as fe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02866</link><description>&lt;p&gt;
&#20998;&#23618;Transformer&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;Transformer&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23454;&#29616;&#30340;&#22270;Transformer&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23567;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#19978;&#65292;&#20840;&#23616;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#23545;&#20110;&#24212;&#29992;&#20110;&#36739;&#22823;&#35268;&#27169;&#22270;&#30340;&#20840;&#25209;&#37327;&#35757;&#32451;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24517;&#35201;&#30340;&#39640;&#23618;&#27425;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#20316;&#20026;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HSGT&#25104;&#21151;&#22320;&#23558;Transformer&#26550;&#26500;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#31895;&#21270;&#25216;&#26415;&#26500;&#24314;&#30340;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#26377;&#25928;&#22320;&#26356;&#26032;&#21644;&#23384;&#20648;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;HSGT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
&lt;/p&gt;</description></item><item><title>Cuttlefish &#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21152;&#36895;&#65292;&#21487;&#29983;&#25104;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02538</link><description>&lt;p&gt;
Cuttlefish: &#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20302;&#31209;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02538
&lt;/p&gt;
&lt;p&gt;
Cuttlefish &#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21152;&#36895;&#65292;&#21487;&#29983;&#25104;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#20302;&#31209;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24635;&#25968;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#21152;&#36895;&#12290;&#20294;&#26159;&#65292;&#20302;&#31209;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Cuttlefish&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#30340;&#38656;&#35201;&#12290;Cuttlefish &#21033;&#29992;&#20102;&#19968;&#31181;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#22312;&#20960;&#20010; epoch &#30340;&#23436;&#20840;&#31209;&#35757;&#32451;&#21518;&#65292;&#27599;&#20010;&#23618;&#30340;&#31283;&#23450;&#31209;&#31283;&#23450;&#22312;&#19968;&#20010;&#24120;&#25968;&#20540;&#12290;&#19968;&#26086;&#25152;&#26377;&#23618;&#30340;&#31283;&#23450;&#31209;&#37117;&#25910;&#25947;&#65292;Cuttlefish &#23601;&#20174;&#23436;&#20840;&#31209;&#35757;&#32451;&#20999;&#25442;&#21040;&#20302;&#31209;&#35757;&#32451;&#65292;&#23558;&#27599;&#20010;&#22240;&#24335;&#20998;&#35299;&#30340;&#32500;&#25968;&#35774;&#32622;&#20026;&#20854;&#30456;&#24212;&#30340;&#31283;&#23450;&#31209;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992; Cuttlefish &#29983;&#25104;&#30340;&#27169;&#22411;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;</title><link>http://arxiv.org/abs/2305.02470</link><description>&lt;p&gt;
&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65306;&#20351;&#29992;ExoMiner&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#39564;&#35777;69&#20010;&#26032;&#34892;&#26143;
&lt;/p&gt;
&lt;p&gt;
Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02470
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24050;&#30693;&#30340;&#31995;&#22806;&#34892;&#26143;&#26159;&#36890;&#36807;&#39564;&#35777;&#25216;&#26415;&#32780;&#19981;&#26159;&#36890;&#36807;&#34917;&#20805;&#35266;&#27979;&#36827;&#34892;&#30830;&#35748;&#30340;&#12290;&#36825;&#20123;&#25216;&#26415;&#29983;&#25104;&#30340;&#20998;&#25968;&#36890;&#24120;&#20195;&#34920;&#20102;&#26377;&#20851;&#20449;&#21495;&#30340;&#26576;&#20123;&#20449;&#24687;&#65288;&#29992;x&#34920;&#31034;&#65289;&#32473;&#20986;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#27010;&#29575;&#65288;y&#65288;x&#65289;=&#34892;&#26143;&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30830;&#35748;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;vespa&#65288;Morton&#31561;&#20154;2016&#65289;&#12289;Robovetter&#65288;Coughlin&#31561;&#20154;2017&#65289;&#12289;AstroNet&#65288;Shallue&#21644;Vanderburg 2018&#65289;&#12289;ExoNet&#65288;Ansdel&#31561;&#20154;2018&#65289;&#12289;GPC&#21644;RFC&#65288;Armstrong&#31561;&#20154;2020&#65289;&#20197;&#21450;ExoMiner&#65288;Valizadegan&#31561;&#20154;2022&#65289;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.02459</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19982;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20849;&#40483;&#26816;&#27979;&#65306;&#35299;&#20915;&#31232;&#26377;&#31867;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31995;&#32479;&#20351;&#24471;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#20363;&#33021;&#22815;&#24471;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23545;&#20110;&#31232;&#26377;&#31867;&#20219;&#21153;&#65288;&#21363;&#31867;&#21035;&#26631;&#31614;&#38750;&#24120;&#23569;&#35265;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&lt;5%&#30340;&#26679;&#26412;&#65289;&#65292;&#25968;&#25454;&#37319;&#38598;&#38556;&#30861;&#20173;&#28982;&#23384;&#22312;&#12290;&#20027;&#21160;&#23398;&#20064;&#19968;&#33324;&#34987;&#25552;&#20986;&#29992;&#20110;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#65292;&#20294;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;&#36873;&#25321;&#31232;&#26377;&#31867;&#31034;&#20363;&#30340;&#26631;&#20934;&#65292;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25552;&#20986;&#30340;&#31232;&#26377;&#31867;&#27010;&#29575;&#65288;PRC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65288;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25910;&#38598;&#35748;&#30693;&#20849;&#25391;&#30340;&#35821;&#35328;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;PRC&#26159;&#25351;&#23548;&#27880;&#37322;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#36716;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01864</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25913;&#36827;&#38899;&#39057;-&#25991;&#26412;&#36328;&#27169;&#24577;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#33719;&#24471;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#31038;&#21306;&#33021;&#22815;&#22312;&#38646;-shot&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#21542;&#21017;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#30340;&#34920;&#24449;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26410;&#37197;&#23545;&#25991;&#26412;&#21644;&#38899;&#39057;&#25913;&#36827;&#36825;&#20123;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#39046;&#22495;&#38750;&#29305;&#23450;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#26041;&#27861;&#65292;&#21019;&#24314;&#25105;&#20204;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#19982;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#32467;&#21512;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#31579;&#36873;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19979;&#28216;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#25110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01807</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#23398;&#20064;&#21644;&#24212;&#29992;&#20110;&#35299;&#37322;&#24615;&#33041;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21033;&#29992;&#22522;&#20110;&#25299;&#25169;&#22270;&#30340;&#21367;&#31215;&#25805;&#20316;&#26469;&#32452;&#21512;&#22270;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#22270;&#26469;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20256;&#32479;PCA&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#30340;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;VNN&#65289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#21487;&#36716;&#31227;&#24615;&#30340;&#27010;&#24565;&#26159;&#20174;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#8220;&#20860;&#23481;&#8221;&#30340;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#30340;&#30452;&#35266;&#26399;&#26395;&#20013;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VNN&#20174;GCN&#32487;&#25215;&#30340;&#26080;&#26631;&#24230;&#25968;&#25454;&#22788;&#29702;&#26550;&#26500;&#65292;&#24182;&#35777;&#26126;&#24403;&#25968;&#25454;&#38598;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#23545;&#35937;&#26102;&#65292;VNN&#33021;&#22815;&#23637;&#29616;&#20986;&#24615;&#33021;&#21487;&#36716;&#31227;&#24615;&#12290;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30740;&#31350;&#33041;&#37096;&#65292;&#24182;&#19988;&#21487;&#20197;&#39564;&#35777;VNN&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCN) leverage topology-driven graph convolutional operations to combine information across the graph for inference tasks. In our recent work, we have studied GCNs with covariance matrices as graphs in the form of coVariance neural networks (VNNs) that draw similarities with traditional PCA-driven data analysis approaches while offering significant advantages over them. In this paper, we first focus on theoretically characterizing the transferability of VNNs. The notion of transferability is motivated from the intuitive expectation that learning models could generalize to "compatible" datasets (possibly of different dimensionalities) with minimal effort. VNNs inherit the scale-free data processing architecture from GCNs and here, we show that VNNs exhibit transferability of performance over datasets whose covariance matrices converge to a limit object. Multi-scale neuroimaging datasets enable the study of the brain at multiple scales and hence, can validate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01095</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22120;&#65292;&#22312;&#36710;&#36947;&#21464;&#25442;&#26102;&#21487;&#20197;&#39044;&#27979;&#20808;&#21069;&#36710;&#36742;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#35843;&#33410;&#36710;&#36895;&#26469;&#30830;&#20445;&#19982;&#21069;&#36710;&#23433;&#20840;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ACC&#31995;&#32479;&#26080;&#27861;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39550;&#39542;&#26465;&#20214;&#21644;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#39550;&#39542;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23454;&#26102;&#22320;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#30340;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#39640;D&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#35013;&#22791;&#26377;&#25668;&#20687;&#22836;&#30340;&#26080;&#20154;&#26426;&#22312;&#24503;&#22269;&#39640;&#36895;&#20844;&#36335;&#19978;&#33719;&#21462;&#30340;&#12290;&#25105;&#20204;&#22312;&#20391;&#38754;&#36710;&#36947;&#21069;&#36710;&#21098;&#20999;&#24182;&#24378;&#21046;&#30446;&#26631;&#39550;&#39542;&#21592;&#20943;&#36895;&#30340;&#28608;&#36827;&#36710;&#36947;&#21464;&#21270;&#26102;&#35780;&#20272;&#20102;ACC&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#39304;&#36865;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13761</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13761
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#20013;&#30340;&#23567;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#29420;&#28909;&#32534;&#30721;&#23558;GBDT&#27169;&#22411;&#36716;&#25442;&#20026;&#32447;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26641;&#21494;&#32534;&#30721;&#20026;&#19968;&#20010;&#34394;&#25311;&#21464;&#37327;&#12290;&#36825;&#20801;&#35768;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#20998;&#35299;&#26041;&#27861;&#26469;&#35780;&#20272;GBDT&#27169;&#22411;&#23545;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#37325;&#26032;&#25311;&#21512;&#20854;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#65292;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#20102;&#27491;&#21017;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29420;&#28909;&#32534;&#30721;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DC-GANs&#65289;&#65292;&#20351;&#29992;&#37051;&#22495;&#36807;&#31243;&#20316;&#20026;&#20851;&#38190;&#27493;&#39588;&#29983;&#25104;&#21516;&#26679;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.13131</link><description>&lt;p&gt;
&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DC-GANs&#65289;&#65292;&#20351;&#29992;&#37051;&#22495;&#36807;&#31243;&#20316;&#20026;&#20851;&#38190;&#27493;&#39588;&#29983;&#25104;&#21516;&#26679;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#36890;&#24120;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20363;&#22914;&#25551;&#36848;&#31038;&#21306;&#24847;&#35265;&#20998;&#27495;&#12289;&#31070;&#32463;&#20803;&#30340;&#38388;&#38548;&#20998;&#24067;&#20197;&#21450;&#25391;&#33633;&#22120;&#30340;&#22266;&#26377;&#39057;&#29575;&#30340;&#25968;&#25454;&#12290;&#29983;&#25104;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#24050;&#25104;&#20026;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#23558;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#35270;&#20026;&#26080;&#38480;&#32500;GAN&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25104;&#21151;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#29992;&#20110;&#29983;&#25104;&#21333;&#23792;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22120;&#8212;&#8212;&#26377;&#21521;&#38142;GAN&#65288;DC-GAN&#65289;&#65292;&#23427;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#26377;&#21521;&#38142;&#30340;&#37051;&#22495;&#36807;&#31243;&#25110;&#36755;&#20837;&#65289;&#25554;&#20837;&#20855;&#26377;&#20998;&#24067;&#32422;&#26463;&#30340;&#26377;&#21521;&#38142;SDE&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#20013;&#12290;DC-GAN&#21487;&#20197;&#29983;&#25104;&#19982;&#37051;&#22495;&#36807;&#31243;&#30456;&#21516;&#20998;&#24067;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#65292;&#32780;&#37051;&#22495;&#36807;&#31243;&#23558;&#25552;&#20379;&#23398;&#20064;&#21644;&#29983;&#25104;&#22810;&#27169;&#24577;&#20998;&#24067;&#25968;&#25454;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data can be multimodal distributed, e.g., data describing the opinion divergence in a community, the interspike interval distribution of neurons, and the oscillators natural frequencies. Generating multimodal distributed real-world data has become a challenge to existing generative adversarial networks (GANs). For example, neural stochastic differential equations (Neural SDEs), treated as infinite-dimensional GANs, have demonstrated successful performance mainly in generating unimodal time series data. In this paper, we propose a novel time series generator, named directed chain GANs (DC-GANs), which inserts a time series dataset (called a neighborhood process of the directed chain or input) into the drift and diffusion coefficients of the directed chain SDEs with distributional constraints. DC-GANs can generate new time series of the same distribution as the neighborhood process, and the neighborhood process will provide the key step in learning and generating multimodal di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.04033</link><description>&lt;p&gt;
&#25506;&#31350;&#40065;&#26834;&#24615;&#27169;&#22411;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36890;&#36807;&#20998;&#35299;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#33021;&#37327;&#22522;&#27169;&#22411;(EBM)&#24418;&#24335;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#23545;&#25239;&#28857;&#31163;&#24320;&#20102;&#36755;&#20837;&#25968;&#25454;&#30340;&#27969;&#24418;&#65292;&#20294;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35777;&#25454;:&#38750;&#23450;&#21521;&#25915;&#20987;&#30340;&#27010;&#29575;&#29978;&#33267;&#27604;&#33258;&#28982;&#25968;&#25454;&#36824;&#35201;&#39640;&#65292;&#24182;&#19988;&#38543;&#30528;&#25915;&#20987;&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#20854;&#27010;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#26816;&#27979;&#23427;&#20204;&#24182;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#65292;&#33021;&#22815;&#27450;&#39575;&#20998;&#31867;&#22120;&#20294;&#20855;&#26377;&#19982;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20854;&#39118;&#38505;&#22240;&#32032;&#65292;&#20854;&#20013;&#24515;&#34880;&#31649;&#30142;&#30149;&#26368;&#20026;&#26222;&#36941;&#12290;&#30007;&#24615;&#21442;&#19982;&#32773;&#36739;&#22899;&#24615;&#26356;&#23481;&#26131;&#24739;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20294;&#31958;&#23615;&#30149;&#19981;&#20855;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;CVD&#21644;DM&#37117;&#20250;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#24739;&#26377;&#32933;&#32982;&#30151;&#30340;&#20303;&#38498;&#30149;&#20154;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2303.04808</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#36798;&#21345;&#24066;&#22522;&#20110;&#21307;&#38498;&#30340;&#27178;&#26029;&#38754;&#30740;&#31350;&#65306;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20027;&#35201;&#39118;&#38505;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh. (arXiv:2303.04808v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20854;&#39118;&#38505;&#22240;&#32032;&#65292;&#20854;&#20013;&#24515;&#34880;&#31649;&#30142;&#30149;&#26368;&#20026;&#26222;&#36941;&#12290;&#30007;&#24615;&#21442;&#19982;&#32773;&#36739;&#22899;&#24615;&#26356;&#23481;&#26131;&#24739;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20294;&#31958;&#23615;&#30149;&#19981;&#20855;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;CVD&#21644;DM&#37117;&#20250;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#24739;&#26377;&#32933;&#32982;&#30151;&#30340;&#20303;&#38498;&#30149;&#20154;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#23547;&#27714;&#33829;&#20859;&#25351;&#23548;&#30340;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#65288;NCD&#65289;&#30340;&#24739;&#30149;&#29575;&#65292;&#20998;&#26512;&#20854;&#39118;&#38505;&#22240;&#32032;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#21035;&#12289;&#24180;&#40836;&#32452;&#12289;&#32933;&#32982;&#19982;NCD&#65288;&#31958;&#23615;&#30149;&#12289;CKD&#12289;IBS&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#12289;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#12289;&#30002;&#29366;&#33146;&#30142;&#30149;&#65289;&#20043;&#38388;&#26377;&#20851;&#32852;&#12290;NCD&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#24515;&#34880;&#31649;&#38382;&#39064;&#65288;CVD&#65289;&#65292;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#21344;83.56%&#12290;CVD&#22312;&#30007;&#24615;&#21442;&#19982;&#32773;&#20013;&#26356;&#20026;&#26222;&#36941;&#12290;&#30456;&#24212;&#22320;&#65292;&#30007;&#24615;&#21442;&#19982;&#32773;&#30340;&#34880;&#21387;&#20998;&#24067;&#27604;&#22899;&#24615;&#26356;&#39640;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31958;&#23615;&#30149;&#24182;&#27809;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;&#26080;&#35770;CVD&#36824;&#26159;DM&#65292;&#37117;&#20855;&#26377;&#24180;&#40836;&#19978;&#30340;&#36827;&#23637;&#12290;&#24930;&#24615;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#22312;&#20013;&#24180;&#21442;&#19982;&#32773;&#20013;&#27604;&#24180;&#36731;&#25110;&#32769;&#24180;&#20154;&#26356;&#20026;&#24120;&#35265;&#12290;&#22522;&#20110;&#25968;&#25454;&#65292;&#20116;&#20998;&#20043;&#19968;&#30340;&#20303;&#38498;&#24739;&#32773;&#24739;&#26377;&#32933;&#32982;&#30151;&#12290;&#25105;&#20204;&#23545;&#21512;&#24182;&#30151;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;31.5%&#30340;&#20154;&#21475;&#20165;&#24739;&#26377;&#19968;&#31181;NCD&#65292;30.1%&#30340;&#20154;&#24739;&#26377;&#20004;&#31181;&#25110;&#20004;&#31181;&#20197;&#19978;&#30340;NCD&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The study aimed to determine the prevalence of several non-communicable diseases (NCD) and analyze risk factors among adult patients seeking nutritional guidance in Dhaka, Bangladesh. Result: Our study observed the relationships between gender, age groups, obesity, and NCDs (DM, CKD, IBS, CVD, CRD, thyroid). The most frequently reported NCD was cardiovascular issues (CVD), which was present in 83.56% of all participants. CVD was more common in male participants. Consequently, male participants had a higher blood pressure distribution than females. Diabetes mellitus (DM), on the other hand, did not have a gender-based inclination. Both CVD and DM had an age-based progression. Our study showed that chronic respiratory illness was more frequent in middle-aged participants than in younger or elderly individuals. Based on the data, every one in five hospitalized patients was obese. We analyzed the co-morbidities and found that 31.5% of the population has only one NCD, 30.1% has t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#22522;&#20110;&#21442;&#25968;&#21270;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#26368;&#22823;&#20313;&#37327;&#23450;&#20041;&#30340;&#21807;&#19968;&#32676;&#19981;&#21464;&#20998;&#31867;&#22120;&#30340;&#26041;&#21521;, &#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#26059;&#32593;&#32476;&#22312;&#36739;&#23569;&#21464;&#24773;&#20917;&#19979;&#30456;&#23545;&#20110;&#19981;&#21464;&#32593;&#32476;&#30340;&#25913;&#21892;&#20313;&#37327;&#19982;&#24191;&#20041;&#30028;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.04198</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#31561;&#21464;&#21487;&#26059;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Linear Equivariant Steerable Networks. (arXiv:2303.04198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#22522;&#20110;&#21442;&#25968;&#21270;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#26368;&#22823;&#20313;&#37327;&#23450;&#20041;&#30340;&#21807;&#19968;&#32676;&#19981;&#21464;&#20998;&#31867;&#22120;&#30340;&#26041;&#21521;, &#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#26059;&#32593;&#32476;&#22312;&#36739;&#23569;&#21464;&#24773;&#20917;&#19979;&#30456;&#23545;&#20110;&#19981;&#21464;&#32593;&#32476;&#30340;&#25913;&#21892;&#20313;&#37327;&#19982;&#24191;&#20041;&#30028;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32676;&#19981;&#21464;&#20108;&#36827;&#21046;&#20998;&#31867;&#20013;&#30340;&#32447;&#24615;&#31561;&#21464;&#21487;&#26059;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#21442;&#25968;&#21270;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#26368;&#22823;&#20313;&#37327;&#23450;&#20041;&#30340;&#21807;&#19968;&#32676;&#19981;&#21464;&#20998;&#31867;&#22120;&#30340;&#26041;&#21521;&#65292;&#35813;&#26041;&#21521;&#30001;&#36755;&#20837;&#32676;&#25805;&#20316;&#23450;&#20041;&#12290;&#22312;&#36755;&#20837;&#34920;&#31034;&#30340;&#37193;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#26059;&#32593;&#32476;&#21644;&#25968;&#25454;&#22686;&#24191;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#26059;&#32593;&#32476;&#22312;&#36739;&#23569;&#21464;&#24773;&#20917;&#19979;&#30456;&#23545;&#20110;&#19981;&#21464;&#32593;&#32476;&#30340;&#25913;&#21892;&#20313;&#37327;&#19982;&#24191;&#20041;&#30028;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the implicit bias of gradient flow on linear equivariant steerable networks in group-invariant binary classification. Our findings reveal that the parameterized predictor converges in direction to the unique group-invariant classifier with a maximum margin defined by the input group action. Under a unitary assumption on the input representation, we establish the equivalence between steerable networks and data augmentation. Furthermore, we demonstrate the improved margin and generalization bound of steerable networks over their non-invariant counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12692</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#26159;&#21307;&#30103;&#20445;&#20581;&#34892;&#19994;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24314;&#31435;&#22312;&#29702;&#35770;&#26694;&#26550;Transformers&#20043;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#24182;&#26410;&#24310;&#20280;&#21040;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#20998;&#23376;&#29305;&#24449;&#25506;&#32034;&#20102;Transformers&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20020;&#24202;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#32597;&#35265;&#30142;&#30149;&#39046;&#22495;&#19979;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#30284;&#30151;&#31867;&#22411;&#30340;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#19981;&#21516;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.05763</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36741;&#21161;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#36880;&#28176;&#20851;&#27880;&#22810;&#26041;&#38754;&#22330;&#26223;&#65292;&#21363;&#26426;&#22120;&#20154;&#19982;&#22810;&#20010;&#20154;&#29992;&#25143;&#21516;&#26102;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290; &#28982;&#32780;&#65292;&#22312;&#20154;&#26426;&#21327;&#20316;&#26041;&#38754;&#65292;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22788;&#29702;&#27492;&#31867;&#21512;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#30340;&#25968;&#25454;&#27604;&#20856;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#35774;&#32622;&#20013;&#26356;&#19981;&#21487;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#20108;&#20803;&#20154;&#26426;&#21327;&#20316;&#24212;&#29992;&#30340;&#24182;&#34892;&#20219;&#21153;&#22330;&#26223;&#65292;&#24182;&#25552;&#35758;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25910;&#38598;&#19982;&#22810;&#29992;&#25143;&#27963;&#21160;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#21363;&#25910;&#38598;&#19982;&#21333;&#20010;&#29992;&#25143;&#30456;&#20851;&#30340;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#23427;&#20204;&#65292;&#20197;&#20943;&#23569;&#20135;&#29983;&#25104;&#21452;&#35774;&#32622;&#24405;&#21046;&#30340;&#21162;&#21147;&#12290;&#25910;&#38598;&#20102;&#21333;&#20010;&#29992;&#25143;&#30340;&#27963;&#21160;&#19977;&#32500;&#39592;&#26550;&#23039;&#21183;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#23545;&#26469;&#39564;&#35777;&#35813;&#35821;&#21477;&#65292;&#38543;&#21518;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#34987;&#29992;&#20110;&#20998;&#21035;&#35757;&#32451;&#30001;LSTM&#32593;&#32476;&#21644;VAE &#28151;&#21512;&#32780;&#25104;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03857</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65306;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#20294;&#21487;&#20197;&#36755;&#20986;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#40065;&#26834;&#24615;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;ACL&#38656;&#35201;&#24040;&#22823;&#30340;&#36816;&#34892;&#26102;&#38388;&#25165;&#33021;&#29983;&#25104;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#21152;&#36895;ACL&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#12290;RCS&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#65292;&#25628;&#32034;&#26368;&#23567;&#21270;&#34920;&#31034;&#20998;&#27495;&#30340;&#20449;&#24687;&#23376;&#38598;&#65292;&#21363;&#33258;&#28982;&#25968;&#25454;&#21644;&#20854;&#34394;&#25311;&#23545;&#25239;&#21464;&#20307;&#20043;&#38388;&#34920;&#31034;&#30340;&#36317;&#31163;&#12290;RCS&#30340;&#22522;&#26412;&#35299;&#27861;&#26159;&#36941;&#21382;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#38598;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;RCS&#36716;&#21270;&#20026;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#38382;&#39064;&#65292;&#21033;&#29992;&#36138;&#24515;&#25628;&#32034;&#26159;&#21407;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20855;&#26377;&#21407;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RCS&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21152;&#36895;ACL&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#26426;&#22120;&#20013;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#30456;&#20114;&#32806;&#21512;&#30340;&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.02605</link><description>&lt;p&gt;
&#21521;&#22823;&#26680;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Toward Large Kernel Models. (arXiv:2302.02605v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#26426;&#22120;&#20013;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#30456;&#20114;&#32806;&#21512;&#30340;&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30456;&#27604;&#65292;&#26680;&#26426;&#22120;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;DNN&#12290;&#26680;&#26426;&#22120;&#30340;&#20852;&#36259;&#21463;&#21040;&#20854;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#31561;&#25928;&#20110;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#29616;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#20204;&#33021;&#22815;&#29420;&#31435;&#22320;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#32780;&#22312;&#20256;&#32479;&#30340;&#26680;&#26426;&#22120;&#20013;&#65292;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#26159;&#30456;&#20114;&#32806;&#21512;&#30340;&#12290;&#30001;&#20110;&#36825;&#31181;&#32806;&#21512;&#65292;&#23558;&#26680;&#26426;&#22120;&#25193;&#23637;&#21040;&#22823;&#25968;&#25454;&#26159;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#26680;&#26426;&#22120;&#30340;&#19968;&#33324;&#21270;&#65292;&#36890;&#36807;&#35299;&#32806;&#27169;&#22411;&#21644;&#25968;&#25454;&#65292;&#20801;&#35768;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#21452;&#37325;&#39044;&#22788;&#29702;SGD&#30340;EigenPro 3.0&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#26680;&#26041;&#27861;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes which have not been possible with existing kernel methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#32508;&#36848;&#20102;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#22238;&#39038;&#20102;250&#22810;&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;</title><link>http://arxiv.org/abs/2211.14997</link><description>&lt;p&gt;
&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#30475;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective. (arXiv:2211.14997v3 [q-fin.RM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#32508;&#36848;&#20102;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#22238;&#39038;&#20102;250&#22810;&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#26088;&#22312;&#39044;&#27979;&#20225;&#19994;&#26410;&#26469;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#32780;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#19968;&#30452;&#26159;&#37329;&#34701;&#21644;&#31649;&#29702;&#39046;&#22495;&#30340;&#26680;&#24515;&#30740;&#31350;&#20027;&#39064;&#12290;&#22522;&#20110;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#30740;&#31350;&#27491;&#22312;&#32463;&#21382;&#24555;&#36895;&#21457;&#23637;&#24182;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#20840;&#38754;&#35780;&#20272;&#30456;&#20851;&#30740;&#31350;&#26082;&#26377;&#24517;&#35201;&#24615;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#26377;&#20215;&#20540;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20851;&#20110;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#30340;&#32508;&#36848;&#65292;&#20294;&#36825;&#20123;&#32508;&#36848;&#21333;&#29420;&#20171;&#32461;&#20102;&#26041;&#27861;&#65292;&#32570;&#20047;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#23581;&#35797;&#20174;&#22823;&#25968;&#25454;&#30340;&#35282;&#24230;&#25552;&#20379;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#26041;&#27861;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#36229;&#36807;250&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprise financial risk analysis aims at predicting the future financial risk of enterprises. Due to its wide and significant application, enterprise financial risk analysis has always been the core research topic in the fields of Finance and Management. Based on advanced computer science and artificial intelligence technologies, enterprise risk analysis research is experiencing rapid developments and making significant progress. Therefore, it is both necessary and challenging to comprehensively review the relevant studies. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise financial risk analysis. In contrast, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from Big Data perspective, which reviews more than 250 representative articles in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;LR-FFT-RP-MLP/LSTM&#65292;&#21033;&#29992;&#25490;&#24207;&#27744;&#21270;&#21644;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#36827;&#34892;&#22810;&#27493;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.14434</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#24207;&#27744;&#21270;&#21644;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#22810;&#27493;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Short-Term Wind Speed Prediction with Rank Pooling and Fast Fourier Transformation. (arXiv:2211.14434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;LR-FFT-RP-MLP/LSTM&#65292;&#21033;&#29992;&#25490;&#24207;&#27744;&#21270;&#21644;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#36827;&#34892;&#22810;&#27493;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#23545;&#20110;&#32463;&#27982;&#21033;&#29992;&#39118;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#39118;&#36895;&#25968;&#25454;&#36890;&#24120;&#26159;&#38388;&#27463;&#24615;&#21644;&#27874;&#21160;&#24615;&#30340;&#65292;&#23545;&#29616;&#26377;&#27973;&#23618;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#22810;&#27493;&#39118;&#36895;&#39044;&#27979;&#65292;&#21517;&#20026;LR-FFT-RP-MLP/LSTM&#65288;&#32447;&#24615;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#25490;&#24207;&#27744;&#21270;&#22810;&#23618;&#24863;&#30693;/&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25490;&#24207;&#27744;&#21270;&#65288;RP&#65289;&#36827;&#34892;&#26412;&#22320;&#29305;&#24449;&#25552;&#21462;&#20197;&#25429;&#25417;&#26102;&#38388;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#26102;&#38388;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20102;&#35299;&#39118;&#30340;&#21608;&#26399;&#24615;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#25552;&#21462;&#20840;&#23616;&#29305;&#24449;&#21644;&#39118;&#36895;&#25968;&#25454;&#20013;&#30456;&#20851;&#30340;&#39057;&#29575;&#20998;&#37327;&#12290;&#20135;&#29983;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20998;&#21035;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#25104;&#65292;&#28982;&#21518;&#36755;&#20837;MLP/LSTM&#23618;&#36827;&#34892;&#21021;&#22987;&#30340;&#39118;&#36895;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#39044;&#27979;&#32467;&#26524;&#22522;&#20110;&#20808;&#21069;&#30340;&#22810;&#27493;&#39044;&#27979;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term wind speed prediction is essential for economical wind power utilization. The real-world wind speed data is typically intermittent and fluctuating, presenting great challenges to existing shallow models. In this paper, we present a novel deep hybrid model for multi-step wind speed prediction, namely LR-FFT-RP-MLP/LSTM (Linear Fast Fourier Transformation Rank Pooling Multiple-Layer Perception/Long Short-Term Memory). Our hybrid model processes the local and global input features simultaneously. We leverage Rank Pooling (RP) for the local feature extraction to capture the temporal structure while maintaining the temporal order. Besides, to understand the wind periodic patterns, we exploit Fast Fourier Transformation (FFT) to extract global features and relevant frequency components in the wind speed data. The resulting local and global features are respectively integrated with the original data and are fed into an MLP/LSTM layer for the initial wind speed predictions. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;SNAFUE&#25216;&#26415;&#65292;&#29992;&#20110;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#35813;&#25216;&#26415;&#23545;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21457;&#29616;&#35768;&#22810;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2211.10024</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;SNAFUE&#25216;&#26415;&#65292;&#29992;&#20110;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#35813;&#25216;&#26415;&#23545;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21457;&#29616;&#35768;&#22810;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#24110;&#21161;&#20154;&#31867;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#36890;&#36807;&#25581;&#31034;DNN&#30340;&#24369;&#28857;&#26469;&#24110;&#21161;&#65292;&#20294;&#24456;&#38590;&#35299;&#37322;&#25110;&#20174;&#20013;&#24471;&#20986;&#21487;&#23454;&#26045;&#30340;&#32467;&#35770;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21253;&#25324;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#24352;&#33258;&#28982;&#22270;&#20687;&#31896;&#36148;&#21040;&#21478;&#19968;&#24352;&#22270;&#20687;&#20250;&#23548;&#33268;&#24847;&#22806;&#30340;&#20998;&#31867;&#38169;&#35823;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#23884;&#20837;&#30340;SNAFUE&#65288;Search for Natural Adversarial Features Using Embeddings&#65289;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#30340;&#20840;&#33258;&#21160;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SNAFUE&#26469;&#27979;&#35797;ImageNet&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#37325;&#29616;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20102;&#25968;&#30334;&#20010;&#20854;&#20182;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#65292;&#20840;&#36807;&#31243;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#20195;&#30721;&#24050;&#22312; https://github.com/thestephencasper/snafue &#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#21355;&#26143;&#21644;&#22320;&#38754;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#22312;&#27809;&#26377;&#30417;&#27979;&#31449;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31354;&#27668;&#27745;&#26579;&#29289;&#20998;&#24067;&#21644;&#25913;&#21464;&#31038;&#20250;&#24037;&#19994;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#27431;&#27954;&#27745;&#26579;&#30417;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.00780</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;AI&#21644;&#21355;&#26143;&#24433;&#20687;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Predicting air quality via multimodal AI and satellite imagery. (arXiv:2211.00780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#21355;&#26143;&#21644;&#22320;&#38754;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#22312;&#27809;&#26377;&#30417;&#27979;&#31449;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31354;&#27668;&#27745;&#26579;&#29289;&#20998;&#24067;&#21644;&#25913;&#21464;&#31038;&#20250;&#24037;&#19994;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#27431;&#27954;&#27745;&#26579;&#30417;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#24403;&#21069;&#22320;&#29699;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#30340;&#29615;&#22659;&#38382;&#39064;&#65292;&#24433;&#21709;&#30528;&#22320;&#29699;&#19978;&#30340;&#25152;&#26377;&#29983;&#29289;&#12290;&#30001;&#20110;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#31449;&#36890;&#24120;&#26159;&#22320;&#38754;&#31449;&#65292;&#23427;&#20204;&#26816;&#27979;&#27745;&#26579;&#29289;&#20998;&#24067;&#30340;&#33021;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#21355;&#26143;&#20855;&#26377;&#22312;&#22823;&#33539;&#22260;&#20869;&#30740;&#31350;&#22823;&#27668;&#30340;&#28508;&#21147;&#12290;&#27431;&#27954;&#31354;&#38388;&#23616;&#30340;&#21733;&#30333;&#23612;&#35745;&#21010;&#21355;&#26143;&#8220;Sentinel-5P&#8221;&#26159;&#19968;&#39063;&#26032;&#36817;&#21457;&#23556;&#30340;&#21355;&#26143;&#65292;&#33021;&#22815;&#27979;&#37327;&#21508;&#31181;&#27745;&#26579;&#29289;&#20449;&#24687;&#24182;&#25552;&#20379;&#20844;&#24320;&#30340;&#25968;&#25454;&#36755;&#20986;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#19981;&#23384;&#22312;&#30417;&#27979;&#31449;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20837;&#23558;&#21253;&#25324;&#22320;&#38754;&#27979;&#37327;&#21644;&#21355;&#26143;&#25968;&#25454;&#30340;&#34701;&#21512;&#65292;&#26088;&#22312;&#31361;&#20986;&#27745;&#26579;&#29289;&#20998;&#24067;&#24182;&#20419;&#36827;&#31038;&#20250;&#21644;&#24037;&#19994;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#19968;&#20010;&#26032;&#30340;&#27431;&#27954;&#27745;&#26579;&#30417;&#27979;&#31449;&#27979;&#37327;&#25968;&#25454;&#38598;&#20063;&#34987;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change may be classified as the most important environmental problem that the Earth is currently facing, and affects all living species on Earth. Given that air-quality monitoring stations are typically ground-based their abilities to detect pollutant distributions are often restricted to wide areas. Satellites however have the potential for studying the atmosphere at large; the European Space Agency (ESA) Copernicus project satellite, "Sentinel-5P" is a newly launched satellite capable of measuring a variety of pollutant information with publicly available data outputs. This paper seeks to create a multi-modal machine learning model for predicting air-quality metrics where monitoring stations do not exist. The inputs of this model will include a fusion of ground measurements and satellite data with the goal of highlighting pollutant distribution and motivating change in societal and industrial behaviors. A new dataset of European pollution monitoring station measurements is cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#21152;&#26435;&#22240;&#23376;&#969;&#65292;&#23398;&#20064;&#21644;&#28151;&#21512;&#22810;&#20010;&#24179;&#28369;&#21644;&#38160;&#21270;&#28388;&#27874;&#22120;&#65292;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#24179;&#28369;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2210.17224</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20256;&#25773;&#31639;&#23376;&#65292;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Graph Neural Networks with Learnable Propagation Operators. (arXiv:2210.17224v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#21152;&#26435;&#22240;&#23376;&#969;&#65292;&#23398;&#20064;&#21644;&#28151;&#21512;&#22810;&#20010;&#24179;&#28369;&#21644;&#38160;&#21270;&#28388;&#27874;&#22120;&#65292;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#24179;&#28369;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20256;&#25773;&#31639;&#23376;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#23376;&#36890;&#24120;&#21482;&#21547;&#26377;&#38750;&#36127;&#20803;&#32032;&#65292;&#24182;&#19988;&#22312;&#36890;&#36947;&#20043;&#38388;&#20849;&#20139;&#65292;&#38480;&#21046;&#20102;GNNs&#30340;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;GNNs&#21463;&#21040;&#20840;&#23616;&#24179;&#28369;&#30340;&#38480;&#21046;&#65292;&#32780;&#26080;&#27861;&#24456;&#22909;&#22320;&#34920;&#36798;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21487;&#20197;&#23398;&#20064;&#22810;&#26679;&#30340;&#20256;&#25773;&#28388;&#27874;&#22120;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#20250;&#34920;&#29616;&#20986;&#20840;&#23616;&#24179;&#28369;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#21152;&#26435;&#22240;&#23376;&#969;&#32435;&#20837;&#27599;&#23618;&#20013;&#65292;&#23398;&#20064;&#21644;&#28151;&#21512;&#22810;&#20010;&#24179;&#28369;&#21644;&#38160;&#21270;&#28388;&#27874;&#22120;&#65292;&#26469;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#31216;&#20026;&#969;GNN&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#969;GCN&#21644;&#969;GAT&#12290;&#23545;&#20110;&#969;GCN&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20854;&#34892;&#20026;&#21644;&#969;&#23545;&#25152;&#24471;&#21040;&#30340;&#33410;&#28857;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#35299;&#37322;&#20102;&#36825;&#20004;&#31181;&#21464;&#20307;&#22914;&#20309;&#36991;&#20813;&#20840;&#23616;&#24179;&#28369;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are limited in their propagation operators. In many cases, these operators often contain non-negative elements only and are shared across channels, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs. In this paper, we bridge these gaps by incorporating trainable channel-wise weighting factors $\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\omega$GNN, and is easy to implement. We study two variants: $\omega$GCN and $\omega$GAT. For $\omega$GCN, we theoretically analyse its behaviour and the impact of $\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth. Additionally, we ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PR-ENT&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20013;&#25552;&#21462;&#20107;&#20214;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22635;&#20805;&#20107;&#20214;&#25551;&#36848;&#20013;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#25991;&#26412;&#38544;&#21547;&#20851;&#31995;&#20219;&#21153;&#20013;&#36873;&#25321;&#31572;&#26696;&#20505;&#36873;&#39033;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.05257</link><description>&lt;p&gt;
&#29992;&#25552;&#31034;&#34164;&#28085;&#37325;&#26032;&#24605;&#32771;&#20107;&#20214;&#32534;&#30721;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PR-ENT&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20013;&#25552;&#21462;&#20107;&#20214;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22635;&#20805;&#20107;&#20214;&#25551;&#36848;&#20013;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#25991;&#26412;&#38544;&#21547;&#20851;&#31995;&#20219;&#21153;&#20013;&#36873;&#25321;&#31572;&#26696;&#20505;&#36873;&#39033;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30417;&#27979;&#21361;&#26426;&#65292;&#25919;&#27835;&#20107;&#20214;&#20174;&#26032;&#38395;&#20013;&#25552;&#21462;&#12290;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20351;&#24471;&#36880;&#26696;&#20998;&#26512;&#38590;&#20197;&#25511;&#21046;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#30340;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#32452;&#32455;&#26469;&#35828;&#12290;&#36825;&#23601;&#38656;&#35201;&#23545;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#19968;&#20219;&#21153;&#34987;&#31216;&#20026;&#20107;&#20214;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PR-ENT&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#26356;&#21152;&#28789;&#27963;&#21644;&#36164;&#28304;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as "Military injured two civilians'' by a template, e.g. "People were [Z]" and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select answer candidates Z* = {"injured'', "hurt"...} by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. This allo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;FLamby&#65292;&#19968;&#20010;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#36924;&#30495;&#20581;&#24247;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#36328;&#35774;&#22791;FL&#30340;&#24212;&#29992;&#21644;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2210.04620</link><description>&lt;p&gt;
FLamby&#65306;&#36866;&#29992;&#20110;&#29616;&#23454;&#21307;&#30103;&#29615;&#22659;&#20013;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings. (arXiv:2210.04620v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;FLamby&#65292;&#19968;&#20010;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#36924;&#30495;&#20581;&#24247;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#36328;&#35774;&#22791;FL&#30340;&#24212;&#29992;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#25345;&#26377;&#25935;&#24863;&#25968;&#25454;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#21327;&#20316;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#20013;&#25968;&#25454;&#12290;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#26159;&#25351;&#23569;&#37327;&#65288;2-50&#65289;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#20013;&#21040;&#22823;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#36890;&#24120;&#20986;&#29616;&#22312;&#21307;&#30103;&#12289;&#37329;&#34701;&#25110;&#24037;&#19994;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#36328;&#35774;&#22791;FL&#25552;&#20986;&#20102;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#65292;&#20294;&#24456;&#23569;&#26377;&#36924;&#30495;&#30340;&#20581;&#24247;&#21307;&#30103;&#36328;&#35774;&#22791;FL&#25968;&#25454;&#38598;&#23384;&#22312;&#65292;&#20174;&#32780;&#20943;&#32531;&#20102;&#36825;&#19968;&#20851;&#38190;&#24212;&#29992;&#30340;&#31639;&#27861;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#35774;&#22791;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;FLamby&#65288;&#24744;&#30340;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#20805;&#36275;&#22522;&#20934;&#65289;&#65292;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#24357;&#21512;&#36328;&#35774;&#22791;FL&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;FLamby&#21253;&#25324;7&#20010;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#33258;&#28982;&#25286;&#20998;&#65292;&#28085;&#30422;&#22810;&#20010;&#20219;&#21153;&#12289;&#27169;&#24577;&#21644;&#25968;&#25454;&#37327;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#37197;&#26377;&#22522;&#32447;&#35757;&#32451;&#20195;&#30721;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#36824;&#23545;FLamby&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL. FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26641;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#36798;&#21040;&#39640;&#31934;&#24230;&#30340;&#20998;&#31867;&#25928;&#26524;&#65307;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.08675</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#26641;&#31181;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree species classification from hyperspectral data using graph-regularized neural networks. (arXiv:2208.08675v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26641;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#36798;&#21040;&#39640;&#31934;&#24230;&#30340;&#20998;&#31867;&#25928;&#26524;&#65307;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26641;&#31181;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#22522;&#20110;&#36229;&#20687;&#32032;&#30340;&#20998;&#21106;&#29992;&#20110;&#22270;&#20687;&#26500;&#24314;&#12289;&#22522;&#20110;&#20687;&#32032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#20934;&#30830;&#19988;&#30495;&#23454;&#30340;&#20998;&#31867;&#22320;&#22270;&#65288;&#27169;&#25311;&#26641;&#20896;&#65289;&#12290;&#19982;&#22810;&#31181;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;GRNN&#22312;&#26631;&#20934;&#30340;&#21360;&#24230;&#29577;&#31859;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26032;&#30340;&#22312;&#27861;&#23646;&#22317;&#20122;&#37027;&#26862;&#26519;&#25910;&#38598;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#65288;&#24403;&#26631;&#27880;&#20687;&#32032;&#23569;&#20110;1&#65285;&#26102;&#65289;&#65292;&#36798;&#21040;&#39640;&#36798;92&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;GRNN&#22312;&#21322;&#30417;&#30563;&#26041;&#27861;&#26041;&#38754;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#22810;&#27425;&#38543;&#26426;&#25277;&#26679;&#35757;&#32451;&#26631;&#35760;&#20687;&#32032;&#30340;&#37325;&#22797;&#35797;&#39564;&#20013;&#34920;&#29616;&#20986;&#23567;&#30340;&#31934;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel graph-regularized neural network (GRNN) algorithm for tree species classification. The proposed algorithm encompasses superpixel-based segmentation for graph construction, a pixel-wise neural network classifier, and the label propagation technique to generate an accurate and realistic (emulating tree crowns) classification map on a sparsely annotated data set. GRNN outperforms several state-of-the-art techniques not only for the standard Indian Pines HSI but also achieves a high classification accuracy (approx. 92%) on a new HSI data set collected over the heterogeneous forests of French Guiana (FG) when less than 1% of the pixels are labeled. We further show that GRNN is competitive with the state-of-the-art semi-supervised methods and exhibits a small deviation in accuracy for different numbers of training samples and over repeated trials with randomly sampled labeled pixels for training.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#32467;&#26524;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30456;&#20284;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.13765</link><description>&lt;p&gt;
&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#65306;&#22522;&#20110;&#29420;&#31435;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Classification of Thyroid Nodules on Ultrasound: Validation on an Independent Dataset. (arXiv:2207.13765v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#32467;&#26524;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30456;&#20284;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#23558;&#20808;&#21069;&#39564;&#35777;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#26032;&#30340;&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#26041;&#27861;&#65306;&#20808;&#21069;&#30340;&#30740;&#31350;&#21576;&#29616;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26816;&#27979;&#30002;&#29366;&#33146;&#32467;&#33410;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#20010;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#24694;&#24615;&#20998;&#31867;&#12290;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#20174;1278&#20010;&#32467;&#33410;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#26368;&#21021;&#20351;&#29992;99&#20010;&#19981;&#21516;&#30340;&#32467;&#33410;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35813;&#31639;&#27861;&#38543;&#21518;&#20351;&#29992;&#20102;378&#20010;&#32467;&#33410;&#30340;&#36229;&#22768;&#26426;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#26696;&#20363;&#30340;&#21046;&#36896;&#21830;&#21644;&#20135;&#21697;&#31867;&#22411;&#19981;&#21516;&#12290;&#35201;&#27714;4&#21517;&#32463;&#39564;&#20016;&#23500;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#35780;&#20272;&#32467;&#33410;&#65292;&#20197;&#19982;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;&#20351;&#29992;&#21442;&#25968;&#21270;&#12289;&#21452;&#27491;&#24577;&#20272;&#35745;&#35745;&#31639;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21644;4&#21517;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;AUC&#12290;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;AUC&#20026;0.69&#65288;95&#65285;CI&#65306;0.64 0.75&#65289;&#12290;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;AUC&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Objectives: The purpose is to apply a previously validated deep learning algorithm to a new thyroid nodule ultrasound image dataset and compare its performances with radiologists. Methods: Prior study presented an algorithm which is able to detect thyroid nodules and then make malignancy classifications with two ultrasound images. A multi-task deep convolutional neural network was trained from 1278 nodules and originally tested with 99 separate nodules. The results were comparable with that of radiologists. The algorithm was further tested with 378 nodules imaged with ultrasound machines from different manufacturers and product types than the training cases. Four experienced radiologists were requested to evaluate the nodules for comparison with deep learning. Results: The Area Under Curve (AUC) of the deep learning algorithm and four radiologists were calculated with parametric, binormal estimation. For the deep learning algorithm, the AUC was 0.69 (95% CI: 0.64 0.75). The AUC of ra
&lt;/p&gt;</description></item><item><title>BigIssue&#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#28431;&#27934;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#28431;&#27934;&#23450;&#20301;&#33021;&#21147;&#21644;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#26356;&#22909;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2207.10739</link><description>&lt;p&gt;
BigIssue&#65306;&#19968;&#20010;&#30495;&#23454;&#30340;&#28431;&#27934;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BigIssue: A Realistic Bug Localization Benchmark. (arXiv:2207.10739v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10739
&lt;/p&gt;
&lt;p&gt;
BigIssue&#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#28431;&#27934;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#28431;&#27934;&#23450;&#20301;&#33021;&#21147;&#21644;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#26356;&#22909;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#36827;&#27493;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#25105;&#20204;&#32534;&#20889;&#26356;&#22909;&#30340;&#20195;&#30721;&#65311;&#38543;&#30528;GPT-3&#21644;Bert&#31561;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#24320;&#22987;&#25506;&#32034;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;&#20195;&#30721;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#19978;&#65292;&#34429;&#28982;&#22312;&#21512;&#25104;&#25110;&#39640;&#24230;&#36807;&#28388;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#65292;&#22240;&#20026;&#23384;&#22312;&#23450;&#20301;&#28431;&#27934;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;BigIssue&#65306;&#19968;&#20010;&#30495;&#23454;&#30340;&#28431;&#27934;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20934;&#27979;&#35797;&#30340;&#30446;&#26631;&#26159;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23454;&#38469;&#21644;&#21512;&#25104;Java&#28431;&#27934;&#30340;&#36890;&#29992;&#22522;&#20934;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#27880;&#24847;&#21040;&#23436;&#25972;&#23384;&#20648;&#24211;&#19978;&#19979;&#25991;&#65292;&#28608;&#21169;&#25552;&#39640;&#27169;&#22411;&#30340;&#28431;&#27934;&#23450;&#20301;&#33021;&#21147;&#12290;&#24341;&#20837;BigIssue&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;&#25512;&#36827;&#28431;&#27934;&#23450;&#20301;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20174;&#32780;&#25552;&#39640;APR&#30340;&#24615;&#33021;&#65292;&#24182;&#26368;&#32456;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#26356;&#22909;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning tools progress, the inevitable question arises: How can machine learning help us write better code? With significant progress being achieved in natural language processing with models like GPT-3 and Bert, the applications of natural language processing techniques to code are starting to be explored. Most of the research has been focused on automatic program repair (APR), and while the results on synthetic or highly filtered datasets are promising, such models are hard to apply in real-world scenarios because of inadequate bug localization. We propose BigIssue: a benchmark for realistic bug localization. The goal of the benchmark is two-fold. We provide (1) a general benchmark with a diversity of real and synthetic Java bugs and (2) a motivation to improve bug localization capabilities of models through attention to the full repository context. With the introduction of BigIssue, we hope to advance the state of the art in bug localization, in turn improving APR perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39640;&#20998;&#36776;&#36136;&#35889;&#25968;&#25454;&#36827;&#34892;&#22810;&#23610;&#24230;&#27491;&#24358;&#23884;&#20837;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545; MS2 &#25968;&#25454;&#30340;&#20809;&#35889;&#24211;&#25628;&#32034;&#21644;&#26032;&#30340;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#21534;&#21520;&#23454;&#39564;&#20197;&#21450;&#21307;&#33647;&#21270;&#23398;&#23478;&#20851;&#27880;&#30340; 10 &#31181;&#21270;&#23398;&#24615;&#36136;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.02980</link><description>&lt;p&gt;
&#23567;&#20998;&#23376;&#36136;&#35889;&#22810;&#23610;&#24230;&#27491;&#24358;&#23884;&#20837;&#33021;&#22815;&#25552;&#39640;&#39640;&#20998;&#36776;&#36136;&#35889;&#25968;&#25454;&#30340;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution Mass Spectrometry Data. (arXiv:2207.02980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39640;&#20998;&#36776;&#36136;&#35889;&#25968;&#25454;&#36827;&#34892;&#22810;&#23610;&#24230;&#27491;&#24358;&#23884;&#20837;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545; MS2 &#25968;&#25454;&#30340;&#20809;&#35889;&#24211;&#25628;&#32034;&#21644;&#26032;&#30340;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#21534;&#21520;&#23454;&#39564;&#20197;&#21450;&#21307;&#33647;&#21270;&#23398;&#23478;&#20851;&#27880;&#30340; 10 &#31181;&#21270;&#23398;&#24615;&#36136;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26679;&#26412;&#20013;&#30340;&#23567;&#20998;&#23376;&#21487;&#29992;&#20110;&#25552;&#20379;&#20851;&#20110;&#30142;&#30149;&#29366;&#24577;&#12289;&#29615;&#22659;&#27602;&#32032;&#12289;&#22825;&#28982;&#20135;&#29289;&#33647;&#29289;&#21457;&#29616;&#21450;&#35768;&#22810;&#20854;&#20182;&#24212;&#29992;&#30340;&#20449;&#24687;&#12290;&#20018;&#32852;&#36136;&#35889;&#65288;MS2&#65289;&#26159;&#20102;&#35299;&#23567;&#20998;&#23376;&#28151;&#21512;&#29289;&#32452;&#25104;&#30340;&#20027;&#35201;&#30028;&#38754;&#65292;&#23427;&#20135;&#29983;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#28789;&#25935;&#24230;&#21644;&#30334;&#19975;&#20998;&#20043;&#19968;&#65288;ppm&#32423;&#21035;&#65289;&#30340;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#38024;&#23545; MS2 &#36136;&#35889;&#25968;&#25454;&#20013;&#36136;&#37327;&#25968;&#25454;&#30340;&#22810;&#23610;&#24230;&#27491;&#24358;&#23884;&#20837;&#65292;&#26088;&#22312;&#28385;&#36275;&#20174;&#23436;&#25972; MS2 &#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#65292;&#25105;&#20204;&#20026;&#20809;&#35889;&#22270;&#20070;&#39302;&#25628;&#32034;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#36825;&#26159;&#35780;&#20272; MS2 &#25968;&#25454;&#30340;&#26631;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#36890;&#36807; MS2 &#25968;&#25454;&#36827;&#34892;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#8212;&#8212;&#35813;&#20219;&#21153;&#22312;&#39640;&#36890;&#37327; MS2 &#23454;&#39564;&#20013;&#20855;&#26377;&#33258;&#28982;&#24212;&#29992;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22312;&#21307;&#33647;&#21270;&#23398;&#23478;&#20248;&#20808;&#32771;&#34385;&#30340; 10 &#31181;&#21270;&#23398;&#24615;&#36136;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#26032;&#21270;&#21512;&#29289;&#24179;&#22343; R&#178; &#20540;&#20026; 80&#65285;&#12290;&#25105;&#20204;&#20351;&#29992;&#38477;&#32500;&#25216;&#26415;&#24182;&#36827;&#34892;&#23454;&#39564;&#26469;&#30740;&#31350;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small molecules in biological samples are studied to provide information about disease states, environmental toxins, natural product drug discovery, and many other applications. The primary window into the composition of small molecule mixtures is tandem mass spectrometry (MS2), which produces data that are of high sensitivity and part per million resolution. We adopt multi-scale sinusoidal embeddings of the mass data in MS2 designed to meet the challenge of learning from the full resolution of MS2 data. Using these embeddings, we provide a new state of the art model for spectral library search, the standard task for initial evaluation of MS2 data. We also introduce a new task, chemical property prediction from MS2 data, that has natural applications in high-throughput MS2 experiments and show that an average $R^2$ of 80\% for novel compounds can be achieved across 10 chemical properties prioritized by medicinal chemists. We use dimensionality reduction techniques and experiments with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65288;DMSC&#65289;&#65292;&#23427;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#22810;&#31181;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;&#21253;&#25324;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#12289;&#21322;&#30417;&#30563;&#25104;&#23545;&#32422;&#26463;&#25439;&#22833;&#21644;&#22810;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#12290;&#20854;&#20013;&#65292;KL&#25955;&#24230;&#22522;&#30784;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#34987;&#29305;&#21035;&#24378;&#35843;&#29992;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20849;&#21516;&#34920;&#31034;&#65292;&#20197;&#36798;&#21040;&#24322;&#26500;&#29305;&#24449;&#20248;&#21270;&#12289;&#22810;&#35270;&#22270;&#21152;&#26435;&#21644;&#32858;&#31867;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.04949</link><description>&lt;p&gt;
&#24102;&#26377;&#26679;&#26412;&#25104;&#23545;&#32422;&#26463;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#21322;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-View Semi-Supervised Clustering with Sample Pairwise Constraints. (arXiv:2206.04949v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65288;DMSC&#65289;&#65292;&#23427;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#22810;&#31181;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;&#21253;&#25324;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#12289;&#21322;&#30417;&#30563;&#25104;&#23545;&#32422;&#26463;&#25439;&#22833;&#21644;&#22810;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#12290;&#20854;&#20013;&#65292;KL&#25955;&#24230;&#22522;&#30784;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#34987;&#29305;&#21035;&#24378;&#35843;&#29992;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20849;&#21516;&#34920;&#31034;&#65292;&#20197;&#36798;&#21040;&#24322;&#26500;&#29305;&#24449;&#20248;&#21270;&#12289;&#22810;&#35270;&#22270;&#21152;&#26435;&#21644;&#32858;&#31867;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#22240;&#20854;&#38598;&#25104;&#22810;&#28304;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#36890;&#24120;&#24573;&#35270;&#20102;&#24369;&#30417;&#30563;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#30041;&#22810;&#20010;&#35270;&#22270;&#30340;&#29305;&#24449;&#23646;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#32858;&#31867;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65288;DMSC&#65289;&#65292;&#22312;&#32593;&#32476;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#20248;&#21270;&#19977;&#31181;&#25439;&#22833;&#65292;&#21253;&#25324;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#12289;&#21322;&#30417;&#30563;&#25104;&#23545;&#32422;&#26463;&#25439;&#22833;&#21644;&#22810;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KL&#25955;&#24230;&#22522;&#30784;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#25439;&#22833;&#34987;&#24378;&#21152;&#22312;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20849;&#21516;&#34920;&#31034;&#19978;&#65292;&#21516;&#26102;&#25191;&#34892;&#24322;&#26500;&#29305;&#24449;&#20248;&#21270;&#12289;&#22810;&#35270;&#22270;&#21152;&#26435;&#21644;&#32858;&#31867;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#23558;&#25104;&#23545;&#32422;&#26463;&#25972;&#21512;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted much attention thanks to the capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally overlook the significance of weakly-supervised information and fail to preserve the feature properties of multiple views, thus resulting in unsatisfactory clustering performance. To address these issues, in this paper, we propose a novel Deep Multi-view Semi-supervised Clustering (DMSC) method, which jointly optimizes three kinds of losses during networks finetuning, including multi-view clustering loss, semi-supervised pairwise constraint loss and multiple autoencoders reconstruction loss. Specifically, a KL divergence based multi-view clustering loss is imposed on the common representation of multi-view data to perform heterogeneous feature optimization, multi-view weighting and clustering prediction simultaneously. Then, we innovatively propose to integrate pairwise constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;U-NO&#65292;&#19968;&#31181;U&#24418;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;&#65292;&#20801;&#35768;&#26356;&#28145;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#21033;&#29992;&#20989;&#25968;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#32467;&#26500;&#65292;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#24555;&#36895;&#35757;&#32451;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.11127</link><description>&lt;p&gt;
U-NO&#65306;U&#24418;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
U-NO: U-shaped Neural Operators. (arXiv:2204.11127v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;U-NO&#65292;&#19968;&#31181;U&#24418;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;&#65292;&#20801;&#35768;&#26356;&#28145;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#21033;&#29992;&#20989;&#25968;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#32467;&#26500;&#65292;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#24555;&#36895;&#35757;&#32451;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#23558;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#20102;&#26080;&#38480;&#32500;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20363;&#22914;&#20989;&#25968;&#31354;&#38388;&#12290;&#20808;&#21069;&#30340;&#31070;&#32463;&#31639;&#23376;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#36825;&#26679;&#30340;&#26144;&#23556;&#65292;&#24182;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#26041;&#38754;&#21462;&#24471;&#20102;&#31354;&#21069;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#23427;&#20204;&#19982;&#20840;&#36830;&#25509;&#26550;&#26500;&#38750;&#24120;&#25509;&#36817;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#21463;&#21040;&#39640;&#20869;&#23384;&#20351;&#29992;&#29575;&#30340;&#22256;&#25200;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#38480;&#20110;&#27973;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;U-NO&#65288;U&#24418;&#31070;&#32463;&#31639;&#23376;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;U&#24418;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;&#65292;&#20801;&#35768;&#26356;&#28145;&#30340;&#31070;&#32463;&#31639;&#23376;&#12290;U-NO&#21033;&#29992;&#20989;&#25968;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#24555;&#36895;&#35757;&#32451;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;U-NO&#22312;PDE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;Darcy&#27969;&#21160;&#23450;&#24459;&#21644;Navier-Stokes&#26041;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;U-NO&#22312;Darcy&#27969;&#21644;Tu&#30340;&#24179;&#22343;&#39044;&#27979;&#25913;&#36827;&#20998;&#21035;&#36798;&#21040;26&#65285;&#21644;44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g., function spaces. Prior works on neural operators proposed a series of novel methods to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-NO results in an average of 26% and 44% prediction improvement on Darcy's flow and tu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;BCQ&#12289;CQL&#21644;TD3-BC&#31639;&#27861;&#26377;&#25928;&#22320;&#31649;&#29702;&#34880;&#31958;&#27700;&#24179;&#65292;&#20811;&#26381;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.03376</link><description>&lt;p&gt;
&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#26356;&#23433;&#20840;&#22320;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;BCQ&#12289;CQL&#21644;TD3-BC&#31639;&#27861;&#26377;&#25928;&#22320;&#31649;&#29702;&#34880;&#31958;&#27700;&#24179;&#65292;&#20811;&#26381;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28151;&#21512;&#38381;&#29615;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#23558;&#26159;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#25252;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;&#36825;&#20123;&#35774;&#22791;&#36890;&#24120;&#21033;&#29992;&#31616;&#21333;&#30340;&#25511;&#21046;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#33008;&#23707;&#32032;&#21058;&#37327;&#26469;&#32500;&#25345;&#34880;&#31958;&#27700;&#24179;&#22312;&#20581;&#24247;&#33539;&#22260;&#20869;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#29992;&#20316;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#20123;&#35774;&#22791;&#30340;&#34880;&#31958;&#25511;&#21046;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#19982;&#20256;&#32479;&#25511;&#21046;&#31639;&#27861;&#30456;&#27604;&#38477;&#20302;&#20102;&#24739;&#32773;&#39118;&#38505;&#65292;&#24182;&#25913;&#21892;&#20102;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#65292;&#20294;&#24448;&#24448;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#31283;&#23450;&#65292;&#23548;&#33268;&#36873;&#25321;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#24320;&#21457;&#26377;&#25928;&#30340;&#21058;&#37327;&#31574;&#30053;&#65292;&#26080;&#38656;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#28508;&#22312;&#21361;&#38505;&#30340;&#24739;&#32773;&#20132;&#20114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BCQ&#12289;CQL&#21644;TD3-BC&#22312;&#31649;&#29702;FDA&#25209;&#20934;&#30340;30&#21517;&#34394;&#25311;&#30149;&#20154;&#30340;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (T1D). These devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. Online reinforcement learning (RL) has been utilised as a method for further enhancing glucose control in these devices. Previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. This work presents an evaluation of offline RL for developing effective dosing policies without the need for potentially dangerous patient interaction during training. This paper examines the utility of BCQ, CQL and TD3-BC in managing the blood glucose of the 30 virtual patients available within the FDA-approved
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#29305;&#24449;&#21450;&#20854;&#23545;&#36801;&#31227;&#23398;&#20064;&#30340;&#25903;&#25345;&#33021;&#21147;&#65292;&#24182;&#22312;&#19968;&#20010;&#20687;&#32032;&#23548;&#33322;&#29615;&#22659;&#20013;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#21516;&#36741;&#21161;&#25439;&#22833;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2203.15955</link><description>&lt;p&gt;
&#25506;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Investigating the Properties of Neural Network Representations in Reinforcement Learning. (arXiv:2203.15955v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#29305;&#24449;&#21450;&#20854;&#23545;&#36801;&#31227;&#23398;&#20064;&#30340;&#25903;&#25345;&#33021;&#21147;&#65292;&#24182;&#22312;&#19968;&#20010;&#20687;&#32032;&#23548;&#33322;&#29615;&#22659;&#20013;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#21516;&#36741;&#21161;&#25439;&#22833;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#29305;&#24449;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#30340;&#26089;&#26399;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22266;&#23450;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#36798;&#21040;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22914;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24565;&#26159;&#20195;&#29702;&#35774;&#35745;&#32773;&#19981;&#24212;&#32534;&#30721;&#34920;&#31034;&#29305;&#24449;&#65292;&#32780;&#24212;&#35813;&#35753;&#25968;&#25454;&#27969;&#20915;&#23450;&#34920;&#31034;&#30340;&#29305;&#24449;&#8212;&#8212;&#22312;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#26696;&#19979;&#65292;&#33391;&#22909;&#30340;&#34920;&#31034;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#20010;&#35270;&#35282;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#23545;&#36229;&#36807;25,000&#20010;&#20195;&#29702;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#25903;&#25345;&#24378;&#21270;&#23398;&#20064;&#20013;&#36801;&#31227;&#24615;&#30340;&#34920;&#31034;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#27979;&#37327;&#20102;&#20845;&#20010;&#34920;&#24449;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#20687;&#32032;&#30340;&#23548;&#33322;&#29615;&#22659;&#20013;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#21516;&#36741;&#21161;&#25439;&#22833;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#20195;&#29702;&#65292;&#21253;&#25324;&#28304;&#21644;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation -- good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25 thousand agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and tran
&lt;/p&gt;</description></item><item><title>ADATIME&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#20854;&#25152;&#25552;&#20379;&#30340;&#26631;&#20934;&#21270;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#12289;&#29616;&#23454;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2203.08321</link><description>&lt;p&gt;
ADATIME&#65306;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data. (arXiv:2203.08321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08321
&lt;/p&gt;
&lt;p&gt;
ADATIME&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#20854;&#25152;&#25552;&#20379;&#30340;&#26631;&#20934;&#21270;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#12289;&#29616;&#23454;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26088;&#22312;&#38024;&#23545;&#21487;&#33021;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#65288;&#36716;&#31227;&#65289;&#30340;&#26410;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#22270;&#20687;&#25968;&#25454;&#19978;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#24037;&#20316;&#23384;&#22312;&#35780;&#20272;&#26041;&#26696;&#12289;&#25968;&#25454;&#38598;&#21644;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#32463;&#24120;&#29992;&#20110;&#27169;&#22411;&#36873;&#25321;&#65292;&#36825;&#36829;&#21453;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65288;AdaTime&#65289;&#26469;&#31995;&#32479;&#22320;&#21644;&#20844;&#27491;&#22320;&#35780;&#20272;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26631;&#20934;&#21270;&#20102;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25506;&#32034;&#26356;&#29616;&#23454;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#19981;&#21516;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#30340;&#36866;&#24212;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation methods aim to generalize well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite (AdaTime) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting st
&lt;/p&gt;</description></item><item><title>PyNET-QxQ&#26159;&#19968;&#31181;&#29992;&#20110;QxQ Bayer CFA&#27169;&#24335;&#30340;&#36731;&#37327;&#32423;&#35299;&#26512;&#27169;&#22411;&#65292;&#21442;&#25968;&#23569;&#20110;&#21407;&#22987;PyNET&#30340;2.5&#65285;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#28176;&#36827;&#33976;&#39311;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PyNET-QxQ&#22312;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.04314</link><description>&lt;p&gt;
PyNET-QxQ: &#19968;&#31181;&#39640;&#25928;&#30340;&#29992;&#20110;CMOS&#22270;&#20687;&#20256;&#24863;&#22120;&#20013;QxQ Bayer&#27169;&#24335;&#35299;&#26512;&#30340;PyNET&#21464;&#20307;&#65288;arXiv:2203.04314v2 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
PyNET-QxQ: An Efficient PyNET Variant for QxQ Bayer Pattern Demosaicing in CMOS Image Sensors. (arXiv:2203.04314v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04314
&lt;/p&gt;
&lt;p&gt;
PyNET-QxQ&#26159;&#19968;&#31181;&#29992;&#20110;QxQ Bayer CFA&#27169;&#24335;&#30340;&#36731;&#37327;&#32423;&#35299;&#26512;&#27169;&#22411;&#65292;&#21442;&#25968;&#23569;&#20110;&#21407;&#22987;PyNET&#30340;2.5&#65285;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#28176;&#36827;&#33976;&#39311;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PyNET-QxQ&#22312;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#30456;&#26426;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#65288;ISP&#65289;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#19987;&#19994;DSLR&#30456;&#26426;&#30456;&#23218;&#32654;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#35201;&#27714;&#36890;&#24120;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#31227;&#21160;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#29616;&#20195;&#31227;&#21160;&#30456;&#26426;&#37319;&#29992;&#38750;Bayer&#33394;&#24425;&#28388;&#33394;&#38453;&#21015;&#65288;CFA&#65289;&#65292;&#22914;Quad Bayer&#65292;Nona Bayer&#21644;QxQ Bayer&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ISP&#65288;&#25110;&#35299;&#30721;&#65289;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#26631;&#20934;Bayer CFAs&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PyNET-QxQ&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;QxQ Bayer CFA&#27169;&#24335;&#35774;&#35745;&#30340;&#36731;&#37327;&#32423;&#35299;&#26512;&#27169;&#22411;&#65292;&#23427;&#26159;&#20174;&#21407;&#22987;&#30340;PyNET&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#31216;&#20026;&#28176;&#36827;&#33976;&#39311;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#20943;&#23569;&#30340;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;PyNET-QxQ&#30340;&#21442;&#25968;&#23569;&#20110;&#21407;&#22987;PyNET&#30340;2.5&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#20351;&#29992;&#21407;&#22411;QxQ&#30456;&#26426;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;QxQ&#22270;&#20687;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PyNET-QxQ&#22312;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based image signal processor (ISP) models for mobile cameras can generate high-quality images that rival those of professional DSLR cameras. However, their computational demands often make them unsuitable for mobile settings. Additionally, modern mobile cameras employ non-Bayer color filter arrays (CFA) such as Quad Bayer, Nona Bayer, and QxQ Bayer to enhance image quality, yet most existing deep learning-based ISP (or demosaicing) models focus primarily on standard Bayer CFAs. In this study, we present PyNET-QxQ, a lightweight demosaicing model specifically designed for QxQ Bayer CFA patterns, which is derived from the original PyNET. We also propose a knowledge distillation method called progressive distillation to train the reduced network more effectively. Consequently, PyNET-QxQ contains less than 2.5% of the parameters of the original PyNET while preserving its performance. Experiments using QxQ images captured by a proto type QxQ camera sensor show that PyNET-QxQ o
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23601;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26694;&#26550;&#29992;&#20110;&#27604;&#36739;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2201.09354</link><description>&lt;p&gt;
3D Object Detection Models and Methods &#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23601;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26694;&#26550;&#29992;&#20110;&#27604;&#36739;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#28872;&#38656;&#27714;&#21644;3D&#20256;&#24863;&#22120;&#24191;&#27867;&#26222;&#21450;&#65292;&#19981;&#26029;&#25512;&#21160;&#30528;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#30740;&#20102;2012&#24180;&#33267;2021&#24180;&#38388;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21253;&#25324;&#36755;&#20837;&#25968;&#25454;&#12289;&#25968;&#25454;&#34920;&#31034;&#21644;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#23454;&#38469;&#26816;&#27979;&#27169;&#22359;&#30340;&#23436;&#25972;&#27969;&#31243;&#12290;&#20171;&#32461;&#20102;&#22522;&#26412;&#27010;&#24565;&#65292;&#20851;&#27880;&#20102;&#36807;&#21435;&#21313;&#24180;&#37324;&#28044;&#29616;&#30340;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#20026;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#26694;&#26550;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#27963;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#23558;3D&#30446;&#26631;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.03860</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#39640;&#26031;&#21270;&#23618;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;GAN&#12289;&#26631;&#20934;&#21270;&#27969;&#21644;&#25193;&#25955;&#27169;&#22411;&#26159;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#19981;&#36866;&#23450;&#24615;&#24182;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36870;&#25512;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#24352;&#37327;&#21487;&#33021;&#20250;&#20174;&#26399;&#26395;&#30340;&#39640;&#32500;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#20013;&#33073;&#31163;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#30340;&#27491;&#21521;&#27169;&#22411;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20250;&#23548;&#33268;&#20302;&#20445;&#30495;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26032;&#39062;&#30340;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#65292;&#20854;&#20013;&#20351;&#29992;&#33258;&#23450;&#20041;&#25805;&#20316;&#31526;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#25311;&#35758;&#30340;&#23618;&#23558;&#36870;&#38382;&#39064;&#32422;&#26463;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21453;&#28436;&#20219;&#21153;&#65288;&#21387;&#32553;&#24863;&#30693;MRI&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#20934;&#30830;&#24230;&#21463;&#38480;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21453;&#28436;&#38382;&#39064;&#8220;eikonal tomography&#8221;&#65289;&#19978;&#20351;&#29992;&#20004;&#31181;&#20856;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2112.01404</link><description>&lt;p&gt;
LOGEN&#65306;&#22522;&#20110;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#30340;&#33258;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#22312;&#23569;&#26679;&#26412;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#38754;&#23618;&#38754;&#25551;&#36848;&#65292;&#20854;&#23384;&#22312;&#25511;&#21046;&#20869;&#23481;&#36873;&#25321;&#22256;&#38590;&#21644;&#20302;&#20445;&#30495;&#24230;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#26469;&#20419;&#36827;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#37327;&#36739;&#22823;&#65292;&#36825;&#20351;&#24471;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#36923;&#36753;&#24418;&#24335;&#65288;&#22914;20/100&#31181;&#23376;&#65289; &#65292;&#24182;&#21033;&#29992;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#20934;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#23454;&#29616;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#24471;PredProp&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#27604;Adam&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;PredProp&#21487;&#20197;&#36890;&#36807;&#35823;&#24046;&#31934;&#24230;&#25552;&#39640;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#21487;&#20197;&#20998;&#35299;&#31934;&#24230;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2111.08792</link><description>&lt;p&gt;
PredProp: &#24102;&#31934;&#24230;&#21152;&#26435;&#39044;&#27979;&#32534;&#30721;&#30340;&#21452;&#21521;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#23454;&#29616;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#24471;PredProp&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#27604;Adam&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;PredProp&#21487;&#20197;&#36890;&#36807;&#35823;&#24046;&#31934;&#24230;&#25552;&#39640;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#21487;&#20197;&#20998;&#35299;&#31934;&#24230;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20256;&#25773;&#30340;&#35823;&#24046;&#21644;&#31070;&#32463;&#27963;&#21160;&#30340;&#31934;&#24230;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;(PCN)&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#30001;&#20110;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#19982;Fisher&#20449;&#24687;&#30697;&#38453;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;PredProp&#23454;&#29616;&#20102;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#23494;&#38598;&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#31616;&#21333;&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;PredProp&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#37197;&#32622;&#20013;&#65292;PredProp&#34920;&#29616;&#20248;&#20110;Adam&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20248;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#20110;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21463;&#30410;&#20110;&#20351;&#29992;PredProp&#30340;&#35823;&#24046;&#31934;&#24230;&#12290;&#30001;&#20110;&#23618;&#27425;&#39044;&#27979;&#32534;&#30721;&#23618;&#26159;&#36890;&#36807;&#23616;&#37096;&#35823;&#24046;&#21333;&#29420;&#20248;&#21270;&#30340;&#65292;&#25152;&#20197;&#25152;&#38656;&#31934;&#24230;&#22312;&#23618;&#27425;&#32467;&#26500;&#19978;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex
&lt;/p&gt;</description></item><item><title>CaloFlow v2&#26159;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#39640;&#20445;&#30495;&#37327;&#33021;&#22120;&#28107;&#28020;&#20223;&#30495;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#21407;&#29256;&#23558;&#28107;&#28020;&#29983;&#25104;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;500&#20493;&#65292;&#24182;&#19988;&#22312;&#20445;&#30495;&#24230;&#26041;&#38754;&#22823;&#22823;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.11377</link><description>&lt;p&gt;
CaloFlow II: &#37319;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#19968;&#27493;&#25552;&#39640;&#36895;&#24230;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#37327;&#33021;&#22120;&#28107;&#28020;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
CaloFlow II: Even Faster and Still Accurate Generation of Calorimeter Showers with Normalizing Flows. (arXiv:2110.11377v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11377
&lt;/p&gt;
&lt;p&gt;
CaloFlow v2&#26159;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#39640;&#20445;&#30495;&#37327;&#33021;&#22120;&#28107;&#28020;&#20223;&#30495;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#21407;&#29256;&#23558;&#28107;&#28020;&#29983;&#25104;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;500&#20493;&#65292;&#24182;&#19988;&#22312;&#20445;&#30495;&#24230;&#26041;&#38754;&#22823;&#22823;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#39640;&#20445;&#30495;&#37327;&#33021;&#22120;&#28107;&#28020;&#20223;&#30495;&#29983;&#25104;&#27169;&#22411;CaloFlow&#12290;&#29616;&#22312;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CaloFlow v2&#65292;&#23427;&#26159;&#25105;&#20204;&#21407;&#26469;&#26694;&#26550;&#30340;&#25913;&#36827;&#29256;&#65292;&#30456;&#23545;&#20110;&#21407;&#29256;&#23558;&#28107;&#28020;&#29983;&#25104;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;500&#20493;&#12290;&#36825;&#31181;&#25913;&#36827;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;&#27010;&#29575;&#23494;&#24230;&#33976;&#39311;&#30340;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#26368;&#21021;&#29992;&#20110;ML&#39046;&#22495;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#24378;&#22823;&#30340;&#26032;&#25439;&#22833;&#39033;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#36825;&#31181;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#65288;&#24179;&#22343;&#22270;&#20687;&#12289;&#39640;&#32423;&#29305;&#24449;&#30340;&#30452;&#26041;&#22270;&#65289;&#21644;&#23450;&#37327;&#65288;GEANT4&#21644;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#31867;&#22120;&#25351;&#26631;&#65289;&#24230;&#37327;&#35777;&#26126;CaloFlow v2&#20445;&#25345;&#20102;&#19982;&#21407;&#29256;&#30456;&#21516;&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;&#32467;&#26524;&#26159;&#19968;&#31181;&#37327;&#33021;&#22120;&#28107;&#28020;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36895;&#24230;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;&#27604;GEANT4&#24555;$10^4$&#20493;&#65289;&#65292;&#24182;&#19988;&#22312;&#20445;&#30495;&#24230;&#26041;&#38754;&#22823;&#22823;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we introduced CaloFlow, a high-fidelity generative model for GEANT4 calorimeter shower emulation based on normalizing flows. Here, we present CaloFlow v2, an improvement on our original framework that speeds up shower generation by a further factor of 500 relative to the original. The improvement is based on a technique called Probability Density Distillation, originally developed for speech synthesis in the ML literature, and which we develop further by introducing a set of powerful new loss terms. We demonstrate that CaloFlow v2 preserves the same high fidelity of the original using qualitative (average images, histograms of high level features) and quantitative (classifier metric between GEANT4 and generated samples) measures. The result is a generative model for calorimeter showers that matches the state-of-the-art in speed (a factor of $10^4$ faster than GEANT4) and greatly surpasses the previous state-of-the-art in fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer GAN&#29983;&#25104;&#31526;&#21495;&#25512;&#29702;&#38382;&#39064;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#29992;&#20110;&#26367;&#20195;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#20462;&#25913;&#30446;&#26631;&#20998;&#24067;&#65292;&#36798;&#21040;&#26356;&#22909;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.10054</link><description>&lt;p&gt;
&#21033;&#29992;Transformer GAN&#29983;&#25104;&#31526;&#21495;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generating Symbolic Reasoning Problems with Transformer GANs. (arXiv:2110.10054v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer GAN&#29983;&#25104;&#31526;&#21495;&#25512;&#29702;&#38382;&#39064;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#29992;&#20110;&#26367;&#20195;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#20462;&#25913;&#30446;&#26631;&#20998;&#24067;&#65292;&#36798;&#21040;&#26356;&#22909;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#30340;GAN&#21644;Wasserstein GAN&#30340;&#33021;&#21147;&#65292;&#22312;&#31526;&#21495;&#25512;&#29702;&#39046;&#22495;&#29983;&#25104;&#26377;&#24847;&#20041;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38382;&#39064;&#39046;&#22495;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#31526;&#21495;&#25968;&#23398;&#21644;&#39564;&#35777;&#20013;&#30340;&#26102;&#38388;&#35268;&#33539;&#12290;&#21363;&#20351;&#27809;&#26377;&#33258;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#20063;&#20250;&#20135;&#29983;&#35821;&#27861;&#27491;&#30830;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#29992;&#20316;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#26367;&#20195;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#38598;&#22826;&#23567;&#26080;&#27861;&#30452;&#25509;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#20351;&#29992;GAN&#30340;&#35774;&#32622;&#36824;&#20801;&#35768;&#25105;&#20204;&#20462;&#25913;&#30446;&#26631;&#20998;&#24067;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#29983;&#25104;&#22120;&#30446;&#26631;&#20013;&#28155;&#21152;&#20998;&#31867;&#22120;&#19981;&#30830;&#23450;&#24615;&#37096;&#20998;&#65292;&#25105;&#20204;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#27604;&#21407;&#22987;&#25968;&#25454;&#38598;&#26356;&#38590;&#20197;&#20026;&#19968;&#20010;&#26102;&#38388;&#36923;&#36753;&#20998;&#31867;&#22120;&#25152;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the capabilities of GANs and Wasserstein GANs equipped with Transformer encoders to generate sensible and challenging training data for symbolic reasoning domains. We conduct experiments on two problem domains where Transformers have been successfully applied recently: symbolic mathematics and temporal specifications in verification. Even without autoregression, our GAN models produce syntactically correct instances. We show that the generated data can be used as a substitute for real training data when training a classifier, and, especially, that training data can be generated from a dataset that is too small to be trained on directly. Using a GAN setting also allows us to alter the target distribution: We show that by adding a classifier uncertainty part to the generator objective, we obtain a dataset that is even harder to solve for a temporal logic classifier than our original dataset.
&lt;/p&gt;</description></item><item><title>CaloFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#33021;&#37327;&#37327;&#33021;&#22120;&#28107;&#28020;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#26032;&#26041;&#27861;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2106.05285</link><description>&lt;p&gt;
CaloFlow: &#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#33021;&#37327;&#37327;&#33021;&#22120;&#28107;&#28020;
&lt;/p&gt;
&lt;p&gt;
CaloFlow: Fast and Accurate Generation of Calorimeter Showers with Normalizing Flows. (arXiv:2106.05285v3 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05285
&lt;/p&gt;
&lt;p&gt;
CaloFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#33021;&#37327;&#37327;&#33021;&#22120;&#28107;&#28020;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#26032;&#26041;&#27861;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;&#65292;&#31216;&#20026;CaloFlow&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#65292;&#27491;&#21017;&#21270;&#27969;&#33021;&#22815;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#37325;&#26032;&#20135;&#29983;&#20986;&#22810;&#36890;&#36947;&#33021;&#37327;&#37327;&#33021;&#22120;&#28107;&#28020;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;GEANT4&#27169;&#25311;&#20197;&#21450;&#22522;&#20110;GANs&#21644;VAE&#30340;&#26368;&#26032;&#24555;&#36895;&#27169;&#25311;&#26694;&#26550;&#19981;&#21516;&#30340;&#26032;&#36873;&#25321;&#12290;&#38500;&#20102;&#21407;&#26377;&#30340;&#29289;&#29702;&#29305;&#24449;&#30340;&#30452;&#26041;&#22270;&#21644;&#33021;&#37327;&#37327;&#33021;&#22120;&#28107;&#28020;&#30340;&#22270;&#20687;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#29983;&#25104;&#24314;&#27169;&#36136;&#37327;&#30340;&#25351;&#26631;&#65306;&#23545;&#21306;&#20998;&#30495;&#23454;&#19982;&#34394;&#26500;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#34987;&#20998;&#31867;&#22120;&#20197;&#25509;&#36817;100&#65285;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#65292;&#32780;&#30001;CaloFlow&#29983;&#25104;&#30340;&#22270;&#20687;&#21017;&#26356;&#23481;&#26131;&#27450;&#39575;&#20998;&#31867;&#22120;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#26041;&#27861;&#65288;GAN&#21644;VAE&#65289;&#30456;&#27604;&#65292;&#27491;&#21017;&#21270;&#27969;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#21253;&#25324;&#65306;&#21487;&#36861;&#36394;&#30340;&#20284;&#28982;&#20989;&#25968;&#65292;&#31283;&#23450;&#21644;&#25910;&#25947;&#30340;&#35757;&#32451;&#20197;&#21450;&#20027;&#35201;&#30340;&#20248;&#21270;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CaloFlow, a fast detector simulation framework based on normalizing flows. For the first time, we demonstrate that normalizing flows can reproduce many-channel calorimeter showers with extremely high fidelity, providing a fresh alternative to computationally expensive GEANT4 simulations, as well as other state-of-the-art fast simulation frameworks based on GANs and VAEs. Besides the usual histograms of physical features and images of calorimeter showers, we introduce a new metric for judging the quality of generative modeling: the performance of a classifier trained to differentiate real from generated images. We show that GAN-generated images can be identified by the classifier with nearly 100% accuracy, while images generated from CaloFlow are better able to fool the classifier. More broadly, normalizing flows offer several advantages compared to other state-of-the-art approaches (GANs and VAEs), including: tractable likelihoods; stable and convergent training; and princ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#32423;&#23545;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2105.06903</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#20013;&#30340;&#21518;&#39564;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Posterior Regularization on Bayesian Hierarchical Mixture Clustering. (arXiv:2105.06903v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#32423;&#23545;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#29992;&#23618;&#32423;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;(HDPMM)&#26367;&#25442;&#20256;&#32479;&#30340;&#39640;&#26031;-&#39640;&#26031;&#26680;&#26469;&#23454;&#29616;&#20174;&#29238;&#33410;&#28857;&#21040;&#23376;&#33410;&#28857;&#30340;&#25193;&#25955;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20998;&#23618;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;BHMC&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#39640;&#33410;&#28857;&#26041;&#24046;&#30340;&#26641;&#65292;&#34920;&#26126;&#22312;&#36739;&#39640;&#23618;&#32423;&#20043;&#38388;&#30340;&#33410;&#28857;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#20998;&#31163;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21518;&#39564;&#27491;&#21017;&#21270;(Posterior Regularization)&#65292;&#23427;&#23545;&#27599;&#20010;&#23618;&#32423;&#30340;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22914;&#20309;&#23558;PR&#24212;&#29992;&#20110;BHMC&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#36827;BHMC&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesian hierarchical clustering by replacing conventional Gaussian-to-Gaussian kernels with a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-child diffusion in the generative process. However,BHMC may produce trees with high nodal variance, indicating weak separation between nodes at higher levels. To address this issue, we employ Posterior Regularization, which imposes max-margin constraints on nodes at every level to enhance cluster separation. We illustrate how to apply PR toBHMC and demonstrate its effectiveness in improving the BHMC model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#22312;GNN&#20013;&#38024;&#23545;&#25200;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;-&#21487;&#35782;&#21035;GNN&#21453;&#23545;&#25200;&#21160; (SIGNNAP) &#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#24418;&#24335;&#23398;&#20064;&#21487;&#38752;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2008.11416</link><description>&lt;p&gt;
&#38024;&#23545;&#25200;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Node Representations against Perturbations. (arXiv:2008.11416v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#22312;GNN&#20013;&#38024;&#23545;&#25200;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;-&#21487;&#35782;&#21035;GNN&#21453;&#23545;&#25200;&#21160; (SIGNNAP) &#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#24418;&#24335;&#23398;&#20064;&#21487;&#38752;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#22312;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;GNN&#30340;&#25104;&#21151;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#33410;&#28857;&#34920;&#31034;&#19978;&#30340;&#8220;&#24179;&#28369;&#8221;&#23646;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;GNN&#27169;&#22411;&#23545;&#22270;&#36755;&#20837;&#30340;&#25200;&#21160;&#24456;&#33030;&#24369;&#65292;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#19981;&#21487;&#38752;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;GNN&#20013;&#38024;&#23545;&#25200;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#33410;&#28857;&#34920;&#31034;&#24212;&#22312;&#36755;&#20837;&#30053;&#24494;&#25200;&#21160;&#26102;&#20445;&#25345;&#31283;&#23450;&#65292;&#24182;&#19988;&#24212;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20004;&#32773;&#20998;&#21035;&#34987;&#31216;&#20026;&#33410;&#28857;&#34920;&#31034;&#30340;&#8220;&#31283;&#23450;&#24615;&#8221;&#21644;&#8220;&#21487;&#35782;&#21035;&#24615;&#8221;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31283;&#23450;-&#21487;&#35782;&#21035;GNN&#21453;&#23545;&#25200;&#21160; (SIGNNAP) &#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21487;&#38752;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;SIGNNAP&#36890;&#36807;&#23545;&#27604;&#30446;&#26631;&#26469;&#24418;&#24335;&#21270;&#8220;&#31283;&#23450;&#24615;&#8221;&#21644;&#8220;&#21487;&#35782;&#21035;&#24615;&#8221;&#65292;&#24182;&#20445;&#30041;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent graph neural networks (GNN) has achieved remarkable performance in node representation learning. One key factor of GNN's success is the \emph{smoothness} property on node representations. Despite this, most GNN models are fragile to the perturbations on graph inputs and could learn unreliable node representations. In this paper, we study how to learn node representations against perturbations in GNN. Specifically, we consider that a node representation should remain stable under slight perturbations on the input, and node representations from different structures should be identifiable, which two are termed as the \emph{stability} and \emph{identifiability} on node representations, respectively. To this end, we propose a novel model called Stability-Identifiability GNN Against Perturbations (SIGNNAP) that learns reliable node representations in an unsupervised manner. SIGNNAP formalizes the \emph{stability} and \emph{identifiability} by a contrastive objective and preserves the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoarsenRank&#65292;&#20854;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#27169;&#22411;&#38169;&#35823;&#29305;&#21270;&#12290;&#23427;&#37319;&#29992;&#30001;&#31895;&#21040;&#31934;&#30340;&#26041;&#26696;&#26469;&#22788;&#29702;&#29992;&#25143;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#25490;&#21517;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#26356;&#22909;&#22320;&#27169;&#25311;&#32858;&#21512;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;CoarsenRank&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/1905.12341</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#40065;&#26834;&#30340;&#25490;&#21517;&#32858;&#21512;&#31639;&#27861;&#22312;&#27169;&#22411;&#38169;&#35823;&#29305;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast and Robust Rank Aggregation against Model Misspecification. (arXiv:1905.12341v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.12341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoarsenRank&#65292;&#20854;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#27169;&#22411;&#38169;&#35823;&#29305;&#21270;&#12290;&#23427;&#37319;&#29992;&#30001;&#31895;&#21040;&#31934;&#30340;&#26041;&#26696;&#26469;&#22788;&#29702;&#29992;&#25143;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#25490;&#21517;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#26356;&#22909;&#22320;&#27169;&#25311;&#32858;&#21512;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;CoarsenRank&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#32858;&#21512;&#31639;&#27861;&#65288;RA&#65289;&#29992;&#20110;&#23558;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#20559;&#22909;&#24635;&#32467;&#25104;&#19968;&#20010;&#24635;&#25490;&#24207;&#12290;&#22312;&#20551;&#35774;&#29992;&#25143;&#21516;&#36136;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#24037;&#20316;&#22522;&#26412;&#26080;&#35823;&#12290;&#20294;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#21516;&#36136;&#20551;&#35774;&#26080;&#27861;&#39564;&#35777;&#65292;RA&#30340;&#27169;&#22411;&#38169;&#35823;&#29305;&#21270;&#20986;&#29616;&#20102;&#12290;&#29616;&#26377;&#30340;&#20581;&#22766;RA&#36890;&#24120;&#37319;&#29992;&#25490;&#21517;&#27169;&#22411;&#25193;&#20805;&#26469;&#35299;&#37322;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;&#20854;&#20013;&#25910;&#38598;&#21040;&#30340;&#20559;&#22909;&#21487;&#20197;&#34987;&#35270;&#20026;&#21333;&#20010;&#38468;&#21152;&#21040;&#29702;&#24819;&#20559;&#22909;&#19978;&#30340;&#25200;&#21160;&#12290;&#30001;&#20110;&#20581;&#22766;&#38383;&#38383;&#20142;RAs&#22823;&#22810;&#20381;&#36182;&#20110;&#26576;&#20123;&#25200;&#21160;&#20551;&#35774;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#23545;&#19981;&#30830;&#23450;&#22122;&#22768;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoarsenRank&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#38169;&#35823;&#29305;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;CoarsenRank&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#65306;&#65288;1&#65289;CoarsenRank&#26159;&#38024;&#23545;&#36731;&#24494;&#30340;&#27169;&#22411;&#38169;&#35823;&#29305;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#20551;&#35774;&#19982;&#27169;&#22411;&#20551;&#35774;&#19968;&#33268;&#30340;&#29702;&#24819;&#20559;&#22909;&#20301;&#20110;&#25910;&#38598;&#21040;&#30340;&#20559;&#22909;&#30340;&#38468;&#36817;&#12290;&#65288;2&#65289;CoarsenRank&#37319;&#29992;&#30001;&#31895;&#21040;&#31934;&#30340;&#26041;&#26696;&#26469;&#25429;&#25417;&#25910;&#38598;&#21040;&#30340;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292; &#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#12290;&#65288;3&#65289;CoarsenRank&#21033;&#29992;&#25490;&#21517;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#26356;&#22909;&#22320;&#27169;&#25311;&#32858;&#21512;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;CoarsenRank&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In rank aggregation (RA), a collection of preferences from different users are summarized into a total order under the assumption of homogeneity of users. Model misspecification in RA arises since the homogeneity assumption fails to be satisfied in the complex real-world situation. Existing robust RAs usually resort to an augmentation of the ranking model to account for additional noises, where the collected preferences can be treated as a noisy perturbation of idealized preferences. Since the majority of robust RAs rely on certain perturbation assumptions, they cannot generalize well to agnostic noise-corrupted preferences in the real world. In this paper, we propose CoarsenRank, which possesses robustness against model misspecification. Specifically, the properties of our CoarsenRank are summarized as follows: (1) CoarsenRank is designed for mild model misspecification, which assumes there exist the ideal preferences (consistent with model assumption) that locates in a neighborhood o
&lt;/p&gt;</description></item></channel></rss>