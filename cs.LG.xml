<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22312;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#24182;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#25193;&#23637;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17165</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training. (arXiv:2306.17165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22312;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#24182;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#25193;&#23637;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#20854;&#20182;&#21518;&#32493;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20174;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#21363;&#21333;&#20010;&#22270;&#20687;&#38598;&#21512;&#20855;&#26377;&#22810;&#20010;&#20219;&#21153;&#26631;&#31614;&#12290;&#36825;&#31181;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#24456;&#23569;&#12289;&#35268;&#27169;&#23567;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#23558;&#24322;&#26500;&#25351;&#30340;&#26159;&#20855;&#26377;&#19981;&#21516;&#20219;&#21153;&#26631;&#31614;&#30340;&#22270;&#20687;&#38598;&#65292;&#25110;&#32773;&#26159;&#21333;&#19968;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#12290;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#22312;&#36825;&#31181;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#29992;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#20197;&#21333;&#19968;&#20219;&#21153;&#39044;&#35757;&#32451;&#20026;&#20027;&#23548;&#65292;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#35774;&#35745;&#29992;&#20110;&#19981;&#21516;&#30446;&#30340;&#30340;&#20027;&#27969;&#35270;&#35273;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#22810;&#20219;&#21153;&#27169;&#22411;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#21253;&#25324;&#25968;&#25454;&#20998;&#24067;&#12289;&#26550;&#26500;&#12289;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#37319;&#26679;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20462;&#25913;&#21644;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;(MoE)&#35270;&#35273;&#36716;&#25442;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a model that can perform multiple vision tasks and can be adapted to other downstream tasks efficiently. Despite considerable progress in multi-task learning, most efforts focus on learning from multi-label data: a single image set with multiple task labels. Such multi-label data sets are rare, small, and expensive. We say heterogeneous to refer to image sets with different task labels, or to combinations of single-task datasets. Few have explored training on such heterogeneous datasets. General-purpose vision models are still dominated by single-task pretraining, and it remains unclear how to scale up multi-task models by leveraging mainstream vision datasets designed for different purposes. The challenges lie in managing large intrinsic differences among vision tasks, including data distribution, architectures, task-specific modules, dataset scales, and sampling strategies. To address these challenges, we propose to modify and scale up mixture-of-experts (MoE) vision trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20840;&#23616;&#22797;&#26434;&#24230;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#26469;&#37325;&#26032;&#23457;&#35270;&#21644;&#21152;&#24378;&#20102;&#32479;&#35745;&#32858;&#21512;&#29702;&#35770;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17151</link><description>&lt;p&gt;
&#32479;&#35745;&#32858;&#21512;&#30340;&#26412;&#22320;&#39118;&#38505;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Local Risk Bounds for Statistical Aggregation. (arXiv:2306.17151v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20840;&#23616;&#22797;&#26434;&#24230;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#26469;&#37325;&#26032;&#23457;&#35270;&#21644;&#21152;&#24378;&#20102;&#32479;&#35745;&#32858;&#21512;&#29702;&#35770;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#21512;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#32473;&#23450;&#31867;&#21035;&#30340;&#22522;&#26412;&#39044;&#27979;&#22120;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#20960;&#20046;&#19982;&#26368;&#20339;&#39044;&#27979;&#22120;&#19968;&#26679;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#20013;&#65292;&#23545;&#31867;&#21035;&#30340;&#32467;&#26500;&#25110;&#30446;&#26631;&#30340;&#24615;&#36136;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#32858;&#21512;&#22312;&#39034;&#24207;&#21644;&#32479;&#35745;&#19978;&#19979;&#25991;&#20013;&#37117;&#26377;&#30740;&#31350;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#26377;&#19968;&#20123;&#37325;&#35201;&#30340;&#24046;&#24322;&#65292;&#20294;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#32463;&#20856;&#32467;&#26524;&#20855;&#26377;&#30456;&#21516;&#30340;&#20840;&#23616;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#29992;&#36739;&#23567;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#26367;&#25442;&#20840;&#23616;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#21644;&#21152;&#24378;&#20102;&#32479;&#35745;&#32858;&#21512;&#29702;&#35770;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#19968;&#20123;&#35777;&#26126;&#22522;&#20110;Catoni&#24341;&#20837;&#30340;PAC-Bayes&#26412;&#22320;&#21270;&#25216;&#26415;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;Leung&#21644;Barron&#25552;&#20986;&#30340;&#25351;&#25968;&#26435;&#37325;&#20272;&#35745;&#22120;&#30340;&#23616;&#37096;&#29256;&#26412;&#30340;&#32463;&#20856;&#30028;&#38480;&#65292;&#20197;&#21450;Q-&#32858;&#21512;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#26368;&#20248;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#25913;&#36827;&#20102;Dai&#65292;Rigollet&#21644;Zhang&#20851;&#20110;&#22266;&#23450;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of aggregation, the aim is to combine a given class of base predictors to achieve predictions nearly as accurate as the best one. In this flexible framework, no assumption is made on the structure of the class or the nature of the target. Aggregation has been studied in both sequential and statistical contexts. Despite some important differences between the two problems, the classical results in both cases feature the same global complexity measure. In this paper, we revisit and tighten classical results in the theory of aggregation in the statistical setting by replacing the global complexity with a smaller, local one. Some of our proofs build on the PAC-Bayes localization technique introduced by Catoni. Among other results, we prove localized versions of the classical bound for the exponential weights estimator due to Leung and Barron and deviation-optimal bounds for the Q-aggregation estimator. These bounds improve over the results of Dai, Rigollet and Zhang for fixed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#25991;&#23454;&#29616;&#20102;&#24452;&#21521;&#22522;&#20989;&#25968;&#25903;&#25345;&#21521;&#37327;&#26426;(RBF SVM)&#29992;&#20110;&#23545;&#23567;&#34892;&#26143;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;RBF SVM&#31639;&#27861;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#21442;&#25968;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.17138</link><description>&lt;p&gt;
&#21033;&#29992;&#24452;&#21521;&#22522;&#20989;&#25968;&#25903;&#25345;&#21521;&#37327;&#26426;&#23545;&#23567;&#34892;&#26143;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Orbit Classification of asteroids using implementation of radial Basis Function on Support Vector Machines. (arXiv:2306.17138v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#23454;&#29616;&#20102;&#24452;&#21521;&#22522;&#20989;&#25968;&#25903;&#25345;&#21521;&#37327;&#26426;(RBF SVM)&#29992;&#20110;&#23545;&#23567;&#34892;&#26143;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;RBF SVM&#31639;&#27861;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#21442;&#25968;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#24452;&#21521;&#22522;&#20989;&#25968;&#25903;&#25345;&#21521;&#37327;&#26426;(RBF SVM)&#23545;&#23567;&#34892;&#26143;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#30340;&#23454;&#29616;&#12290;&#23567;&#34892;&#26143;&#26159;&#37325;&#35201;&#30340;&#22825;&#20307;&#23545;&#35937;&#65292;&#20854;&#36712;&#36947;&#23545;&#20110;&#29702;&#35299;&#22826;&#38451;&#31995;&#21160;&#21147;&#23398;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22269;&#38469;&#22825;&#25991;&#23398;&#32852;&#21512;&#20250;&#32500;&#25252;&#30528;&#25552;&#20379;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#39564;&#30340;&#25968;&#25454;&#26723;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24212;&#29992;RBF SVM&#31639;&#27861;&#23545;&#23567;&#34892;&#26143;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RBF SVM&#31639;&#27861;&#23545;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#21442;&#25968;&#23545;RBF SVM&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#21442;&#25968;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#23567;&#34892;&#26143;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;RBF SVM&#31639;&#27861;&#22312;&#36825;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the implementation of radial Basis Function (RBF) Support Vector Machines (SVM) for classifying asteroid orbits. Asteroids are important astronomical objects, and their orbits play a crucial role in understanding the dynamics of the solar system. The International Astronomical Union maintains data archives that provide a playground to experiment with various machine-learning techniques. In this study, we explore the application of RBF SVM algorithm to classify asteroids. The results show that the RBF SVM algorithm provides a good efficiency and accuracy to the dataset. We also analyze the impact of various parameters on the performance of the RBF SVM algorithm and present the optimal parameter settings. Our study highlights the importance of using machine learning techniques for classifying asteroid orbits and the effectiveness of the RBF SVM algorithm in this regard.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#20197;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#21319;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17109</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#29983;&#25104;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Demographic Data Generation for Card Fraud Detection Using GANs. (arXiv:2306.17109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#20197;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#21319;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#12290;&#29983;&#25104;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#27450;&#35784;&#30340;&#21512;&#25104;&#20132;&#26131;&#25968;&#25454;&#30340;&#25216;&#26415;&#20063;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#21482;&#21253;&#21547;&#20132;&#26131;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#26102;&#38388;&#12289;&#22320;&#28857;&#21644;&#37329;&#39069;&#12290;&#36890;&#24120;&#19981;&#21253;&#21547;&#20010;&#20307;&#29992;&#25143;&#30340;&#29305;&#24449;&#65288;&#24180;&#40836;&#21644;&#24615;&#21035;&#20598;&#23572;&#20250;&#21253;&#21547;&#65289;&#12290;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#21487;&#33021;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#36229;&#36807;&#20854;&#20182;&#25104;&#29087;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65288;&#22914;&#24494;&#27169;&#25311;&#65289;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#20154;&#21475;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#29983;&#25104;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using machine learning models to generate synthetic data has become common in many fields. Technology to generate synthetic transactions that can be used to detect fraud is also growing fast. Generally, this synthetic data contains only information about the transaction, such as the time, place, and amount of money. It does not usually contain the individual user's characteristics (age and gender are occasionally included). Using relatively complex synthetic demographic data may improve the complexity of transaction data features, thus improving the fraud detection performance. Benefiting from developments of machine learning, some deep learning models have potential to perform better than other well-established synthetic data generation methods, such as microsimulation. In this study, we built a deep-learning Generative Adversarial Network (GAN), called DGGAN, which will be used for demographic data generation. Our model generates samples during model training, which we found importan
&lt;/p&gt;</description></item><item><title>ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17108</link><description>&lt;p&gt;
ManimML&#65306;&#29992;&#21160;&#30011;&#28436;&#31034;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17108
&lt;/p&gt;
&lt;p&gt;
ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#26032;&#39062;&#30340;ML&#31639;&#27861;&#30340;&#24037;&#20855;&#36824;&#36828;&#36828;&#33853;&#21518;&#12290;&#21160;&#30011;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21046;&#20316;&#20986;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#30340;&#31995;&#32479;&#30340;&#21560;&#24341;&#20154;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#27807;&#36890;ML&#31639;&#27861;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21160;&#30011;&#21270;ML&#31639;&#27861;&#30340;&#26041;&#27861;&#26159;&#25163;&#24037;&#21046;&#20316;&#31361;&#20986;&#29305;&#23450;&#31639;&#27861;&#25110;&#20351;&#29992;&#22797;&#26434;&#30340;&#36890;&#29992;&#21160;&#30011;&#36719;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ManimML&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20195;&#30721;&#20013;&#36731;&#26494;&#29983;&#25104;ML&#31639;&#27861;&#30340;&#21160;&#30011;&#12290;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;ML&#20174;&#19994;&#32773;&#23545;&#32534;&#31243;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#35201;&#27714;&#20182;&#20204;&#23398;&#20064;&#22797;&#26434;&#30340;&#21160;&#30011;&#36719;&#20214;&#12290;ManimML&#20855;&#26377;&#29087;&#24713;&#30340;&#35821;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#27169;&#20223;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;Pytorch&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#8220;&#31070;&#32463;&#25240;&#21472;&#8221;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#34920;&#38754;&#19978;&#25240;&#21472;&#30340;&#34920;&#31034;&#23454;&#38469;&#19978;&#20173;&#38544;&#34255;&#26377;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17105</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#23454;&#38469;&#19978;&#26159;&#25240;&#21472;&#30340;&#21527;&#65311;&#20851;&#20110;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#32454;&#31890;&#24230;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations. (arXiv:2306.17105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17105
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#8220;&#31070;&#32463;&#25240;&#21472;&#8221;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#34920;&#38754;&#19978;&#25240;&#21472;&#30340;&#34920;&#31034;&#23454;&#38469;&#19978;&#20173;&#38544;&#34255;&#26377;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#20805;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#8220;&#31070;&#32463;&#25240;&#21472;&#8221;&#29616;&#35937;&#65292;&#21363;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#26368;&#21518;&#19968;&#23618;&#34920;&#31034;&#20114;&#30456;&#25240;&#21472;&#22312;&#19968;&#36215;&#12290;&#36825;&#20284;&#20046;&#34920;&#26126;&#26368;&#21518;&#19968;&#23618;&#30340;&#34920;&#31034;&#23436;&#20840;&#30001;&#26631;&#31614;&#20915;&#23450;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#20998;&#24067;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#36825;&#19981;&#26159;&#19968;&#20010;&#23436;&#20840;&#30340;&#25551;&#36848;&#65292;&#34920;&#38754;&#19978;&#30340;&#25240;&#21472;&#38544;&#34255;&#20102;&#34920;&#31034;&#20013;&#30340;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21363;&#20351;&#34920;&#31034;&#34920;&#38754;&#19978;&#25240;&#21472;&#22312;&#19968;&#36215;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#21097;&#20313;&#30340;&#21464;&#24322;&#24615;&#33021;&#22815;&#24544;&#23454;&#32780;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#36755;&#20837;&#20998;&#24067;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#21482;&#20351;&#29992;5&#20010;&#31895;&#31890;&#24230;&#26631;&#31614;&#65288;&#23558;&#20004;&#20010;&#31867;&#21035;&#32452;&#21512;&#25104;&#19968;&#20010;&#36229;&#31867;&#65289;&#23545;CIFAR-10&#36827;&#34892;&#35757;&#32451;&#30452;&#21040;&#25910;&#25947;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#20174;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20013;&#37325;&#26500;&#20986;&#21407;&#22987;&#30340;10&#20010;&#31867;&#21035;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has observed an intriguing ''Neural Collapse'' phenomenon in well-trained neural networks, where the last-layer representations of training samples with the same label collapse into each other. This appears to suggest that the last-layer representations are completely determined by the labels, and do not depend on the intrinsic structure of input distribution. We provide evidence that this is not a complete description, and that the apparent collapse hides important fine-grained structure in the representations. Specifically, even when representations apparently collapse, the small amount of remaining variation can still faithfully and accurately captures the intrinsic structure of input distribution. As an example, if we train on CIFAR-10 using only 5 coarse-grained labels (by combining two classes into one super-class) until convergence, we can reconstruct the original 10-class labels from the learned representations via unsupervised clustering. The reconstructed labels a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#26426;&#22120;&#20154;&#36816;&#21160;&#25216;&#33021;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26174;&#33879;&#24615;&#20998;&#26512;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#21453;&#39304;&#29366;&#24577;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20851;&#33410;&#20301;&#32622;&#12289;&#37325;&#21147;&#21521;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#29366;&#24577;&#21487;&#20197;&#23454;&#29616;&#19982;&#20351;&#29992;&#25152;&#26377;&#29366;&#24577;&#30456;&#24403;&#30340;&#27493;&#24577;&#25216;&#33021;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17101</link><description>&lt;p&gt;
&#37492;&#23450;&#23398;&#20064;&#21160;&#20316;&#25216;&#33021;&#20013;&#37325;&#35201;&#30340;&#24863;&#35273;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Identifying Important Sensory Feedback for Learning Locomotion Skills. (arXiv:2306.17101v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#26426;&#22120;&#20154;&#36816;&#21160;&#25216;&#33021;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26174;&#33879;&#24615;&#20998;&#26512;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#21453;&#39304;&#29366;&#24577;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20851;&#33410;&#20301;&#32622;&#12289;&#37325;&#21147;&#21521;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#29366;&#24577;&#21487;&#20197;&#23454;&#29616;&#19982;&#20351;&#29992;&#25152;&#26377;&#29366;&#24577;&#30456;&#24403;&#30340;&#27493;&#24577;&#25216;&#33021;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#29366;&#24577;-&#21160;&#20316;&#26144;&#23556;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#21487;&#20197;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25216;&#33021;&#12290;&#34429;&#28982;&#29366;&#24577;&#35266;&#27979;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26174;&#33879;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#23450;&#37327;&#35780;&#20272;&#36890;&#36807;DRL&#23398;&#20064;&#21040;&#30340;&#21508;&#31181;&#21453;&#39304;&#29366;&#24577;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#23450;&#23545;&#20110;&#27493;&#24577;&#25216;&#33021;&#65288;&#21253;&#25324;&#24179;&#34913;&#24674;&#22797;&#12289;&#23567;&#36305;&#12289;&#36339;&#36291;&#12289;&#27493;&#36895;&#21644;&#22868;&#33150;&#65289;&#26368;&#37325;&#35201;&#30340;&#21453;&#39304;&#29366;&#24577;&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#20851;&#38190;&#29366;&#24577;&#65292;&#21253;&#25324;&#20851;&#33410;&#20301;&#32622;&#12289;&#37325;&#21147;&#21521;&#37327;&#12289;&#22522;&#24231;&#30340;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34394;&#25311;&#22235;&#36275;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#27979;&#35797;&#22330;&#26223;&#20013;&#21487;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#20219;&#21153;&#24615;&#33021;&#25351;&#26631;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#20351;&#29992;&#20851;&#38190;&#29366;&#24577;&#23398;&#20064;&#21040;&#30340;&#27493;&#24577;&#25216;&#33021;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#29366;&#24577;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#20219;&#21153;&#24615;&#33021;&#25110;&#23398;&#20064;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot motor skills can be learned through deep reinforcement learning (DRL) by neural networks as state-action mappings. While the selection of state observations is crucial, there has been a lack of quantitative analysis to date. Here, we present a systematic saliency analysis that quantitatively evaluates the relative importance of different feedback states for motor skills learned through DRL. Our approach can identify the most essential feedback states for locomotion skills, including balance recovery, trotting, bounding, pacing and galloping. By using only key states including joint positions, gravity vector, base linear and angular velocities, we demonstrate that a simulated quadruped robot can achieve robust performance in various test scenarios across these distinct skills. The benchmarks using task performance metrics show that locomotion skills learned with key states can achieve comparable performance to those with all states, and the task performance or learning success rat
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#29305;&#24449;&#25968;&#25454;&#38598;&#21457;&#29616;&#65292;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#23567;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#20984;&#26174;&#20986;&#22686;&#24378;&#40065;&#26834;&#29305;&#24449;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17091</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#29305;&#24449;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Robust Features in Mitigating Catastrophic Forgetting. (arXiv:2306.17091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#29305;&#24449;&#25968;&#25454;&#38598;&#21457;&#29616;&#65292;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#23567;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#20984;&#26174;&#20986;&#22686;&#24378;&#40065;&#26834;&#29305;&#24449;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#31181;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#22312;&#26032;&#20219;&#21153;&#25110;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#26102;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#40065;&#26834;&#24615;&#23545;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#35299;&#65292;&#23558;&#20854;&#20998;&#20026;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#31867;&#22411;&#65292;&#24182;&#34920;&#26126;&#22312;&#40065;&#26834;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#27809;&#26377;&#30740;&#31350;&#20851;&#20110;&#20174;CL&#27169;&#22411;&#35282;&#24230;&#35780;&#20272;&#40065;&#26834;&#29305;&#24449;&#22312;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CL&#40065;&#26834;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;CL&#40065;&#26834;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;CL&#40065;&#26834;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CL&#27169;&#22411;&#23545;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36739;&#23569;&#65292;&#32780;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#36951;&#24536;&#36739;&#22810;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#24378;&#35843;&#20102;&#25552;&#20379;&#32473;&#24213;&#23618;CL&#27169;&#22411;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26174;&#31034;CL&#40065;&#26834;&#29305;&#24449;&#21487;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an approach to address catastrophic forgetting, which refers to forgetting previously learned knowledge by neural networks when trained on new tasks or data distributions. The adversarial robustness has decomposed features into robust and non-robust types and demonstrated that models trained on robust features significantly enhance adversarial robustness. However, no study has been conducted on the efficacy of robust features from the lens of the CL model in mitigating catastrophic forgetting in CL. In this paper, we introduce the CL robust dataset and train four baseline models on both the standard and CL robust datasets. Our results demonstrate that the CL models trained on the CL robust dataset experienced less catastrophic forgetting of the previously learned tasks than when trained on the standard dataset. Our observations highlight the significance of the features provided to the underlying CL models, showing that CL robust features can alleviate catast
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#27169;&#22411;&#36827;&#34892;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#29702;&#35299;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#31232;&#30095;&#27169;&#24335;&#26500;&#24314;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#26174;&#24335;&#20808;&#39564;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#22270;&#19981;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17090</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#22270;&#27169;&#22411;&#21033;&#29992;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sparsity exploitation via discovering graphical models in multi-variate time-series forecasting. (arXiv:2306.17090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#27169;&#22411;&#36827;&#34892;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#29702;&#35299;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#31232;&#30095;&#27169;&#24335;&#26500;&#24314;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#26174;&#24335;&#20808;&#39564;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#22270;&#19981;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;MTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#21644;&#29702;&#35299;&#34920;&#31034;&#25968;&#25454;&#30456;&#20851;&#24615;&#30340;&#24213;&#23618;&#22270;&#32467;&#26500;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#24403;&#26174;&#24335;&#30340;&#20808;&#39564;&#22270;&#32467;&#26500;&#19981;&#21487;&#29992;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#24471;&#25972;&#20307;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22270;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;GNN&#39044;&#27979;&#27169;&#22359;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#24418;Lasso&#65288;&#25110;GraphLASSO&#65289;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#21033;&#29992;&#31232;&#30095;&#27169;&#24335;&#26500;&#24314;&#38745;&#24577;&#21644;&#26102;&#21464;&#24773;&#20917;&#19979;&#30340;&#22270;&#32467;&#26500;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22270;&#32467;&#26500;&#21644;&#36755;&#20837;&#25968;&#25454;&#25311;&#21512;&#21040;&#19968;&#20010;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;GCRN&#65289;&#20013;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been widely applied in multi-variate time-series forecasting (MTSF) tasks because of their capability in capturing the correlations among different time-series. These graph-based learning approaches improve the forecasting performance by discovering and understanding the underlying graph structures, which represent the data correlation. When the explicit prior graph structures are not available, most existing works cannot guarantee the sparsity of the generated graphs that make the overall model computational expensive and less interpretable. In this work, we propose a decoupled training method, which includes a graph generating module and a GNNs forecasting module. First, we use Graphical Lasso (or GraphLASSO) to directly exploit the sparsity pattern from data to build graph structures in both static and time-varying cases. Second, we fit these graph structures and the input data into a Graph Convolutional Recurrent Network (GCRN) to train a forecasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17089</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#20204;&#20063;&#26159;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#65288;CODL&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21069;&#25552;&#26159;LLMs&#35201;&#29702;&#35299;&#27010;&#24565;&#24182;&#30830;&#20445;&#27010;&#24565;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#21450;LLMs&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#21644;&#27010;&#24565;&#23398;&#20064;&#12290;&#20154;&#31867;&#30693;&#35782;&#21253;&#25324;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#21644;&#20855;&#20307;&#65288;&#24863;&#24615;&#65289;&#30693;&#35782;&#12290;&#32780;&#20165;&#25991;&#26412;&#30340;LLMs&#21482;&#33021;&#34920;&#31034;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;LLMs&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30693;&#35782;&#30340;&#23436;&#25972;&#33539;&#22260;&#65288;&#27010;&#24565;&#24615;&#21644;&#24863;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35270;&#35273;-&#35821;&#35328;LLMs&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#36825;&#26159;&#26368;&#37325;&#35201;&#30340;&#22810;&#27169;&#24577;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.17066</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#26102;&#24577;&#28857;&#36807;&#31243;&#27169;&#22411;&#22312;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data. (arXiv:2306.17066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26159;&#36830;&#32493;&#26102;&#38388;&#20013;&#24314;&#27169;&#24322;&#27493;&#20107;&#20214;&#24207;&#21015;&#30340;&#26631;&#20934;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;TPP&#27169;&#22411;&#24120;&#24120;&#21463;&#38480;&#20110;&#24378;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#21160;&#24577;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#31070;&#32463;TPP&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#24314;&#27169;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;TPP&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#32479;&#19968;&#30340;&#35774;&#32622;&#65292;&#20381;&#36182;&#19981;&#21516;&#30340;&#22522;&#32447;&#12289;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#37197;&#32622;&#12290;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;&#25512;&#21160;&#39044;&#27979;&#20934;&#30830;&#24615;&#25913;&#36827;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;mmWave Wi-Fi&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;/&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#25552;&#21462;&#20449;&#21495;-to-noise ratios (SNRs) &#21644;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#19968;&#29615;&#22659;&#20013;96.7%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17062</link><description>&lt;p&gt;
&#20351;&#29992;mmWave Wi-Fi&#25509;&#20837;&#28857;&#30340;&#23039;&#21183;&#35782;&#21035;&#65306;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Gesture Recognition with mmWave Wi-Fi Access Points: Lessons Learned. (arXiv:2306.17062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;mmWave Wi-Fi&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;/&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#25552;&#21462;&#20449;&#21495;-to-noise ratios (SNRs) &#21644;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#19968;&#29615;&#22659;&#20013;96.7%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;6 GHz&#20197;&#19979;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;Wi-Fi&#24863;&#30693;&#65292;&#29305;&#21035;&#26159;&#27963;&#21160;&#21644;&#25163;&#21183;&#35782;&#21035;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;mmWave&#65288;60 GHz&#65289;Wi-Fi&#20449;&#21495;&#29992;&#20110;&#25163;&#21183;&#35782;&#21035;/&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;mmWave Wi-Fi&#20449;&#21495;&#65292;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#39640;&#36895;&#25968;&#25454;&#36890;&#20449;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#24863;&#30693;&#65292;&#22312;XR&#24212;&#29992;&#20013;&#26356;&#26377;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;IEEE 802.11ad&#35774;&#22791;&#20351;&#29992;&#30340;&#21608;&#26399;&#24615;&#27874;&#26463;&#35757;&#32451;&#20013;&#25552;&#21462;&#31354;&#38388;&#27874;&#26463;&#20449;&#21495;-to-noise ratios (SNRs) &#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#30001;XR&#24212;&#29992;&#39537;&#21160;&#30340;10&#20010;&#25163;&#21183;/&#23039;&#21183;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#29615;&#22659;&#21644;&#19977;&#20154;&#36523;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;IEEE 802.11ac&#35774;&#22791;&#30340;CSI&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20174;CSI&#21644;&#27874;&#26463;SNR&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;DNN&#20998;&#31867;&#22120;&#22312;&#21333;&#19968;&#29615;&#22659;&#20013;&#30340;&#27874;&#26463;SNR&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;96.7%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, channel state information (CSI) at sub-6 GHz has been widely exploited for Wi-Fi sensing, particularly for activity and gesture recognition. In this work, we instead explore mmWave (60 GHz) Wi-Fi signals for gesture recognition/pose estimation. Our focus is on the mmWave Wi-Fi signals so that they can be used not only for high data rate communication but also for improved sensing e.g., for extended reality (XR) applications. For this reason, we extract spatial beam signal-to-noise ratios (SNRs) from the periodic beam training employed by IEEE 802.11ad devices. We consider a set of 10 gestures/poses motivated by XR applications. We conduct experiments in two environments and with three people.As a comparison, we also collect CSI from IEEE 802.11ac devices. To extract features from the CSI and the beam SNR, we leverage a deep neural network (DNN). The DNN classifier achieves promising results on the beam SNR task with state-of-the-art 96.7% accuracy in a single environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17052</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#20849;&#20139;&#20132;&#36890;&#65292;&#38656;&#35201;&#21327;&#35843;&#22823;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20248;&#21270;&#20195;&#34920;&#24615;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#26469;&#24212;&#23545;&#30001;&#27492;&#24102;&#26469;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20998;&#24067;&#23384;&#22312;&#20840;&#23616;&#32422;&#26463;&#30340;&#24773;&#20917;&#65288;&#20363;&#22914;&#38656;&#35201;&#28385;&#36275;&#23481;&#37327;&#32422;&#26463;&#25110;&#26368;&#23567;&#35206;&#30422;&#35201;&#27714;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23427;&#22312;&#20445;&#35777;&#24754;&#35266;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#36716;&#31227;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#30830;&#20445;&#39640;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20849;&#20139;&#20132;&#36890;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#36710;&#36742;&#37325;&#23450;&#20301;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#22323;&#20986;&#31199;&#36710;&#36712;&#36857;&#25968;&#25454;&#30340;&#20223;&#30495;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#28385;&#36275;&#20851;&#38190;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#36827;&#21270;&#26041;&#31243;&#21457;&#29616;&#20013;&#30340;&#36136;&#37327;&#65307;&#21333;&#30446;&#26631;&#20248;&#21270;&#20165;&#32771;&#34385;&#26041;&#31243;&#20013;&#25152;&#36873;&#39033;&#30340;&#24046;&#24322;&#65292;&#32780;&#22810;&#30446;&#26631;&#20248;&#21270;&#36824;&#32771;&#34385;&#20102;&#25152;&#33719;&#24471;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17038</link><description>&lt;p&gt;
&#21333;&#30446;&#26631;&#19982;&#22810;&#30446;&#26631;&#20248;&#21270;&#36136;&#37327;&#22312;&#36827;&#21270;&#26041;&#31243;&#21457;&#29616;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Single- and Multi- Objective Optimization Quality for Evolutionary Equation Discovery. (arXiv:2306.17038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#36827;&#21270;&#26041;&#31243;&#21457;&#29616;&#20013;&#30340;&#36136;&#37327;&#65307;&#21333;&#30446;&#26631;&#20248;&#21270;&#20165;&#32771;&#34385;&#26041;&#31243;&#20013;&#25152;&#36873;&#39033;&#30340;&#24046;&#24322;&#65292;&#32780;&#22810;&#30446;&#26631;&#20248;&#21270;&#36824;&#32771;&#34385;&#20102;&#25152;&#33719;&#24471;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24494;&#20998;&#26041;&#31243;&#30340;&#21457;&#29616;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#33021;&#22815;&#33719;&#24471;&#19981;&#38656;&#35201;&#24120;&#35268;&#26041;&#27861;&#65288;&#22914;&#36890;&#36807;&#23436;&#25972;&#21487;&#33021;&#39033;&#24211;&#30340;&#31232;&#30095;&#31526;&#21495;&#22238;&#24402;&#65289;&#30340;&#20808;&#39564;&#20551;&#35774;&#30340;&#26041;&#31243;&#30340;&#24037;&#20855;&#12290;&#26041;&#31243;&#21457;&#29616;&#39046;&#22495;&#21253;&#21547;&#20004;&#20010;&#29420;&#31435;&#30340;&#26041;&#21521;&#12290;&#31532;&#19968;&#20010;&#26041;&#21521;&#26159;&#32431;&#25968;&#23398;&#30340;&#65292;&#28041;&#21450;&#21040;&#24494;&#20998;&#12289;&#20248;&#21270;&#30340;&#30446;&#26631;&#20197;&#21450;&#23427;&#20204;&#19982;&#20989;&#25968;&#31354;&#38388;&#31561;&#30340;&#20851;&#31995;&#12290;&#31532;&#20108;&#20010;&#26041;&#21521;&#21017;&#23436;&#20840;&#33268;&#21147;&#20110;&#20248;&#21270;&#38382;&#39064;&#30340;&#38472;&#36848;&#12290;&#25506;&#32034;&#36825;&#20004;&#20010;&#20027;&#39064;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#22788;&#29702;&#23454;&#39564;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#19968;&#31181;&#26356;&#31526;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#24335;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#26174;&#33879;&#30340;&#39044;&#22788;&#29702;&#21644;&#23545;&#20854;&#26412;&#36136;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#21333;&#30446;&#26631;&#20248;&#21270;&#65288;&#20165;&#32771;&#34385;&#26041;&#31243;&#20013;&#25152;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;&#36824;&#32771;&#34385;&#25152;&#33719;&#24471;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#65289;&#20043;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary differential equation discovery proved to be a tool to obtain equations with less a priori assumptions than conventional approaches, such as sparse symbolic regression over the complete possible terms library. The equation discovery field contains two independent directions. The first one is purely mathematical and concerns differentiation, the object of optimization and its relation to the functional spaces and others. The second one is dedicated purely to the optimizational problem statement. Both topics are worth investigating to improve the algorithm's ability to handle experimental data a more artificial intelligence way, without significant pre-processing and a priori knowledge of their nature. In the paper, we consider the prevalence of either single-objective optimization, which considers only the discrepancy between selected terms in the equation, or multi-objective optimization, which additionally takes into account the complexity of the obtained equation. The pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24067;&#23572;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#21644;&#25299;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#23433;&#20840;&#24863;&#30693;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.17033</link><description>&lt;p&gt;
&#38754;&#21521;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#24863;&#30693;&#20219;&#21153;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Safety-Aware Task Composition for Discrete and Continuous Reinforcement Learning. (arXiv:2306.17033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24067;&#23572;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#21644;&#25299;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#23433;&#20840;&#24863;&#30693;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#21487;&#25193;&#23637;&#31995;&#32479;&#35774;&#35745;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26368;&#36817;&#22312;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#30495;&#27491;&#21033;&#29992;&#32452;&#21512;&#26041;&#38754;&#25165;&#21018;&#21018;&#24320;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#23398;&#20064;&#20219;&#21153;&#30340;&#24067;&#23572;&#32452;&#21512;&#65292;&#32780;&#19981;&#26159;&#21151;&#33021;&#24615;&#25110;&#39034;&#24207;&#24615;&#32452;&#21512;&#12290;&#29616;&#26377;&#30340;RL&#24067;&#23572;&#32452;&#21512;&#20391;&#37325;&#20110;&#22312;&#20855;&#26377;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#36798;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#21560;&#25910;&#29366;&#24577;&#65292;&#20294;&#19981;&#25903;&#25345;&#21487;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#65288;&#21363;&#36991;&#20813;&#65289;&#32422;&#26463;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#25512;&#36827;&#20102;&#23398;&#20064;&#20219;&#21153;&#24067;&#23572;&#32452;&#21512;&#30340;&#26368;&#26032;&#25216;&#26415;&#65306;i&#65289;&#22312;&#27492;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#65307;ii&#65289;&#23637;&#31034;&#22914;&#20309;&#24378;&#21046;&#25191;&#34892;&#23433;&#20840;&#35821;&#20041;&#65292;&#35777;&#26126;&#27491;&#30830;&#24615;&#65288;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65289;&#65292;&#24182;&#20998;&#26512;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#26435;&#34913;&#65307;iii&#65289;&#23558;&#24067;&#23572;&#32452;&#21512;&#20174;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#25193;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#29256;&#30340;&#20215;&#20540;&#36845;&#20195;&#31639;&#27861;&#26469;&#28436;&#31034;&#36825;&#20123;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is a critical aspect of scalable system design. Reinforcement learning (RL) has recently shown substantial success in task learning, but has only recently begun to truly leverage composition. In this paper, we focus on Boolean composition of learned tasks as opposed to functional or sequential composition. Existing Boolean composition for RL focuses on reaching a satisfying absorbing state in environments with discrete action spaces, but does not support composable safety (i.e., avoidance) constraints. We advance the state of the art in Boolean composition of learned tasks with three contributions: i) introduce two distinct notions of safety in this framework; ii) show how to enforce either safety semantics, prove correctness (under some assumptions), and analyze the trade-offs between the two safety notions; and iii) extend Boolean composition from discrete action spaces to continuous action spaces. We demonstrate these techniques using modified versions of value iter
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#35889;&#25209;&#37327;&#24402;&#19968;&#21270;(SBN)&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#24402;&#19968;&#21270;&#29305;&#24449;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23613;&#31649;&#26377;&#25209;&#37327;&#24402;&#19968;&#21270;(BN)&#65292;&#29305;&#24449;&#26144;&#23556;&#22312;&#32593;&#32476;&#24320;&#22987;&#38454;&#27573;&#20173;&#28982;&#20250;&#29190;&#28856;&#12290;</title><link>http://arxiv.org/abs/2306.16999</link><description>&lt;p&gt;
&#39057;&#22495;&#24402;&#19968;&#21270;&#65306;&#35889;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Spectral Batch Normalization: Normalization in the Frequency Domain. (arXiv:2306.16999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#35889;&#25209;&#37327;&#24402;&#19968;&#21270;(SBN)&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#24402;&#19968;&#21270;&#29305;&#24449;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23613;&#31649;&#26377;&#25209;&#37327;&#24402;&#19968;&#21270;(BN)&#65292;&#29305;&#24449;&#26144;&#23556;&#22312;&#32593;&#32476;&#24320;&#22987;&#38454;&#27573;&#20173;&#28982;&#20250;&#29190;&#28856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#26159;&#19968;&#32452;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#25928;&#26041;&#27861;&#8212;&#8212;&#35889;&#25209;&#37327;&#24402;&#19968;&#21270;(SBN)&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#24402;&#19968;&#21270;&#29305;&#24449;&#26144;&#23556;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#22312;&#32593;&#32476;&#28145;&#24230;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#26080;&#25209;&#37327;&#24402;&#19968;&#21270;(BN)&#30340;&#27531;&#24046;&#32593;&#32476;&#30340;&#28608;&#27963;&#24448;&#24448;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23548;&#33268;&#29305;&#24449;&#26144;&#23556;&#33539;&#25968;&#38750;&#24120;&#22823;&#65292;&#23613;&#31649;&#21442;&#25968;&#30456;&#23545;&#36739;&#23567;&#12290;&#36825;&#31181;&#29190;&#28856;&#24615;&#21160;&#24577;&#23545;&#23398;&#20064;&#38750;&#24120;&#26377;&#23475;&#12290;BN&#20351;&#24471;&#23545;&#32553;&#25918;&#22240;&#23376; &#947;, &#946; &#36827;&#34892;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#36817;&#20284;&#31561;&#25928;&#20110;&#23545;&#29305;&#24449;&#26144;&#23556;&#33539;&#25968;&#36827;&#34892;&#21152;&#24615;&#24809;&#32602;&#65292;&#20174;&#19968;&#23450;&#31243;&#24230;&#19978;&#38450;&#27490;&#29305;&#24449;&#26144;&#23556;&#33539;&#25968;&#21464;&#24471;&#36807;&#22823;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23613;&#31649; BN &#36827;&#34892;&#20102;&#36817;&#20284;&#30340;&#21152;&#24615;&#24809;&#32602;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#30340;&#29305;&#24449;&#26144;&#23556;&#24448;&#24448;&#22312;&#32593;&#32476;&#24320;&#22987;&#38454;&#27573;&#20173;&#28982;&#20250;&#29190;&#28856;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce spectral batch normalization (SBN), a novel effective method to improve generalization by normalizing feature maps in the frequency (spectral) domain. The activations of residual networks without batch normalization (BN) tend to explode exponentially in the depth of the network at initialization. This leads to extremely large feature map norms even though the parameters are relatively small. These explosive dynamics can be very detrimental to learning. BN makes weight decay regularization on the scaling factors $\gamma, \beta$ approximately equivalent to an additive penalty on the norm of the feature maps, which prevents extremely large feature map norms to a certain degree. However, we show experimentally that, despite the approximate additive penalty of BN, feature maps in deep neural networks (DNNs) tend to explode at the beginning of the net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26435;&#37325;&#21387;&#32553;&#22120;&#65288;WC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.16993</link><description>&lt;p&gt;
Weight Compander: &#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#30340;&#31616;&#21333;&#26435;&#37325;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weight Compander: A Simple Weight Reparameterization for Regularization. (arXiv:2306.16993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26435;&#37325;&#21387;&#32553;&#22120;&#65288;WC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#38598;&#21512;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#26435;&#37325;&#21387;&#32553;&#22120;&#65288;WC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#26159;&#19968;&#31181;&#36890;&#29992;&#12289;&#30452;&#35266;&#12289;&#24265;&#20215;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22823;&#26435;&#37325;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#22797;&#26434;&#32593;&#32476;&#30340;&#26631;&#24535;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#32593;&#32476;&#24448;&#24448;&#20855;&#26377;&#26356;&#24191;&#33539;&#22260;&#30340;&#25509;&#36817;&#38646;&#30340;&#26435;&#37325;&#65292;&#32780;&#20013;&#24515;&#25509;&#36817;&#38646;&#30340;&#26435;&#37325;&#36739;&#23569;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26435;&#37325;&#37325;&#26032;&#21442;&#25968;&#21270;&#20989;&#25968;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#27599;&#20010;&#26435;&#37325;&#65292;&#36890;&#36807;&#38480;&#21046;&#26435;&#37325;&#30340;&#24133;&#20540;&#21516;&#26102;&#20351;&#20854;&#36828;&#31163;&#38646;&#26469;&#38544;&#24335;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#20013;&#26356;&#21152;&#27665;&#20027;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#20010;&#20307;&#26435;&#37325;&#30340;&#24433;&#21709;&#21147;&#19981;&#22826;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce weight compander (WC), a novel effective method to improve generalization by reparameterizing each weight in deep neural networks using a nonlinear function. It is a general, intuitive, cheap and easy to implement method, which can be combined with various other regularization techniques. Large weights in deep neural networks are a sign of a more complex network that is overfitted to the training data. Moreover, regularized networks tend to have a greater range of weights around zero with fewer weights centered at zero. We introduce a weight reparameterization function which is applied to each weight and implicitly reduces overfitting by restricting the magnitude of the weights while forcing them away from zero at the same time. This leads to a more democratic decision-making in the network. Firstly, individual weights cannot have too much influ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16978</link><description>&lt;p&gt;
&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26159;&#23547;&#25214;&#35206;&#30422;&#32473;&#23450;&#23553;&#38381;&#21306;&#22495;&#25972;&#20010;&#33258;&#30001;&#31354;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#26426;&#22120;&#20154;&#21106;&#33609;&#21644;&#21560;&#23576;&#21040;&#22320;&#38647;&#28165;&#38500;&#21644;&#25628;&#25937;&#20219;&#21153;&#12290;&#34429;&#28982;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#20026;&#24050;&#30693;&#29615;&#22659;&#25214;&#21040;&#21487;&#35777;&#26126;&#23436;&#22791;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#20294;&#22312;&#22312;&#32447;&#22330;&#26223;&#19979;&#65292;&#29615;&#22659;&#20107;&#20808;&#26410;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38750;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#20215;&#20540;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#26500;&#24314;&#35266;&#23519;&#31354;&#38388;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#35268;&#21010;&#38271;&#26399;&#36335;&#24452;&#65292;&#24182;&#21516;&#26102;&#23545;&#30701;&#26399;&#38556;&#30861;&#29289;&#36827;&#34892;&#34892;&#21160;&#12290;&#20026;&#20102;&#32771;&#34385;&#22823;&#35268;&#27169;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#23610;&#24230;&#22320;&#22270;&#36755;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#36335;&#24452;&#20559;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-Jump GNNs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35843;&#33410;&#30340;&#24230;&#37327;&#36807;&#28388;&#22120;&#65292;&#26469;&#25552;&#39640;&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#21270;&#22330;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36339;&#36291;&#24335;&#30340;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#29983;&#25104;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#65292;&#20197;&#23547;&#25214;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.16976</link><description>&lt;p&gt;
Diffusion-Jump GNNs: &#21487;&#23398;&#20064;&#24230;&#37327;&#36807;&#28388;&#22120;&#30340;&#21516;&#36136;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters. (arXiv:2306.16976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-Jump GNNs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35843;&#33410;&#30340;&#24230;&#37327;&#36807;&#28388;&#22120;&#65292;&#26469;&#25552;&#39640;&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#21270;&#22330;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36339;&#36291;&#24335;&#30340;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#29983;&#25104;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#65292;&#20197;&#23547;&#25214;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HO-GNNs&#65289;&#34987;&#24320;&#21457;&#29992;&#20110;&#22312;&#24322;&#36136;&#24615;&#33539;&#22260;&#20013;&#25512;&#26029;&#19968;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20854;&#20013;&#26631;&#31614;&#20998;&#24067;&#19982;&#22270;&#32467;&#26500;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HO-GNNs&#26159;&#22522;&#20110;&#36339;&#25968;&#30340;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#36716;&#31227;&#30697;&#38453;&#30340;&#24130;&#27425;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26500;&#23545;&#20998;&#31867;&#25439;&#22833;&#30340;&#21453;&#24212;&#19981;&#23436;&#20840;&#65292;&#24182;&#19988;&#25152;&#36798;&#21040;&#30340;&#32467;&#26500;&#21270;&#36807;&#28388;&#22120;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#32593;&#32476;&#19981;&#33021;&#23398;&#20064;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#25110;&#31995;&#25968;&#65292;&#32780;&#21482;&#33021;&#23398;&#20064;&#36807;&#28388;&#22120;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#30340;&#36339;&#36291;&#25193;&#25955;GNNs&#26041;&#27861;&#12290;&#25193;&#25955;&#36339;&#36291;&#29983;&#25104;&#19968;&#23545;&#19968;&#30340;&#36317;&#31163;&#65292;&#20854;&#25237;&#24433;&#30830;&#23450;&#27599;&#20010;&#32467;&#26500;&#21270;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#12290;&#36825;&#20123;&#36807;&#28388;&#22120;&#31216;&#20026;&#36339;&#36291;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#23610;&#24230;&#33539;&#22260;&#20869;&#25506;&#32034;&#20197;&#25214;&#21040;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-order Graph Neural Networks (HO-GNNs) have been developed to infer consistent latent spaces in the heterophilic regime, where the label distribution is not correlated with the graph structure. However, most of the existing HO-GNNs are hop-based, i.e., they rely on the powers of the transition matrix. As a result, these architectures are not fully reactive to the classification loss and the achieved structural filters have static supports. In other words, neither the filters' supports nor their coefficients can be learned with these networks. They are confined, instead, to learn combinations of filters. To address the above concerns, we propose Diffusion-jump GNNs a method relying on asymptotic diffusion distances that operates on jumps. A diffusion-pump generates pairwise distances whose projections determine both the support and coefficients of each structural filter. These filters are called jumps because they explore a wide range of scales in order to find bonds between scatter
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#21333;&#32431;&#24490;&#29615;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20174;&#30456;&#20851;&#25968;&#25454;&#38598;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#29699;&#30340;&#21516;&#20262;&#32676;&#30340;&#29983;&#25104;&#20803;&#30340;&#32676;&#35770;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.16951</link><description>&lt;p&gt;
&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#21040;&#20195;&#25968;&#25299;&#25169;&#23398;&#65306;&#21033;&#29992;Wu&#20844;&#24335;&#20013;&#22810;&#26631;&#31614;&#30340;&#26041;&#27861;&#29983;&#25104;&#21333;&#32431;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula. (arXiv:2306.16951v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#21333;&#32431;&#24490;&#29615;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20174;&#30456;&#20851;&#25968;&#25454;&#38598;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#29699;&#30340;&#21516;&#20262;&#32676;&#30340;&#29983;&#25104;&#20803;&#30340;&#32676;&#35770;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#29699;&#30340;&#21516;&#20262;&#32676;&#19968;&#30452;&#26159;&#20195;&#25968;&#25299;&#25169;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#21457;&#23637;&#20102;&#21508;&#31181;&#29702;&#35770;&#21644;&#31639;&#27861;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#21521;&#29702;&#35299;&#36825;&#20123;&#21516;&#20262;&#32676;&#29983;&#25104;&#20803;&#30340;&#32676;&#35770;&#32467;&#26500;&#30340;&#30446;&#26631;&#36808;&#36827;&#20102;&#19968;&#27493;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;Wu&#20844;&#24335;&#30340;&#21333;&#32431;&#32676;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#21333;&#32431;&#24490;&#29615;&#30340;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20174;&#19982;Dyck&#35821;&#35328;&#30456;&#20851;&#30340;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;&#20132;&#38598;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#22810;&#26631;&#31614;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#24207;&#21015;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21450;&#24517;&#35201;&#30340;&#32676;&#35770;&#24037;&#20855;&#21644;&#38750;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing homotopy groups of spheres has long been a fundamental objective in algebraic topology. Various theoretical and algorithmic approaches have been developed to tackle this problem. In this paper we take a step towards the goal of comprehending the group-theoretic structure of the generators of these homotopy groups by leveraging the power of machine learning. Specifically, in the simplicial group setting of Wu's formula, we reformulate the problem of generating simplicial cycles as a problem of sampling from the intersection of algorithmic datasets related to Dyck languages. We present and evaluate language modelling approaches that employ multi-label information for input sequences, along with the necessary group-theoretic toolkit and non-neural baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24674;&#22797;&#24050;&#32763;&#35793;&#65288;&#29978;&#33267;&#26059;&#36716;&#65289;&#30340;&#36755;&#20837;&#21040;&#21407;&#22987;&#36755;&#20837;&#65292;&#24182;&#23558;&#20854;&#20256;&#36865;&#21040;&#20219;&#24847;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16938</link><description>&lt;p&gt;
&#20351;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Restore Translation Using Equivariant Neural Networks. (arXiv:2306.16938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24674;&#22797;&#24050;&#32763;&#35793;&#65288;&#29978;&#33267;&#26059;&#36716;&#65289;&#30340;&#36755;&#20837;&#21040;&#21407;&#22987;&#36755;&#20837;&#65292;&#24182;&#23558;&#20854;&#20256;&#36865;&#21040;&#20219;&#24847;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#23545;&#20110;&#31354;&#38388;&#21464;&#25442;&#65288;&#22914;&#24179;&#31227;&#21644;&#26059;&#36716;&#65289;&#30340;&#19981;&#21464;&#24615;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#23646;&#24615;&#21644;&#22522;&#26412;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23454;&#38469;&#19978;&#23545;&#20110;&#21363;&#20351;&#26159;&#24494;&#23567;&#30340;&#24179;&#31227;&#20063;&#38750;&#24120;&#25935;&#24863;&#12290;&#23384;&#22312;&#30528;&#22823;&#37327;&#30340;&#24037;&#20316;&#26469;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#25110;&#35780;&#20272;&#21464;&#25442;&#26469;&#23454;&#29616;&#31934;&#30830;&#25110;&#36817;&#20284;&#30340;&#21464;&#25442;&#19981;&#21464;&#24615;&#12290;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#23545;&#26631;&#20934;CNNs&#36827;&#34892;&#26356;&#25913;&#65292;&#24182;&#23545;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#20854;&#20462;&#25913;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#20998;&#31867;&#24674;&#22797;&#22120;&#65292;&#23558;&#24050;&#32763;&#35793;&#65288;&#29978;&#33267;&#26059;&#36716;&#65289;&#30340;&#36755;&#20837;&#24674;&#22797;&#21040;&#21407;&#22987;&#36755;&#20837;&#65292;&#24182;&#23558;&#20854;&#20256;&#36865;&#21040;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#20998;&#31867;&#22120;&#20013;&#12290;&#27492;&#24674;&#22797;&#22120;&#22522;&#20110;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#32473;&#20986;&#20102;&#19968;&#20010;&#20223;&#23556;&#31639;&#23376;&#22312;&#24352;&#37327;&#31354;&#38388;&#19978;&#26159;&#24179;&#31227;&#31561;&#21464;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance to spatial transformations such as translations and rotations is a desirable property and a basic design principle for classification neural networks. However, the commonly used convolutional neural networks (CNNs) are actually very sensitive to even small translations. There exist vast works to achieve exact or approximate transformation invariance by designing transformation-invariant models or assessing the transformations. These works usually make changes to the standard CNNs and harm the performance on standard datasets. In this paper, rather than modifying the classifier, we propose a pre-classifier restorer to recover translated (or even rotated) inputs to the original ones which will be fed into any classifier for the same dataset. The restorer is based on a theoretical result which gives a sufficient and necessary condition for an affine operator to be translational equivariant on a tensor space.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>OSP&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#21644;&#26412;&#22320;&#26799;&#24230;&#20462;&#27491;&#26469;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#31934;&#24230;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.16926</link><description>&lt;p&gt;
OSP: &#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#25552;&#21319;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16926
&lt;/p&gt;
&lt;p&gt;
OSP&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#21644;&#26412;&#22320;&#26799;&#24230;&#20462;&#27491;&#26469;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#31934;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#26159;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#38543;&#30528;DDL&#33410;&#28857;&#30340;&#35745;&#31639;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#32593;&#32476;&#36830;&#25509;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#21442;&#25968;&#26381;&#21153;&#22120;&#24335;DDL&#20013;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#26799;&#24230;&#21387;&#32553;&#21644;&#25913;&#36827;&#30340;&#27169;&#22411;&#21516;&#27493;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20002;&#22833;&#26799;&#24230;&#32780;&#20986;&#29616;&#31934;&#24230;&#25439;&#22833;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#21516;&#27493;&#30340;&#21534;&#21520;&#37327;&#30340;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21516;&#27493;&#26041;&#27861;&#65292;&#21517;&#20026;Overlapped Synchronization Parallel&#65288;OSP&#65289;&#65292;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#26799;&#24230;&#30340;&#21442;&#25968;&#20462;&#27491;&#65288;LGP&#65289;&#26469;&#36991;&#20813;&#30001;&#36807;&#26399;&#21442;&#25968;&#24341;&#36215;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;OSP&#30340;&#21407;&#22411;&#20351;&#29992;PyTo&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo
&lt;/p&gt;</description></item><item><title>NAUTILUS&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#36125;&#21494;&#26031;&#37325;&#35201;&#23884;&#22871;&#37319;&#26679;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#21644;&#35777;&#25454;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#23558;INS&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37325;&#35201;&#37319;&#26679;&#12290;&#22312;&#21508;&#31181;&#21512;&#25104;&#38382;&#39064;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;NAUTILUS&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;NS&#21644;MCMC&#36719;&#20214;&#21253;&#12290;</title><link>http://arxiv.org/abs/2306.16923</link><description>&lt;p&gt;
NAUTILUS:&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#36125;&#21494;&#26031;&#37325;&#35201;&#23884;&#22871;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
NAUTILUS: boosting Bayesian importance nested sampling with deep learning. (arXiv:2306.16923v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16923
&lt;/p&gt;
&lt;p&gt;
NAUTILUS&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#36125;&#21494;&#26031;&#37325;&#35201;&#23884;&#22871;&#37319;&#26679;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#21644;&#35777;&#25454;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#23558;INS&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37325;&#35201;&#37319;&#26679;&#12290;&#22312;&#21508;&#31181;&#21512;&#25104;&#38382;&#39064;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;NAUTILUS&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;NS&#21644;MCMC&#36719;&#20214;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#36125;&#21494;&#26031;&#37325;&#35201;&#23884;&#22871;&#37319;&#26679;&#65288;INS&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#21644;&#35777;&#25454;&#20272;&#35745;&#12290;&#19982;&#22522;&#20110;&#25298;&#32477;&#30340;&#37319;&#26679;&#26041;&#27861;&#65288;&#22914;&#22522;&#26412;&#23884;&#22871;&#37319;&#26679;NS&#25110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;MCMC&#31639;&#27861;&#65289;&#19981;&#21516;&#65292;&#37325;&#35201;&#37319;&#26679;&#25216;&#26415;&#21487;&#20197;&#20351;&#29992;&#25152;&#26377;&#20284;&#28982;&#24230;&#37327;&#20540;&#36827;&#34892;&#21518;&#39564;&#21644;&#35777;&#25454;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#37325;&#35201;&#37319;&#26679;&#65292;&#38656;&#35201;&#20351;&#29992;&#19982;&#21518;&#39564;&#20998;&#24067;&#30456;&#20284;&#30340;&#24314;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#23558;INS&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;NAUTILUS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#21644;&#35777;&#25454;&#20272;&#35745;&#30340;&#21442;&#32771;&#24320;&#28304;Python&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#38382;&#39064;&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22914;&#31995;&#22806;&#34892;&#26143;&#26816;&#27979;&#65292;&#26143;&#31995;SED&#25311;&#21512;&#21644;&#23431;&#23449;&#23398;&#30740;&#31350;&#65292;&#23558;NAUTILUS&#19982;&#27969;&#34892;&#30340;NS&#21644;MCMC&#36719;&#20214;&#21253;&#65288;&#21253;&#25324;EMCEE&#65292;DYNESTY&#65292;ULTRANEST&#21644;POCOMC&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel approach to boost the efficiency of the importance nested sampling (INS) technique for Bayesian posterior and evidence estimation using deep learning. Unlike rejection-based sampling methods such as vanilla nested sampling (NS) or Markov chain Monte Carlo (MCMC) algorithms, importance sampling techniques can use all likelihood evaluations for posterior and evidence estimation. However, for efficient importance sampling, one needs proposal distributions that closely mimic the posterior distributions. We show how to combine INS with deep learning via neural network regression to accomplish this task. We also introduce NAUTILUS, a reference open-source Python implementation of this technique for Bayesian posterior and evidence estimation. We compare NAUTILUS against popular NS and MCMC packages, including EMCEE, DYNESTY, ULTRANEST and POCOMC, on a variety of challenging synthetic problems and real-world applications in exoplanet detection, galaxy SED fitting and cosmo
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#65292;&#37319;&#29992;&#31232;&#30095;&#31034;&#20363;&#20808;&#23398;&#20064;&#30340;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#28151;&#21512;&#36755;&#20837;&#30340;&#22855;&#20598;&#30446;&#26631;&#19978;&#23398;&#20064;&#21040;&#36275;&#22815;&#22823;&#38454;&#25968;&#30340;&#22855;&#20598;&#24615;&#65292;&#32780;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#22312;&#30456;&#21516;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.16921</link><description>&lt;p&gt;
&#23545;&#28151;&#21512;&#36755;&#20837;&#30340;&#22855;&#20598;&#30446;&#26631;&#65292;&#35838;&#31243;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs. (arXiv:2306.16921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#65292;&#37319;&#29992;&#31232;&#30095;&#31034;&#20363;&#20808;&#23398;&#20064;&#30340;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#28151;&#21512;&#36755;&#20837;&#30340;&#22855;&#20598;&#30446;&#26631;&#19978;&#23398;&#20064;&#21040;&#36275;&#22815;&#22823;&#38454;&#25968;&#30340;&#22855;&#20598;&#24615;&#65292;&#32780;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#22312;&#30456;&#21516;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#65292;&#21363;&#20808;&#21576;&#29616;&#31616;&#21333;&#31034;&#20363;&#65292;&#28982;&#21518;&#20877;&#21576;&#29616;&#26356;&#22797;&#26434;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#20063;&#34920;&#26126;&#65292;&#25913;&#21464;&#37319;&#26679;&#20998;&#24067;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22855;&#20598;&#24615;&#65292;&#20294;&#21482;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#21333;&#27493;&#21442;&#25968;&#30340;&#24418;&#24335;&#32467;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#65288;&#26377;&#30028;&#65289;&#23398;&#20064;&#29575;&#21644;&#24120;&#35265;&#26679;&#26412;&#20998;&#24067;&#30340;&#35757;&#32451;&#27493;&#39588;&#25968;&#37327;&#19978;&#30340;&#20998;&#31163;&#32467;&#26524;&#65306;&#22914;&#26524;&#25968;&#25454;&#20998;&#24067;&#26159;&#31232;&#30095;&#21644;&#23494;&#38598;&#36755;&#20837;&#30340;&#28151;&#21512;&#29289;&#65292;&#21017;&#23384;&#22312;&#19968;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35838;&#31243;&#22024;&#26434;&#26799;&#24230;&#19979;&#38477;&#65288;&#25110;SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#20808;&#20351;&#29992;&#31232;&#30095;&#31034;&#20363;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#36275;&#22815;&#22823;&#38454;&#25968;&#30340;&#22855;&#20598;&#24615;&#65292;&#32780;&#20219;&#20309;&#30001;&#22024;&#26434;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#23485;&#24230;&#25110;&#28145;&#24230;&#21487;&#33021;&#26356;&#22823;&#65289;&#22312;&#20081;&#24207;&#26679;&#26412;&#19978;&#37117;&#19981;&#33021;&#22312;&#27809;&#26377;&#39069;&#22806;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#25903;&#25345;&#36229;&#20986;&#30340;&#23450;&#24615;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21464;&#24418;&#22330;&#26223;&#20013;&#20272;&#35745;&#30456;&#26426;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#26500;&#25104;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#21487;&#21464;&#24418;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#23548;&#33322;&#21644;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2306.16917</link><description>&lt;p&gt;
&#37257;&#27721;&#23450;&#21521;&#65306;&#20272;&#35745;&#21464;&#24418;&#22330;&#26223;&#20013;&#30456;&#26426;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes. (arXiv:2306.16917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21464;&#24418;&#22330;&#26223;&#20013;&#20272;&#35745;&#30456;&#26426;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#26500;&#25104;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#21487;&#21464;&#24418;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#23548;&#33322;&#21644;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21464;&#24418;&#22330;&#26223;&#20013;&#30340;&#30456;&#26426;&#36816;&#21160;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#24320;&#25918;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38750;&#21018;&#24615;&#32467;&#26500;&#36816;&#21160;&#25216;&#26415;&#20551;&#35774;&#22312;&#24314;&#31435;&#38170;&#23450;&#21442;&#32771;&#26102;&#35266;&#23519;&#21040;&#38500;&#21464;&#24418;&#22330;&#26223;&#37096;&#20998;&#22806;&#36824;&#23384;&#22312;&#38745;&#24577;&#22330;&#26223;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#30456;&#20851;&#24212;&#29992;&#26696;&#20363;&#20013;&#65292;&#22914;&#20869;&#31397;&#38236;&#26816;&#26597;&#65292;&#36825;&#20010;&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;&#35299;&#20915;&#25506;&#32034;&#22411;&#36712;&#36857;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#21644;&#36866;&#24403;&#30340;&#23450;&#37327;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#21464;&#24418;&#37324;&#31243;&#35745;&#21644;SLAM&#31649;&#36947;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20849;&#21516;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#37257;&#27721;&#25968;&#25454;&#38598;&#8221;&#65292;&#23427;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#21487;&#21464;&#24418;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#23548;&#33322;&#21644;&#37325;&#24314;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#30340;&#19977;&#32500;&#22330;&#26223;&#20869;&#27599;&#20010;&#34920;&#38754;&#38543;&#26102;&#38388;&#21576;&#29616;&#38750;&#21018;&#24615;&#21464;&#24418;&#30340;&#22823;&#22411;&#25506;&#32034;&#30456;&#26426;&#36712;&#36857;&#38598;&#21512;&#12290;&#36890;&#36807;&#22312;&#36924;&#30495;&#30340;&#19977;&#32500;&#24314;&#31569;&#20013;&#36827;&#34892;&#27169;&#25311;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26377;&#24207;&#20256;&#36755;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;OTHPO&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#20219;&#21153;&#39034;&#24207;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21313;&#20010;&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16916</link><description>&lt;p&gt;
&#36981;&#23432;&#35746;&#21333;&#65306;&#24341;&#20837;&#26377;&#24207;&#20256;&#36755;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation. (arXiv:2306.16916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16916
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26377;&#24207;&#20256;&#36755;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;OTHPO&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#20219;&#21153;&#39034;&#24207;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21313;&#20010;&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#24207;&#20256;&#36755;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;OTHPO&#65289;&#65292;&#36825;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#29256;&#26412;&#65292;&#20854;&#20013;&#20219;&#21153;&#25353;&#39034;&#24207;&#36827;&#34892;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;HPO&#19981;&#21516;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#20219;&#21153;&#19982;&#20043;&#21069;&#30340;&#20219;&#21153;&#26368;&#30456;&#20851;&#12290;&#36825;&#31526;&#21512;&#35768;&#22810;&#37096;&#32626;&#35774;&#32622;&#65292;&#20854;&#20013;&#38543;&#30528;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#65292;&#36229;&#21442;&#25968;&#20250;&#37325;&#26032;&#35843;&#25972;&#65307;&#20363;&#22914;&#65292;&#35843;&#25972;&#19968;&#31995;&#21015;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#65292;&#38543;&#30528;&#28155;&#21152;&#26356;&#22810;&#30005;&#24433;&#21644;&#35780;&#32423;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#23450;&#20041;&#65292;&#27010;&#36848;&#20102;&#19982;&#30456;&#20851;&#38382;&#39064;&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;OTHPO&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;HPO&#12290;&#25105;&#20204;&#36890;&#36807;&#21313;&#20010;&#22522;&#20934;&#27979;&#35797;&#23454;&#35777;&#22320;&#23637;&#31034;&#20102;&#32771;&#34385;&#39034;&#24207;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#28041;&#21450;&#36880;&#28176;&#32047;&#31215;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#24182;&#28085;&#30422;&#20102;XGBoost&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#36817;&#20284;k&#26368;&#36817;&#37051;&#65292;&#24377;&#24615;&#32593;&#32476;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#19968;&#20010;&#29420;&#31435;&#30340;&#29616;&#23454;&#19990;&#30028;&#21160;&#26426;&#39537;&#21160;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ordered transfer hyperparameter optimisation (OTHPO), a version of transfer learning for hyperparameter optimisation (HPO) where the tasks follow a sequential order. Unlike for state-of-the-art transfer HPO, the assumption is that each task is most correlated to those immediately before it. This matches many deployed settings, where hyperparameters are retuned as more data is collected; for instance tuning a sequence of movie recommendation systems as more movies and ratings are added. We propose a formal definition, outline the differences to related problems and propose a basic OTHPO method that outperforms state-of-the-art transfer HPO. We empirically show the importance of taking order into account using ten benchmarks. The benchmarks are in the setting of gradually accumulating data, and span XGBoost, random forest, approximate k-nearest neighbor, elastic net, support vector machines and a separate real-world motivated optimisation problem. We open source the benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.16913</link><description>&lt;p&gt;
&#20005;&#26684;&#32422;&#26463;&#24212;&#29992;&#20013;&#30340;AutoML
&lt;/p&gt;
&lt;p&gt;
AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#38656;&#35201;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#37197;&#32622;&#65292;&#36890;&#24120;&#30001;AutoML&#31995;&#32479;&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#20248;&#21270;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;AutoML&#31995;&#32479;&#30340;&#20108;&#38454;&#20803;&#37197;&#32622;&#65292;AutoML&#36807;&#31243;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#30446;&#21069;&#30340;AutoML&#31995;&#32479;&#26080;&#27861;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#29992;&#20363;&#30340;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#26080;&#27861;&#32534;&#35793;&#29992;&#25143;&#23450;&#20041;&#30340;&#24212;&#29992;&#32422;&#26463;&#65292;&#20197;&#30830;&#20445;&#27969;&#31243;&#21450;&#20854;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Caml&#65292;&#23427;&#20351;&#29992;&#20803;&#23398;&#20064;&#33258;&#21160;&#36866;&#24212;&#20854;&#33258;&#36523;&#30340;AutoML&#21442;&#25968;&#65292;&#27604;&#22914;&#25628;&#32034;&#31574;&#30053;&#12289;&#39564;&#35777;&#31574;&#30053;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;Caml&#30340;&#21160;&#24577;AutoML&#31574;&#30053;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;&#21644;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16906</link><description>&lt;p&gt;
&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;:&#19968;&#31181;&#27010;&#29575;&#26368;&#36817;&#37051;&#26680;&#23494;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach. (arXiv:2306.16906v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;&#21644;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#26367;&#25442;&#32570;&#22833;&#30340;&#20540;&#20197;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#12290;&#24403;&#21069;&#30340;&#22635;&#34917;&#26041;&#27861;&#35797;&#22270;&#26368;&#23567;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#20540;&#21644;&#22635;&#34917;&#20540;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#20294;&#26159;&#65292;&#22312;&#22810;&#27169;&#24577;&#25110;&#22797;&#26434;&#20998;&#24067;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#33021;&#20250;&#20135;&#29983;&#20266;&#20687;&#65292;&#23548;&#33268;&#22635;&#34917;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$k$NN$\times$KDE&#31639;&#27861;: &#19968;&#31181;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;($k$NN)&#21644;&#20351;&#29992;&#39640;&#26031;&#26680;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;(KDE)&#32467;&#21512;&#30340;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#19982;&#20043;&#21069;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#28041;&#21450;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#32570;&#22833;&#24773;&#20917;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#32570;&#22833;&#29575;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21407;&#22987;&#25968;&#25454;&#32467;&#26500;&#65292;&#20135;&#29983;&#26356;&#20302;&#30340;&#25968;&#25454;&#22635;&#34917;&#35823;&#24046;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#20197;&#24320;&#28304;&#24418;&#24335;&#21457;&#24067;&#32473;&#31038;&#21306;&#65306;https://github.com/DeltaFloflo/knnxkde
&lt;/p&gt;
&lt;p&gt;
Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community: https://github.com/DeltaFloflo/knnxkde
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36861;&#28335;&#30340;&#32676;&#20307;&#21270;&#33258;&#20248;&#21270;&#29305;&#24449;&#36716;&#25442;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#20010;&#32423;&#32852;&#24378;&#21270;&#20195;&#29702;&#33258;&#21160;&#36873;&#25321;&#20505;&#36873;&#29305;&#24449;&#21644;&#25805;&#20316;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#29305;&#24449;&#36716;&#25442;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.16893</link><description>&lt;p&gt;
&#21487;&#36861;&#28335;&#30340;&#32676;&#20307;&#21270;&#33258;&#20248;&#21270;&#29305;&#24449;&#36716;&#25442;&#23398;&#20064;&#65306;&#21452;&#37325;&#20248;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective. (arXiv:2306.16893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36861;&#28335;&#30340;&#32676;&#20307;&#21270;&#33258;&#20248;&#21270;&#29305;&#24449;&#36716;&#25442;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#20010;&#32423;&#32852;&#24378;&#21270;&#20195;&#29702;&#33258;&#21160;&#36873;&#25321;&#20505;&#36873;&#29305;&#24449;&#21644;&#25805;&#20316;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#29305;&#24449;&#36716;&#25442;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36716;&#25442;&#26088;&#22312;&#36890;&#36807;&#25968;&#23398;&#19978;&#30340;&#25913;&#36827;&#29616;&#26377;&#29305;&#24449;&#26469;&#37325;&#26500;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#23427;&#26159;&#24212;&#23545;&#32500;&#24230;&#28798;&#38590;&#12289;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12289;&#20943;&#23569;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#25193;&#23637;&#32463;&#20856;&#27169;&#22411;&#36866;&#29992;&#24615;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#24449;&#24037;&#31243;&#25110;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#26377;&#35265;&#22320;&#65292;&#20294;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#26080;&#27861;&#20135;&#29983;&#21487;&#36861;&#28335;&#19988;&#26368;&#20248;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#19968;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#26102;&#65292;&#25105;&#20204;&#33021;&#21542;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65311;&#25105;&#20204;&#30340;&#21021;&#27493;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20248;&#21270;&#26694;&#26550;&#26397;&#30528;&#36825;&#20010;&#25361;&#25112;&#36808;&#20986;&#20102;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#19977;&#20010;&#32423;&#32852;&#24378;&#21270;&#20195;&#29702;&#30340;&#21147;&#37327;&#65292;&#33258;&#21160;&#36873;&#25321;&#20505;&#36873;&#29305;&#24449;&#21644;&#25805;&#20316;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#29305;&#24449;&#36716;&#25442;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine-learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16884</link><description>&lt;p&gt;
&#38750;&#20256;&#36882;&#24615;&#28216;&#25103;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Policy Space Diversity for Non-Transitive Games. (arXiv:2306.16884v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16884
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#22312;&#22810;&#26234;&#33021;&#20307;&#38750;&#20256;&#36882;&#24615;&#28216;&#25103;&#20013;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#37325;&#35201;&#31639;&#27861;&#26694;&#26550;&#12290;&#35768;&#22810;&#20043;&#21069;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#23581;&#35797;&#20419;&#36827;PSRO&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#22312;&#20110;&#26356;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#65288;&#26681;&#25454;&#22810;&#26679;&#24615;&#24230;&#37327;&#65289;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#26356;&#22909;&#22320;&#36924;&#36817;NE&#65288;&#27491;&#22914;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#35777;&#26126;&#30340;&#37027;&#26679;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#20854;&#25913;&#36827;&#20445;&#35777;&#20102;&#26356;&#22909;&#22320;&#36924;&#36817;NE&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#27491;&#24403;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#29366;&#24577;-&#34892;&#21160;&#26679;&#26412;&#26469;&#20248;&#21270;&#25105;&#20204;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#27491;&#21017;&#21270;&#32435;&#20837;PSRO&#20013;&#30340;&#26368;&#20339;&#24212;&#31572;&#27714;&#35299;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;PSRO&#21464;&#31181;&#65292;&#21363;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;PSRO&#65288;PSD-PSRO&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PSD-PSRO&#30340;&#25910;&#25947;&#24615;&#12290;&#32463;&#39564;&#19978;&#65292;&#22312;&#21508;&#31181;&#28216;&#25103;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;PSD-PSRO&#22312;&#20419;&#36827;&#31574;&#30053;&#22810;&#26679;&#24615;&#12289;&#25552;&#39640;&#36924;&#36817;NE&#25928;&#26524;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games. Many previous studies have been trying to promote policy diversity in PSRO. A major weakness in existing diversity metrics is that a more diverse (according to their diversity metrics) population does not necessarily mean (as we proved in the paper) a better approximation to a NE. To alleviate this problem, we propose a new diversity metric, the improvement of which guarantees a better approximation to a NE. Meanwhile, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, Policy Space Diversity PSRO (PSD-PSRO). We present the convergence property of PSD-PSRO. Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in prod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#21010;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.16879</link><description>&lt;p&gt;
&#25163;&#26415;&#38454;&#27573;&#21644;&#22120;&#26800;&#35782;&#21035;&#65306;&#22914;&#20309;&#35782;&#21035;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits. (arXiv:2306.16879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#24320;&#21457;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#30340;&#22797;&#26434;&#24615;&#36136;&#65292;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#26159;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#65292;&#23558;&#25968;&#25454;&#20180;&#32454;&#21010;&#20998;&#20026;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#20197;&#21450;&#36873;&#25321;&#21512;&#36866;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#38598;&#21010;&#20998;&#30340;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#25152;&#25552;&#20986;&#30340;&#21487;&#35270;&#21270;&#26694;&#26550;&#26377;&#21161;&#20110;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#12290;&#30446;&#21069;&#65292;&#23427;&#25903;&#25345;&#25163;&#26415;&#38454;&#27573;&#21644;&#22120;&#26800;&#27880;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#32467;&#26524;&#65306;&#20026;&#20102;&#39564;&#35777;&#19987;&#29992;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Purpose: The development of machine learning models for surgical workflow and instrument recognition from temporal data represents a challenging task due to the complex nature of surgical workflows. In particular, the imbalanced distribution of data is one of the major challenges in the domain of surgical workflow recognition. In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial. Methods: In this work, we present an openly available web-based application that enables interactive exploration of dataset partitions. The proposed visual framework facilitates the assessment of dataset splits for surgical workflow recognition, especially with regard to identifying sub-optimal dataset splits. Currently, it supports visualization of surgical phase and instrument annotations. Results: In order to validate the dedicated interactive visualizations, we use a dataset split of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#20445;&#30041;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#65292;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.16873</link><description>&lt;p&gt;
&#29702;&#35299;&#36845;&#20195;&#20803;&#35757;&#32451;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#20445;&#30041;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#65292;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20004;&#38454;&#27573;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#36973;&#21463;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#36807;&#24230;&#21306;&#20998;&#36896;&#25104;&#30340;&#65292;&#21363;&#27169;&#22411;&#23398;&#20064;&#36807;&#20110;&#20381;&#36182;&#36866;&#21512;&#22522;&#31867;&#21306;&#20998;&#30340;&#34920;&#38754;&#29305;&#24449;&#65292;&#32780;&#25233;&#21046;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#30041;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#39564;&#35777;&#20934;&#30830;&#29575;&#26368;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#38480;&#21046;&#20102;&#23398;&#29983;&#27169;&#22411;&#32447;&#24615;&#20998;&#31867;&#22120;&#36755;&#20986;&#20998;&#24067;&#19982;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#31216;Kullback-Leibler&#65288;SKL&#65289;&#25955;&#24230;&#12290;&#36825;&#19968;&#31616;&#21333;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29992;&#20110;&#20803;&#35757;&#32451;&#30340;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#65292;&#20197;&#25512;&#21160;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of two-stage few-shot classification methods, in the episodic meta-training stage, the model suffers severe overfitting. We hypothesize that it is caused by over-discrimination, i.e., the model learns to over-rely on the superficial features that fit for base class discrimination while suppressing the novel class generalization. To penalize over-discrimination, we introduce knowledge distillation techniques to keep novel generalization knowledge from the teacher model during training. Specifically, we select the teacher model as the one with the best validation accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL) divergence between the output distribution of the linear classifier of the teacher model and that of the student model. This simple approach outperforms the standard meta-training process. We further propose the Nearest Neighbor Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the limits of knowledge distill
&lt;/p&gt;</description></item><item><title>NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16869</link><description>&lt;p&gt;
NeuralFuse: &#23398;&#20064;&#25913;&#21892;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16869
&lt;/p&gt;
&lt;p&gt;
NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#33021;&#37327;&#28040;&#32791;&#20173;&#28982;&#26159;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#26159;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#20013;&#65292;&#32780;SRAM&#20013;&#20250;&#21457;&#29983;&#38543;&#26426;&#20301;&#32763;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NeuralFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#22312;&#20302;&#30005;&#21387;&#29615;&#22659;&#20013;&#35299;&#20915;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;NeuralFuse&#22312;&#26631;&#31216;&#30005;&#21387;&#21644;&#20302;&#30005;&#21387;&#24773;&#20917;&#19979;&#37117;&#33021;&#20445;&#25252;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;NeuralFuse&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#26377;&#38480;&#35775;&#38382;&#30340;DNN&#65292;&#20363;&#22914;&#19981;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#25110;&#20113;&#31471;API&#30340;&#36828;&#31243;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1%&#30340;&#20301;&#38169;&#35823;&#29575;&#19979;&#65292;NeuralFuse&#21487;&#20197;&#23558;SRAM&#20869;&#23384;&#35775;&#38382;&#33021;&#37327;&#38477;&#20302;&#39640;&#36798;24%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
&lt;/p&gt;</description></item><item><title>ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16857</link><description>&lt;p&gt;
ArrayBot: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch. (arXiv:2306.16857v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16857
&lt;/p&gt;
&lt;p&gt;
ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ArrayBot&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;16&#215;16&#30340;&#31446;&#21521;&#28369;&#21160;&#26609;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#32452;&#25104;&#30340;&#20998;&#24067;&#24335;&#25805;&#20316;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#12289;&#24863;&#30693;&#21644;&#25805;&#20316;&#26700;&#38754;&#19978;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#25511;&#21046;&#31574;&#30053;&#12290;&#38754;&#23545;&#22823;&#37327;&#20887;&#20313;&#30340;&#21160;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#31354;&#38388;&#23616;&#37096;&#21160;&#20316;&#22270;&#22359;&#21644;&#39057;&#22495;&#20013;&#20302;&#39057;&#21160;&#20316;&#26469;&#37325;&#26032;&#23450;&#20041;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#20010;&#37325;&#26032;&#23450;&#20041;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25105;&#20204;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21482;&#36890;&#36807;&#35302;&#35273;&#35266;&#23519;&#21363;&#21487;&#37325;&#26032;&#23450;&#20301;&#19981;&#21516;&#30340;&#29289;&#20307;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21457;&#29616;&#30340;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#27169;&#25311;&#22120;&#20013;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#36716;&#31227;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#22495;&#38543;&#26426;&#21270;&#12290;&#21033;&#29992;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20016;&#23500;&#30340;&#30495;&#23454;&#19990;&#30028;&#25805;&#20316;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#20854;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ArrayBot, a distributed manipulation system consisting of a $16 \times 16$ array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Surprisingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also transfer to the physical robot without any domain randomization. Leveraging the deployed policy, we present abundant real-world manipulation tasks, illustrating the vast potential of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#26159;&#21542;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#35782;&#21035;&#24120;&#35268;&#35821;&#35328;&#30340;RNNs&#26469;&#39564;&#35777;&#32858;&#31867;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16854</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#21644;&#35821;&#20041;&#22522;&#26412;&#20107;&#23454;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth. (arXiv:2306.16854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#26159;&#21542;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#35782;&#21035;&#24120;&#35268;&#35821;&#35328;&#30340;RNNs&#26469;&#39564;&#35777;&#32858;&#31867;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#26159;&#21542;&#20542;&#21521;&#20110;&#24418;&#25104;&#35821;&#20041;&#30456;&#20284;&#21521;&#37327;&#30340;&#31751;&#32676;&#65292;&#36825;&#20010;&#20551;&#35774;&#34987;&#31216;&#20026;&#32858;&#31867;&#20551;&#35774;&#12290;&#34429;&#28982;&#36825;&#20010;&#20551;&#35774;&#22312;&#36817;&#24180;&#26469;&#23545;RNNs&#30340;&#20998;&#26512;&#20013;&#24050;&#32463;&#34987;&#20551;&#23450;&#65292;&#20294;&#20854;&#22312;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#24120;&#35268;&#35821;&#35328;&#30340;RNNs&#30340;&#32972;&#26223;&#19979;&#32771;&#23519;&#20102;&#32858;&#31867;&#20551;&#35774;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#35780;&#20272;&#20013;&#21033;&#29992;&#23436;&#32654;&#30340;&#22522;&#20934;&#33258;&#21160;&#26426;&#65292;&#19982;&#20854;&#27604;&#36739;RNN&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#23558;RNN&#30340;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#22312;&#35821;&#20041;&#19978;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#30340;&#65288;&#20998;&#27573;&#32447;&#24615;&#65289;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#26368;&#26032;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#35745;&#31639;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#31751;&#32676;&#12290;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#20102;&#35745;&#31639;&#32858;&#31867;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;&#32858;&#31867;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the assumption that the hidden-state vectors of recurrent neural networks (RNNs) tend to form clusters of semantically similar vectors, which we dub the clustering hypothesis. While this hypothesis has been assumed in the analysis of RNNs in recent years, its validity has not been studied thoroughly on modern neural network architectures. We examine the clustering hypothesis in the context of RNNs that were trained to recognize regular languages. This enables us to draw on perfect ground-truth automata in our evaluation, against which we can compare the RNN's accuracy and the distribution of the hidden-state vectors.  We start with examining the (piecewise linear) separability of an RNN's hidden-state vectors into semantically different classes. We continue the analysis by computing clusters over the hidden-state vector space with multiple state-of-the-art unsupervised clustering approaches. We formally analyze the accuracy of computed clustering functions and the validity o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;</title><link>http://arxiv.org/abs/2306.16844</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#40657;&#30418;&#20248;&#21270;&#23454;&#29616;&#23439;&#21333;&#20803;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22823;&#35268;&#27169;&#38598;&#25104;&#65288;VLSI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#33455;&#29255;&#24067;&#23616;&#20013;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#25216;&#26415;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#23439;&#21333;&#20803;&#24067;&#23616;&#20316;&#20026;&#35813;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#23376;&#38382;&#39064;&#65292;&#35797;&#22270;&#30830;&#23450;&#25152;&#26377;&#23439;&#21333;&#20803;&#30340;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#21322;&#21608;&#38271;&#32447;&#38271;&#65288;HPWL&#65289;&#24182;&#36991;&#20813;&#37325;&#21472;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#25171;&#21253;&#12289;&#20998;&#26512;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#26694;&#26550;&#65288;&#31216;&#20026;WireMask-BBO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#30446;&#26631;&#35780;&#20272;&#26469;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#12290;&#37197;&#22791;&#19981;&#21516;&#30340;BBO&#31639;&#27861;&#65292;WireMask-BBO&#22312;&#23454;&#36341;&#20013;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#26102;&#38388;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#30701;&#30340;HPWL&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#35270;&#20026;&#21021;&#22987;&#35299;&#26469;&#24494;&#35843;&#29616;&#26377;&#30340;&#24067;&#23616;&#65292;&#20174;&#32780;&#20351;HPWL&#25913;&#21892;&#22810;&#36798;50%&#12290;WireMask-BBO&#20855;&#26377;&#24341;&#39046;&#33455;&#29255;&#24067;&#23616;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#26799;&#24230;&#65292;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#30340;&#21464;&#25442;&#21644;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16830</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37319;&#26679;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16830
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#26799;&#24230;&#65292;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#30340;&#21464;&#25442;&#21644;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#21644;&#26377;&#25928;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#19981;&#38656;&#35201;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#20869;&#37096;&#32593;&#32476;&#21442;&#25968;&#30340;&#26799;&#24230;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#37319;&#26679;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#24605;&#24819;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#36755;&#20837;&#21644;&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#23545;&#27973;&#23618;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25968;&#25454;&#26080;&#20851;&#30340;&#20998;&#24067;&#65292;&#22914;&#27491;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26500;&#36896;&#30340;&#37319;&#26679;&#32593;&#32476;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#26041;&#26696;&#23545;&#21018;&#20307;&#21464;&#25442;&#21644;&#36755;&#20837;&#25968;&#25454;&#30340;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#24120;&#29992;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#19981;&#20877;&#38656;&#35201;&#12290;&#23545;&#20110;&#24052;&#40857;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37319;&#26679;&#27973;&#23618;&#32593;&#32476;&#30340;L^2&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#20943;&#23567;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex
&lt;/p&gt;</description></item><item><title>SaGess&#26159;&#19968;&#31181;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#65288;DiGress&#65289;&#19982;&#24191;&#20041;&#30340;&#20998;&#27835;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#22411;&#23454;&#38469;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#29983;&#25104;&#22823;&#22411;&#22270;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16827</link><description>&lt;p&gt;
SaGess&#65306;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#29983;&#25104;&#30340;&#37319;&#26679;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SaGess: Sampling Graph Denoising Diffusion Model for Scalable Graph Generation. (arXiv:2306.16827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16827
&lt;/p&gt;
&lt;p&gt;
SaGess&#26159;&#19968;&#31181;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#65288;DiGress&#65289;&#19982;&#24191;&#20041;&#30340;&#20998;&#27835;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#22411;&#23454;&#38469;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#29983;&#25104;&#22823;&#22411;&#22270;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#34920;&#26684;&#21644;&#22270;&#24418;&#25968;&#25454;&#29983;&#25104;&#31561;&#20854;&#20182;&#24212;&#29992;&#20013;&#20063;&#35777;&#26126;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#23567;&#22411;&#22270;&#65292;&#20363;&#22914;&#22312;&#20998;&#23376;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SaGess&#65292;&#19968;&#31181;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#65288;DiGress&#65289;&#19982;&#24191;&#20041;&#30340;&#20998;&#27835;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#22411;&#23454;&#38469;&#32593;&#32476;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#21021;&#22987;&#22270;&#30340;&#23376;&#22270;&#36827;&#34892;&#37319;&#26679;&#26469;&#35757;&#32451;DiGress&#65292;&#28982;&#21518;&#20351;&#29992;DiGress&#29983;&#25104;&#30340;&#23376;&#22270;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, denoising diffusion generative models have come to be considered as state-of-the-art methods for synthetic data generation, especially in the case of generating images. These approaches have also proved successful in other applications such as tabular and graph data generation. However, due to computational complexity, to this date, the application of these techniques to graph data has been restricted to small graphs, such as those used in molecular modeling. In this paper, we propose SaGess, a discrete denoising diffusion approach, which is able to generate large real-world networks by augmenting a diffusion model (DiGress) with a generalized divide-and-conquer framework. The algorithm is capable of generating larger graphs by sampling a covering of subgraphs of the initial graph in order to train DiGress. SaGess then constructs a synthetic graph using the subgraphs that have been generated by DiGress. We evaluate the quality of the synthetic data sets against sever
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#26469;&#39044;&#27979;&#21307;&#38498;&#31649;&#29702;&#20013;&#30340;&#20303;&#38498;&#26102;&#38271;&#65292;&#20197;&#24110;&#21161;&#21307;&#38498;&#26377;&#25928;&#35268;&#21010;&#20837;&#38498;&#12289;&#20998;&#37197;&#36164;&#28304;&#21644;&#25913;&#21892;&#25252;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.16823</link><description>&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#21307;&#38498;&#31649;&#29702;&#20013;&#30340;&#20303;&#38498;&#26102;&#38271;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Length of Stay prediction for Hospital Management using Domain Adaptation. (arXiv:2306.16823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#26469;&#39044;&#27979;&#21307;&#38498;&#31649;&#29702;&#20013;&#30340;&#20303;&#38498;&#26102;&#38271;&#65292;&#20197;&#24110;&#21161;&#21307;&#38498;&#26377;&#25928;&#35268;&#21010;&#20837;&#38498;&#12289;&#20998;&#37197;&#36164;&#28304;&#21644;&#25913;&#21892;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20303;&#38498;&#26102;&#38271;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#31649;&#29702;&#25351;&#26631;&#65292;&#22914;&#26524;&#25552;&#21069;&#30693;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#26377;&#25928;&#22320;&#35268;&#21010;&#20837;&#38498;&#12289;&#20998;&#37197;&#36164;&#28304;&#21644;&#25913;&#21892;&#25252;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#24739;&#32773;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#20303;&#38498;&#26102;&#38271;&#39044;&#27979;&#27169;&#22411;&#12290;&#20174;&#20262;&#29702;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#29992;&#20110;&#26367;&#20195;&#37096;&#38376;&#20027;&#31649;&#23545;&#20110;&#24739;&#32773;&#20986;&#38498;&#30340;&#20915;&#31574;&#65292;&#20294;&#23545;&#20110;&#36127;&#36131;&#26377;&#25928;&#21307;&#38498;&#35268;&#21010;&#30340;&#21307;&#38498;&#31649;&#29702;&#31995;&#32479;&#26469;&#35828;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#31995;&#32479;&#30340;&#35774;&#35745;&#24212;&#35813;&#36866;&#24212;&#30495;&#23454;&#30340;&#21307;&#38498;&#29615;&#22659;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#65292;&#22312;&#20837;&#38498;&#21333;&#20301;&#30340;&#32454;&#31890;&#24230;&#23618;&#38754;&#19978;&#39044;&#27979;&#26089;&#26399;&#20303;&#38498;&#26102;&#38271;&#65292;&#20197;&#21033;&#29992;&#20174;&#28508;&#22312;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#20174;eICU-CRD&#21644;MIMIC-IV&#20998;&#21035;&#25552;&#21462;&#20102;110,079&#27425;&#21644;60,492&#27425;&#24739;&#32773;&#20837;&#20303;9&#20010;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#30340;&#26102;&#21464;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#34987;&#36755;&#20837;&#21040;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#65292;&#35757;&#32451;&#20986;&#19968;&#20010;&#28304;&#39046;&#22495;&#27169;&#22411;&#65292;wei
&lt;/p&gt;
&lt;p&gt;
Inpatient length of stay (LoS) is an important managerial metric which if known in advance can be used to efficiently plan admissions, allocate resources and improve care. Using historical patient data and machine learning techniques, LoS prediction models can be developed. Ethically, these models can not be used for patient discharge in lieu of unit heads but are of utmost necessity for hospital management systems in charge of effective hospital planning. Therefore, the design of the prediction system should be adapted to work in a true hospital setting. In this study, we predict early hospital LoS at the granular level of admission units by applying domain adaptation to leverage information learned from a potential source domain. Time-varying data from 110,079 and 60,492 patient stays to 8 and 9 intensive care units were respectively extracted from eICU-CRD and MIMIC-IV. These were fed into a Long-Short Term Memory and a Fully connected network to train a source domain model, the wei
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.16817</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#38598;&#25104;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#36845;&#20195;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#21644;&#22312;&#32447;&#26041;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65306;(1)&#22312;&#32447;&#35774;&#32622;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;(2)&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#20960;&#31687;&#26368;&#36817;&#30340;&#25991;&#31456;&#34920;&#26126;&#36830;&#32493;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#37325;&#25918;&#26041;&#27861;&#22312;&#27169;&#22411;&#25345;&#32493;&#35780;&#20272;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31616;&#21333;&#22320;&#38598;&#25104;&#26469;&#33258;&#21508;&#31181;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#20174;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.16805</link><description>&lt;p&gt;
CLIPAG: &#36208;&#21521;&#26080;&#38656;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#23545;&#40784;&#26799;&#24230; (Perceptually Aligned Gradients, PAG) &#26159;&#22312;&#20581;&#22766;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#26377;&#36259;&#23646;&#24615;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#36755;&#20837;&#28176;&#21464;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#24182;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#12290;&#34429;&#28982;&#36825;&#19968;&#29616;&#35937;&#24341;&#36215;&#20102;&#26174;&#30528;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20165;&#20165;&#22312;&#21333;&#27169;&#24577;&#32431;&#35270;&#35273;&#26550;&#26500;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558; PAG &#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#36825;&#26159;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#40065;&#26834;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102; PAG&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102; CLIPAG &#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#32541;&#38598;&#25104; CLIPAG &#30340; "&#21363;&#25554;&#21363;&#29992;" &#26041;&#24335;&#26174;&#33879;&#25913;&#36827;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20854; PAG &#23646;&#24615;&#65292;CLIPAG &#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#27979;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#22870;&#21169;&#23545;&#35937;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#33719;&#24471;&#20102;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16803</link><description>&lt;p&gt;
&#38271;&#26399;&#20449;&#29992;&#24402;&#22240;&#36890;&#36807;&#21453;&#20107;&#23454;&#36129;&#29486;&#20998;&#26512;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. (arXiv:2306.16803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#27979;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#22870;&#21169;&#23545;&#35937;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#33719;&#24471;&#20102;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22909;&#30340;&#20449;&#29992;&#24402;&#22240;&#26041;&#27861;&#26469;&#34913;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#22312;&#24724;&#26827;&#20449;&#29992;&#24402;&#22240;&#65288;HCA&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36129;&#29486;&#20998;&#26512;&#65288;COCOA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#24402;&#22240;&#31639;&#27861;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#37327;&#21270;&#19968;&#20010;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#20449;&#29992;&#20998;&#37197;&#65306;&#8220;&#22914;&#26524;&#20195;&#29702;&#36873;&#25321;&#21478;&#19968;&#20010;&#21160;&#20316;&#65292;&#23427;&#20173;&#28982;&#20250;&#33719;&#24471;&#36825;&#20010;&#22870;&#21169;&#21527;&#65311;&#8221;&#36890;&#36807;&#27979;&#37327;&#21160;&#20316;&#23545;&#33719;&#24471;&#21518;&#32493;&#22870;&#21169;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#22870;&#21169;&#29366;&#24577;&#27979;&#37327;&#36129;&#29486;&#65288;&#21363;HCA&#20013;&#25152;&#20570;&#30340;&#65289;&#20250;&#23548;&#33268;&#36129;&#29486;&#30340;&#38169;&#35823;&#20272;&#35745;&#65292;&#20351;&#24471;HCA&#22312;&#35768;&#22810;&#30456;&#20851;&#29615;&#22659;&#20013;&#21521;&#39640;&#26041;&#24046;&#30340;REINFORCE&#20272;&#35745;&#22120;&#36864;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#25152;&#23398;&#20064;&#30340;&#22870;&#21169;&#23545;&#35937;&#30340;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#24471;&#21040;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#29305;&#23450;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: "Would the agent still have reached this reward if it had taken another action?". We show that measuring contributions w.r.t. rewarding states, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#37319;&#26679;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#20998;&#23376;-&#23646;&#24615;&#20851;&#31995;&#22270;&#24182;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#35843;&#24230;&#23376;&#22270;&#37319;&#26679;&#36807;&#31243;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#23545;&#22810;&#20851;&#31995;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16780</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#37319;&#26679;&#30340;&#20803;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Sampling-based Meta-Learning for Molecular Property Prediction. (arXiv:2306.16780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#37319;&#26679;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#20998;&#23376;-&#23646;&#24615;&#20851;&#31995;&#22270;&#24182;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#35843;&#24230;&#23376;&#22270;&#37319;&#26679;&#36807;&#31243;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#23545;&#22810;&#20851;&#31995;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#26377;&#38480;&#25968;&#30446;&#30340;&#26679;&#26412;&#35266;&#27979;&#21040;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#23646;&#24615;&#39044;&#27979;&#35270;&#20026;&#23569;&#26679;&#26412;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#21363;&#27599;&#20010;&#20998;&#23376;&#21487;&#20197;&#21516;&#26102;&#35760;&#24405;&#22810;&#20010;&#19981;&#21516;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#20998;&#23376;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#22810;&#23545;&#22810;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#37319;&#26679;&#30340;&#20803;&#23398;&#20064;&#65288;GS-Meta&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#23376;-&#23646;&#24615;&#20851;&#31995;&#22270;&#65288;MPG&#65289;&#65306;&#20998;&#23376;&#21644;&#23646;&#24615;&#20316;&#20026;&#33410;&#28857;&#65292;&#32780;&#23646;&#24615;&#26631;&#31614;&#20915;&#23450;&#36793;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#21033;&#29992;MPG&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20803;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;episode&#37325;&#26032;&#23450;&#20041;&#20026;MPG&#30340;&#23376;&#22270;&#65292;&#20854;&#20013;&#21253;&#21547;&#30446;&#26631;&#23646;&#24615;&#33410;&#28857;&#12289;&#20998;&#23376;&#33410;&#28857;&#21644;&#36741;&#21161;&#23646;&#24615;&#33410;&#28857;&#12290;&#31532;&#19977;&#65292;&#30001;&#20110;&#23376;&#22270;&#24418;&#24335;&#30340;episode&#19981;&#20877;&#30456;&#20114;&#29420;&#31435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26469;&#35843;&#24230;&#23376;&#22270;&#37319;&#26679;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#37319;&#26679;&#30340;episode&#20855;&#26377;&#23545;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property is usually observed with a limited number of samples, and researchers have considered property prediction as a few-shot problem. One important fact that has been ignored by prior works is that each molecule can be recorded with several different properties simultaneously. To effectively utilize many-to-many correlations of molecules and properties, we propose a Graph Sampling-based Meta-learning (GS-Meta) framework for few-shot molecular property prediction. First, we construct a Molecule-Property relation Graph (MPG): molecule and properties are nodes, while property labels decide edges. Then, to utilize the topological information of MPG, we reformulate an episode in meta-learning as a subgraph of the MPG, containing a target property node, molecule nodes, and auxiliary property nodes. Third, as episodes in the form of subgraphs are no longer independent of each other, we propose to schedule the subgraph sampling process with a contrastive loss function, which cons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SimDIT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24615;&#33021;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#29992;ASIC&#31995;&#32479;&#32423;&#30828;&#20214;&#21152;&#36895;&#22120;&#24179;&#21488;&#19978;&#30340;CNN&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;SimDIT&#32508;&#21512;&#32771;&#34385;&#20102;&#21367;&#31215;&#21644;&#38750;&#21367;&#31215;&#25805;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#24615;&#33021;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.16767</link><description>&lt;p&gt;
DNN&#25512;&#29702;/&#35757;&#32451;&#20013;&#21367;&#31215;&#21644;&#38750;&#21367;&#31215;&#25805;&#20316;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of DNN Inference/Training with Convolution and non-Convolution Operations. (arXiv:2306.16767v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SimDIT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24615;&#33021;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#29992;ASIC&#31995;&#32479;&#32423;&#30828;&#20214;&#21152;&#36895;&#22120;&#24179;&#21488;&#19978;&#30340;CNN&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;SimDIT&#32508;&#21512;&#32771;&#34385;&#20102;&#21367;&#31215;&#21644;&#38750;&#21367;&#31215;&#25805;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#24615;&#33021;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#20998;&#26512;&#26694;&#26550;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23613;&#31649;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38500;&#20102;&#21367;&#31215;&#23618;&#20043;&#22806;&#36824;&#21253;&#25324;&#35768;&#22810;&#20854;&#20182;&#31867;&#22411;&#30340;&#23618;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23588;&#20854;&#22914;&#27492;&#65292;&#20294;&#36825;&#20123;&#26694;&#26550;&#20027;&#35201;&#38598;&#20013;&#22312;&#21367;&#31215;&#23618;&#19978;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#26694;&#26550;&#36890;&#24120;&#38024;&#23545;&#25512;&#29702;&#65292;&#32570;&#20047;&#23545;&#35757;&#32451;&#25805;&#20316;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24615;&#33021;&#20998;&#26512;&#26694;&#26550;SimDIT&#65292;&#29992;&#20110;&#36890;&#29992;ASIC&#31995;&#32479;&#32423;&#30828;&#20214;&#21152;&#36895;&#22120;&#24179;&#21488;&#12290;SimDIT&#30340;&#24314;&#27169;&#24037;&#20316;&#20840;&#38754;&#35206;&#30422;&#20102;CNN&#25512;&#29702;&#21644;&#35757;&#32451;&#20013;&#30340;&#21367;&#31215;&#21644;&#38750;&#21367;&#31215;&#25805;&#20316;&#65292;&#24182;&#22312;&#39640;&#24230;&#21487;&#21442;&#25968;&#21270;&#30340;&#30828;&#20214;&#24213;&#23618;&#19978;&#12290;SimDIT&#19982;&#21518;&#31471;&#30789;&#23454;&#29616;&#27969;&#31243;&#38598;&#25104;&#65292;&#20026;&#25191;&#34892;CNN&#25512;&#29702;&#21644;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#32479;&#35745;&#65288;&#22914;&#25968;&#25454;&#35775;&#38382;&#25104;&#26412;&#12289;&#24490;&#29615;&#35745;&#25968;&#12289;&#33021;&#37327;&#21644;&#21151;&#29575;&#65289;&#12290;SimDIT&#30340;&#24615;&#33021;&#20998;&#26512;&#21151;&#33021;&#20026;&#24615;&#33021;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's performance analysis frameworks for deep learning accelerators suffer from two significant limitations. First, although modern convolutional neural network (CNNs) consist of many types of layers other than convolution, especially during training, these frameworks largely focus on convolution layers only. Second, these frameworks are generally targeted towards inference, and lack support for training operations. This work proposes a novel performance analysis framework, SimDIT, for general ASIC-based systolic hardware accelerator platforms. The modeling effort of SimDIT comprehensively covers convolution and non-convolution operations of both CNN inference and training on a highly parameterizable hardware substrate. SimDIT is integrated with a backend silicon implementation flow and provides detailed end-to-end performance statistics (i.e., data access cost, cycle counts, energy, and power) for executing CNN inference and training workloads. SimDIT-enabled performance analysis r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16761</link><description>&lt;p&gt;
&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. (arXiv:2306.16761v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ye&#31561;&#20154;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#20915;&#29305;&#23450;&#31867;&#21035;&#21452;&#23618;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#37325;&#28857;&#26159;&#19982;&#36229;&#21442;&#25968;&#36873;&#25321;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22522;&#20110;&#20540;&#20989;&#25968;&#26041;&#27861;&#25913;&#20889;&#30340;&#24046;&#20998;&#20984;&#31639;&#27861;&#12290;&#22312;&#19979;&#23618;&#38382;&#39064;&#23436;&#20840;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#25110;&#26368;&#23567;&#32477;&#23545;&#25910;&#32553;&#21644;&#36873;&#25321;&#31639;&#23376;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36866;&#24212;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#19979;&#23618;&#23436;&#20840;&#20984;&#24615;&#30340;&#22522;&#26412;&#20551;&#35774;&#22823;&#22823;&#21066;&#24369;&#20026;&#24369;&#20984;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19979;&#23618;&#38382;&#39064;&#30340;Moreau&#21253;&#32476;&#36827;&#34892;&#37325;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#37325;&#26500;&#26159;&#19968;&#20010;&#24369;&#20984;&#24046;&#20998;&#35268;&#21010;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36880;&#27493;&#25910;&#25947;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24369;&#20984;&#24046;&#20998;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ye et al. (Mathematical Programming 2023) designed an algorithm for solving a specific class of bilevel programs with an emphasis on applications related to hyperparameter selection, utilizing the difference of convex algorithm based on the value function approach reformulation. The proposed algorithm is particularly powerful when the lower level problem is fully convex , such as a support vector machine model or a least absolute shrinkage and selection operator model. In this paper, to suit more applications related to machine learning and statistics, we substantially weaken the underlying assumption from lower level full convexity to weak convexity. Accordingly, we propose a new reformulation using Moreau envelope of the lower level problem and demonstrate that this reformulation is a difference of weakly convex program. Subsequently, we develop a sequentially convergent algorithm for solving this difference of weakly convex program. To evaluate the effectiveness of our app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#20915;&#40479;&#40483;&#20998;&#31867;&#27604;&#36187;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16760</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#40479;&#40483;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification. (arXiv:2306.16760v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16760
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#20915;&#40479;&#40483;&#20998;&#31867;&#27604;&#36187;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#40479;&#40483;&#20998;&#31867;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#24037;&#20316;&#31508;&#35760;&#65292;&#37325;&#28857;&#26159;&#22312;&#35760;&#24405;&#30340;&#22768;&#26223;&#20013;&#35782;&#21035;&#38750;&#27954;&#40479;&#31867;&#29289;&#31181;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#29616;&#25104;&#27169;&#22411;BirdNET&#21644;MixIT&#26469;&#35299;&#20915;&#27604;&#36187;&#20013;&#30340;&#34920;&#31034;&#21644;&#26631;&#27880;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;BirdNET&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#21508;&#31181;&#27169;&#22411;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#20197;&#22312;&#27604;&#36187;&#25490;&#34892;&#27036;&#19978;&#36798;&#21040;&#26368;&#20339;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes. Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition. We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning. Our experiments involve various models and feature engineering approaches to maximize performance on the competition leaderboard. The results demonstrate the effectiveness of our approach in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.
&lt;/p&gt;</description></item><item><title>ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16750</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#21450;&#20854;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16750
&lt;/p&gt;
&lt;p&gt;
ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#29305;&#24449;&#23376;&#31354;&#38388;&#35268;&#33539;&#21270;&#25209;&#35780;&#23478;&#65288;ERC&#65289;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290; ERC&#21463;&#21040;&#20102;&#23545;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#26041;&#27861;&#20013;Q&#20540;&#20272;&#35745;&#35823;&#24046;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#30001;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30456;&#20851;&#30340;&#36716;&#31227;&#26680;&#20851;&#32852;&#30340;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#23450;&#20041;&#30340;&#36335;&#24452;&#12290;&#23427;&#25581;&#31034;&#20102;TD&#23398;&#20064;&#30340;&#19968;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#22312;&#20808;&#21069;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#20013;&#26410;&#34987;&#20351;&#29992;&#12290;&#22312;ERC&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#25351;&#23548;&#36817;&#20284;&#35823;&#24046;&#36235;&#21521;&#20110;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#39640;&#25928;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ERC&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#22312;DMControl&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#20219;&#21153;&#20013;&#65292;ERC&#20248;&#20110;20&#20010;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;Q&#20540;&#20272;&#35745;&#26041;&#38754;&#20063;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel value approximation method, namely Eigensubspace Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated by an analysis of the dynamics of Q-value approximation error in the Temporal-Difference (TD) method, which follows a path defined by the 1-eigensubspace of the transition kernel associated with the Markov Decision Process (MDP). It reveals a fundamental property of TD learning that has remained unused in previous deep RL approaches. In ERC, we propose a regularizer that guides the approximation error tending towards the 1-eigensubspace, resulting in a more efficient and stable path of value approximation. Moreover, we theoretically prove the convergence of the ERC method. Besides, theoretical analysis and experiments demonstrate that ERC effectively reduces the variance of value functions. Among 26 tasks in the DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides, it shows significant advantages in Q-value approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FRAT&#65292;&#29992;&#20110;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#20013;&#20248;&#21270;&#38543;&#26426;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#32500;&#25252;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#28151;&#21512;&#26469;&#36798;&#21040;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.16738</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#25239;&#20013;&#20248;&#21270;&#38543;&#26426;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Randomized Strategies in Adversarial Example Game. (arXiv:2306.16738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FRAT&#65292;&#29992;&#20110;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#20013;&#20248;&#21270;&#38543;&#26426;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#32500;&#25252;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#28151;&#21512;&#26469;&#36798;&#21040;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#20351;&#29992;&#38543;&#26426;&#21270;&#26159;&#25214;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#23436;&#20840;&#38543;&#26426;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38450;&#23432;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#31574;&#30053;&#65292;&#30446;&#21069;&#27809;&#26377;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;FRAT&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;&#32500;&#36830;&#32493;&#26102;&#38388;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#30340;&#27969;&#26469;&#24314;&#27169;&#38382;&#39064;&#12290;FRAT&#20026;&#38450;&#23432;&#32773;&#32500;&#25252;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#28151;&#21512;&#65292;&#20855;&#26377;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#26377;&#25928;&#26356;&#26032;&#28151;&#21512;&#26435;&#37325;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;FRAT&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;&#37319;&#26679;&#23376;&#20363;&#31243;&#26469;&#26500;&#24314;&#25915;&#20987;&#32773;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FRAT&#30340;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications. A recent line of work shows that the use of randomization in adversarial training is the key to find optimal strategies against adversarial example attacks. However, in a fully randomized setting where both the defender and the attacker can use randomized strategies, there are no efficient algorithm for finding such an optimal strategy. To fill the gap, we propose the first algorithm of its kind, called FRAT, which models the problem with a new infinite-dimensional continuous-time flow on probability distribution spaces. FRAT maintains a lightweight mixture of models for the defender, with flexibility to efficiently update mixing weights and model parameters at each iteration. Furthermore, FRAT utilizes lightweight sampling subroutines to construct a random strategy for the attacker. We prove that the continuous-time limit of FRAT converg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#19981;&#20165;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#19988;&#24050;&#32463;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#23384;&#22312;&#12290;&#25968;&#20540;&#27714;&#35299;&#32467;&#26524;&#19982;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2306.16717</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#24322;&#26041;&#24046;&#22238;&#24402;&#30340;&#30149;&#24577;
&lt;/p&gt;
&lt;p&gt;
Understanding Pathologies of Deep Heteroskedastic Regression. (arXiv:2306.16717v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#19981;&#20165;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#19988;&#24050;&#32463;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#23384;&#22312;&#12290;&#25968;&#20540;&#27714;&#35299;&#32467;&#26524;&#19982;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#20351;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24314;&#27169;&#26102;&#20986;&#29616;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#22343;&#20540;&#32593;&#32476;&#21644;&#26041;&#24046;&#32593;&#32476;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#25311;&#21512;&#27599;&#20010;&#25968;&#25454;&#28857;&#65288;&#21516;&#26102;&#23558;&#39044;&#27979;&#30340;&#26041;&#24046;&#25910;&#32553;&#21040;&#38646;&#65289;&#65292;&#25110;&#32773;&#23398;&#20064;&#19968;&#20010;&#24658;&#23450;&#30340;&#39044;&#27979;&#65292;&#36755;&#20986;&#26041;&#24046;&#24688;&#22909;&#21305;&#37197;&#27599;&#20010;&#39044;&#27979;&#27531;&#24046;&#65288;&#21363;&#23558;&#30446;&#26631;&#35299;&#37322;&#20026;&#32431;&#22122;&#22768;&#65289;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20123;&#22256;&#38590;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35266;&#23519;&#21040;&#30340;&#19981;&#31283;&#23450;&#24615;&#19981;&#29305;&#23450;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#26159;&#24050;&#32463;&#23384;&#22312;&#20110;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#12290;&#22312;&#36731;&#24494;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#30340;&#38750;&#21442;&#25968;&#33258;&#30001;&#33021;&#12290;&#24471;&#21040;&#30340;&#35299;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#20855;&#26377;&#33391;&#22909;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#29305;&#21035;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent studies have reported negative results when using heteroskedastic neural regression models to model real-world data. In particular, for overparameterized models, the mean and variance networks are powerful enough to either fit every single data point (while shrinking the predicted variances to zero), or to learn a constant prediction with an output variance exactly matching every predicted residual (i.e., explaining the targets as pure noise). This paper studies these difficulties from the perspective of statistical physics. We show that the observed instabilities are not specific to any neural network architecture but are already present in a field theory of an overparameterized conditional Gaussian likelihood model. Under light assumptions, we derive a nonparametric free energy that can be solved numerically. The resulting solutions show excellent qualitative agreement with empirical model fits on real-world data and, in particular, prove the existence of phase transit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20174;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;Multi Image BART (MI-BART)&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#20351;&#29992;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26368;&#22823;&#30340;RETVQA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;</title><link>http://arxiv.org/abs/2306.16713</link><description>&lt;p&gt;
&#26469;&#33258;&#22270;&#20687;&#27744;&#30340;&#31572;&#26696;&#25366;&#25496;&#65306;&#38754;&#21521;&#22522;&#20110;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering. (arXiv:2306.16713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20174;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;Multi Image BART (MI-BART)&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#20351;&#29992;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26368;&#22823;&#30340;RETVQA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#19968;&#20010;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#39318;&#20808;&#20174;&#22270;&#20687;&#27744;&#20013;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#65292;&#24182;&#20174;&#36825;&#20123;&#26816;&#32034;&#21040;&#30340;&#22270;&#20687;&#20013;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;&#25110;&#31616;&#31216;&#20026;RETVQA&#65289;&#12290;RETVQA&#19982;&#20256;&#32479;&#30740;&#31350;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#30528;&#26126;&#26174;&#19981;&#21516;&#21644;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;VQA&#35201;&#27714;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#20010;&#30456;&#20851;&#22270;&#20687;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;RETVQA&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;Multi Image BART&#65288;MI-BART&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25105;&#20204;&#30340;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;RETVQA&#65292;&#20855;&#26377;&#20197;&#19979;&#26174;&#33879;&#29305;&#28857;&#65306;VQA&#30340;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;Translated from Abstract&#65289;</title><link>http://arxiv.org/abs/2306.16700</link><description>&lt;p&gt;
&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic-Resolution Model Learning for Object Pile Manipulation. (arXiv:2306.16700v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;Translated from Abstract&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#23398;&#20064;&#36825;&#31181;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20351;&#29992;&#20160;&#20040;&#22330;&#26223;&#34920;&#31034;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22266;&#23450;&#32500;&#24230;&#25110;&#20998;&#36776;&#29575;&#30340;&#34920;&#31034;&#65292;&#36825;&#23545;&#31616;&#21333;&#20219;&#21153;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#23545;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#26080;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23398;&#20064;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#25928;&#29575;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23398;&#20064;&#20102;&#32479;&#19968;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20801;&#35768;&#36830;&#32493;&#36873;&#25321;&#25277;&#35937;&#23618;&#27425;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#20195;&#29702;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#27599;&#20010;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27493;&#39588;&#30340;&#26368;&#20339;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#25105;&#20204;&#22312;&#28921;&#39274;, &#20892;&#19994;&#31561;&#39046;&#22495;&#32463;&#24120;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item><item><title>BinaryViT&#26159;&#19968;&#31181;&#38024;&#23545;&#20108;&#20540;&#21270;&#35270;&#35273;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#20511;&#37492;CNN&#30340;&#26550;&#26500;&#29305;&#24615;&#65292;&#25552;&#39640;&#20102;&#20108;&#20540;&#21270;ViT&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16678</link><description>&lt;p&gt;
BinaryViT&#65306;&#23558;&#20108;&#36827;&#21046;&#35270;&#35273;Transformer&#25512;&#21521;&#21367;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models. (arXiv:2306.16678v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16678
&lt;/p&gt;
&lt;p&gt;
BinaryViT&#26159;&#19968;&#31181;&#38024;&#23545;&#20108;&#20540;&#21270;&#35270;&#35273;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#20511;&#37492;CNN&#30340;&#26550;&#26500;&#29305;&#24615;&#65292;&#25552;&#39640;&#20102;&#20108;&#20540;&#21270;ViT&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#26085;&#30410;&#27969;&#34892;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#22914;&#20309;&#20351;&#23427;&#20204;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#39640;&#25928;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;&#36890;&#36807;&#20351;&#29992;&#20108;&#20540;&#21270;&#65292;&#22312;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#20026;&#20108;&#36827;&#21046;&#26102;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;ViT&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#29992;popcount&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#19982;CNN&#22312;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet-1k&#65289;&#19978;&#30452;&#25509;&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20108;&#20540;&#21270;&#26041;&#27861;&#25110;&#29616;&#26377;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ViT&#30340;&#24615;&#33021;&#19979;&#38477;&#26356;&#22823;&#12290;&#32463;&#36807;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20108;&#20540;&#21270;&#30340;&#22522;&#30784;ViT&#65288;&#22914;DeiT&#65289;&#32570;&#23569;&#35768;&#22810;CNN&#25152;&#20855;&#26377;&#30340;&#20851;&#38190;&#26550;&#26500;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#20351;&#20108;&#20540;&#21270;&#30340;CNN&#20855;&#26377;&#27604;&#22522;&#30784;ViT&#26356;&#39640;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BinaryViT&#65292;&#21463;CNN&#26550;&#26500;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity and the increasing size of vision transformers (ViTs), there has been an increasing interest in making them more efficient and less computationally costly for deployment on edge devices with limited computing resources. Binarization can be used to help reduce the size of ViT models and their computational cost significantly, using popcount operations when the weights and the activations are in binary. However, ViTs suffer a larger performance drop when directly applying convolutional neural network (CNN) binarization methods or existing binarization methods to binarize ViTs compared to CNNs on datasets with a large number of classes such as ImageNet-1k. With extensive analysis, we find that binary vanilla ViTs such as DeiT miss out on a lot of key architectural properties that CNNs have that allow binary CNNs to have much higher representational capability than binary vanilla ViT. Therefore, we propose BinaryViT, in which inspired by the CNN architecture,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#31867;-based &#24179;&#38138;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#26469;&#23454;&#29616;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#65292;&#20026;&#26410;&#27880;&#37322;&#30340;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.16666</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#36827;&#34892;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Game Level Blending using a Learned Level Representation. (arXiv:2306.16666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#31867;-based &#24179;&#38138;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#26469;&#23454;&#29616;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#65292;&#20026;&#26410;&#27880;&#37322;&#30340;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#30340;&#26041;&#27861;&#22312;&#28216;&#25103;&#20135;&#29983;&#25216;&#26415;&#39046;&#22495;&#36880;&#28176;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#30340;&#25968;&#37327;&#12290;&#21363;&#20351;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#28216;&#25103;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#39069;&#22806;&#30340;&#20849;&#20139;&#34920;&#31034;&#25165;&#33021;&#36827;&#34892;&#28151;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#24179;&#38138;&#23884;&#20837;&#65288;CTE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#25216;&#26415;&#65292;&#21487;&#20197;&#20026;&#38750;&#27880;&#37322;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game level blending via machine learning, the process of combining features of game levels to create unique and novel game levels using Procedural Content Generation via Machine Learning (PCGML) techniques, has gained increasing popularity in recent years. However, many existing techniques rely on human-annotated level representations, which limits game level blending to a limited number of annotated games. Even with annotated games, researchers often need to author an additional shared representation to make blending possible. In this paper, we present a novel approach to game level blending that employs Clustering-based Tile Embeddings (CTE), a learned level representation technique that can serve as a level representation for unannotated games and a unified level representation across games without the need for human annotation. CTE represents game level tiles as a continuous vector representation, unifying their visual, contextual, and behavioral information. We apply this approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28216;&#25103;&#35270;&#39057;&#36827;&#34892;&#20851;&#21345;&#29983;&#25104;&#21644;&#32763;&#35793;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20851;&#21345;&#32763;&#35793;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#20851;&#21345;&#29983;&#25104;&#25216;&#26415;&#20013;&#21463;&#38480;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16662</link><description>&lt;p&gt;
&#21033;&#29992;&#28216;&#25103;&#35270;&#39057;&#36827;&#34892;&#20851;&#21345;&#29983;&#25104;&#21644;&#32763;&#35793;&#30340;&#32852;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Joint Level Generation and Translation Using Gameplay Videos. (arXiv:2306.16662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28216;&#25103;&#35270;&#39057;&#36827;&#34892;&#20851;&#21345;&#29983;&#25104;&#21644;&#32763;&#35793;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20851;&#21345;&#32763;&#35793;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#20851;&#21345;&#29983;&#25104;&#25216;&#26415;&#20013;&#21463;&#38480;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31243;&#24207;&#29983;&#25104;&#25216;&#26415;&#38754;&#20020;&#30528;&#19968;&#20010;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#22270;&#20687;&#25110;&#25991;&#26412;&#29983;&#25104;&#65289;&#19981;&#21516;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#21363;&#21463;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#20851;&#21345;&#29983;&#25104;&#26041;&#27861;&#38656;&#35201;&#38500;&#20102;&#20851;&#21345;&#22270;&#20687;&#20043;&#22806;&#30340;&#36741;&#21161;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#20123;&#34920;&#31034;&#30340;&#24403;&#21069;&#26041;&#27861;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#20154;&#24037;&#27880;&#37322;&#28216;&#25103;&#30340;&#28216;&#25103;&#35270;&#39057;&#26469;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#26694;&#26550;&#65292;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20851;&#21345;&#32763;&#35793;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32763;&#35793;&#37096;&#20998;&#21487;&#20197;&#23558;&#28216;&#25103;&#35270;&#39057;&#24103;&#36716;&#25442;&#20026;&#31561;&#25928;&#30340;&#36741;&#21161;&#34920;&#31034;&#65292;&#32780;&#29983;&#25104;&#37096;&#20998;&#21487;&#20197;&#20135;&#29983;&#26032;&#30340;&#20851;&#21345;&#27573;&#33853;&#12290;&#35780;&#20272;&#32467;&#26524;&#21644;&#19982;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#32467;&#21512;&#20851;&#21345;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation via Machine Learning (PCGML) faces a significant hurdle that sets it apart from other fields, such as image or text generation, which is limited annotated data. Many existing methods for procedural level generation via machine learning require a secondary representation besides level images. However, the current methods for obtaining such representations are laborious and time-consuming, which contributes to this problem. In this work, we aim to address this problem by utilizing gameplay videos of two human-annotated games to develop a novel multi-tail framework that learns to perform simultaneous level translation and generation. The translation tail of our framework can convert gameplay video frames to an equivalent secondary representation, while its generation tail can produce novel level segments. Evaluation results and comparisons between our framework and baselines suggest that combining the level generation and translation tasks can lead to an over
&lt;/p&gt;</description></item><item><title>NaturalInversion &#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#12289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#21512;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#19968;&#33268;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16661</link><description>&lt;p&gt;
NaturalInversion: &#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#25552;&#21319;&#29616;&#23454;&#19990;&#30028;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency. (arXiv:2306.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16661
&lt;/p&gt;
&lt;p&gt;
NaturalInversion &#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#12289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#21512;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#19968;&#33268;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; NaturalInversion &#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30456;&#31526;&#30340;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312; NaturalInversion &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#20960;&#28857;&#21019;&#26032;&#65306;&#65288;1&#65289;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#65292;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#25552;&#21462;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#21407;&#22987;&#25968;&#25454;&#22270;&#20687;&#20808;&#39564;&#20449;&#24687;&#65307;&#65288;2&#65289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#21482;&#21512;&#25104;&#19968;&#20010;&#25209;&#27425;&#30340;&#22270;&#20687;&#65292;&#20197;&#24341;&#20837;&#38750;&#32447;&#24615;&#24230;&#37327;&#24182;&#31616;&#21270;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#65307;&#65288;3&#65289;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#20197;&#35843;&#25972;&#36755;&#20986;&#22270;&#20687;&#36890;&#36947;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#21407;&#22987;&#22270;&#20687;&#20808;&#39564;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340; NaturalInversion&#65292;&#25105;&#20204;&#20174;&#22312; CIFAR-10/100 &#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#38468;&#21152;&#20998;&#26512;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#22270;&#20687;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data. In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further. With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 and show that our images are more consistent with original data distribution than prior works by visualization and additional analysis. Furthermore, our synthesized images outperform prior w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#65292;&#21033;&#29992;&#22797;&#39640;&#26031;&#26426;&#21046;&#36817;&#20284;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#20026;&#31209;k&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#26512;&#22797;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#19979;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;M&#30340;&#31532;k&#20010;&#29305;&#24449;&#20540;&#21644;&#31532;k+1&#20010;&#29305;&#24449;&#20540;&#20043;&#38388;&#26377;&#36275;&#22815;&#22823;&#30340;&#38388;&#38548;&#26102;&#65292;&#22797;&#39640;&#26031;&#26426;&#21046;&#20135;&#29983;&#30340;&#30697;&#38453;&#19982;M&#30340;&#26368;&#20339;&#31209;k&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#30028;&#38480;&#20026;O(&#8730;kd)&#12290;</title><link>http://arxiv.org/abs/2306.16648</link><description>&lt;p&gt;
&#22797;&#39640;&#26031;&#25200;&#21160;&#19979;&#30340;&#31169;&#26377;&#21327;&#26041;&#24046;&#36817;&#20284;&#21644;&#29305;&#24449;&#20540;&#38388;&#38553;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Private Covariance Approximation and Eigenvalue-Gap Bounds for Complex Gaussian Perturbations. (arXiv:2306.16648v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#65292;&#21033;&#29992;&#22797;&#39640;&#26031;&#26426;&#21046;&#36817;&#20284;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#20026;&#31209;k&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#26512;&#22797;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#19979;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;M&#30340;&#31532;k&#20010;&#29305;&#24449;&#20540;&#21644;&#31532;k+1&#20010;&#29305;&#24449;&#20540;&#20043;&#38388;&#26377;&#36275;&#22815;&#22823;&#30340;&#38388;&#38548;&#26102;&#65292;&#22797;&#39640;&#26031;&#26426;&#21046;&#20135;&#29983;&#30340;&#30697;&#38453;&#19982;M&#30340;&#26368;&#20339;&#31209;k&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#30028;&#38480;&#20026;O(&#8730;kd)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#19979;&#65292;&#36817;&#20284;&#19968;&#20010;d&#215;d&#21327;&#26041;&#24046;&#30697;&#38453;M&#20026;&#31209;&#20026;k&#30340;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#22797;&#39640;&#26031;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#26426;&#21046;&#20135;&#29983;&#30340;&#30697;&#38453;&#19982;M&#30340;&#26368;&#20339;&#31209;k&#36817;&#20284;&#20043;&#38388;&#30340;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#24046;&#30340;&#30028;&#38480;&#32422;&#20026;O(&#8730;kd)&#65292;&#21482;&#35201;M&#30340;&#31532;k&#20010;&#29305;&#24449;&#20540;&#21644;&#31532;k+1&#20010;&#29305;&#24449;&#20540;&#20043;&#38388;&#26377;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#38388;&#38548;&#12290;&#36825;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20197;&#31867;&#20284;&#30340;&#30028;&#38480;&#38656;&#35201;M&#30340;&#25152;&#26377;&#21069;k&#20010;&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#38388;&#38548;&#33267;&#23569;&#20026;&#8730;d&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#22797;&#30697;&#38453;&#24067;&#26391;&#36816;&#21160;&#30340;&#29305;&#24449;&#20540;&#27604;&#23454;&#25968;&#24773;&#20917;&#19979;&#26356;&#23481;&#26131;&#36828;&#31163;&#65292;&#21033;&#29992;&#25140;&#26862;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#29305;&#24449;&#20540;&#30340;&#28436;&#21270;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22797;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#19979;&#30340;&#30697;&#38453;M&#30340;&#29305;&#24449;&#20540;&#20855;&#26377;&#24456;&#22823;&#30340;&#38388;&#38548;&#30340;&#27010;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating a $d \times d$ covariance matrix $M$ with a rank-$k$ matrix under $(\varepsilon,\delta)$-differential privacy. We present and analyze a complex variant of the Gaussian mechanism and show that the Frobenius norm of the difference between the matrix output by this mechanism and the best rank-$k$ approximation to $M$ is bounded by roughly $\tilde{O}(\sqrt{kd})$, whenever there is an appropriately large gap between the $k$'th and the $k+1$'th eigenvalues of $M$. This improves on previous work that requires that the gap between every pair of top-$k$ eigenvalues of $M$ is at least $\sqrt{d}$ for a similar bound. Our analysis leverages the fact that the eigenvalues of complex matrix Brownian motion repel more than in the real case, and uses Dyson's stochastic differential equations governing the evolution of its eigenvalues to show that the eigenvalues of the matrix $M$ perturbed by complex Gaussian noise have large gaps with high probability. Our resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.16636</link><description>&lt;p&gt;
CMATH&#65306;&#20320;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#36890;&#36807;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1.7k&#20010;&#20855;&#26377;&#35814;&#32454;&#27880;&#37322;&#30340;&#23567;&#23398;&#27700;&#24179;&#25968;&#23398;&#24212;&#29992;&#39064;&#65292;&#26469;&#28304;&#20110;&#20013;&#22269;&#23454;&#38469;&#30340;&#32451;&#20064;&#21644;&#32771;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36798;&#21040;&#23567;&#23398;&#25968;&#23398;&#21738;&#20010;&#24180;&#32423;&#27700;&#24179;&#30340;&#22522;&#20934;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;LLMs&#65292;&#21253;&#25324;&#21830;&#19994;&#21644;&#24320;&#28304;&#36873;&#39033;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#20845;&#20010;&#23567;&#23398;&#24180;&#32423;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65288;&#20934;&#30830;&#29575;&#8805;60%&#65289;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#24178;&#25200;&#20449;&#24687;&#26469;&#35780;&#20272;&#20960;&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;GPT-4&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22833;&#36133;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#25581;&#31034;LLMs&#22312;&#31639;&#26415;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\geq$ 60\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16635</link><description>&lt;p&gt;
&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness in Deepfake Detection. (arXiv:2306.16635v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#26377;&#25928;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24320;&#21457;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#26102;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#31181;&#26063;&#21644;/&#25110;&#24615;&#21035;&#30340;&#20154;&#32676;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#32676;&#20307;&#21463;&#21040;&#19981;&#20844;&#24179;&#30340;&#23450;&#20301;&#25110;&#34987;&#25490;&#38500;&#22312;&#26816;&#27979;&#20043;&#22806;&#65292;&#20174;&#32780;&#35753;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#28145;&#24230;&#20266;&#36896;&#25805;&#32437;&#33286;&#35770;&#24182;&#30772;&#22351;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#30528;&#37325;&#20110;&#30830;&#23450;&#21644;&#35780;&#20272;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#24320;&#21457;&#20986;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#23618;&#38754;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#22312;&#19981;&#32771;&#34385;&#25110;&#32771;&#34385;&#20154;&#21475;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20844;&#24179;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#12290;&#23545;&#22235;&#20010;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the development of effective deepfake detection models in recent years, several recent studies have demonstrated that biases in the training data utilized to develop deepfake detection models can lead to unfair performance for demographic groups of different races and/or genders. Such can result in these groups being unfairly targeted or excluded from detection, allowing misclassified deepfakes to manipulate public opinion and erode trust in the model. While these studies have focused on identifying and evaluating the unfairness in deepfake detection, no methods have been developed to address the fairness issue of deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions to train fair deepfake detection models in ways that are agnostic or aware of demographic factors. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexi
&lt;/p&gt;</description></item><item><title>MNISQ &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110; NISQ &#26102;&#20195;&#30340;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001; 4,950,000 &#20010;&#25968;&#25454;&#28857;&#32452;&#25104;&#65292;&#20197;&#37327;&#23376;&#21644;&#32463;&#20856;&#24418;&#24335;&#21576;&#29616;&#65292;&#26088;&#22312;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#20419;&#36827;&#37327;&#23376;&#35745;&#31639;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.16627</link><description>&lt;p&gt;
MNISQ&#65306;&#29992;&#20110;NISQ&#26102;&#20195;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;/&#20026;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MNISQ: A Large-Scale Quantum Circuit Dataset for Machine Learning on/for Quantum Computers in the NISQ era. (arXiv:2306.16627v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16627
&lt;/p&gt;
&lt;p&gt;
MNISQ &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110; NISQ &#26102;&#20195;&#30340;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001; 4,950,000 &#20010;&#25968;&#25454;&#28857;&#32452;&#25104;&#65292;&#20197;&#37327;&#23376;&#21644;&#32463;&#20856;&#24418;&#24335;&#21576;&#29616;&#65292;&#26088;&#22312;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#20419;&#36827;&#37327;&#23376;&#35745;&#31639;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;MNISQ&#65292;&#29992;&#20110;NISQ&#26102;&#20195;&#30340;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#12290;MNISQ&#21253;&#21547;&#20102;4,950,000&#20010;&#25968;&#25454;&#28857;&#65292;&#20998;&#20026;9&#20010;&#23376;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#20449;&#24687;&#65288;&#20363;&#22914;MNIST&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#37327;&#23376;&#32534;&#30721;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21452;&#37325;&#24418;&#24335;&#30340;&#25968;&#25454;&#38598;&#65306;&#37327;&#23376;&#24418;&#24335;&#30340;&#30005;&#36335;&#21644;&#32463;&#20856;&#24418;&#24335;&#30340;&#37327;&#23376;&#30005;&#36335;&#25551;&#36848;&#65288;&#37327;&#23376;&#32534;&#31243;&#35821;&#35328;&#65292;QASM&#65289;&#12290;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#21452;&#37325;&#25361;&#25112;&#65306;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33021;&#21147;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#37327;&#23376;&#35745;&#31639;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30005;&#36335;&#20998;&#31867;&#65292;&#21516;&#26102;&#37319;&#29992;&#37327;&#23376;&#21644;&#32463;&#20856;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#22788;&#29702;&#12290;&#22312;&#37327;&#23376;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#26680;&#26041;&#27861;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We introduce the first large-scale dataset, MNISQ, for both the Quantum and the Classical Machine Learning community during the Noisy Intermediate-Scale Quantum era. MNISQ consists of 4,950,000 data points organized in 9 subdatasets. Building our dataset from the quantum encoding of classical information (e.g., MNIST dataset), we deliver a dataset in a dual form: in quantum form, as circuits, and in classical form, as quantum circuit descriptions (quantum programming language, QASM). In fact, also the Machine Learning research related to quantum computers undertakes a dual challenge: enhancing machine learning exploiting the power of quantum computers, while also leveraging state-of-the-art classical machine learning methodologies to help the advancement of quantum computing. Therefore, we perform circuit classification on our dataset, tackling the task with both quantum and classical models. In the quantum endeavor, we test our circuit dataset with Quantum Kernel methods, and we show 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;1D-&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#25289;&#26364;&#20809;&#35889;&#39044;&#27979;&#28151;&#21512;&#32452;&#20998;&#27987;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;Python&#36719;&#20214;&#21253;RaMix&#65292;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25289;&#26364;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#21270;&#23398;&#35745;&#37327;&#23398;&#31639;&#27861;&#22312;&#23454;&#26102;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16621</link><description>&lt;p&gt;
&#35780;&#20272;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#25289;&#26364;&#20809;&#35889;&#39044;&#27979;&#28151;&#21512;&#32452;&#20998;&#27987;&#24230;&#19978;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Assessing the Performance of 1D-Convolution Neural Networks to Predict Concentration of Mixture Components from Raman Spectra. (arXiv:2306.16621v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;1D-&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#25289;&#26364;&#20809;&#35889;&#39044;&#27979;&#28151;&#21512;&#32452;&#20998;&#27987;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;Python&#36719;&#20214;&#21253;RaMix&#65292;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25289;&#26364;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#21270;&#23398;&#35745;&#37327;&#23398;&#31639;&#27861;&#22312;&#23454;&#26102;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26364;&#20809;&#35889;&#26159;&#30417;&#27979;&#29983;&#29289;&#33647;&#29289;&#29983;&#20135;&#36807;&#31243;&#20013;&#21270;&#23398;&#21453;&#24212;&#22120;&#29366;&#24577;&#30340;&#26032;&#20852;&#24212;&#29992;&#12290;&#25289;&#26364;&#39057;&#31227;&#24378;&#24230;&#19982;&#21270;&#23398;&#29289;&#31181;&#30340;&#27987;&#24230;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38750;&#30772;&#22351;&#24615;&#30340;&#20809;&#29031;&#23556;&#20197;&#26080;&#26631;&#35760;&#30340;&#26041;&#24335;&#20998;&#26512;&#23454;&#26102;&#27987;&#24230;&#12290;&#21270;&#23398;&#35745;&#37327;&#23398;&#31639;&#27861;&#29992;&#20110;&#35299;&#37322;&#38543;&#30528;&#21453;&#24212;&#30340;&#36827;&#23637;&#32780;&#20135;&#29983;&#30340;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#30340;&#25289;&#26364;&#20809;&#35889;&#12290;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25289;&#26364;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20026;&#29305;&#23450;&#30340;&#29983;&#29289;&#21453;&#24212;&#22120;&#29615;&#22659;&#25214;&#21040;&#26368;&#20339;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;RaMix Python&#36719;&#20214;&#21253;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25289;&#26364;&#28151;&#21512;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#21270;&#23398;&#35745;&#37327;&#23398;&#31639;&#27861;&#31867;&#22411;&#22312;&#23454;&#26102;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#35813;&#36719;&#20214;&#21253;&#30340;&#21151;&#33021;&#24182;&#27604;&#36739;&#19981;&#21516;&#21270;&#23398;&#35745;&#37327;&#23398;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20316;&#32773;&#20351;&#29992;48&#20010;&#21512;&#25104;&#30340;&#25289;&#26364;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging application of Raman spectroscopy is monitoring the state of chemical reactors during biologic drug production. Raman shift intensities scale linearly with the concentrations of chemical species and thus can be used to analytically determine real-time concentrations using non-destructive light irradiation in a label-free manner. Chemometric algorithms are used to interpret Raman spectra produced from complex mixtures of bioreactor contents as a reaction evolves. Finding the optimal algorithm for a specific bioreactor environment is challenging due to the lack of freely available Raman mixture datasets. The RaMix Python package addresses this challenge by enabling the generation of synthetic Raman mixture datasets with controllable noise levels to assess the utility of different chemometric algorithm types for real-time monitoring applications. To demonstrate the capabilities of this package and compare the performance of different chemometric algorithms, 48 datasets of simu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#65292;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16617</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#28216;&#25103;&#20013;&#26080;&#20851;&#26354;&#29575;&#30340;&#26368;&#21518;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds. (arXiv:2306.16617v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#65292;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#35768;&#22810;&#24212;&#29992;&#21487;&#20197;&#20197;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#22343;&#34913;&#35745;&#31639;&#24418;&#24335;&#21270;&#12290;&#23613;&#31649;&#23545;&#23427;&#20204;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#30340;&#21407;&#22987;&#26041;&#26696;&#65292;&#24182;&#22312;&#23545;&#27979;&#22320;&#32447;&#21333;&#35843;&#24615;&#20551;&#35774;&#36827;&#34892;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#30740;&#31350;&#20805;&#20998;&#30340;&#27979;&#22320;&#32447;&#20984;&#20985;&#26497;&#20540;&#20248;&#21270;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#36317;&#31163;&#22833;&#30495;&#29616;&#35937;&#65292;&#20294;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#21069;&#20174;&#26410;&#32771;&#34385;&#36807;&#22312;&#40654;&#26364;&#35774;&#32622;&#20013;&#23384;&#22312;&#26354;&#29575;&#26080;&#20851;&#36895;&#29575;&#21644;/&#25110;&#26368;&#21518;&#25910;&#25947;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous applications in machine learning and data analytics can be formulated as equilibrium computation over Riemannian manifolds. Despite the extensive investigation of their Euclidean counterparts, the performance of Riemannian gradient-based algorithms remain opaque and poorly understood. We revisit the original scheme of Riemannian gradient descent (RGD) and analyze it under a geodesic monotonicity assumption, which includes the well-studied geodesically convex-concave min-max optimization problem as a special case. Our main contribution is to show that, despite the phenomenon of distance distortion, the RGD scheme, with a step size that is agnostic to the manifold's curvature, achieves a curvature-independent and linear last-iterate convergence rate in the geodesically strongly monotone setting. To the best of our knowledge, the possibility of curvature-independent rates and/or last-iterate convergence in the Riemannian setting has not been considered before.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16614</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#20013;&#23450;&#21046;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#26469;&#24341;&#36215;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#24230;&#37327;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30495;&#23454;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#26356;&#36866;&#21512;&#35780;&#20272;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#33021;&#22815;&#22312;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#20934;&#30830;&#22320;&#34913;&#37327;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#36890;&#36807;&#20248;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16612</link><description>&lt;p&gt;
GuidedMixup&#65306;&#30001;&#26174;&#33879;&#24615;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;Mixup&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps. (arXiv:2306.16612v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16612
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#36890;&#36807;&#20248;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#29616;&#22312;&#26159;&#22270;&#20687;&#35757;&#32451;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#26377;&#25928;&#22320;&#38450;&#27490;&#20102;&#36807;&#25311;&#21512;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#22122;&#22768;&#25968;&#25454;&#38598;&#26356;&#21152;&#40065;&#26834;&#12290;&#26368;&#36817;&#30340;&#28151;&#21512;&#22686;&#24378;&#31574;&#30053;&#21457;&#23637;&#20986;&#20102;&#33021;&#22815;&#20016;&#23500;&#26174;&#33879;&#24615;&#20449;&#24687;&#30340;&#28151;&#21512;&#25513;&#27169;&#65292;&#36825;&#26159;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#26469;&#20248;&#21270;&#28151;&#21512;&#25513;&#27169;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#23427;&#26088;&#22312;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37197;&#23545;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#24182;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#23454;&#29616;&#20016;&#23500;&#30340;&#26174;&#33879;&#24615;&#12290;&#27492;&#22806;&#65292;GuidedMixup&#36890;&#36807;&#24179;&#28369;&#22320;&#25554;&#20540;&#20004;&#20010;&#37197;&#23545;&#22270;&#20687;&#26469;&#25511;&#21046;&#27599;&#20010;&#20687;&#32032;&#30340;&#28151;&#21512;&#27604;&#20363;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#20010;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is now an essential part of the image training process, as it effectively prevents overfitting and makes the model more robust against noisy datasets. Recent mixing augmentation strategies have advanced to generate the mixup mask that can enrich the saliency information, which is a supervisory signal. However, these methods incur a significant computational burden to optimize the mixup mask. From this motivation, we propose a novel saliency-aware mixup method, GuidedMixup, which aims to retain the salient regions in mixup images with low computational overhead. We develop an efficient pairing algorithm that pursues to minimize the conflict of salient regions of paired images and achieve rich saliency in mixup images. Moreover, GuidedMixup controls the mixup ratio for each pixel to better preserve the salient region by interpolating two paired images smoothly. The experiments on several datasets demonstrate that GuidedMixup provides a good trade-off between augmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.16601</link><description>&lt;p&gt;
&#29992;&#20110;CPU&#19978;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#20005;&#26684;&#30340;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#35201;&#27714;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36816;&#34892;&#26102;&#23545;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#32570;&#20047;&#20805;&#20998;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#36719;&#20214;&#22534;&#26632;&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#20351;&#29992;&#24658;&#23450;&#30340;&#22359;&#22823;&#23567;&#36827;&#34892;&#21098;&#26525;&#12290;&#25105;&#20204;&#30340;&#31232;&#30095;&#36719;&#20214;&#21152;&#36895;&#22120;&#21033;&#29992;Intel Deep Learning Boost&#22312;CPU&#19978;&#26368;&#22823;&#21270;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;&#36890;&#24120;&#34987;&#32553;&#20889;&#20026;SpMM&#65289;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#27867;&#30340;GEMM&#24418;&#29366;&#21644;5&#20010;&#20195;&#34920;&#24615;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;SpMM&#20869;&#26680;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65288;oneMKL&#12289;TVM&#21644;LIBXSMM&#65289;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based language models have become the standard approach for natural language processing tasks. However, stringent throughput and latency requirements in industrial applications are limiting their adoption. To mitigate the gap, model compression techniques such as structured pruning are being used to improve inference efficiency. However, most existing neural network inference runtimes lack adequate support for structured sparsity. In this paper, we propose an efficient sparse deep learning inference software stack for Transformer-based language models where the weights are pruned with constant block size. Our sparse software accelerator leverages Intel Deep Learning Boost to maximize the performance of sparse matrix - dense matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an order of magnitude on a wide range of GEMM shapes under 5 representative sparsity ra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#21644;&#32570;&#22833;&#21464;&#37327;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#21464;&#37327;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.16593</link><description>&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#21457;&#23637;&#39044;&#27979;&#22312;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#30340;&#24110;&#21161;&#19979;
&lt;/p&gt;
&lt;p&gt;
Forecasting of the development of a partially-observed dynamical time series with the aid of time-invariance and linearity. (arXiv:2306.16593v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#21644;&#32570;&#22833;&#21464;&#37327;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#21464;&#37327;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20135;&#29983;&#19968;&#31181;&#20381;&#36182;&#22810;&#20803;&#24207;&#21015;&#65292;&#31216;&#20026;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#65292;&#36890;&#36807;&#28436;&#21270;&#20989;&#25968;&#21457;&#23637;&#32780;&#26469;&#12290;&#30001;&#20110;&#24403;&#21069;&#26102;&#38388;&#28857;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#36890;&#24120;&#20381;&#36182;&#20110;&#21069;&#19968;&#20010;&#26102;&#38388;&#28857;&#30340;&#25152;&#26377;&#21464;&#37327;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#19968;&#20123;&#21464;&#37327;&#26159;&#32570;&#22833;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#12290;ARS&#27169;&#22411;&#28041;&#21450;&#28436;&#21270;&#20989;&#25968;&#21644;&#20316;&#20026;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#32570;&#22833;&#21464;&#37327;&#30340;&#21516;&#26102;&#20272;&#35745;&#65292;&#20511;&#21161;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#12290;&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#25552;&#20986;&#30340;ARS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dynamical system produces a dependent multivariate sequence called dynamical time series, developed with an evolution function. As variables in the dynamical time series at the current time-point usually depend on the whole variables in the previous time-point, existing studies forecast the variables at the future time-point by estimating the evolution function. However, some variables in the dynamical time-series are missing in some practical situations. In this study, we propose an autoregressive with slack time series (ARS) model. ARS model involves the simultaneous estimation of the evolution function and the underlying missing variables as a slack time series, with the aid of the time-invariance and linearity of the dynamical system. This study empirically demonstrates the effectiveness of the proposed ARS model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#21333;&#20301;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#30340;&#38382;&#39064;&#65292;&#33218;&#19978;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#65292;&#32780;&#19988;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#32780;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#38454;&#25968;&#19979;&#30340;&#26368;&#20248;&#26377;&#30028;&#21644;&#26080;&#30028;&#36951;&#25022;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#38454;&#25968;&#20026;1/2&#26102;&#23384;&#22312;&#30456;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.16578</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26410;&#30693;&#21644;&#38543;&#26426;&#22870;&#21169;&#30340;&#33218;&#19978;&#20998;&#37197;&#21487;&#20998;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Allocating Divisible Resources on Arms with Unknown and Random Rewards. (arXiv:2306.16578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#21333;&#20301;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#30340;&#38382;&#39064;&#65292;&#33218;&#19978;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#65292;&#32780;&#19988;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#32780;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#38454;&#25968;&#19979;&#30340;&#26368;&#20248;&#26377;&#30028;&#21644;&#26080;&#30028;&#36951;&#25022;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#38454;&#25968;&#20026;1/2&#26102;&#23384;&#22312;&#30456;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20915;&#31574;&#32773;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#20010;&#21487;&#20877;&#29983;&#21644;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#12290;&#36825;&#20123;&#33218;&#20855;&#26377;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#22870;&#21169;&#65292;&#20854;&#22343;&#20540;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;$b$&#25104;&#27604;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#20915;&#31574;&#32773;&#22312;&#19968;&#20010;&#21608;&#26399;&#23558;&#36164;&#28304;$A_i$&#20998;&#37197;&#32473;&#33218;$i$&#65292;&#37027;&#20040;&#22870;&#21169;$Y_i$&#26159;$Y_i(A_i)=A_i\mu_i+A_i^b\xi_i$&#65292;&#20854;&#20013;$\mu_i$&#26159;&#26410;&#30693;&#30340;&#22343;&#20540;&#65292;&#22122;&#22768;$\xi_i$&#26159;&#29420;&#31435;&#19988;&#23376;&#39640;&#26031;&#30340;&#12290;&#24403;&#38454;&#25968;$b$&#20174;0&#21040;1&#21464;&#21270;&#26102;&#65292;&#35813;&#26694;&#26550;&#24179;&#28369;&#22320;&#36830;&#25509;&#20102;&#26631;&#20934;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#24102;&#26377;&#23436;&#20840;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23427;&#20204;&#23454;&#29616;&#20102;$b\in[0,1]$&#26102;&#30340;&#26368;&#20248;&#26377;&#30028;&#24046;&#21644;&#26080;&#30028;&#24046;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;$b=1/2$&#22788;&#30340;&#30456;&#21464;&#12290;&#29702;&#35770;&#32467;&#26524;&#20381;&#36182;&#20110;&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#26032;&#22411;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#23427;&#38480;&#21046;&#20102;&#23376;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decision maker allocating one unit of renewable and divisible resource in each period on a number of arms. The arms have unknown and random rewards whose means are proportional to the allocated resource and whose variances are proportional to an order $b$ of the allocated resource. In particular, if the decision maker allocates resource $A_i$ to arm $i$ in a period, then the reward $Y_i$ is$Y_i(A_i)=A_i \mu_i+A_i^b \xi_{i}$, where $\mu_i$ is the unknown mean and the noise $\xi_{i}$ is independent and sub-Gaussian. When the order $b$ ranges from 0 to 1, the framework smoothly bridges the standard stochastic multi-armed bandit and online learning with full feedback. We design two algorithms that attain the optimal gap-dependent and gap-independent regret bounds for $b\in [0,1]$, and demonstrate a phase transition at $b=1/2$. The theoretical results hinge on a novel concentration inequality we have developed that bounds a linear combination of sub-Gaussian random variables w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26679;&#26412;&#19979;&#23545;&#31216;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#20445;&#35777;&#12290;&#23545;&#20110;&#23545;&#31216;&#20998;&#24067;&#65292;&#21487;&#20197;&#33719;&#24471;&#25910;&#25947;&#21040;&#27425;&#39640;&#26031;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#28176;&#36817;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.16573</link><description>&lt;p&gt;
&#26377;&#38480;&#26679;&#26412;&#19979;&#20855;&#26377;&#36153;&#33293;&#23572;&#20449;&#24687;&#36895;&#29575;&#30340;&#23545;&#31216;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Symmetric Mean Estimation with Fisher Information Rate. (arXiv:2306.16573v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26679;&#26412;&#19979;&#23545;&#31216;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#20445;&#35777;&#12290;&#23545;&#20110;&#23545;&#31216;&#20998;&#24067;&#65292;&#21487;&#20197;&#33719;&#24471;&#25910;&#25947;&#21040;&#27425;&#39640;&#26031;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#28176;&#36817;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#26410;&#30693;&#26041;&#24046;&#20026;$\sigma^2$&#30340;&#20998;&#24067;$f$&#65292;&#21487;&#20197;&#36890;&#36807;$n$&#20010;&#26679;&#26412;&#20197;&#26041;&#24046;$\frac{\sigma^2}{n}$&#21644;&#20960;&#20046;&#30456;&#23545;&#24212;&#30340;&#27425;&#39640;&#26031;&#36895;&#29575;&#26469;&#20272;&#35745;&#22343;&#20540;&#12290;&#24403;$f$&#24050;&#30693;&#19988;&#23545;&#31216;&#26102;&#65292;&#21487;&#20197;&#22312;&#28176;&#36817;&#26465;&#20214;&#19979;&#23558;&#20854;&#25913;&#36827;&#20026;$\frac{1}{n\mathcal I}$&#65292;&#20854;&#20013;$\mathcal I$&#20026;&#35813;&#20998;&#24067;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#26410;&#30693;&#20998;&#24067;$f$&#65292;&#36825;&#26679;&#30340;&#25913;&#36827;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20294;&#26159;&#65292;Stone(1975)&#35777;&#26126;&#20102;&#24403;$f$&#20851;&#20110;&#20854;&#22343;&#20540;&#23545;&#31216;&#26102;&#65292;&#36825;&#31181;&#28176;&#36817;&#25910;&#25947;&#26159;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;Stone&#30340;&#30028;&#38480;&#26159;&#28176;&#36817;&#30340;&#65292;&#21363;&#25910;&#25947;&#25152;&#38656;&#30340;$n$&#20197;&#26410;&#25351;&#23450;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#20998;&#24067;$f$&#21644;&#22833;&#36133;&#27010;&#29575;$\delta$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23601;&#23545;&#31216;&#22343;&#20540;&#20272;&#35745;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#32473;&#20986;&#26377;&#38480;&#26679;&#26412;&#30340;&#20445;&#35777;&#12290;&#23545;&#20110;&#27599;&#20010;$f,n,\delta$&#28385;&#36275;$n &gt; \log \frac{1}{\delta}$&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#25910;&#25947;&#21040;&#26041;&#24046;&#20026;$\frac{1}{n \mathcal I_r}$&#30340;&#27425;&#39640;&#26031;&#38468;&#36817;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$\mathcal I_r$&#26159;$r$-$\textit{&#24179;&#28369;&#21270;}$&#36153;&#33293;&#23572;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mean of an unknown variance-$\sigma^2$ distribution $f$ can be estimated from $n$ samples with variance $\frac{\sigma^2}{n}$ and nearly corresponding subgaussian rate. When $f$ is known up to translation, this can be improved asymptotically to $\frac{1}{n\mathcal I}$, where $\mathcal I$ is the Fisher information of the distribution. Such an improvement is not possible for general unknown $f$, but [Stone, 1975] showed that this asymptotic convergence $\textit{is}$ possible if $f$ is $\textit{symmetric}$ about its mean. Stone's bound is asymptotic, however: the $n$ required for convergence depends in an unspecified way on the distribution $f$ and failure probability $\delta$. In this paper we give finite-sample guarantees for symmetric mean estimation in terms of Fisher information. For every $f, n, \delta$ with $n &gt; \log \frac{1}{\delta}$, we get convergence close to a subgaussian with variance $\frac{1}{n \mathcal I_r}$, where $\mathcal I_r$ is the $r$-$\textit{smoothed}$ Fisher in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16559</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65306;&#23545;&#23646;&#24615;&#38388;&#21327;&#20316;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Selection: A perspective on inter-attribute cooperation. (arXiv:2306.16559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#23545;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#20219;&#21153;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#32500;&#24230;&#32553;&#20943;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#65292;&#36890;&#24120;&#26159;&#22312;&#24212;&#29992;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#30340;&#37325;&#35201;&#25968;&#25454;&#22788;&#29702;&#27493;&#39588;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#30456;&#20851;&#24615;&#25490;&#24207;&#31639;&#27861;&#21457;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;-&#20887;&#20313;&#26435;&#34913;&#21644;&#22522;&#20110;&#22810;&#20803;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25429;&#25417;&#22810;&#21464;&#37327;&#20381;&#36182;&#30340;&#36235;&#21183;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#38388;&#30340;&#20114;&#30456;&#21512;&#20316;&#33719;&#21462;&#20851;&#20110;&#31867;&#21035;&#30340;&#29420;&#29305;&#20449;&#24687;&#12290;&#26412;&#25991;&#23545;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#19981;&#21516;&#26041;&#27861;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#30697;&#38453;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20004;&#20010;&#27969;&#34892;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16557</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20984;&#20248;&#21270;&#65306;&#31283;&#20581;&#30697;&#38453;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Optimizations for Machine Learning with Theoretical Guarantee: Robust Matrix Completion and Neural Network Learning. (arXiv:2306.16557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#30697;&#38453;&#34917;&#20840;&#21644;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20004;&#20010;&#27969;&#34892;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#23398;&#20064;&#31995;&#32479;&#20173;&#28982;&#26159;"&#40657;&#30418;"&#30340;&#27010;&#24565;&#65292;&#20854;&#24615;&#33021;&#26080;&#27861;&#29702;&#35299;&#21644;&#25512;&#23548;&#12290;&#38543;&#30528;&#20844;&#20849;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#26085;&#30410;&#24341;&#36215;&#20851;&#27880;&#65292;&#35774;&#35745;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#31995;&#32479;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#36235;&#21183;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34987;&#34920;&#36848;&#20026;&#26368;&#23567;&#21270;&#65288;&#25110;&#26368;&#22823;&#21270;&#65289;&#26576;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#30001;&#20110;&#30495;&#23454;&#25968;&#25454;&#24456;&#21487;&#33021;&#26469;&#33258;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#25439;&#22833;&#20989;&#25968;&#19968;&#33324;&#26159;&#38750;&#20984;&#30340;&#12290;&#19982;&#20984;&#20248;&#21270;&#38382;&#39064;&#19981;&#21516;&#65292;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#26102;&#20250;&#38519;&#20837;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#22312;&#30740;&#31350;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#38750;&#20984;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#21644;&#65288;2&#65289;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent development in machine learning, most learning systems are still under the concept of "black box", where the performance cannot be understood and derived. With the rise of safety and privacy concerns in public, designing an explainable learning system has become a new trend in machine learning. In general, many machine learning problems are formulated as minimizing (or maximizing) some loss function. Since real data are most likely generated from non-linear models, the loss function is non-convex in general. Unlike the convex optimization problem, gradient descent algorithms will be trapped in spurious local minima in solving non-convex optimization. Therefore, it is challenging to provide explainable algorithms when studying non-convex optimization problems. In this thesis, two popular non-convex problems are studied: (1) low-rank matrix completion and (2) neural network learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;F-&#25955;&#24230;&#34913;&#37327;&#20844;&#24179;&#24615;&#65292;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.16552</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Fair Classifiers via Min-Max F-divergence Regularization. (arXiv:2306.16552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;F-&#25955;&#24230;&#34913;&#37327;&#20844;&#24179;&#24615;&#65292;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#22312;&#25191;&#27861;&#12289;&#21009;&#20107;&#21496;&#27861;&#12289;&#37329;&#34701;&#12289;&#25307;&#32856;&#21644;&#24405;&#21462;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#65292;&#30830;&#20445;ML&#36741;&#21161;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20851;&#27880;&#20844;&#24179;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#27169;&#22411;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#32452;&#25104;&#65292;&#21363;&#20998;&#31867;&#22120;&#32593;&#32476;&#21644;&#20559;&#24046;/&#20844;&#24179;&#24615;&#20272;&#35745;&#22120;&#32593;&#32476;&#65292;&#20854;&#20013;&#20844;&#24179;&#24615;&#20351;&#29992;&#32479;&#35745;&#27010;&#24565;&#30340;F-&#25955;&#24230;&#36827;&#34892;&#34913;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;F-&#25955;&#24230;&#24230;&#37327;&#20855;&#26377;&#20984;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21464;&#20998;&#34920;&#31034;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26041;&#20415;&#22320;&#36866;&#24212;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;F-&#25955;&#24230;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) based systems are adopted in domains such as law enforcement, criminal justice, finance, hiring and admissions, ensuring the fairness of ML aided decision-making is becoming increasingly important. In this paper, we focus on the problem of fair classification, and introduce a novel min-max F-divergence regularization framework for learning fair classification models while preserving high accuracy. Our framework consists of two trainable networks, namely, a classifier network and a bias/fairness estimator network, where the fairness is measured using the statistical notion of F-divergence. We show that F-divergence measures possess convexity and differentiability properties, and their variational representation make them widely applicable in practical gradient based training methods. The proposed framework can be readily adapted to multiple sensitive attributes and for high dimensional datasets. We study the F-divergence based training paradigm for two types of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19979;&#19968;&#20195;&#22686;&#24378;&#29616;&#23454;&#20250;&#35758;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30495;&#23454;&#20154;&#20307;&#28210;&#26579;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#24212;&#29992;&#22312;&#23454;&#26102;&#20250;&#35758;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20256;&#36882;&#30495;&#23454;&#20154;&#20307;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#20511;&#37492;&#20102;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#21333;&#30446;&#35270;&#39057;&#37319;&#38598;&#21644;&#33258;&#30001;&#35270;&#28857;&#21512;&#25104;&#26469;&#25552;&#39640;&#20250;&#35758;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#30495;&#23454;&#24863;&#21644;&#20132;&#20114;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16541</link><description>&lt;p&gt;
&#39640;&#25928;&#30495;&#23454;&#20154;&#20307;&#28210;&#26579;&#30340;&#19979;&#19968;&#20195;&#22686;&#24378;&#29616;&#23454;&#20250;&#35758;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering. (arXiv:2306.16541v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19979;&#19968;&#20195;&#22686;&#24378;&#29616;&#23454;&#20250;&#35758;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30495;&#23454;&#20154;&#20307;&#28210;&#26579;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#24212;&#29992;&#22312;&#23454;&#26102;&#20250;&#35758;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20256;&#36882;&#30495;&#23454;&#20154;&#20307;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#20511;&#37492;&#20102;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#21333;&#30446;&#35270;&#39057;&#37319;&#38598;&#21644;&#33258;&#30001;&#35270;&#28857;&#21512;&#25104;&#26469;&#25552;&#39640;&#20250;&#35758;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#30495;&#23454;&#24863;&#21644;&#20132;&#20114;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20250;&#35758;&#27491;&#22312;&#25104;&#20026;&#26032;&#30340;&#24120;&#24577;&#65292;&#20026;&#22312;&#32447;&#20250;&#35758;&#21019;&#36896;&#36523;&#20020;&#20854;&#22659;&#30340;&#20307;&#39564;&#24050;&#32463;&#25104;&#20026;&#21051;&#19981;&#23481;&#32531;&#30340;&#38656;&#27714;&#12290;&#39640;&#25928;&#30495;&#23454;&#20154;&#20307;&#28210;&#26579;&#26159;&#23454;&#29616;&#36523;&#20020;&#20854;&#22659;&#20250;&#35758;&#30340;&#26680;&#24515;&#12290;&#30446;&#21069;&#27969;&#34892;&#30340;&#24212;&#29992;&#34429;&#28982;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#20250;&#35758;&#65292;&#20294;&#22312;&#20256;&#36882;&#30495;&#23454;&#20154;&#20307;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#31354;&#38388;&#26377;&#38480;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#20351;&#29992;&#32570;&#20047;&#30495;&#23454;&#20114;&#21160;&#30340;&#21270;&#36523;&#12290;&#26368;&#36817;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#27604;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#65292;&#20026;&#20803;&#23431;&#23449;&#20250;&#35758;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#30495;&#23454;&#24863;&#12290;&#28982;&#32780;&#65292;NeRF &#30340;&#28210;&#26579;&#36895;&#24230;&#36739;&#24930;&#65292;&#23545;&#20110;&#23454;&#26102;&#20250;&#35758;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#26410;&#26469;&#30340;&#22686;&#24378;&#29616;&#23454;&#20803;&#23431;&#23449;&#20250;&#35758;&#31995;&#32479;&#30340;&#27969;&#31243;&#65292;&#23427;&#21033;&#29992;&#21333;&#30446;&#35270;&#39057;&#37319;&#38598;&#21644;&#33258;&#30001;&#35270;&#28857;&#21512;&#25104;&#26469;&#25552;&#39640;&#25968;&#25454;&#21644;&#30828;&#20214;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#23454;&#29616;&#36523;&#20020;&#20854;&#22659;&#30340;&#20250;&#35758;&#20307;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#21152;&#36895;&#30340;&#22522;&#20110; NeRF &#30340;&#33258;&#30001;&#35270;&#28857;&#28210;&#26579; pipeline&#65292;&#36890;&#36807;&#22312;&#28210;&#26579;&#36807;&#31243;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#30495;&#23454;&#24863;&#21644;&#20132;&#20114;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meeting online is becoming the new normal. Creating an immersive experience for online meetings is a necessity towards more diverse and seamless environments. Efficient photorealistic rendering of human 3D dynamics is the core of immersive meetings. Current popular applications achieve real-time conferencing but fall short in delivering photorealistic human dynamics, either due to limited 2D space or the use of avatars that lack realistic interactions between participants. Recent advances in neural rendering, such as the Neural Radiance Field (NeRF), offer the potential for greater realism in metaverse meetings. However, the slow rendering speed of NeRF poses challenges for real-time conferencing. We envision a pipeline for a future extended reality metaverse conferencing system that leverages monocular video acquisition and free-viewpoint synthesis to enhance data and hardware efficiency. Towards an immersive conferencing experience, we explore an accelerated NeRF-based free-viewpoint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35745;&#31639;&#21644;&#32479;&#35745;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#33014;&#36136;&#30244;&#24739;&#32773;&#30340;MRI&#24207;&#21015;&#21644;&#20998;&#23376;&#29305;&#24449;&#65292;&#39044;&#27979;WHO 4&#32423;&#33014;&#36136;&#30244;&#24739;&#32773;&#30340;&#24555;&#36895;&#26089;&#26399;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19982;&#29983;&#23384;&#27010;&#29575;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.16531</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#25918;&#23556;MRI&#39044;&#27979;WHO 4&#32423;&#33014;&#36136;&#30244;&#24739;&#32773;&#30340;&#24555;&#36895;&#26089;&#26399;&#36827;&#23637;&#21644;&#29983;&#23384;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prediction of Rapid Early Progression and Survival Risk with Pre-Radiation MRI in WHO Grade 4 Glioma Patients. (arXiv:2306.16531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35745;&#31639;&#21644;&#32479;&#35745;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#33014;&#36136;&#30244;&#24739;&#32773;&#30340;MRI&#24207;&#21015;&#21644;&#20998;&#23376;&#29305;&#24449;&#65292;&#39044;&#27979;WHO 4&#32423;&#33014;&#36136;&#30244;&#24739;&#32773;&#30340;&#24555;&#36895;&#26089;&#26399;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19982;&#29983;&#23384;&#27010;&#29575;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20020;&#24202;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#37096;&#20998;&#22312;&#25918;&#23556;&#27835;&#30103;&#24320;&#22987;&#21069;&#34920;&#29616;&#20986;&#24555;&#36895;&#26089;&#26399;&#36827;&#23637;&#65288;REP&#65289;&#30340;&#33041;&#33014;&#36136;&#30244;&#24739;&#32773;&#12290;&#30446;&#21069;&#30340;&#25991;&#29486;&#36804;&#20170;&#20026;&#27490;&#20165;&#25551;&#36848;&#20102;&#36825;&#31181;&#20154;&#32676;&#30340;&#20020;&#24202;&#30149;&#29702;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#20256;&#32479;&#25918;&#23556;&#35299;&#21078;&#23398;&#12289;&#22797;&#26434;&#22810;&#20998;&#36776;&#29575;&#20998;&#24418;&#32441;&#29702;&#29305;&#24449;&#21644;&#19981;&#21516;&#20998;&#23376;&#29305;&#24449;&#65288;MGMT&#12289;IDH&#31361;&#21464;&#65289;&#20316;&#20026;&#35786;&#26029;&#21644;&#39044;&#21518;&#24037;&#20855;&#65292;&#20351;&#29992;&#35745;&#31639;&#21644;&#32479;&#35745;&#24314;&#27169;&#26041;&#27861;&#39044;&#27979;&#26469;&#33258;&#38750;REP&#30149;&#20363;&#30340;REP&#24739;&#32773;&#30340;&#28508;&#21147;&#12290;&#20998;&#26512;&#20102;70&#21517;&#24739;&#32773;&#30340;&#25918;&#23556;&#35268;&#21010;T1&#22686;&#24378;&#65288;T1C&#65289;MRI&#24207;&#21015;&#12290;&#32463;&#36807;1000&#27425;&#36845;&#20195;&#30340;5&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;&#38598;&#25104;&#26041;&#27861;&#25552;&#20379;&#20102;0.793&#30340;AUC&#65292;&#26631;&#20934;&#20559;&#24046;&#20026;0.082&#65292;&#29992;&#20110;REP&#21644;&#38750;REP&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#22312;&#20381;&#36182;&#24615;&#25130;&#26631;&#30340;&#22522;&#30784;&#19978;&#65292;&#22522;&#20110;Copula&#27169;&#22411;&#30340;&#24314;&#27169;&#65288;&#20854;&#20013;&#19968;&#37096;&#20998;&#24739;&#32773;&#21487;&#33021;&#22312;&#27515;&#20129;&#21069;&#26410;&#36319;&#36394;&#65289;&#30830;&#23450;&#20102;&#29983;&#23384;&#27010;&#29575;&#30340;&#26174;&#33879;&#29305;&#24449;&#65288;p&#20540;&lt;0.05&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent clinical research describes a subset of glioblastoma patients that exhibit REP prior to start of radiation therapy. Current literature has thus far described this population using clinicopathologic features. To our knowledge, this study is the first to investigate the potential of conventional ra-diomics, sophisticated multi-resolution fractal texture features, and different molecular features (MGMT, IDH mutations) as a diagnostic and prognostic tool for prediction of REP from non-REP cases using computational and statistical modeling methods. Radiation-planning T1 post-contrast (T1C) MRI sequences of 70 patients are analyzed. Ensemble method with 5-fold cross validation over 1000 iterations offers AUC of 0.793 with standard deviation of 0.082 for REP and non-REP classification. In addition, copula-based modeling under dependent censoring (where a subset of the patients may not be followed up until death) identifies significant features (p-value &lt;0.05) for survival probability a
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16528</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Food Recommender System in Academic Environments Based on Machine Learning Models. (arXiv:2306.16528v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20154;&#20204;&#30340;&#20581;&#24247;&#21462;&#20915;&#20110;&#36866;&#24403;&#30340;&#39278;&#39135;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#22914;&#20170;&#65292;&#38543;&#30528;&#20154;&#20204;&#29983;&#27963;&#30340;&#26426;&#26800;&#21270;&#22686;&#21152;&#65292;&#36866;&#24403;&#30340;&#39278;&#39135;&#20064;&#24815;&#21644;&#34892;&#20026;&#34987;&#24573;&#35270;&#20102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#39135;&#29289;&#25512;&#33616;&#20063;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#26159;&#38543;&#30528;&#35199;&#26041;&#39278;&#39135;&#39118;&#26684;&#30340;&#24341;&#20837;&#21644;&#35199;&#26041;&#21270;&#23398;&#33647;&#29289;&#30340;&#36827;&#27493;&#65292;&#22312;&#30142;&#30149;&#27835;&#30103;&#21644;&#33829;&#20859;&#26041;&#38754;&#20986;&#29616;&#20102;&#35768;&#22810;&#38382;&#39064;&#12290;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23548;&#33268;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21019;&#24314;&#65292;&#20197;&#25913;&#21892;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#26041;&#27861;&#65306;&#37319;&#29992;&#28151;&#21512;&#25512;&#33616;&#31995;&#32479;&#65292;&#21253;&#25324;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22312;2519&#21517;&#23398;&#29983;&#30340;&#33829;&#20859;&#31649;&#29702;&#31995;&#32479;&#20013;&#65292;&#30740;&#31350;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: People's health depends on the use of proper diet as an important factor. Today, with the increasing mechanization of people's lives, proper eating habits and behaviors are neglected. On the other hand, food recommendations in the field of health have also tried to deal with this issue. But with the introduction of the Western nutrition style and the advancement of Western chemical medicine, many issues have emerged in the field of disease treatment and nutrition. Recent advances in technology and the use of artificial intelligence methods in information systems have led to the creation of recommender systems in order to improve people's health. Methods: A hybrid recommender system including, collaborative filtering, content-based, and knowledge-based models was used. Machine learning models such as Decision Tree, k-Nearest Neighbors (kNN), AdaBoost, and Bagging were investigated in the field of food recommender systems on 2519 students in the nutrition management system of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;</title><link>http://arxiv.org/abs/2306.16526</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#30340;&#40657;&#30418;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Shilling Black-box Review-based Recommender Systems through Fake Review Generation. (arXiv:2306.16526v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RBRS&#65289;&#30001;&#20110;&#33021;&#22815;&#32531;&#35299;&#20247;&#25152;&#21608;&#30693;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;RBRS&#21033;&#29992;&#35780;&#35770;&#26469;&#26500;&#24314;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#23545;&#35780;&#35770;&#30340;&#20381;&#36182;&#21487;&#33021;&#20250;&#20351;&#31995;&#32479;&#38754;&#20020;&#34987;&#25805;&#32437;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;RBRS&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#19968;&#20010;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#65292;&#23427;&#36890;&#36807;&#21521;&#31995;&#32479;&#28155;&#21152;&#29983;&#25104;&#30340;&#35780;&#35770;&#23548;&#33268;&#39044;&#27979;&#20559;&#31227;&#20174;&#32780;&#24694;&#24847;&#25512;&#24191;&#29289;&#21697;&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#22870;&#21169;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#26041;&#38754;&#39044;&#27979;&#22120;&#26469;&#22686;&#21152;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29983;&#25104;&#30340;&#35780;&#35770;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#25805;&#32437;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;
&lt;/p&gt;
&lt;p&gt;
Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.16524</link><description>&lt;p&gt;
HNO&#65306;&#29992;&#20110;&#35299;&#20915;PDE&#30340;&#39715;&#29399;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#31163;&#25955;&#21270;&#20197;&#35299;&#26512;&#24517;&#35201;&#30340;&#26102;&#31354;&#23610;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;PDE&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#12290;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#39715;&#29399;&#65288;Hyena&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#37319;&#29992;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#12290;&#39715;&#29399;&#31639;&#23376;&#26159;&#19968;&#31181;&#20855;&#26377;&#27425;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#25805;&#20316;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#21442;&#25968;&#21270;&#20855;&#26377;&#20840;&#23616;&#24863;&#21463;&#37326;&#30340;&#38271;&#21367;&#31215;&#12290;&#36825;&#31181;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#12290;&#20026;&#20102;&#34913;&#37327;&#21508;&#20010;&#23618;&#22312;&#35299;&#20915;PDE&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#949;-&#35206;&#30422;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16516</link><description>&lt;p&gt;
&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#21482;&#38656;&#35201;&#22266;&#23450;&#25968;&#37327;&#30340;&#26597;&#35810;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
For Kernel Range Spaces a Constant Number of Queries Are Sufficient. (arXiv:2306.16516v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16516
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#949;-&#35206;&#30422;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#33539;&#22260;&#31354;&#38388;&#30340;&#949;-&#35206;&#30422;&#27010;&#24565;&#12290;&#26680;&#33539;&#22260;&#31354;&#38388;&#28041;&#21450;&#19968;&#20010;&#28857;&#38598;X&#8834;R^d&#21644;&#30001;&#22266;&#23450;&#26680;&#20989;&#25968;&#65288;&#20363;&#22914;&#39640;&#26031;&#26680;&#20989;&#25968;K(p,&#183;)=exp(-||p-&#183;||^2)&#65289;&#23450;&#20041;&#30340;&#26597;&#35810;&#31354;&#38388;&#12290;&#23545;&#20110;&#22823;&#23567;&#20026;n&#30340;&#28857;&#38598;X&#65292;&#26597;&#35810;&#36820;&#22238;&#19968;&#20010;&#20540;&#21521;&#37327;Rp&#8712;R^n&#65292;&#20854;&#20013;&#31532;i&#20010;&#22352;&#26631;(Rp)_i=K(p,x_i)&#65292;&#20854;&#20013;x_i&#8712;X&#12290;&#949;-&#35206;&#30422;&#26159;&#28857;&#38598;Q&#8834;R^d&#30340;&#23376;&#38598;&#65292;&#23545;&#20110;&#20219;&#24847;p&#8712;R^d&#65292;&#23384;&#22312;q&#8712;Q&#20351;&#24471;||(Rp-Rq)/n||_1&#8804;&#949;&#12290;&#36825;&#26159;Haussler&#22312;&#32452;&#21512;&#33539;&#22260;&#31354;&#38388;&#65288;&#20363;&#22914;&#30001;&#29699;&#26597;&#35810;&#23450;&#20041;&#30340;&#28857;&#38598;&#23376;&#38598;&#65289;&#20013;&#949;-&#35206;&#30422;&#27010;&#24565;&#30340;&#24179;&#28369;&#27169;&#25311;&#65292;&#20854;&#20013;&#24471;&#21040;&#30340;&#21521;&#37327;Rp&#26159;{0,1}^n&#32780;&#19981;&#26159;[0,1]^n&#12290;&#36825;&#20123;&#33539;&#22260;&#31354;&#38388;&#30340;&#26680;&#29256;&#26412;&#20986;&#29616;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#22352;&#26631;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#65292;&#22240;&#27492;&#24076;&#26395;&#22312;&#33539;&#22260;&#26597;&#35810;&#20013;&#28155;&#21152;&#19968;&#20123;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of an $\varepsilon$-cover for a kernel range space. A kernel range space concerns a set of points $X \subset \mathbb{R}^d$ and the space of all queries by a fixed kernel (e.g., a Gaussian kernel $K(p,\cdot) = \exp(-\|p-\cdot\|^2)$). For a point set $X$ of size $n$, a query returns a vector of values $R_p \in \mathbb{R}^n$, where the $i$th coordinate $(R_p)_i = K(p,x_i)$ for $x_i \in X$. An $\varepsilon$-cover is a subset of points $Q \subset \mathbb{R}^d$ so for any $p \in \mathbb{R}^d$ that $\frac{1}{n} \|R_p R_q\|_1\leq \varepsilon$ for some $q \in Q$. This is a smooth analog of Haussler's notion of $\varepsilon$-covers for combinatorial range spaces (e.g., defined by subsets of points within a ball query) where the resulting vectors $R_p$ are in $\{0,1\}^n$ instead of $[0,1]^n$. The kernel versions of these range spaces show up in data analysis tasks where the coordinates may be uncertain or imprecise, and hence one wishes to add some flexibility in the not
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.16504</link><description>&lt;p&gt;
&#21160;&#37327;&#31616;&#21333;&#32780;&#21487;&#35777;&#23454;&#22320;&#22686;&#24378;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#33539;&#20363;&#65292;&#20294;&#30001;&#20110;&#19981;&#21487;&#38752;&#30340;&#32593;&#32476;&#36830;&#25509;&#12289;&#32531;&#24930;&#30340;&#36890;&#20449;&#20197;&#21450;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;FedAvg&#21644;SCAFFOLD&#26159;&#20004;&#31181;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22522;&#26412;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;FedAvg&#22312;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20043;&#21069;&#37319;&#29992;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65292;&#32780;SCAFFOLD&#22312;&#20854;&#26412;&#22320;&#26356;&#26032;&#20013;&#32500;&#25252;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#25511;&#21046;&#21464;&#37327;&#20197;&#34917;&#20607;&#8220;&#23458;&#25143;&#31471;&#28418;&#31227;&#8221;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#23545;&#31639;&#27861;&#32467;&#26500;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#35843;&#25972;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#26377;&#30028;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21160;&#37327;&#26469;&#22686;&#24378;FedAvg&#21644;SCAFFOLD&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#24403;&#25152;&#26377;&#23458;&#25143;&#31471;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
&lt;/p&gt;</description></item><item><title>SARC&#26159;&#19968;&#20010;&#22522;&#20110;SAC&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#35770;&#32773;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#26799;&#24230;&#20272;&#35745;&#65292;&#20026;&#28436;&#21592;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16503</link><description>&lt;p&gt;
SARC: &#36719;&#21442;&#28436;&#32773;&#22238;&#39038;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
SARC: Soft Actor Retrospective Critic. (arXiv:2306.16503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16503
&lt;/p&gt;
&lt;p&gt;
SARC&#26159;&#19968;&#20010;&#22522;&#20110;SAC&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#35770;&#32773;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#26799;&#24230;&#20272;&#35745;&#65292;&#20026;&#28436;&#21592;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SAC&#26159;&#19968;&#20010;&#28436;&#21592;&#35780;&#35770;&#32773;&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#29305;&#24615;&#22312;&#20110;&#35780;&#35770;&#32773;&#20272;&#35745;&#22312;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#37117;&#27809;&#26377;&#25910;&#25947;&#20110;&#28436;&#21592;&#65292;&#20294;&#30001;&#20110;&#35780;&#35770;&#32773;&#23398;&#20064;&#36895;&#24230;&#27604;&#28436;&#21592;&#24555;&#65292;&#23427;&#30830;&#20445;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26368;&#32456;&#19968;&#33268;&#24615;&#12290;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#30001;&#20110;&#26799;&#24230;&#20272;&#35745;&#20381;&#36182;&#20110;&#35780;&#35770;&#32773;&#65292;&#25105;&#20204;&#35748;&#20026;&#25913;&#36827;&#35780;&#35770;&#32773;&#21487;&#20197;&#20026;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#28436;&#21592;&#25552;&#20379;&#26356;&#22909;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#21442;&#28436;&#32773;&#22238;&#39038;&#35780;&#35770;&#32773;(SARC)&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;SAC&#35780;&#35770;&#32773;&#25439;&#22833;&#19982;&#21478;&#19968;&#20010;&#25439;&#22833;&#39033;&#22238;&#39038;&#25439;&#22833;&#30456;&#32467;&#21512; - &#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35780;&#35770;&#32773;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#28436;&#21592;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;SAC&#23454;&#29616;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;SARC&#65292;&#21482;&#38656;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SARC&#30456;&#27604;S&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The two-time scale nature of SAC, which is an actor-critic algorithm, is characterised by the fact that the critic estimate has not converged for the actor at any given time, but since the critic learns faster than the actor, it ensures eventual consistency between the two. Various strategies have been introduced in literature to learn better gradient estimates to help achieve better convergence. Since gradient estimates depend upon the critic, we posit that improving the critic can provide a better gradient estimate for the actor at each time. Utilizing this, we propose Soft Actor Retrospective Critic (SARC), where we augment the SAC critic loss with another loss term retrospective loss - leading to faster critic convergence and consequently, better policy gradient estimates for the actor. An existing implementation of SAC can be easily adapted to SARC with minimal modifications. Through extensive experimentation and analysis, we show that SARC provides consistent improvement over S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21464;&#20998;&#19981;&#31561;&#24335;&#20013;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#22823;&#25968;&#23450;&#24459;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#25581;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;VIP&#38382;&#39064;&#65292;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010;&#21807;&#19968;&#30340;&#19981;&#21464;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.16502</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#31561;&#24335;&#20013;&#30340;&#38543;&#26426;&#26041;&#27861;&#65306;&#36941;&#21382;&#24615;&#12289;&#20559;&#24046;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements. (arXiv:2306.16502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21464;&#20998;&#19981;&#31561;&#24335;&#20013;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#22823;&#25968;&#23450;&#24459;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#25581;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;VIP&#38382;&#39064;&#65292;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010;&#21807;&#19968;&#30340;&#19981;&#21464;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36935;&#21040;&#30340;min-max&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;(VIP)&#65292;&#38543;&#26426;&#22806;&#25512;&#26799;&#24230;(SEG)&#21644;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;(SGDA)&#31639;&#27861;&#24050;&#25104;&#20026;&#26480;&#20986;&#30340;&#31639;&#27861;&#12290;SEG/SGDA&#30340;&#24658;&#23450;&#27493;&#38271;&#21464;&#31181;&#24191;&#21463;&#27426;&#36814;&#65292;&#20855;&#26377;&#26131;&#20110;&#35843;&#33410;&#21644;&#21407;&#22987;&#26465;&#20214;&#36805;&#36895;&#36866;&#24212;&#30340;&#20248;&#28857;&#65292;&#20294;&#21363;&#20351;&#22312;&#22522;&#26412;&#30340;&#21452;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#23427;&#20204;&#30340;&#25910;&#25947;&#34892;&#20026;&#20063;&#26356;&#21152;&#22797;&#26434;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#38416;&#26126;&#21644;&#37327;&#21270;&#36825;&#20123;&#31639;&#27861;&#20869;&#22312;&#30340;&#27010;&#29575;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#24658;&#23450;&#27493;&#38271;SEG/SGDA&#37325;&#26032;&#26500;&#36896;&#20026;&#26102;&#38388;&#40784;&#27425;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39318;&#20010;&#22823;&#25968;&#23450;&#24459;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#22312;&#24191;&#27867;&#30340;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;VIP&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010;&#21807;&#19968;&#30340;&#19981;&#21464;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20984;&#20985;min-max&#20248;&#21270;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#36830;&#25509;VIP&#21644;&#20248;&#21270;&#20559;&#24046;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
For min-max optimization and variational inequalities problems (VIP) encountered in diverse machine learning tasks, Stochastic Extragradient (SEG) and Stochastic Gradient Descent Ascent (SGDA) have emerged as preeminent algorithms. Constant step-size variants of SEG/SGDA have gained popularity, with appealing benefits such as easy tuning and rapid forgiveness of initial conditions, but their convergence behaviors are more complicated even in rudimentary bilinear models. Our work endeavors to elucidate and quantify the probabilistic structures intrinsic to these algorithms. By recasting the constant step-size SEG/SGDA as time-homogeneous Markov Chains, we establish a first-of-its-kind Law of Large Numbers and a Central Limit Theorem, demonstrating that the average iterate is asymptotically normal with a unique invariant distribution for an extensive range of monotone and non-monotone VIPs. Specializing to convex-concave min-max optimization, we characterize the relationship between the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;IST&#19982;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.16484</link><description>&lt;p&gt;
&#26397;&#30528;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#30340;&#26356;&#22909;&#29702;&#35770;&#29702;&#35299;&#36808;&#36827; (arXiv:2306.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;IST&#19982;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#31163;&#19981;&#24320;&#25968;&#25454;&#24182;&#34892;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#33539;&#24335;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#23545;&#36890;&#20449;&#36890;&#36947;&#26045;&#21152;&#20102;&#24040;&#22823;&#21387;&#21147;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20849;&#21516;&#35774;&#35745;&#36890;&#20449;&#21387;&#32553;&#31574;&#30053;&#21644;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#23613;&#31649;&#32431;&#25968;&#25454;&#24182;&#34892;&#24615;&#20801;&#35768;&#26356;&#22909;&#30340;&#25968;&#25454;&#25193;&#23637;&#24615;&#65292;&#20294;&#20854;&#22312;&#27169;&#22411;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#35745;&#31639;&#33410;&#28857;&#21463;&#20869;&#23384;&#38480;&#21046;&#20005;&#37325;&#38480;&#21046;&#65292;&#38459;&#27490;&#20102;&#27169;&#22411;&#23610;&#23544;&#30340;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#24040;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#26524;&#20063;&#20381;&#36182;&#20110;&#26576;&#31181;&#24418;&#24335;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;IST&#21644;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#29305;&#24449;&#24402;&#22240;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16431</link><description>&lt;p&gt;
&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Increasing Performance And Sample Efficiency With Model-agnostic Interactive Feature Attributions. (arXiv:2306.16431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#29305;&#24449;&#24402;&#22240;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26222;&#36866;&#30340;&#29305;&#24449;&#24402;&#22240;&#21487;&#20197;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#23616;&#37096;&#27934;&#23519;&#21147;&#12290;&#22914;&#26524;&#35299;&#37322;&#26159;&#27491;&#30830;&#30340;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#39564;&#35777;&#21644;&#20449;&#20219;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23427;&#19982;&#19987;&#23478;&#30340;&#30693;&#35782;&#30456;&#30683;&#30462;&#65292;&#30456;&#20851;&#24037;&#20316;&#21482;&#32416;&#27491;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#20197;&#25913;&#36827;&#27169;&#22411;&#12290;&#20026;&#20102;&#20801;&#35768;&#26080;&#38480;&#30340;&#20132;&#20114;&#65292;&#26412;&#25991;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;&#36974;&#34109;&#27861;&#21644;&#27801;&#26222;&#21033;&#20540;&#65289;&#25552;&#20379;&#20102;&#27169;&#22411;&#26222;&#36866;&#30340;&#23454;&#29616;&#65292;&#20197;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#24378;&#21046;&#25191;&#34892;&#23436;&#20840;&#19981;&#21516;&#30340;&#24402;&#22240;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#26679;&#26412;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#32416;&#27491;&#30340;&#29305;&#24449;&#24402;&#22240;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#23616;&#37096;&#25968;&#25454;&#65292;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#26679;&#26412;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#36827;&#34892;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#22522;&#20110;&#32416;&#27491;&#30340;&#35299;&#37322;&#26469;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23558;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#28155;&#21152;&#21040;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#20013;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic feature attributions can provide local insights in complex ML models. If the explanation is correct, a domain expert can validate and trust the model's decision. However, if it contradicts the expert's knowledge, related work only corrects irrelevant features to improve the model. To allow for unlimited interaction, in this paper we provide model-agnostic implementations for two popular explanation methods (Occlusion and Shapley values) to enforce entirely different attributions in the complex model. For a particular set of samples, we use the corrected feature attributions to generate extra local data, which is used to retrain the model to have the right explanation for the samples. Through simulated and real data experiments on a variety of models we show how our proposed approach can significantly improve the model's performance only by augmenting its training dataset based on corrected explanations. Adding our interactive explanations to active learning settings incr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;DNN&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#26041;&#27861;DNA-TEQ&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16430</link><description>&lt;p&gt;
DNA-TEQ&#65306;&#19968;&#31181;&#29992;&#20110;DNN&#25512;&#29702;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;DNN&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#26041;&#27861;DNA-TEQ&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#28608;&#27963;&#21644;&#26435;&#37325;&#65288;&#21363;&#24352;&#37327;&#65289;&#30340;&#31639;&#26415;&#31934;&#24230;&#26469;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#39640;&#25928;&#30340;&#30828;&#20214;&#26550;&#26500;&#37319;&#29992;&#32447;&#24615;&#37327;&#21270;&#65292;&#20197;&#20415;&#23558;&#26368;&#26032;&#30340;DNN&#37096;&#32626;&#21040;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#22343;&#21248;&#37327;&#21270;&#36890;&#24120;&#26080;&#27861;&#23558;&#25968;&#20540;&#31934;&#24230;&#38477;&#20302;&#21040;&#23567;&#20110;8&#20301;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#24352;&#37327;&#24182;&#19981;&#26381;&#20174;&#22343;&#21248;&#20998;&#24067;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DNA-TEQ&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#25351;&#25968;&#37327;&#21270;DNN&#24352;&#37327;&#65292;&#20197;&#23454;&#29616;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#26696;&#30456;&#27604;&#65292;DNA-TEQ&#25552;&#20379;&#20102;&#26356;&#20302;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26550;&#26500;&#26469;&#22788;&#29702;&#22797;&#25968;&#20449;&#21495;&#65292;&#36229;&#36234;&#20102;&#21407;&#22987;&#26550;&#26500;&#30340;&#22797;&#25968;&#25193;&#23637;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#21482;&#38656;&#31245;&#24494;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#21363;&#21487;&#23454;&#29616;&#22797;&#25968;&#36816;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.16428</link><description>&lt;p&gt;
&#22797;&#25968;&#33258;&#36866;&#24212;&#31995;&#32479;&#35782;&#21035;&#36890;&#36807;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Complex-valued Adaptive System Identification via Low-Rank Tensor Decomposition. (arXiv:2306.16428v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26550;&#26500;&#26469;&#22788;&#29702;&#22797;&#25968;&#20449;&#21495;&#65292;&#36229;&#36234;&#20102;&#21407;&#22987;&#26550;&#26500;&#30340;&#22797;&#25968;&#25193;&#23637;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#21482;&#38656;&#31245;&#24494;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#21363;&#21487;&#23454;&#29616;&#22797;&#25968;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22522;&#20110;&#24352;&#37327;&#30340;&#26041;&#27861;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#19968;&#30452;&#21463;&#21040;&#31185;&#23398;&#30028;&#30340;&#37325;&#22823;&#20851;&#27880;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#31995;&#32479;&#35782;&#21035;&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#20165;&#20351;&#29992;&#24352;&#37327;&#30340;&#26550;&#26500;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21516;&#26102;&#20173;&#33021;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#21482;&#20801;&#35768;&#22788;&#29702;&#23454;&#25968;&#38382;&#39064;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22788;&#29702;&#22797;&#25968;&#31995;&#32479;&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#36890;&#20449;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#20197;&#20801;&#35768;&#22788;&#29702;&#22797;&#25968;&#20449;&#21495;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#25193;&#23637;&#33021;&#22815;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#21407;&#22987;&#26550;&#26500;&#30340;&#24179;&#20961;&#30340;&#22797;&#25968;&#25193;&#23637;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#31245;&#24494;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#26469;&#20801;&#35768;&#22797;&#25968;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) and tensor-based methods have been of significant interest for the scientific community for the last few decades. In a previous work we presented a novel tensor-based system identification framework to ease the computational burden of tensor-only architectures while still being able to achieve exceptionally good performance. However, the derived approach only allows to process real-valued problems and is therefore not directly applicable on a wide range of signal processing and communications problems, which often deal with complex-valued systems. In this work we therefore derive two new architectures to allow the processing of complex-valued signals, and show that these extensions are able to surpass the trivial, complex-valued extension of the original architecture in terms of performance, while only requiring a slight overhead in computational resources to allow for complex-valued operations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#24452;&#21521;&#22522;&#20989;&#25968;&#26680;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#26399;&#36880;&#26102;&#30340;&#39118;&#21147;&#21644;&#22826;&#38451;&#33021;&#21457;&#30005;&#22330;&#26223;&#65292;&#24182;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#33021;&#28304;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16427</link><description>&lt;p&gt;
&#38271;&#26399;&#20851;&#32852;&#39118;&#21147;&#21644;&#22826;&#38451;&#33021;&#21457;&#30005;&#30340;&#36880;&#26102;&#22330;&#26223;&#29983;&#25104;&#65306;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#24452;&#21521;&#22522;&#20989;&#25968;&#26680;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Long-Term Hourly Scenario Generation for Correlated Wind and Solar Power combining Variational Autoencoders with Radial Basis Function Kernels. (arXiv:2306.16427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#24452;&#21521;&#22522;&#20989;&#25968;&#26680;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#26399;&#36880;&#26102;&#30340;&#39118;&#21147;&#21644;&#22826;&#38451;&#33021;&#21457;&#30005;&#22330;&#26223;&#65292;&#24182;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#33021;&#28304;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#29983;&#25104;&#21487;&#25345;&#32493;&#33021;&#28304;&#21457;&#30005;&#30340;&#26410;&#26469;&#22330;&#26223;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#38271;&#26399;&#35268;&#21010;&#21644;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#21487;&#25345;&#32493;&#33021;&#28304;&#30340;&#26085;&#30410;&#20851;&#27880;&#21644;&#22312;&#33021;&#28304;&#30697;&#38453;&#20013;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#28183;&#36879;&#12290;&#36825;&#20123;&#39044;&#27979;&#33021;&#22815;&#24110;&#21161;&#30005;&#21147;&#31995;&#32479;&#36816;&#33829;&#21830;&#21644;&#33021;&#28304;&#35268;&#21010;&#24072;&#26377;&#25928;&#22320;&#31649;&#29702;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#25152;&#24102;&#26469;&#30340;&#21464;&#24322;&#24615;&#21644;&#38388;&#27463;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#30005;&#32593;&#31283;&#23450;&#24615;&#12289;&#25913;&#21892;&#33021;&#28304;&#31649;&#29702;&#21644;&#21152;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39118;&#21147;&#21644;&#22826;&#38451;&#33021;&#21457;&#30005;&#30340;&#38271;&#26399;&#36880;&#26102;&#22330;&#26223;&#65292;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#31181;&#33021;&#28304;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#33021;&#21147;&#19982;&#22312;&#25105;&#20204;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#32435;&#20837;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#26680;&#30340;&#39069;&#22806;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate generation of realistic future scenarios of renewable energy generation is crucial for long-term planning and operation of electrical systems, especially considering the increasing focus on sustainable energy and the growing penetration of renewable generation in energy matrices. These predictions enable power system operators and energy planners to effectively manage the variability and intermittency associated with renewable generation, allowing for better grid stability, improved energy management, and enhanced decision-making processes. In this paper, we propose an innovative method for generating long-term hourly scenarios for wind and solar power generation, taking into consideration the correlation between these two energy sources. To achieve this, we combine the capabilities of a Variational Autoencoder (VAE) with the additional benefits of incorporating the Radial Basis Function (RBF) kernel in our artificial neural network architecture. By incorporating them, we aim 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#39046;&#22495;CTR&#39044;&#27979;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.16425</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#25512;&#33616;&#30340;&#21327;&#20316;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Collaborative Transfer Learning Framework for Cross-domain Recommendation. (arXiv:2306.16425v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#39046;&#22495;CTR&#39044;&#27979;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26377;&#22810;&#20010;&#19981;&#21516;&#30340;&#19994;&#21153;&#39046;&#22495;&#26469;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#19981;&#21516;&#39046;&#22495;&#30340;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;&#19981;&#21516;&#19994;&#21153;&#39046;&#22495;&#36827;&#34892;CTR&#39044;&#27979;&#24314;&#27169;&#12290;&#34892;&#19994;&#35299;&#20915;&#26041;&#26696;&#26159;&#23545;&#27599;&#20010;&#39046;&#22495;&#20351;&#29992;&#29305;&#23450;&#30340;&#27169;&#22411;&#25110;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#21069;&#32773;&#30340;&#32570;&#28857;&#26159;&#21333;&#19968;&#39046;&#22495;&#27169;&#22411;&#27809;&#26377;&#21033;&#29992;&#20854;&#20182;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#32780;&#21518;&#32773;&#21017;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#25152;&#26377;&#25968;&#25454;&#65292;&#20294;&#36801;&#31227;&#23398;&#20064;&#30340;&#24494;&#35843;&#27169;&#22411;&#21487;&#33021;&#20351;&#27169;&#22411;&#38519;&#20837;&#28304;&#39046;&#22495;&#30340;&#23616;&#37096;&#26368;&#20248;&#65292;&#38590;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;&#21516;&#26102;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#29305;&#24449;&#27169;&#24335;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#21363;&#39046;&#22495;&#20559;&#31227;&#65292;&#22312;&#36801;&#31227;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#36127;&#38754;&#36801;&#31227;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#20102;&#31034;&#20363;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.16422</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Neural networks can detect model-free static arbitrage strategies. (arXiv:2306.16422v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#20102;&#31034;&#20363;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29702;&#35770;&#21644;&#25968;&#20540;&#26041;&#27861;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#24066;&#22330;&#23384;&#22312;&#22871;&#21033;&#26426;&#20250;&#26102;&#26816;&#27979;&#20986;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#30830;&#20445;&#30456;&#24212;&#20132;&#26131;&#31574;&#30053;&#30340;&#20960;&#20046;&#21363;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#30340;&#31034;&#20363;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;&#19968;&#31867;&#20984;&#21322;&#26080;&#38480;&#35268;&#21010;&#38382;&#39064;&#65292;&#36825;&#26159;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we demonstrate both theoretically as well as numerically that neural networks can detect model-free static arbitrage opportunities whenever the market admits some. Due to the use of neural networks, our method can be applied to financial markets with a high number of traded securities and ensures almost immediate execution of the corresponding trading strategies. To demonstrate its tractability, effectiveness, and robustness we provide examples using real financial data. From a technical point of view, we prove that a single neural network can approximately solve a class of convex semi-infinite programs, which is the key result in order to derive our theoretical results that neural networks can detect model-free static arbitrage strategies whenever the financial market admits such opportunities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16077</link><description>&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#24322;&#27493;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;:&#22522;&#20110;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;(VFL)&#22240;&#33021;&#22815;&#22312;&#22402;&#30452;&#20998;&#21106;&#30340;&#25968;&#25454;&#19978;&#32852;&#21512;&#35757;&#32451;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;(ZOO)&#22312;&#26500;&#24314;&#23454;&#29992;&#30340;VFL&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;ZOO&#30340;VFL&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#27169;&#22411;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;VFL&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20013;&#65292;&#19979;&#28216;&#27169;&#22411;&#65288;&#23458;&#25143;&#31471;&#65289;&#20351;&#29992;ZOO&#36827;&#34892;&#35757;&#32451;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#30830;&#20445;&#19981;&#20849;&#20139;&#20869;&#37096;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#19978;&#28216;&#27169;&#22411;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#26412;&#22320;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;(FOO)&#36827;&#34892;&#26356;&#26032;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;VFL&#26694;&#26550;&#27604;&#22522;&#20110;ZOO&#30340;VFL&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15786</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#32599;&#29983;&#38376;&#25928;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#29983;&#38376;&#25928;&#24212;&#25551;&#36848;&#20102;&#20197;&#19979;&#29616;&#35937;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#21516;&#33391;&#22909;&#24615;&#33021;&#20294;&#37319;&#29992;&#19981;&#21516;&#35299;&#20915;&#31574;&#30053;&#30340;&#27169;&#22411;&#12290;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#35299;&#37322;&#30340;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#27604;&#36739;&#22330;&#26223;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24402;&#22240;&#26041;&#27861;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#35843;&#25972;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#65292;&#25351;&#26631;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20808;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35889;&#32858;&#31867;&#20013;&#39318;&#27425;&#24212;&#29992;&#37325;&#21551;&#31574;&#30053;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#20998;&#31867;&#26679;&#26412;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15138</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#22823;&#35268;&#27169;&#35889;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation. (arXiv:2306.15138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35889;&#32858;&#31867;&#20013;&#39318;&#27425;&#24212;&#29992;&#37325;&#21551;&#31574;&#30053;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#20998;&#31867;&#26679;&#26412;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#26159;&#26368;&#27969;&#34892;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#12290;&#26500;&#24314;&#30456;&#20284;&#24615;&#30697;&#38453;&#23545;&#20110;&#36825;&#31867;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#30456;&#20284;&#24615;&#30697;&#38453;&#21482;&#35745;&#31639;&#19968;&#27425;&#25110;&#32773;&#26159;&#20132;&#26367;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24456;&#38590;&#21453;&#26144;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20840;&#38754;&#20851;&#31995;&#65292;&#32780;&#21518;&#32773;&#32791;&#26102;&#19988;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#29978;&#33267;&#38590;&#20197;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26694;&#26550;&#12290;&#35813;&#31574;&#30053;&#30340;&#20248;&#21183;&#22312;&#20110;&#23613;&#21487;&#33021;&#20445;&#30041;&#20174;&#20808;&#21069;&#21608;&#26399;&#20013;&#33719;&#24471;&#30340;&#26377;&#29992;&#32858;&#31867;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#37325;&#21551;&#31574;&#30053;&#24212;&#29992;&#20110;&#35889;&#32858;&#31867;&#30340;&#24037;&#20316;&#12290;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#25105;&#20204;&#22312;&#26041;&#27861;&#30340;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#36827;&#34892;&#19968;&#27425;&#20998;&#31867;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering is one of the most popular unsupervised machine learning methods. Constructing similarity matrix is crucial to this type of method. In most existing works, the similarity matrix is computed once for all or is updated alternatively. However, the former is difficult to reflect comprehensive relationships among data points, and the latter is time-consuming and is even infeasible for large-scale problems. In this work, we propose a restarted clustering framework with self-guiding and block diagonal representation. An advantage of the strategy is that some useful clustering information obtained from previous cycles could be preserved as much as possible. To the best of our knowledge, this is the first work that applies restarting strategy to spectral clustering. The key difference is that we reclassify the samples in each cycle of our method, while they are classified only once in existing methods. To further release the overhead, we introduce a block diagonal representa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.14522</link><description>&lt;p&gt;
&#38750;&#20984;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230;&#27861;&#21450;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#38750;&#20984;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;Lipschitz&#24179;&#28369;&#24615;, &#20294;&#36825;&#19968;&#35201;&#27714;&#23545;&#20110;&#21253;&#25324;&#20108;&#27425;&#36870;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38382;&#39064;&#31867;&#21035;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#26063;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230; (SBPG) &#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;&#24179;&#28369;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.13651</link><description>&lt;p&gt;
&#33258;&#24102;&#25968;&#25454;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLM&#22312;&#37326;&#22806;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#22312;&#27169;&#22411;&#37096;&#32626;&#26399;&#38388;&#36827;&#34892;&#30340;&#27969;&#25968;&#25454;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12088</link><description>&lt;p&gt;
&#19968;&#31181;&#20943;&#23569;&#32852;&#37030;&#23398;&#20064;&#36890;&#20449;&#30340;&#39640;&#25928;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#24320;&#38144;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#19968;&#20123;&#32463;&#20856;&#30340;&#26041;&#26696;&#20551;&#35774;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#26412;&#22320;&#27169;&#22411;&#20013;&#25552;&#21462;&#21442;&#19982;&#32773;&#35757;&#32451;&#25968;&#25454;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;&#26381;&#21153;&#22120;&#20351;&#29992;&#34394;&#25311;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#22312;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#20869;&#36798;&#21040;&#30446;&#26631;&#27979;&#35797;&#31934;&#24230;&#12290;&#26412;&#25991;&#23558;&#19978;&#36848;&#35299;&#20915;&#26041;&#26696;&#27010;&#25324;&#20026;&#22522;&#20110;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25552;&#20986;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65288;EM&#65289;&#65292;&#23427;&#30830;&#20445;&#34394;&#25311;&#25968;&#25454;&#38598;&#23545;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#22120;&#26469;&#35774;&#35745;EM&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FedINIBoost&#20511;&#37492;&#20102;&#26799;&#24230;&#21305;&#37197;&#30340;&#24605;&#24819;&#26469;&#26500;&#24314;EM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedINIBoost&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27599;&#20010;&#21442;&#19982;&#32773;&#20013;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#26500;&#24314;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#26381;&#21153;&#22120;&#32858;&#21512;&#25152;&#26377;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#37325;&#28857;&#20851;&#27880;&#24076;&#33098;&#33021;&#28304;&#24066;&#22330;&#65292;&#36890;&#36807;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09129</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Energy Time-Series Analysis and Forecasting. (arXiv:2306.09129v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#37325;&#28857;&#20851;&#27880;&#24076;&#33098;&#33021;&#28304;&#24066;&#22330;&#65292;&#36890;&#36807;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25551;&#36848;&#20102;&#36890;&#36807;&#20998;&#26512;&#36807;&#21435;&#30340;&#33021;&#28304;&#35266;&#27979;&#21644;&#21487;&#33021;&#30340;&#22806;&#37096;&#22240;&#32032;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#36807;&#31243;&#12290;&#22312;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#39044;&#27979;&#30340;&#19968;&#33324;&#39046;&#22495;&#20013;&#28041;&#21450;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#30005;&#21147;&#36127;&#33655;&#38656;&#27714;&#39044;&#27979;&#12289;&#20010;&#24615;&#21270;&#33021;&#28304;&#28040;&#36153;&#39044;&#27979;&#65292;&#20197;&#21450;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#39044;&#27979;&#31561;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#25913;&#36827;&#65292;&#29305;&#21035;&#20851;&#27880;&#24076;&#33098;&#33021;&#28304;&#24066;&#22330;&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24517;&#35201;&#30693;&#35782;.
&lt;/p&gt;
&lt;p&gt;
Energy time-series analysis describes the process of analyzing past energy observations and possibly external factors so as to predict the future. Different tasks are involved in the general field of energy time-series analysis and forecasting, with electric load demand forecasting, personalized energy consumption forecasting, as well as renewable energy generation forecasting being among the most common ones. Following the exceptional performance of Deep Learning (DL) in a broad area of vision tasks, DL models have successfully been utilized in time-series forecasting tasks. This paper aims to provide insight into various DL methods geared towards improving the performance in energy time-series forecasting tasks, with special emphasis in Greek Energy Market, and equip the reader with the necessary knowledge to apply these methods in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ViP&#65292;&#19968;&#20010;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;DP-SGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;LAION400M&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;ViP&#12290;ViP&#22312;&#26631;&#20934;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#23398;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;&#19982;AlexNet&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.08842</link><description>&lt;p&gt;
ViP&#65306;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24046;&#20998;&#38544;&#31169;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ViP: A Differentially Private Foundation Model for Computer Vision. (arXiv:2306.08842v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ViP&#65292;&#19968;&#20010;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;DP-SGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;LAION400M&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;ViP&#12290;ViP&#22312;&#26631;&#20934;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#23398;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;&#19982;AlexNet&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20351;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#30340;&#38750;&#31579;&#36873;&#24615;&#36136;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#38544;&#31169;&#21644;&#27861;&#24459;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#26410;&#32463;&#35768;&#21487;&#19981;&#24212;&#35813;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#25514;&#26045;&#65292;&#21363;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#35757;&#32451;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#20316;&#20026;&#36866;&#21512;&#19982;DP-SGD&#30456;&#21305;&#37197;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;LAION400M&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;$\epsilon=8$&#30340;&#20005;&#26684;&#38544;&#31169;&#39044;&#31639;&#35757;&#32451;&#20102;ViP - &#19968;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#35780;&#20272;&#20102;ViP&#23398;&#21040;&#30340;&#34920;&#31034;&#36136;&#37327;&#65307;&#29305;&#21035;&#22320;&#65292;ViP&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;$55.7\%$&#30340;&#65288;&#38750;&#31169;&#26377;&#65289;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#65292;&#19982;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;AlexNet&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train ViP -- a Vision transformer with differential Privacy -- under a strict privacy budget of $\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of $55.7\%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.07774</link><description>&lt;p&gt;
&#38477;&#31209;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65306;&#22312;&#39640;&#32500;&#20013;&#36827;&#34892;&#36817;&#20284;&#20302;&#31209;&#21160;&#24577;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#30340;&#25512;&#26029;&#21644;&#27169;&#25311;&#20013;&#65292;&#38656;&#35201;&#36827;&#34892;&#26576;&#31181;&#24418;&#24335;&#30340;&#38477;&#32500;&#25165;&#33021;&#20351;&#38382;&#39064;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#39044;&#27979;&#27493;&#39588;&#30456;&#20851;&#30340;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#26469;&#23454;&#29616;&#30340;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#36817;&#24320;&#21457;&#30340;&#25968;&#20540;&#31283;&#23450;&#12289;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#36825;&#20123;&#26041;&#31243;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#27880;&#24847;&#21327;&#26041;&#24046;&#26356;&#26032;&#20165;&#36716;&#25442;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#65292;&#32780;&#35813;&#31354;&#38388;&#30001;&#26500;&#36896;&#24471;&#21040;&#65292;&#20174;&#32780;&#20351;&#26356;&#26032;&#27493;&#39588;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#65292;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#21487;&#20197;&#20943;&#23569;&#22810;&#23567;&#26102;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#23454;&#39564;&#30340;&#35780;&#20272;&#39034;&#24207;&#23436;&#20840;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2305.17595</link><description>&lt;p&gt;
Python&#23553;&#35013;&#22120;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#26080;&#38656;&#31561;&#24453;
&lt;/p&gt;
&lt;p&gt;
Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait. (arXiv:2305.17595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#65292;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#21487;&#20197;&#20943;&#23569;&#22810;&#23567;&#26102;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#23454;&#39564;&#30340;&#35780;&#20272;&#39034;&#24207;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#20248;&#21270;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#24448;&#24448;&#38656;&#35201;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22240;&#27492;&#28145;&#24230;&#23398;&#20064;&#30340;HP&#20248;&#21270;&#36890;&#24120;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#26114;&#36149;&#30340;&#12290;&#36825;&#20419;&#20351;&#20986;&#29616;&#20102;&#34920;&#26684;&#25110;&#26367;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#22312;&#19968;&#23567;&#37096;&#20998;&#26102;&#38388;&#20869;&#26597;&#35810;&#29305;&#23450;HP&#37197;&#32622;&#30340;DL&#30340;&#65288;&#39044;&#27979;&#65289;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DL&#35757;&#32451;&#30340;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#19982;&#26597;&#35810;&#21709;&#24212;&#26102;&#38388;&#26126;&#26174;&#19981;&#21516;&#65292;&#24322;&#27493;HPO&#65288;&#20363;&#22914;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65289;&#30340;&#27169;&#25311;&#22120;&#24517;&#39035;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31561;&#24453;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#65292;&#21542;&#21017;&#27169;&#25311;&#20013;&#30340;&#35780;&#20272;&#39034;&#24207;&#19981;&#31526;&#21512;&#23454;&#38469;&#23454;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#29992;&#27861;&#12290;&#36825;&#20010;&#23553;&#35013;&#22120;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#20197;&#20415;&#25105;&#20204;&#21482;&#38656;&#31561;&#24453;$10^{-2}$&#31186;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#19982;&#23454;&#38469;&#23454;&#39564;&#23436;&#20840;&#30456;&#21516;&#30340;&#35780;&#20272;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#31561;&#24453;&#20960;&#20010;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter (HP) optimization of deep learning (DL) is essential for high performance. As DL often requires several hours to days for its training, HP optimization (HPO) of DL is often prohibitively expensive. This boosted the emergence of tabular or surrogate benchmarks, which enable querying the (predictive) performance of DL with a specific HP configuration in a fraction. However, since the actual runtime of a DL training is significantly different from its query response time, simulators of an asynchronous HPO, e.g. multi-fidelity optimization, must wait for the actual runtime at each iteration in a na\"ive implementation; otherwise, the evaluation order during simulation does not match with the real experiment. To ease this issue, we developed a Python wrapper and describe its usage. This wrapper forces each worker to wait so that we yield exactly the same evaluation order as in the real experiment with only $10^{-2}$ seconds of waiting instead of waiting several hours. Our imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17341</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31354;&#38388;&#30340;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#26469;&#25913;&#36827;&#38544;&#31169;&#20445;&#25252;PCA
&lt;/p&gt;
&lt;p&gt;
Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;PowerMethod&#30340;PCA&#24120;&#35268;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19982;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#20027;&#25104;&#20998;&#23545;&#24212;&#30340;&#36817;&#20284;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;Pandas CSCML 21&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20197;&#19979;&#20248;&#21270;&#65306;&#65288;i&#65289;&#20248;&#21270;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#25216;&#26415;&#65288;Jiang&#31561;&#20154;SIGSAC 2018&#65289;&#65292;&#35813;&#25216;&#26415;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35745;&#31639;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21516;&#24577;&#30005;&#36335;&#26469;&#21516;&#24577;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#29992;&#20110;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
&lt;/p&gt;</description></item><item><title>SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14912</link><description>&lt;p&gt;
SVDinsTN: &#19968;&#31181;&#38598;&#25104;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#21450;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search. (arXiv:2305.14912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14912
&lt;/p&gt;
&lt;p&gt;
SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#34920;&#31034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#20197;&#23454;&#29616;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#30001;&#20110;&#37325;&#22797;&#30340;&#32467;&#26500;&#35780;&#20272;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#65288;&#21333;&#23618;&#65289;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SVDinsTN&#65292;&#28040;&#38500;&#20102;&#37325;&#22797;&#32321;&#29712;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#36890;&#36807;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;TN&#30340;&#27599;&#20010;&#36793;&#25554;&#20837;&#19968;&#20010;&#23545;&#35282;&#22240;&#23376;&#65292;&#25105;&#20204;&#21516;&#26102;&#35745;&#31639;TN&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#22240;&#23376;&#31232;&#30095;&#24615;&#25581;&#31034;&#20102;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#23454;&#29616;&#20102;&#32422;10&#21040;10^3&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) representation is a powerful technique for data analysis and machine learning. It practically involves a challenging TN structure search (TN-SS) problem, which aims to search for the optimal structure to achieve a compact representation. Existing TN-SS methods mainly adopt a bi-level optimization method that leads to excessive computational costs due to repeated structure evaluations. To address this issue, we propose an efficient integrated (single-level) method named SVD-inspired TN decomposition (SVDinsTN), eliminating the need for repeated tedious structure evaluation. By inserting a diagonal factor for each edge of the fully-connected TN, we calculate TN cores and diagonal factors simultaneously, with factor sparsity revealing the most compact TN structure. Experimental results on real-world data demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ times acceleration in runtime compared to the existing TN-SS methods while maintaining a comparable lev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10631</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;U-Net&#22312;&#20998;&#21106;&#30452;&#32928;&#30284;&#27835;&#30103;&#26399;&#38388;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#26102;&#65292;&#30001;&#20110;&#22810;&#27425;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#23548;&#33268;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#38169;&#20301;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;MRI&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#22312;&#32534;&#30721;&#20013;&#20351;&#29992;&#20102;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#12290;2&#65289;&#35774;&#35745;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20445;&#25345;U-Net&#30340;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;&#23545;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20043;&#65292;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21487;&#20197;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#20351;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23384;&#22312;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;&#65306;&#27745;&#26579;&#22826;&#23569;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#65292;&#27745;&#26579;&#22826;&#22810;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20004;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#23545;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#36827;&#34892;&#21518;&#22788;&#29702;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09671</link><description>&lt;p&gt;
&#36873;&#25321;&#20320;&#30340;&#27602;&#33647;&#65306;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#30340;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;
&lt;/p&gt;
&lt;p&gt;
Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09671
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23384;&#22312;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;&#65306;&#27745;&#26579;&#22826;&#23569;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#65292;&#27745;&#26579;&#22826;&#22810;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20004;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#23545;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#36827;&#34892;&#21518;&#22788;&#29702;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#26263;&#34255;&#21518;&#38376;&#30340;&#26426;&#21046;&#12290;&#21363;&#20351;&#22521;&#35757;&#36807;&#31243;&#20013;&#21482;&#26377;&#23569;&#37327;&#27745;&#26579;&#26679;&#26412;&#65292;&#20063;&#36275;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#34429;&#28982;&#24050;&#30693;&#27745;&#26579;&#26356;&#22810;&#30340;&#26679;&#26412;&#21487;&#20197;&#22686;&#24378;&#25915;&#20987;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#27745;&#26579;&#22826;&#22810;&#26679;&#26412;&#26159;&#21542;&#20250;&#20351;&#25915;&#20987;&#21464;&#24471;&#26356;&#26131;&#34987;&#26816;&#27979;&#21040;&#20174;&#32780;&#21066;&#24369;&#25915;&#20987;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26816;&#27979;&#24615;/&#40065;&#26834;&#24615;&#26435;&#34913;&#65306;&#27745;&#26579;&#22826;&#23569;&#30340;&#26679;&#26412;&#20250;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#21644;&#19981;&#40065;&#26834;&#65292;&#20294;&#27745;&#26579;&#22826;&#22810;&#30340;&#26679;&#26412;&#21017;&#20250;&#20351;&#25915;&#20987;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#36825;&#25552;&#39640;&#20102;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32773;&#30340;&#38376;&#27099;&#65292;&#20182;&#20204;&#24517;&#39035;&#26435;&#34913;&#36825;&#31181;&#26435;&#34913;&#20197;&#20445;&#25345;&#40065;&#26834;&#21644;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#20316;&#20026;&#22521;&#35757;&#21518;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#25514;&#26045;&#21487;&#20197;&#20943;&#36731;&#22823;&#37327;&#27745;&#26579;&#25915;&#20987;&#65292;&#21516;&#26102;&#23545;&#36867;&#36991;&#23581;&#35797;&#20445;&#25345;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23457;&#26597;&#29616;&#35937;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#20197;&#21450;&#38543;&#26426;&#25506;&#32034;&#65292;&#20174;&#32780;&#30830;&#20445;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09035</link><description>&lt;p&gt;
&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#30340;&#31639;&#27861;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Censoring in Dynamic Learning Systems. (arXiv:2305.09035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23457;&#26597;&#29616;&#35937;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#20197;&#21450;&#38543;&#26426;&#25506;&#32034;&#65292;&#20174;&#32780;&#30830;&#20445;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#36873;&#25321;&#26631;&#35760;&#24433;&#21709;&#30340;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#20986;&#29616;&#23457;&#26597;&#29616;&#35937;&#65292;&#21363;&#38024;&#23545;&#19968;&#32452;&#25110;&#22810;&#32452;&#25968;&#25454;&#28857;&#20998;&#37197;&#25345;&#32493;&#30340;&#36127;&#38754;&#39044;&#27979;&#12290;&#22312;&#28040;&#36153;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#65292;&#36825;&#20250;&#23548;&#33268;&#19968;&#20123;&#30003;&#35831;&#20154;&#32452;&#34987;&#25345;&#32493;&#25298;&#32477;&#65292;&#24182;&#19988;&#20174;&#26410;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#35268;&#33539;&#21270;&#23457;&#26597;&#29616;&#35937;&#65292;&#23637;&#31034;&#20854;&#21487;&#33021;&#30340;&#20986;&#29616;&#26041;&#24335;&#65292;&#24182;&#24378;&#35843;&#26816;&#27979;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#37319;&#21462;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#65292;&#24182;&#36827;&#34892;&#38543;&#26426;&#25506;&#32034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#30830;&#20445;&#25105;&#20204;&#23545;&#21407;&#26412;&#26410;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25216;&#26415;&#33021;&#22815;&#35753;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23457;&#26597;&#30340;&#19981;&#21487;&#27979;&#37327;&#30340;&#21361;&#23475;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#32531;&#35299;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic learning systems subject to selective labeling exhibit censoring, i.e. persistent negative predictions assigned to one or more subgroups of points. In applications like consumer finance, this results in groups of applicants that are persistently denied and thus never enter into the training data. In this work, we formalize censoring, demonstrate how it can arise, and highlight difficulties in detection. We consider safeguards against censoring recourse and randomized-exploration - both of which ensure we collect labels for points that would otherwise go unobserved. The resulting techniques allow examples from censored groups to enter into the training data and correct the model. Our results highlight the otherwise unmeasured harms of censoring and demonstrate the effectiveness of mitigation strategies across a range of data generating processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PITT&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.08757</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer
&lt;/p&gt;
&lt;p&gt;
Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PITT&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21364;&#24448;&#24448;&#26080;&#27861;&#23436;&#25972;&#22320;&#34701;&#20837;&#31995;&#32479;&#20449;&#24687;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;Transformer&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#24182;&#22312;PDE&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;Transformer&#32570;&#20047;&#19982;&#29289;&#29702;&#21644;&#25512;&#29702;&#30340;&#25972;&#21512;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;PITT&#65306;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;PITT&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#26469;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#12290;PITT&#20351;&#29992;&#26041;&#31243;&#26631;&#35760;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#20998;&#26512;&#39537;&#21160;&#30340;&#25968;&#20540;&#26356;&#26032;&#36816;&#31639;&#31526;&#12290;&#36890;&#36807;&#26631;&#35760;&#21270;PDEs&#21644;&#23884;&#20837;&#20559;&#23548;&#25968;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#29289;&#29702;&#36807;&#31243;&#30340;&#22522;&#26412;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PITT&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#21333;&#20154;&#28216;&#25103;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#35757;&#32451;&#30340;&#20195;&#29702;&#20154;&#21644;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#65292;&#21457;&#29616;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.07687</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#25484;&#25569;&#31867;&#28183;&#36879;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering Percolation-like Games with Deep Learning. (arXiv:2305.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07687
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#21333;&#20154;&#28216;&#25103;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#35757;&#32451;&#30340;&#20195;&#29702;&#20154;&#21644;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#65292;&#21457;&#29616;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32593;&#32476;&#23545;&#38543;&#26426;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#25925;&#24847;&#30772;&#22351;&#24182;&#19981;&#36866;&#29992;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#26230;&#26684;&#19978;&#30340;&#21333;&#20154;&#28216;&#25103;&#65292;&#27169;&#25311;&#25915;&#20987;&#32773;&#35797;&#22270;&#25703;&#27585;&#32593;&#32476;&#30340;&#36923;&#36753;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#26368;&#23569;&#30340;&#27493;&#39588;&#20013;&#31105;&#29992;&#25152;&#26377;&#33410;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#23398;&#20064;&#29609;&#36825;&#20010;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#26368;&#20248;&#22320;&#25915;&#20987;&#32593;&#32476;&#12290;&#30001;&#20110;&#23398;&#20064;&#31639;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#25105;&#20204;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#19978;&#24182;&#27604;&#36739;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#24341;&#23548;&#35757;&#32451;&#20195;&#29702;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#26263;&#31034;&#30528;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29702;&#35299;&#32593;&#32476;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#31163;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though robustness of networks to random attacks has been widely studied, intentional destruction by an intelligent agent is not tractable with previous methods. Here we devise a single-player game on a lattice that mimics the logic of an attacker attempting to destroy a network. The objective of the game is to disable all nodes in the fewest number of steps. We develop a reinforcement learning approach using deep Q-learning that is capable of learning to play this game successfully, and in so doing, to optimally attack a network. Because the learning algorithm is universal, we train agents on different definitions of robustness and compare the learned strategies. We find that superficially similar definitions of robustness induce different strategies in the trained agent, implying that optimally attacking or defending a network is sensitive the particular objective. Our method provides a new approach to understand network robustness, with potential applications to other discrete proces
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#23481;&#24525;&#20002;&#22833;&#20256;&#36755;&#21327;&#35758;&#65288;LTP&#65289;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#36895;&#24230;&#21644;&#21534;&#21520;&#37327;&#65292;&#35813;&#21327;&#35758;&#20801;&#35768;&#37096;&#20998;&#26799;&#24230;&#20002;&#22833;&#65292;&#24182;&#36890;&#36807;&#20081;&#24207;&#20256;&#36755;&#21644;&#20081;&#24207;&#30830;&#35748;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.04279</link><description>&lt;p&gt;
&#36890;&#36807;&#23481;&#24525;&#20002;&#22833;&#30340;&#20256;&#36755;&#21327;&#35758;&#25552;&#21319;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Distributed Machine Learning Training Through Loss-tolerant Transmission Protocol. (arXiv:2305.04279v1 [cs.DC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#23481;&#24525;&#20002;&#22833;&#20256;&#36755;&#21327;&#35758;&#65288;LTP&#65289;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#36895;&#24230;&#21644;&#21534;&#21520;&#37327;&#65292;&#35813;&#21327;&#35758;&#20801;&#35768;&#37096;&#20998;&#26799;&#24230;&#20002;&#22833;&#65292;&#24182;&#36890;&#36807;&#20081;&#24207;&#20256;&#36755;&#21644;&#20081;&#24207;&#30830;&#35748;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#31995;&#32479;&#34987;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#21644;&#36793;&#32536;&#33410;&#28857;&#20013;&#27169;&#22411;&#35757;&#32451;&#30340;&#36895;&#24230;&#12290;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#36890;&#20449;&#26550;&#26500;&#24120;&#34987;&#37319;&#29992;&#65292;&#20294;&#30001;&#20110;&#22810;&#23545;&#19968;&#30340;&#8220;incast&#8221;&#27969;&#37327;&#27169;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#38271;&#23614;&#24310;&#36831;&#65292;&#23545;&#35757;&#32451;&#21534;&#21520;&#37327;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#8220;&#23481;&#24525;&#20002;&#22833;&#20256;&#36755;&#21327;&#35758;&#8221;&#65288;LTP&#65289;&#65292;&#23427;&#20801;&#35768;&#22312;&#21516;&#27493;&#36807;&#31243;&#20013;&#37096;&#20998;&#20002;&#22833;&#26799;&#24230;&#65292;&#20197;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#37325;&#20256;&#65292;&#24182;&#25552;&#39640;&#27599;&#27425;&#36845;&#20195;&#30340;&#21516;&#27493;&#36895;&#24230;&#12290;LTP&#36890;&#36807;&#8220;&#20081;&#24207;&#20256;&#36755;&#8221;&#65288;out-of-order transmission&#65289;&#21644;&#8220;&#20081;&#24207;&#30830;&#35748;&#8221;&#65288;out-of-order ACKs&#65289;&#26469;&#23454;&#29616;&#23481;&#24525;&#20002;&#22833;&#30340;&#20256;&#36755;&#12290;LTP&#21033;&#29992;&#8220;&#25552;&#21069;&#20851;&#38381;&#8221;&#65288;Early Close&#65289;&#26681;&#25454;&#32593;&#32476;&#26465;&#20214;&#35843;&#25972;&#23481;&#24525;&#20002;&#22833;&#30340;&#38408;&#20540;&#65292;&#24182;&#20351;&#29992;&#8220;&#22635;&#20805;&#27668;&#27873;&#8221;&#65288;Bubble Filling&#65289;&#36827;&#34892;&#25968;&#25454;&#26657;&#27491;&#20197;&#20445;&#25345;&#35757;&#32451;&#31934;&#24230;&#12290;LTP&#30001;C++&#23454;&#29616;&#24182;&#38598;&#25104;&#21040;PyTorch&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30001;8&#20010;&#24037;&#20316;&#33410;&#28857;&#32452;&#25104;&#30340;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Machine Learning (DML) systems are utilized to enhance the speed of model training in data centers (DCs) and edge nodes. The Parameter Server (PS) communication architecture is commonly employed, but it faces severe long-tail latency caused by many-to-one "incast" traffic patterns, negatively impacting training throughput. To address this challenge, we design the \textbf{L}oss-tolerant \textbf{T}ransmission \textbf{P}rotocol (LTP), which permits partial loss of gradients during synchronization to avoid unneeded retransmission and contributes to faster synchronization per iteration. LTP implements loss-tolerant transmission through \textit{out-of-order transmission} and \textit{out-of-order Acknowledges (ACKs)}. LTP employs \textit{Early Close} to adjust the loss-tolerant threshold based on network conditions and \textit{Bubble Filling} for data correction to maintain training accuracy. LTP is implemented by C++ and integrated into PyTorch. Evaluations on a testbed of 8 work
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#24456;&#23569;&#30340;&#26679;&#26412;&#19988;&#33021;&#22815;&#23545;&#26435;&#37325;&#21644;&#22343;&#20540;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.04127</link><description>&lt;p&gt;
&#20351;&#29992;&#25130;&#26029;&#25968;&#25454;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Mixtures of Gaussians with Censored Data. (arXiv:2305.04127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#24456;&#23569;&#30340;&#26679;&#26412;&#19988;&#33021;&#22815;&#23545;&#26435;&#37325;&#21644;&#22343;&#20540;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#25130;&#26029;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#21363;&#20174;&#19968;&#20010;&#28151;&#21512;&#21333;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2)$&#20013;&#35266;&#27979;&#21040;&#30340;&#26679;&#26412;&#21482;&#26377;&#24403;&#20854;&#20301;&#20110;$S$&#38598;&#21512;&#20869;&#26102;&#25165;&#20250;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20165;&#38656;&#35201;$\frac{1}{\varepsilon^{O(k)}}$&#20010;&#26679;&#26412;&#21363;&#21487;&#22312;$\varepsilon$&#35823;&#24046;&#20869;&#20272;&#35745;&#26435;&#37325;$w_i$&#21644;&#22343;&#20540;$\mu_i$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning mixtures of Gaussians with censored data. Statistical learning with censored data is a classical problem, with numerous practical applications, however, finite-sample guarantees for even simple latent variable models such as Gaussian mixtures are missing. Formally, we are given censored data from a mixture of univariate Gaussians $$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2),$$ i.e. the sample is observed only if it lies inside a set $S$. The goal is to learn the weights $w_i$ and the means $\mu_i$. We propose an algorithm that takes only $\frac{1}{\varepsilon^{O(k)}}$ samples to estimate the weights $w_i$ and the means $\mu_i$ within $\varepsilon$ error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02506</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#35299;&#23494;&#24230;&#30340;&#23383;&#31526;&#20018;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#25193;&#23637;&#23450;&#21521;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#37117;&#23450;&#20041;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#26377;&#20851;&#27010;&#29575;&#26144;&#23556;&#30340;&#39532;&#23572;&#21487;&#22827;&#33539;&#30068;&#30340;&#24037;&#20316;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33539;&#30068;&#65292;&#20854;&#24577;&#23556;&#23558;&#20998;&#21035;&#30001;&#27599;&#20010;&#26679;&#26412;&#31354;&#38388;&#20998;&#35299;&#30340;&#32852;&#21512;&#23494;&#24230;&#19982;&#20174;&#26679;&#26412;&#21040;&#36820;&#22238;&#20540;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#32452;&#21512;&#12290;&#36825;&#26159;&#36808;&#21521;&#26368;&#36817;&#30340;&#33539;&#30068;&#35770;&#27010;&#29575;&#27979;&#24230;&#25551;&#36848;&#21644;&#36890;&#24120;&#22312;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#20998;&#35299;&#23494;&#24230;&#30340;&#25805;&#20316;&#23450;&#20041;&#20043;&#38388;&#30340;&#32553;&#23567;&#24046;&#36317;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10226</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10226
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#21378;&#21830;&#30340;&#22270;&#20687;&#39118;&#26684;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26679;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#21378;&#21830;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.07645</link><description>&lt;p&gt;
&#38024;&#23545;&#31283;&#23450;&#30340;&#36229;&#32593;&#32476;&#23398;&#20064;&#30340;&#38750;&#27604;&#20363;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Proportional Parametrizations for Stable Hypernetwork Learning. (arXiv:2304.07645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#27604;&#38750;&#36229;&#32593;&#32476;&#27169;&#22411;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#19982;&#20351;&#29992;&#24120;&#35265;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;&#21644;&#21021;&#22987;&#21270;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#26377;&#20851;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#25968;&#20540;&#38382;&#39064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#29978;&#33267;&#38459;&#27490;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24402;&#19968;&#21270;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#35746;&#30340;&#36229;&#32593;&#32476;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#36229;&#32593;&#32476;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#21487;&#20197;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks are neural networks that generate the parameters of another neural network. In many scenarios, current hypernetwork training strategies are unstable, and convergence is often far slower than for non-hypernetwork models. We show that this problem is linked to an issue that arises when using common choices of hypernetwork architecture and initialization. We demonstrate analytically and experimentally how this numerical issue can lead to an instability during training that slows, and sometimes even prevents, convergence. We also demonstrate that popular deep learning normalization strategies fail to address these issues. We then propose a solution to the problem based on a revised hypernetwork formulation that uses non-proportional additive parametrizations. We test the proposed reparametrization on several tasks, and demonstrate that it consistently leads to more stable training, achieving faster convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#32553;&#25918;&#22240;&#23376;&#24555;&#36895;&#29983;&#25104; Pareto &#21069;&#27839;&#65292;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05448</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#32553;&#25918;&#30340;&#20998;&#27573;&#22270;&#20687;&#26041;&#27861;&#30340;&#23454;&#29616;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amortized Learning of Dynamic Feature Scaling for Image Segmentation. (arXiv:2304.05448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#32553;&#25918;&#22240;&#23376;&#24555;&#36895;&#29983;&#25104; Pareto &#21069;&#27839;&#65292;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#21106;&#26550;&#26500;&#36890;&#36807;&#22266;&#23450;&#30340;&#22240;&#23376;&#23558;&#31354;&#38388;&#32500;&#24230;&#35843;&#25972;&#20026;&#20108;&#26469;&#32858;&#21512;&#31354;&#38388;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#25552;&#39640;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20854;&#20182;&#35843;&#25972;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#35843;&#25972;&#22240;&#23376;&#36890;&#24120;&#38656;&#35201;&#20026;&#35768;&#22810;&#19981;&#21516;&#30340;&#22240;&#23376;&#35757;&#32451;&#21333;&#29420;&#30340;&#32593;&#32476;&#65292;&#24182;&#27604;&#36739;&#27599;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#33655;&#24847;&#21619;&#30528;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#36825;&#26679;&#20570;&#65292;&#32780;&#19988;&#21482;&#32771;&#34385;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#29992;&#26469;&#36731;&#26494;&#24555;&#36895;&#22320;&#29983;&#25104;&#22312;&#35843;&#25972;&#22240;&#23376;&#21464;&#21270;&#26102;&#65292;&#22312;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340; Pareto &#21069;&#27839;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#36229;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#20110;&#35843;&#25972;&#22240;&#23376;&#30340; CNN &#21442;&#25968;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#36873;&#25321;&#20182;&#20204;&#30340;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27604;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNN) have become the predominant model for image segmentation tasks. Most CNN segmentation architectures resize spatial dimensions by a fixed factor of two to aggregate spatial context. Recent work has explored using other resizing factors to improve model accuracy for specific applications. However, finding the appropriate rescaling factor most often involves training a separate network for many different factors and comparing the performance of each model. The computational burden of these models means that in practice it is rarely done, and when done only a few different scaling factors are considered.  In this work, we present a hypernetwork strategy that can be used to easily and rapidly generate the Pareto frontier for the trade-off between accuracy and efficiency as the rescaling factor varies. We show how to train a single hypernetwork that generates CNN parameters conditioned on a rescaling factor. This enables a user to quickly choose a rescalin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item><item><title>PeakNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#65292;&#23427;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#26469;&#36866;&#24212;&#36880;&#21457;&#24378;&#32972;&#26223;&#25955;&#23556;&#30340;&#27874;&#21160;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#35823;&#25253;&#23792;&#28857;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.15301</link><description>&lt;p&gt;
PeakNet&#65306;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;
&lt;/p&gt;
&lt;p&gt;
PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks. (arXiv:2303.15301v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15301
&lt;/p&gt;
&lt;p&gt;
PeakNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#65292;&#23427;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#26469;&#36866;&#24212;&#36880;&#21457;&#24378;&#32972;&#26223;&#25955;&#23556;&#30340;&#27874;&#21160;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#35823;&#25253;&#23792;&#28857;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;X&#23556;&#32447;&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#22120;&#65288;XFEL&#65289;&#21644;&#21516;&#27493;&#36752;&#23556;&#35774;&#26045;&#20013;&#30340;&#20018;&#34892;&#26230;&#20307;&#23398;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22823;&#20998;&#23376;&#32467;&#26500;&#21644;&#20998;&#23376;&#36807;&#31243;&#36827;&#34892;&#26032;&#39062;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32473;&#25968;&#25454;&#20943;&#23569;&#21644;&#23454;&#26102;&#21453;&#39304;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;Bragg&#23792;&#28857;&#23547;&#25214;&#31639;&#27861;&#29992;&#20110;&#35782;&#21035;&#26377;&#29992;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#21629;&#20013;&#29575;&#21644;&#20998;&#36776;&#29575;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;&#26469;&#33258;&#32531;&#20914;&#28342;&#28082;&#12289;&#27880;&#23556;&#21943;&#22068;&#21644;&#20854;&#20182;&#23631;&#34109;&#26448;&#26009;&#30340;&#36880;&#21457;&#24378;&#24230;&#27874;&#21160;&#21644;&#24378;&#32972;&#26223;&#25955;&#23556;&#20351;&#24471;&#36825;&#25104;&#20026;&#19968;&#20010;&#32791;&#26102;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeakNet&#65292;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serial crystallography at X-ray free electron laser (XFEL) and synchrotron facilities has experienced tremendous progress in recent times enabling novel scientific investigations into macromolecular structures and molecular processes. However, these experiments generate a significant amount of data posing computational challenges in data reduction and real-time feedback. Bragg peak finding algorithm is used to identify useful images and also provide real-time feedback about hit-rate and resolution. Shot-to-shot intensity fluctuations and strong background scattering from buffer solution, injection nozzle and other shielding materials make this a time-consuming optimization problem. Here, we present PeakNet, an autonomous Bragg peak finder that utilizes deep neural networks. The development of this system 1) eliminates the need for manual algorithm parameter tuning, 2) reduces false-positive peaks by adjusting to shot-to-shot variations in strong background scattering in real-time, 3) e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20803;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;gamma&#32593;&#32476;&#21644;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15057</link><description>&lt;p&gt;
Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20803;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;gamma&#32593;&#32476;&#21644;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#23384;&#22312;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#27010;&#29575;&#19982;&#30495;&#23454;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#26657;&#20934;&#27169;&#22411;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#21516;&#26102;&#20248;&#21270;&#26657;&#20934;&#35823;&#24046;&#30340;&#20195;&#29702;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#20803;&#26657;&#20934;&#65288;MC&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#26657;&#20934;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#25193;&#23637;&#20102;MC&#65306;&#65288;1&#65289;gamma&#32593;&#32476;&#65288;gamma-net&#65289;&#65292;&#19968;&#20010;&#20803;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#31354;&#38388;&#20026;&#35843;&#20248;&#39592;&#24178;&#32593;&#32476;&#30340;focal loss&#23398;&#20064;&#36880;&#26679;&#26412;gamma&#65307;&#65288;2&#65289;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65288;SECE&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#26680;&#30340;&#26080;&#20559;&#21644;&#21487;&#24494;&#30340;ECE&#65292;&#26088;&#22312;&#24179;&#28369;&#35843;&#20248;gamma-net&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#22320;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#65288;a&#65289;&#22312;&#36830;&#32493;&#31354;&#38388;&#23398;&#20064;&#36880;&#26679;&#26412;gamma&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#39592;&#24178;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration-the mismatch between predicted probability and the true correctness likelihood-has been frequently identified in modern deep neural networks. Recent work in the field aims to address this problem by training calibrated models directly by optimizing a proxy of the calibration error alongside the conventional objective. Recently, Meta-Calibration (MC) showed the effectiveness of using meta-learning for learning better calibrated models. In this work, we extend MC with two main components: (1) gamma network (gamma-net), a meta network to learn a sample-wise gamma at a continuous space for focal loss for optimizing backbone network; (2) smooth expected calibration error (SECE), a Gaussian-kernel based unbiased and differentiable ECE which aims to smoothly optimizing gamma-net. The proposed method regularizes neural network towards better calibration meanwhile retain predictive performance. Our experiments show that (a) learning sample-wise gamma at continuous space can effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01170</link><description>&lt;p&gt;
&#26080;&#19987;&#23478;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#19987;&#23478;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#26032;&#25163;&#26234;&#33021;&#20307;&#26469;&#35299;&#20915;&#35757;&#32451;&#38382;&#39064;&#65292;&#22914;&#25506;&#32034;&#25104;&#26412;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36801;&#31227;&#38656;&#35201;&#26032;&#25163;&#26234;&#33021;&#20307;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#25165;&#33021;&#26377;&#25928;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#19987;&#23478;&#22312;&#32447;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;EF-OnTL&#65289;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#27599;&#19968;&#27425;&#36801;&#31227;&#27493;&#39588;&#20013;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#26469;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARS-RND&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#23545;RND&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#12289;&#34892;&#21160;&#12289;&#22870;&#21169;&#21644;&#19979;&#19968;&#29366;&#24577;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.00515</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#21487;&#35299;&#37322;&#27700;&#20301;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27721;&#27743;&#27700;&#20301;&#23545;&#20110;&#20132;&#36890;&#25511;&#21046;&#21644;&#36991;&#20813;&#33258;&#28982;&#28798;&#23475;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#28041;&#21450;&#22810;&#31181;&#21464;&#37327;&#24182;&#30456;&#20114;&#22797;&#26434;&#22320;&#32852;&#31995;&#30528;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#36716;&#25442;&#22120;&#65292;&#21033;&#29992;&#21464;&#37327;&#20808;&#21069;&#30693;&#35782;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#65292;&#39044;&#27979;&#27721;&#27743;&#27982;&#24030;&#26725;&#30340;&#27700;&#20301;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#21040;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#24182;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#12290;&#20973;&#20511;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#33719;&#24471;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;2016&#24180;&#33267;2021&#24180;&#30340;&#27721;&#27743;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#35299;&#20915;convex-concave Lipschitz&#38543;&#26426;Saddle Point&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28385;&#36275;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#36895;&#29575;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.12909</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20339;&#36895;&#29575;&#30340;&#20855;&#26377;&#24378;&#38388;&#38553;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;Saddle Point&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap. (arXiv:2302.12909v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#35299;&#20915;convex-concave Lipschitz&#38543;&#26426;Saddle Point&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28385;&#36275;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#36895;&#29575;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;$(\epsilon,\delta)$-&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#20984;&#20985;Lipschitz&#38543;&#26426;Saddle Point&#38382;&#39064;&#65288;&#20063;&#31216;&#20026;&#38543;&#26426;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65289;&#21487;&#20197;&#34987;&#35299;&#20915;&#65292;&#20854;&#20855;&#26377;&#24378;&#65288;&#21407;&#22987;-&#23545;&#20598;&#65289;&#38388;&#38553;&#29575;&#20026;$\tilde O\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$&#65292;&#20854;&#20013;$n$&#20026;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;$d$&#20026;&#38382;&#39064;&#32500;&#24230;&#12290;&#26681;&#25454;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20248;&#21270;&#30340;&#19979;&#30028;&#65292;&#35813;&#36895;&#29575;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#24182;&#20998;&#26512;&#36866;&#29992;&#20110;Saddle Point&#38382;&#39064;&#30340;&#36882;&#24402;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#24378;&#38388;&#38553;&#30340;&#32039;&#23494;&#19978;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#36895;&#29575;&#21487;&#20197;&#22312;$O\big(\min\big\{\frac{n^2\epsilon^{1.5}}{\sqrt{d}}, n^{3/2}\big\}\big)$&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#20197;&#21450;&#22312;&#25439;&#22833;&#20989;&#25968;&#20809;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;$\tilde{O}(n)$&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#32473;&#23450;&#40657;&#30418;&#35775;&#38382;&#19968;&#20010;&#28385;&#36275;&#26465;&#20214;&#30340;&#23376;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that convex-concave Lipschitz stochastic saddle point problems (also known as stochastic minimax optimization) can be solved under the constraint of $(\epsilon,\delta)$-differential privacy with \emph{strong (primal-dual) gap} rate of $\tilde O\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$, where $n$ is the dataset size and $d$ is the dimension of the problem. This rate is nearly optimal, based on existing lower bounds in differentially private stochastic optimization. Specifically, we prove a tight upper bound on the strong gap via novel implementation and analysis of the recursive regularization technique repurposed for saddle point problems. We show that this rate can be attained with $O\big(\min\big\{\frac{n^2\epsilon^{1.5}}{\sqrt{d}}, n^{3/2}\big\}\big)$ gradient complexity, and $\tilde{O}(n)$ gradient complexity if the loss function is smooth. As a byproduct of our method, we develop a general algorithm that, given a black-box access to a subroutine satisfying
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27169;&#22411;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#65292;&#21019;&#24314;&#21305;&#37197;&#32452;&#24182;&#20272;&#35745;&#27835;&#30103;&#25928;&#24212;&#65292;&#23454;&#29616;&#20102;&#21487;&#23457;&#26680;&#12289;&#26131;&#25490;&#26597;&#12289;&#20934;&#30830;&#20272;&#35745;&#21644;&#21487;&#25193;&#23637;&#30340;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21464;&#37327;&#37325;&#35201;&#24615;&#27979;&#37327;&#26500;&#24314;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;LASSO&#25805;&#20316;&#21270;&#23454;&#26045;&#65292;&#22312;&#19981;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22312;&#28508;&#22312;&#28151;&#28102;&#21464;&#37327;&#25968;&#37327;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#21450;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11715</link><description>&lt;p&gt;
&#21487;&#21464;&#37325;&#35201;&#24615;&#21305;&#37197;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variable Importance Matching for Causal Inference. (arXiv:2302.11715v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27169;&#22411;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#65292;&#21019;&#24314;&#21305;&#37197;&#32452;&#24182;&#20272;&#35745;&#27835;&#30103;&#25928;&#24212;&#65292;&#23454;&#29616;&#20102;&#21487;&#23457;&#26680;&#12289;&#26131;&#25490;&#26597;&#12289;&#20934;&#30830;&#20272;&#35745;&#21644;&#21487;&#25193;&#23637;&#30340;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21464;&#37327;&#37325;&#35201;&#24615;&#27979;&#37327;&#26500;&#24314;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;LASSO&#25805;&#20316;&#21270;&#23454;&#26045;&#65292;&#22312;&#19981;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22312;&#28508;&#22312;&#28151;&#28102;&#21464;&#37327;&#25968;&#37327;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#21450;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#21487;&#23457;&#26680;&#12289;&#26131;&#20110;&#25490;&#26597;&#12289;&#20934;&#30830;&#20272;&#35745;&#27835;&#30103;&#25928;&#24212;&#24182;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#30340;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#27169;&#22411;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20197;&#19979;&#27493;&#39588;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65306;&#65288;i&#65289;&#36890;&#36807;&#32467;&#26524;&#24314;&#27169;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#36317;&#31163;&#24230;&#37327;&#21019;&#24314;&#21305;&#37197;&#32452;&#65292;&#65288;iii&#65289;&#20351;&#29992;&#21305;&#37197;&#32452;&#20272;&#35745;&#27835;&#30103;&#25928;&#24212;&#12290;&#27169;&#22411;&#21305;&#37197;&#20351;&#29992;&#21464;&#37327;&#37325;&#35201;&#24615;&#27979;&#37327;&#26469;&#26500;&#24314;&#36317;&#31163;&#24230;&#37327;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#20851;&#27880;&#38382;&#39064;&#22312;&#28508;&#22312;&#28151;&#28102;&#21464;&#37327;&#25968;&#37327;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#29992;LASSO&#23558;&#27169;&#22411;&#21305;&#37197;&#26694;&#26550;&#25805;&#20316;&#21270;&#12290;&#25105;&#20204;&#22312;LASSO&#32467;&#26524;&#24314;&#27169;&#19968;&#33268;&#22320;&#35782;&#21035;&#20986;&#25152;&#26377;&#28151;&#28102;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#24615;&#33021;&#20445;&#35777;&#65288;&#37325;&#35201;&#30340;&#26159;&#19981;&#35201;&#27714;&#32447;&#24615;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#25193;&#23637;&#24615;&#19982;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to produce methods for observational causal inference that are auditable, easy to troubleshoot, accurate for treatment effect estimation, and scalable to high-dimensional data. We describe a general framework called Model-to-Match that achieves these goals by (i) learning a distance metric via outcome modeling, (ii) creating matched groups using the distance metric, and (iii) using the matched groups to estimate treatment effects. Model-to-Match uses variable importance measurements to construct a distance metric, making it a flexible framework that can be adapted to various applications. Concentrating on the scalability of the problem in the number of potential confounders, we operationalize the Model-to-Match framework with LASSO. We derive performance guarantees for settings where LASSO outcome modeling consistently identifies all confounders (importantly without requiring the linear model to be correctly specified). We also provide experimental results demonstrating the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03684</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#20013;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporal Robustness against Data Poisoning. (arXiv:2302.03684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#32771;&#34385;&#20102;&#36890;&#36807;&#24694;&#24847;&#35757;&#32451;&#25968;&#25454;&#25805;&#32437;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#27745;&#26579;&#23041;&#32961;&#27169;&#22411;&#37117;&#22260;&#32469;&#30528;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65292;&#21363;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#20197;&#21487;&#25215;&#21463;&#30340;&#20195;&#20215;&#27745;&#26579;&#27604;&#39044;&#26399;&#26356;&#22810;&#30340;&#26679;&#26412;&#65292;&#23601;&#20687;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19968;&#26679;&#65292;&#20182;&#20204;&#21487;&#33021;&#33021;&#22815;&#22312;&#24456;&#30701;&#30340;&#26102;&#38388;&#20869;&#20351;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#22833;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#30340;&#20986;&#29983;&#26085;&#26399;&#26102;&#38388;&#25139;&#65292;&#36825;&#20123;&#26102;&#38388;&#25139;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#20294;&#36807;&#21435;&#34987;&#24573;&#30053;&#12290;&#21033;&#29992;&#36825;&#20123;&#26102;&#38388;&#25139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#26032;&#22411;&#25351;&#26631;&#65288;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#65289;&#30340;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#20998;&#21035;&#34913;&#37327;&#25915;&#20987;&#25552;&#21069;&#24320;&#22987;&#30340;&#26102;&#38388;&#21644;&#25915;&#20987;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#20063;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a 
&lt;/p&gt;</description></item><item><title>KDEformer&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#21152;&#36895;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#22312;&#23454;&#35777;&#39564;&#35777;&#20013;&#26174;&#31034;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02451</link><description>&lt;p&gt;
KDEformer: &#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#21152;&#36895;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
KDEformer: Accelerating Transformers via Kernel Density Estimation. (arXiv:2302.02451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02451
&lt;/p&gt;
&lt;p&gt;
KDEformer&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#21152;&#36895;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#22312;&#23454;&#35777;&#39564;&#35777;&#20013;&#26174;&#31034;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#31215;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#29616;&#20195;&#28145;&#24230;&#20307;&#31995;&#32467;&#26500;&#65288;&#20363;&#22914;Transformer&#65289;&#20013;&#23545;&#20110;&#24207;&#21015;&#24314;&#27169;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#65292;&#23545;&#35813;&#27169;&#22411;&#30340;&#26420;&#32032;&#31934;&#30830;&#35745;&#31639;&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#20855;&#26377;&#20108;&#27425;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#22312;&#35757;&#32451;&#38271;&#24207;&#21015;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#38459;&#30861;&#12290;&#20851;&#38190;&#29942;&#39048;&#26159;&#22240;&#20026;&#22312;softmax&#20989;&#25968;&#30340;&#20998;&#27597;&#20013;&#35745;&#31639;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#22312;softmax&#30697;&#38453;&#19982;&#20540;&#30697;&#38453;&#20043;&#38388;&#30340;&#20056;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#21069;&#32773;&#21487;&#20197;&#34987;&#31616;&#21270;&#20026;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#32780;&#39640;&#25928;&#30340;KDE&#27714;&#35299;&#22120;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#24555;&#36895;&#30697;&#38453;&#20056;&#31215;&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#21518;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;KDEformer&#21487;&#20197;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#36817;&#20284;&#35745;&#31639;&#27880;&#24847;&#21147;&#65292;&#24182;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#35889;&#33539;&#25968;&#30028;&#38480;&#65292;&#32780;&#20043;&#21069;&#30340;&#32467;&#26524;&#21482;&#25552;&#20379;&#36880;&#20010;&#20803;&#32032;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;KDEformer&#22312;&#20934;&#30830;&#24615;&#65292;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27880;&#24847;&#21147;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na\"ive exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;</title><link>http://arxiv.org/abs/2302.00049</link><description>&lt;p&gt;
Transformers&#36935;&#35265;&#26377;&#21521;&#22270;
&lt;/p&gt;
&lt;p&gt;
Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#25991;&#26412;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20294;&#29616;&#22312;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#26080;&#21521;&#22270;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#26377;&#21521;&#22270;&#30340;transformers&#21364;&#26159;&#19968;&#20010;&#24847;&#22806;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20027;&#39064;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21253;&#25324;&#28304;&#20195;&#30721;&#21644;&#36923;&#36753;&#30005;&#36335;&#22312;&#20869;&#30340;&#26222;&#36941;&#39046;&#22495;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#65288;1&#65289;&#30913;&#22330;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327; - &#26159;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#26041;&#21521;&#24863;&#30693;&#25512;&#24191;&#65307;&#65288;2&#65289;&#26041;&#21521;&#38543;&#26426;&#28216;&#36208;&#32534;&#30721;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#30340;&#26041;&#21521;&#20449;&#24687;&#22312;&#21253;&#25324;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#21512;&#25968;&#25454;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Open Graph Benchmark Code2&#19978;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#24037;&#31243;&#38382;&#39064;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#23545;&#32500;&#25252;&#38656;&#27714;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#29366;&#24577;&#30340;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#22312;&#37096;&#32626;&#21518;&#19981;&#26029;&#35843;&#25972;&#27169;&#22411;&#20197;&#36866;&#24212;&#28436;&#21464;&#22330;&#26223;&#30340;&#26041;&#26696;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.12467</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20013;&#30340;&#24212;&#29992;: &#27010;&#36848;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Predictive Maintenance: Overview and Challenges. (arXiv:2301.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12467
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#24037;&#31243;&#38382;&#39064;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#23545;&#32500;&#25252;&#38656;&#27714;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#29366;&#24577;&#30340;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#22312;&#37096;&#32626;&#21518;&#19981;&#26029;&#35843;&#25972;&#27169;&#22411;&#20197;&#36866;&#24212;&#28436;&#21464;&#22330;&#26223;&#30340;&#26041;&#26696;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#20026;&#26377;&#25928;&#35299;&#20915;&#24037;&#31243;&#38382;&#39064;&#30340;&#20027;&#35201;&#25512;&#21160;&#21147;&#20043;&#19968;&#12290;&#20363;&#22914;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#25913;&#36827;&#23545;&#19981;&#21516;&#26426;&#22120;&#21644;&#25805;&#20316;&#29615;&#22659;&#20013;&#32500;&#25252;&#38656;&#27714;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#21453;&#26144;&#24403;&#21069;&#38382;&#39064;&#29366;&#24577;&#30340;&#22266;&#23450;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;&#20869;&#37096;&#25110;&#22806;&#37096;&#22240;&#32032;&#65292;&#38382;&#39064;&#29366;&#24577;&#21487;&#33021;&#20250;&#21457;&#29983;&#25913;&#21464;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#27867;&#21270;&#21644;&#36866;&#24212;&#24615;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#19982;&#36825;&#31181;&#22266;&#23450;&#35757;&#32451;&#38598;&#30456;&#21453;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#21464;&#21270;&#20854;&#29615;&#22659;&#65292;&#36825;&#23601;&#38656;&#35201;&#22312;&#37096;&#32626;&#21518;&#19981;&#26029;&#35843;&#25972;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#28436;&#21464;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#22312;&#37096;&#32626;&#21518;&#19981;&#26029;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#24182;&#34701;&#20837;&#26032;&#30693;&#35782;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have become one of the main propellers for solving engineering problems effectively and efficiently. For instance, Predictive Maintenance methods have been used to improve predictions of when maintenance is needed on different machines and operative contexts. However, deep learning methods are not without limitations, as these models are normally trained on a fixed distribution that only reflects the current state of the problem. Due to internal or external factors, the state of the problem can change, and the performance decreases due to the lack of generalization and adaptation. Contrary to this stationary training set, real-world applications change their environments constantly, creating the need to constantly adapt the model to evolving scenarios. To aid in this endeavor, Continual Learning methods propose ways to constantly adapt prediction models and incorporate new knowledge after deployment. Despite the advantages of these techniques, there are still c
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;SWARM&#24182;&#34892;&#24615;&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#27169;&#22411;&#30340;&#27169;&#22411;&#24182;&#34892;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#36830;&#25509;&#24046;&#12289;&#24322;&#26500;&#21644;&#19981;&#21487;&#38752;&#35774;&#22791;&#12290;&#36890;&#36807;&#22312;&#33410;&#28857;&#20043;&#38388;&#21019;&#24314;&#20020;&#26102;&#30340;&#38543;&#26426;&#21270;&#31649;&#36947;&#24182;&#36827;&#34892;&#37325;&#26032;&#24179;&#34913;&#65292;SWARM&#21487;&#20197;&#23454;&#29616;&#26356;&#23569;&#30340;&#36890;&#20449;&#23494;&#38598;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#21387;&#32553;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#26469;&#35757;&#32451;&#22823;&#22411;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.11913</link><description>&lt;p&gt;
SWARM&#24182;&#34892;&#24615;: &#35757;&#32451;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#36890;&#20449;&#25928;&#29575;&#19978;&#26377;&#24778;&#20154;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (arXiv:2301.11913v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11913
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SWARM&#24182;&#34892;&#24615;&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#27169;&#22411;&#30340;&#27169;&#22411;&#24182;&#34892;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#36830;&#25509;&#24046;&#12289;&#24322;&#26500;&#21644;&#19981;&#21487;&#38752;&#35774;&#22791;&#12290;&#36890;&#36807;&#22312;&#33410;&#28857;&#20043;&#38388;&#21019;&#24314;&#20020;&#26102;&#30340;&#38543;&#26426;&#21270;&#31649;&#36947;&#24182;&#36827;&#34892;&#37325;&#26032;&#24179;&#34913;&#65292;SWARM&#21487;&#20197;&#23454;&#29616;&#26356;&#23569;&#30340;&#36890;&#20449;&#23494;&#38598;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#21387;&#32553;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#26469;&#35757;&#32451;&#22823;&#22411;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21463;&#30410;&#20110;&#20351;&#29992;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#27169;&#22411;&#12290;&#30001;&#20110;&#38656;&#35201;&#19987;&#29992;&#30340;HPC&#38598;&#32676;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#35757;&#32451;&#22823;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#20351;&#29992;&#24265;&#20215;&#30340;&#8220;&#21487;&#25250;&#21344;&#8221;&#23454;&#20363;&#25110;&#20174;&#22810;&#20010;&#21306;&#22495;&#27719;&#38598;&#29616;&#26377;&#36164;&#28304;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#26465;&#20214;&#19979;&#29616;&#26377;&#27169;&#22411;&#24182;&#34892;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25214;&#21040;&#20102;&#35757;&#32451;&#26356;&#22823;&#27169;&#22411;&#26102;&#36890;&#20449;&#23494;&#38598;&#24230;&#36739;&#20302;&#30340;&#37197;&#32622;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWARM&#24182;&#34892;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#25509;&#24046;&#12289;&#24322;&#26500;&#21644;&#19981;&#21487;&#38752;&#35774;&#22791;&#30340;&#27169;&#22411;&#24182;&#34892;&#35757;&#32451;&#31639;&#27861;&#12290;SWARM&#22312;&#33410;&#28857;&#20043;&#38388;&#21019;&#24314;&#20020;&#26102;&#30340;&#38543;&#26426;&#21270;&#31649;&#36947;&#65292;&#24182;&#22312;&#20986;&#29616;&#25925;&#38556;&#26102;&#36827;&#34892;&#37325;&#26032;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#23558;SWARM&#24182;&#34892;&#24615;&#19982;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#19982;&#21387;&#32553;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap "preemptible" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26041;&#27861;&#30340;&#25968;&#25454;&#39537;&#21160;&#32447;&#24615;&#22797;&#26434;&#24230;&#20302;&#31209;&#36924;&#36817;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#20219;&#24847;&#20998;&#24067;&#30340;&#30697;&#24418;&#26680;&#30697;&#38453;&#65292;&#21487;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2212.12674</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#19968;&#33324;&#26680;&#30697;&#38453;&#32447;&#24615;&#22797;&#26434;&#24230;&#20302;&#31209;&#36924;&#36817;&#65306;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Linear Complexity Low-Rank Approximation of General Kernel Matrices: A Geometric Approach. (arXiv:2212.12674v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26041;&#27861;&#30340;&#25968;&#25454;&#39537;&#21160;&#32447;&#24615;&#22797;&#26434;&#24230;&#20302;&#31209;&#36924;&#36817;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#20219;&#24847;&#20998;&#24067;&#30340;&#30697;&#24418;&#26680;&#30697;&#38453;&#65292;&#21487;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#30697;&#24418;&#26680;&#30697;&#38453;&#21487;&#20197;&#23450;&#20041;&#20026; $K_{ij} = \kappa(x_i,y_j)$&#65292;&#20854;&#20013; $\kappa(x,y)$ &#26159;&#19968;&#20010;&#26680;&#20989;&#25968;&#65292;$X=\{x_i\}_{i=1}^m$ &#21644; $Y=\{y_i\}_{i=1}^n$ &#26159;&#20004;&#32452;&#28857;&#38598;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#19968;&#20010;&#26680;&#30697;&#38453;&#30340;&#20302;&#31209;&#36924;&#36817;&#65292;&#20854;&#20013;&#28857;&#38598; $X$ &#21644; $Y$ &#26159;&#22823;&#35268;&#27169;&#32780;&#20219;&#24847;&#20998;&#24067;&#30340;&#65292;&#27604;&#22914;&#30456;&#36317;&#36828;&#31163;&#12289;&#20132;&#38169;&#20998;&#24067;&#12289;&#30456;&#21516;&#31561;&#31561;&#12290;&#36825;&#26679;&#30340;&#30697;&#24418;&#26680;&#30697;&#38453;&#21487;&#33021;&#20986;&#29616;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#65292;&#20854;&#20013; $X$ &#23545;&#24212;&#35757;&#32451;&#25968;&#25454;&#65292;$Y$ &#23545;&#24212;&#27979;&#35797;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28857;&#38598;&#36890;&#24120;&#26159;&#39640;&#32500;&#30340;&#12290;&#30001;&#20110;&#28857;&#38598;&#24456;&#22823;&#65292;&#25105;&#20204;&#24517;&#39035;&#21033;&#29992;&#30697;&#38453;&#26469;&#33258;&#20110;&#26680;&#20989;&#25968;&#30340;&#20107;&#23454;&#65292;&#24182;&#36991;&#20813;&#24418;&#25104;&#30697;&#38453;&#65292;&#20174;&#32780;&#25490;&#38500;&#20102;&#22823;&#22810;&#25968;&#20195;&#25968;&#25216;&#26415;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23547;&#27714;&#33021;&#22815;&#20197;&#22266;&#23450;&#36924;&#36817;&#31209;&#20026;&#20195;&#20215;&#32447;&#24615;&#25110;&#36817;&#20046;&#32447;&#24615;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#20960;&#20309;&#26041;&#27861;&#26469;&#36817;&#20284;&#32447;&#24615;&#25110;&#36817;&#20046;&#32447;&#24615;&#22320;&#34920;&#31034;&#30697;&#38453;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general, {\em rectangular} kernel matrix may be defined as $K_{ij} = \kappa(x_i,y_j)$ where $\kappa(x,y)$ is a kernel function and where $X=\{x_i\}_{i=1}^m$ and $Y=\{y_i\}_{i=1}^n$ are two sets of points. In this paper, we seek a low-rank approximation to a kernel matrix where the sets of points $X$ and $Y$ are large and are arbitrarily distributed, such as away from each other, ``intermingled'', identical, etc. Such rectangular kernel matrices may arise, for example, in Gaussian process regression where $X$ corresponds to the training data and $Y$ corresponds to the test data. In this case, the points are often high-dimensional. Since the point sets are large, we must exploit the fact that the matrix arises from a kernel function, and avoid forming the matrix, and thus ruling out most algebraic techniques. In particular, we seek methods that can scale linearly or nearly linear with respect to the size of data for a fixed approximation rank. The main idea in this paper is to {\em geo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;XAI&#30340;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#65306;&#22312;&#22622;&#20869;&#21152;&#23572;&#30340;&#31227;&#21160;&#30005;&#35805;&#25968;&#25454;&#19978;&#22522;&#20110;ML&#27169;&#22411;&#36827;&#34892;&#30005;&#27668;&#21270;&#29575;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#20154;&#21475;&#23494;&#24230;&#20559;&#35265;&#65292;&#24182;&#25351;&#20986;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#37322;&#30340;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.06277</link><description>&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;: &#20174;&#22622;&#20869;&#21152;&#23572;&#30340;&#31227;&#21160;&#30005;&#35805;&#25968;&#25454;&#20013;&#20272;&#35745;&#30005;&#27668;&#21270;&#29575;
&lt;/p&gt;
&lt;p&gt;
Explainability in Practice: Estimating Electrification Rates from Mobile Phone Data in Senegal. (arXiv:2211.06277v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;XAI&#30340;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#65306;&#22312;&#22622;&#20869;&#21152;&#23572;&#30340;&#31227;&#21160;&#30005;&#35805;&#25968;&#25454;&#19978;&#22522;&#20110;ML&#27169;&#22411;&#36827;&#34892;&#30005;&#27668;&#21270;&#29575;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#20154;&#21475;&#23494;&#24230;&#20559;&#35265;&#65292;&#24182;&#25351;&#20986;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#37322;&#30340;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20026;&#19981;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#25216;&#26415;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XAI&#30340;&#19968;&#20010;&#29992;&#20363;&#65306;&#19968;&#20010;&#20351;&#29992;&#22622;&#20869;&#21152;&#23572;&#30340;&#31227;&#21160;&#30005;&#35805;&#25968;&#25454;&#35757;&#32451;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#30005;&#27668;&#21270;&#29575;&#12290;&#36825;&#20123;&#25968;&#25454;&#28304;&#33258;2014/15&#24180;&#27225;&#23376;&#20844;&#21496;&#30340;&#25968;&#25454;&#21457;&#23637;&#25361;&#25112;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#20004;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#21487;&#20197;&#34987;&#39564;&#35777;&#65292;&#20294;&#22312;&#20154;&#21475;&#23494;&#24230;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#25351;&#20986;&#25105;&#20204;&#22312;&#24037;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#26469;&#24635;&#32467;&#26412;&#25991;&#65306;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#22411;&#35774;&#35745;&#21487;&#33021;&#21463;&#21040;&#30446;&#21069;&#21487;&#29992;&#30340;XAI&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#35299;&#37322;&#30340;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) provides explanations for not interpretable machine learning (ML) models. While many technical approaches exist, there is a lack of validation of these techniques on real-world datasets. In this work, we present a use-case of XAI: an ML model which is trained to estimate electrification rates based on mobile phone data in Senegal. The data originate from the Data for Development challenge by Orange in 2014/15. We apply two model-agnostic, local explanation techniques and find that while the model can be verified, it is biased with respect to the population density. We conclude our paper by pointing to the two main challenges we encountered during our work: data processing and model design that might be restricted by currently available XAI methods, and the importance of domain knowledge to interpret explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#65292;&#21516;&#26102;&#37319;&#29992;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#65288;WNR&#65289;&#20943;&#23569;&#23545;&#25239;&#24615;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2211.01579</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#65292;&#21516;&#26102;&#37319;&#29992;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#65288;WNR&#65289;&#20943;&#23569;&#23545;&#25239;&#24615;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#36890;&#36807;API&#20165;&#23558;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#26292;&#38706;&#32473;&#31532;&#19977;&#26041;&#29992;&#25143;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#30340;&#32454;&#33410;&#65288;&#22914;&#26550;&#26500;&#12289;&#23398;&#20064;&#26435;&#37325;&#12289;&#35757;&#32451;&#32454;&#33410;&#31561;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#27169;&#22411;&#22312;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25200;&#21160;&#26679;&#26412;&#19978;&#30340;&#23545;&#25239;&#24615;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#8221;(WNR)&#65292;&#23427;&#22312;&#36755;&#20837;&#22270;&#20687;&#19978;&#25191;&#34892;&#31163;&#25955;&#23567;&#27874;&#20998;&#35299;&#65292;&#24182;&#20165;&#36873;&#25321;&#25105;&#20204;&#30340;&#8220;&#23567;&#27874;&#31995;&#25968;&#36873;&#25321;&#27169;&#22359;&#8221;(WCSM)&#30830;&#23450;&#30340;&#23569;&#25968;&#37325;&#35201;&#31995;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#36807;WNR&#21435;&#38500;&#22122;&#22768;&#21518;&#24674;&#22797;&#22270;&#20687;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;&#19968;&#20010;&#8220;&#20877;&#29983;&#22120;&#8221;&#32593;&#32476;&#65292;&#30446;&#26631;&#26159;&#24674;&#22797;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coeffi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.13148</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#27169;&#22411;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#33021;&#21147;&#23398;&#20064;&#36229;&#20986;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#21040;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22270;&#30340;&#32467;&#26500;&#20559;&#24046;&#27880;&#20837;&#21040;Transformer&#30340;&#26550;&#26500;&#20013;&#65292;&#24182;&#38024;&#23545;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#24212;&#24615;&#30340;&#26550;&#26500;&#25913;&#36827;&#65306;&#65288;1&#65289;&#19968;&#20010;&#27604;&#24120;&#35268;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26356;&#39640;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;DAGs&#30340;&#32467;&#26500;&#65292;&#65288;2&#65289;&#19968;&#20010;&#23545;DAG&#30340;&#20559;&#24207;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#34917;&#20805;&#20102;&#21069;&#32773;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20174;&#23545;&#28304;&#20195;&#30721;&#22270;&#30340;&#20998;&#31867;&#21040;&#23545;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#19982;&#30446;&#26631;&#29615;&#22659;&#20132;&#20114;&#19981;&#20801;&#35768;&#26102;&#65292;&#26368;&#22909;&#30340;&#32467;&#26524;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65307;&#24403;&#20801;&#35768;&#20132;&#20114;&#26102;&#65292;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#26368;&#22810;&#26159;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#26080;&#20851;&#30340;&#30446;&#26631;&#29615;&#22659;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2210.10464</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#35757;&#32451;&#22312;RL&#27867;&#21270;&#20013;&#30340;&#33021;&#21147;&#65306;&#21487;&#35777;&#26126;&#30340;&#22909;&#22788;&#21644;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness. (arXiv:2210.10464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#19982;&#30446;&#26631;&#29615;&#22659;&#20132;&#20114;&#19981;&#20801;&#35768;&#26102;&#65292;&#26368;&#22909;&#30340;&#32467;&#26524;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65307;&#24403;&#20801;&#35768;&#20132;&#20114;&#26102;&#65292;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#26368;&#22810;&#26159;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#26080;&#20851;&#30340;&#30446;&#26631;&#29615;&#22659;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#27867;&#21270;&#30446;&#26631;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#36866;&#29992;&#20110;&#30446;&#26631;&#29615;&#22659;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;RL&#27867;&#21270;&#65306;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#36890;&#36807;&#22312;&#35757;&#32451;&#29615;&#22659;&#19978;&#30340;&#39044;&#35757;&#32451;&#23545;&#27867;&#21270;&#26377;&#22810;&#22823;&#30340;&#24110;&#21161;&#65311;&#24403;&#19982;&#30446;&#26631;&#29615;&#22659;&#30340;&#20132;&#20114;&#19981;&#20801;&#35768;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#26368;&#22810;&#21487;&#20197;&#33719;&#24471;&#30340;&#26159;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#24403;&#20801;&#35768;&#20195;&#29702;&#19982;&#30446;&#26631;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#21363;&#20174;&#39044;&#35757;&#32451;&#20013;&#30340;&#25913;&#36827;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#26080;&#20851;&#30340;&#30446;&#26631;&#29615;&#22659;&#22522;&#20110;&#20998;&#24067;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization in Reinforcement Learning (RL) aims to learn an agent during training that generalizes to the target environment. This paper studies RL generalization from a theoretical aspect: how much can we expect pre-training over training environments to be helpful? When the interaction with the target environment is not allowed, we certify that the best we can obtain is a near-optimal policy in an average sense, and we design an algorithm that achieves this goal. Furthermore, when the agent is allowed to interact with the target environment, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, in the non-asymptotic regime, we design an efficient algorithm and prove a distribution-based regret bound in the target environment that is independent of the state-action space.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#26597;&#35810;&#39537;&#21160;&#22256;&#38590;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#31616;&#21333;&#30452;&#35266;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.11559</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#23545;&#35937;&#26816;&#27979;&#30340;&#26597;&#35810;&#39537;&#21160;&#22256;&#38590;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query-based Hard-Image Retrieval for Object Detection at Test Time. (arXiv:2209.11559v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#26597;&#35810;&#39537;&#21160;&#22256;&#38590;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#31616;&#21333;&#30452;&#35266;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#25429;&#25417;&#23545;&#35937;&#26816;&#27979;&#22120;&#38169;&#35823;&#34892;&#20026;&#30340;&#20852;&#36259;&#19968;&#30452;&#23384;&#22312;&#65292;&#21363;&#25214;&#21040;&#20854;&#24615;&#33021;&#21487;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#12290;&#22312;&#35832;&#22914;&#33258;&#21160;&#39550;&#39542;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23545;&#20110;&#36229;&#20986;&#31616;&#21333;&#26816;&#27979;&#24615;&#33021;&#35201;&#27714;&#30340;&#28508;&#22312;&#25925;&#38556;&#36827;&#34892;&#34920;&#24449;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#19982;&#33258;&#36523;&#36710;&#36742;&#30456;&#36817;&#30340;&#34892;&#20154;&#30340;&#28431;&#26816;&#36890;&#24120;&#38656;&#35201;&#26356;&#20180;&#32454;&#30340;&#26816;&#26597;&#65292;&#32780;&#19982;&#36710;&#36742;&#36739;&#36828;&#22788;&#30340;&#28431;&#26816;&#30456;&#27604;&#36739;&#32780;&#35328;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#39044;&#27979;&#27492;&#31867;&#28508;&#22312;&#25925;&#38556;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#38382;&#39064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#65292;&#32780;&#22522;&#20110;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20256;&#32479;&#26041;&#27861;&#21017;&#26080;&#27861;&#25552;&#20379;&#23545;&#38169;&#35823;&#30340;&#36825;&#31181;&#31934;&#32454;&#21270;&#34920;&#24449;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#25214;&#21040;&#8220;&#22256;&#38590;&#8221;&#22270;&#20687;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#30340;&#22256;&#38590;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#20854;&#20013;&#26597;&#35810;&#26159;&#23545;&#8220;&#22256;&#38590;&#24615;&#8221;&#30340;&#20855;&#20307;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#22823;&#37327;&#30340;&#22256;&#38590;&#22270;&#20687;&#23450;&#20041;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a longstanding interest in capturing the error behaviour of object detectors by finding images where their performance is likely to be unsatisfactory. In real-world applications such as autonomous driving, it is also crucial to characterise potential failures beyond simple requirements of detection performance. For example, a missed detection of a pedestrian close to an ego vehicle will generally require closer inspection than a missed detection of a car in the distance. The problem of predicting such potential failures at test time has largely been overlooked in the literature and conventional approaches based on detection uncertainty fall short in that they are agnostic to such fine-grained characterisation of errors. In this work, we propose to reformulate the problem of finding "hard" images as a query-based hard image retrieval task, where queries are specific definitions of "hardness", and offer a simple and intuitive method that can solve this task for a large family of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.11549</link><description>&lt;p&gt;
MAGIC: &#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#23454;&#29616;&#22522;&#20110;&#25513;&#30721;&#30340;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#24102;&#26377;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#26469;&#25511;&#21046;&#23545;&#21333;&#20010;&#22270;&#20687;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;MAGIC&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#32467;&#26500;&#21270;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36755;&#20837;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#20854;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20445;&#35777;&#21512;&#25104;&#30340;&#21487;&#20449;&#24230;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#22797;&#26434;&#21407;&#35821;&#26469;&#30417;&#30563;&#36807;&#31243;&#25110;&#20351;&#29992;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#24369;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGIC&#36890;&#36807;&#22312;&#36755;&#20837;&#19978;&#32858;&#21512;&#26799;&#24230;&#65292;&#30001;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#25512;&#21160;&#12290;MAGIC&#20197;&#21333;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#24378;&#28872;&#30340;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22312;&#37325;&#22797;&#29289;&#20307;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#32473;&#29992;&#25143;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#36719;&#20214;&#27979;&#35797;&#20219;&#21153;&#20013;&#19981;&#21516;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;DRL&#26694;&#26550;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#21644;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2208.12136</link><description>&lt;p&gt;
&#36719;&#20214;&#27979;&#35797;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks. (arXiv:2208.12136v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#36719;&#20214;&#27979;&#35797;&#20219;&#21153;&#20013;&#19981;&#21516;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;DRL&#26694;&#26550;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#21644;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#27979;&#35797;&#27963;&#21160;&#36890;&#36807;&#23457;&#26597;&#36719;&#20214;&#20135;&#21697;&#30340;&#24037;&#20214;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#21487;&#33021;&#30340;&#32570;&#38519;&#24182;&#30830;&#20445;&#20135;&#21697;&#28385;&#36275;&#39044;&#26399;&#35201;&#27714;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#27979;&#35797;&#20219;&#21153;&#65292;&#22914;&#28216;&#25103;&#27979;&#35797;&#12289;&#22238;&#24402;&#27979;&#35797;&#21644;&#27979;&#35797;&#29992;&#20363;&#20248;&#21270;&#65292;&#20197;&#33258;&#21160;&#21270;&#36807;&#31243;&#24182;&#25552;&#20379;&#25345;&#32493;&#36866;&#24212;&#24615;&#12290;&#24320;&#21457;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#22836;&#23454;&#29616;DRL&#31639;&#27861;&#25110;&#20351;&#29992;DRL&#26694;&#26550;&#26469;&#20351;&#29992;DRL&#12290;DRL&#26694;&#26550;&#25552;&#20379;&#20102;&#32500;&#25252;&#33391;&#22909;&#12289;&#23454;&#29616;&#20102;&#26368;&#26032;DRL&#31639;&#27861;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#21152;&#24555;DRL&#24212;&#29992;&#30340;&#24320;&#21457;&#12290;&#24320;&#21457;&#32773;&#24050;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#26469;&#35299;&#20915;&#21253;&#25324;&#36719;&#20214;&#27979;&#35797;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;DRL&#26694;&#26550;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#19968;&#20123;&#25351;&#23548;&#26041;&#38024;&#65292;&#36825;&#20123;&#25351;&#23548;&#26041;&#38024;&#23558;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#20351;&#29992;DRL&#26694;&#26550;&#26102;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software testing activities scrutinize the artifacts and the behavior of a software product to find possible defects and ensure that the product meets its expected requirements. Recently, Deep Reinforcement Learning (DRL) has been successfully employed in complex testing tasks such as game testing, regression testing, and test case prioritization to automate the process and provide continuous adaptation. Practitioners can employ DRL by implementing from scratch a DRL algorithm or using a DRL framework. DRL frameworks offer well-maintained implemented state-of-the-art DRL algorithms to facilitate and speed up the development of DRL applications. Developers have widely used these frameworks to solve problems in various domains including software testing. However, to the best of our knowledge, there is no study that empirically evaluates the effectiveness and performance of implemented algorithms in DRL frameworks. Moreover, some guidelines are lacking from the literature that would help 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2206.05904</link><description>&lt;p&gt;
GNN&#22312;&#25512;&#24191;&#24102;&#38480;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#27604;NN&#26356;&#21152;&#26126;&#26174;
&lt;/p&gt;
&lt;p&gt;
Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20854;&#25972;&#21512;&#22270;&#24418;&#20449;&#24687;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#20165;&#38024;&#23545;&#22270;&#32423;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33410;&#28857;&#32423;&#20219;&#21153;&#65292;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#65292;&#20854;&#20013;&#35797;&#22270;&#20174;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#26631;&#31614;&#20013;&#25554;&#20540;&#20986;&#32570;&#22833;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#25152;&#36848;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20989;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;GNN&#25554;&#20540;$\mathbb{R}^d$&#20013;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#21644;&#23618;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;GNN&#26550;&#26500;&#20197;$\epsilon$-&#36817;&#20284;&#31163;&#25955;&#24102;&#38480;&#20449;&#21495;&#20165;&#38656;&#35201;$O((\log \epsilon^{-1})^{d})$&#20010;&#26435;&#37325;&#65292;&#36825;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24471;&#21040;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#25152;&#38656;&#26435;&#37325;&#23569;&#24471;&#22810; - &#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#20351;&#29992;$O((\log \epsilon^{-1})^{d})$&#20010;&#26679;&#26412;&#26469;&#35757;&#32451;GNN&#20197;$\epsilon$-&#36924;&#36817;&#24102;&#38480;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#24314;&#27169;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#65292;&#20174;&#32780;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2205.08790</link><description>&lt;p&gt;
&#36890;&#36807;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#29087;&#24713;&#22320;&#28857;&#36827;&#34892;&#35774;&#22791;&#20869;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2205.08790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#24314;&#27169;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#65292;&#20174;&#32780;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20351;&#31227;&#21160;&#21644;&#27867;&#22312;&#35745;&#31639;&#24212;&#29992;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#30340;&#24773;&#22659;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#19978;&#65292;&#36890;&#24120;&#22312;&#38598;&#20013;&#24335;&#26550;&#26500;&#19978;&#22788;&#29702;&#65292;&#21487;&#33021;&#20250;&#26292;&#38706;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#32780;&#19988;&#32570;&#20047;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#35774;&#22791;&#20869;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#20195;&#34920;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#36235;&#21183;&#12290;&#22312;&#31227;&#21160;&#29615;&#22659;&#20013;&#65292;&#29992;&#25143;&#30340;&#31038;&#20132;&#20114;&#21160;&#21644;&#35775;&#38382;&#22320;&#28857;&#26159;&#23545;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#36827;&#34892;&#34920;&#24449;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#22522;&#20110;&#33258;&#25105;&#32593;&#32476;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#36827;&#34892;&#24314;&#27169;&#12290;&#20381;&#38752;&#36825;&#20010;&#27169;&#22411;&#65292;&#31995;&#32479;&#33021;&#22815;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context modeling and recognition represent complex tasks that allow mobile and ubiquitous computing applications to adapt to the user's situation. Current solutions mainly focus on limited context information generally processed on centralized architectures, potentially exposing users' personal data to privacy leakage, and missing personalization features. For these reasons on-device context modeling and recognition represent the current research trend in this area. Among the different information characterizing the user's context in mobile environments, social interactions and visited locations remarkably contribute to the characterization of daily life scenarios. In this paper we propose a novel, unsupervised and lightweight approach to model the user's social context and her locations based on ego networks directly on the user mobile device. Relying on this model, the system is able to extract high-level and semantic-rich context features from smartphone-embedded sensors data. Speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35745;&#31639;&#28304;&#32479;&#35745;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#24863;&#30693;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22810;&#31181;&#22270;&#20687;&#27745;&#26579;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#38754;&#23545;&#22810;&#31181;&#27745;&#26579;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#27745;&#26579;&#30340;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.13263</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#28304;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#21327;&#26041;&#24046;&#24863;&#30693;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22810;&#31181;&#22270;&#20687;&#27745;&#26579;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Covariance-aware Feature Alignment with Pre-computed Source Statistics for Test-time Adaptation to Multiple Image Corruptions. (arXiv:2204.13263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35745;&#31639;&#28304;&#32479;&#35745;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#24863;&#30693;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22810;&#31181;&#22270;&#20687;&#27745;&#26579;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#38754;&#23545;&#22810;&#31181;&#27745;&#26579;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#27745;&#26579;&#30340;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#32463;&#24120;&#36935;&#21040;&#25439;&#22351;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#36825;&#20250;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#24182;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#20013;&#20351;&#29992;&#21333;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#29615;&#22659;&#65288;&#22914;&#20998;&#24067;&#22312;&#22478;&#24066;&#25110;&#27773;&#36710;&#20013;&#30340;&#25668;&#20687;&#22836;&#65289;&#30340;&#22270;&#20687;&#12290;&#36825;&#26679;&#30340;&#21333;&#19968;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#38388;&#38754;&#20020;&#30528;&#20197;&#22810;&#31181;&#19981;&#21516;&#26041;&#24335;&#25439;&#22351;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#31435;&#21363;&#36866;&#24212;&#22810;&#20010;&#21464;&#24418;&#65292;&#32780;&#19981;&#26159;&#20197;&#39640;&#25104;&#26412;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212; (TTA)&#65292;&#26088;&#22312;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#27169;&#22411;&#65292;&#26159;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#30830;&#23454;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#21333;&#19968;&#30340;&#27745;&#26579;&#12290;&#28982;&#32780;&#65292;&#22312;&#20986;&#29616;&#22810;&#31181;&#31867;&#22411;&#30340;&#27745;&#26579;&#26102;&#65292;&#36866;&#24212;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#26356;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#26159;&#22240;&#20026;&#20998;&#24067;&#20559;&#31227;&#26356;&#21152;&#22797;&#26434;&#65292;&#22312;&#22810;&#31181;&#27745;&#26579;&#24773;&#20917;&#19979;&#36866;&#24212;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#39044;&#26399;... (&#26410;&#23436;&#24453;&#32493;)
&lt;/p&gt;
&lt;p&gt;
Real-world image recognition systems often face corrupted input images, which cause distribution shifts and degrade the performance of models. These systems often use a single prediction model in a central server and process images sent from various environments, such as cameras distributed in cities or cars. Such single models face images corrupted in heterogeneous ways in test time. Thus, they require to instantly adapt to the multiple corruptions during testing rather than being re-trained at a high cost. Test-time adaptation (TTA), which aims to adapt models without accessing the training dataset, is one of the settings that can address this problem. Existing TTA methods indeed work well on a single corruption. However, the adaptation ability is limited when multiple types of corruption occur, which is more realistic. We hypothesize this is because the distribution shift is more complicated, and the adaptation becomes more difficult in case of multiple corruptions. In fact, we expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#20998;&#35299;&#24314;&#27169;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65292;&#24212;&#29992;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2204.08247</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#19982;&#22270;&#23398;&#20064;&#30340;&#32852;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#20998;&#35299;&#24314;&#27169;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65292;&#24212;&#29992;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20043;&#21069;&#30340;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#32858;&#31867;&#32467;&#26500;&#25110;&#30456;&#20284;&#24615;&#32467;&#26500;&#26469;&#25351;&#23548;&#29305;&#24449;&#36873;&#25321;&#65292;&#24573;&#30053;&#20102;&#32852;&#21512;&#20844;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#20114;&#24800;&#25928;&#30410;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#20840;&#23616;&#32467;&#26500;&#23398;&#20064;&#25110;&#23616;&#37096;&#32467;&#26500;&#23398;&#20064;&#26469;&#23398;&#20064;&#30456;&#20284;&#24615;&#32467;&#26500;&#65292;&#32570;&#20047;&#21516;&#26102;&#20855;&#22791;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#23398;&#20064;&#33021;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;JMVFG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#27491;&#20132;&#20998;&#35299;&#23545;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#30697;&#38453;&#34987;&#20998;&#35299;&#20026;&#19968;&#20010;&#35270;&#22270;&#29305;&#23450;&#30340;&#22522;&#30697;&#38453;&#21644;&#19968;&#20010;&#35270;&#22270;&#19968;&#33268;&#30340;&#32858;&#31867;&#25351;&#31034;&#22120;&#12290;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#34987;&#24212;&#29992;&#20110;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress, previous multi-view unsupervised feature selection methods mostly suffer from two limitations. First, they generally utilize either cluster structure or similarity structure to guide the feature selection, which neglect the possibility of a joint formulation with mutual benefits. Second, they often learn the similarity structure by either global structure learning or local structure learning, which lack the capability of graph learning with both global and local structural awareness. In light of this, this paper presents a joint multi-view unsupervised feature selection and graph learning (JMVFG) approach. Particularly, we formulate the multi-view feature selection with orthogonal decomposition, where each target matrix is decomposed into a view-specific basis matrix and a view-consistent cluster indicator. The cross-space locality preservation is incorporated to bridge the cluster structure learning in the projected space and the similarity learning (i.e.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.04636</link><description>&lt;p&gt;
&#8220;&#36825;&#26159;&#19968;&#20010;&#21487;&#30097;&#30340;&#21453;&#24212;&#65281;&#8221;&#65306;&#35299;&#35835;&#27010;&#29575;&#21464;&#21270;&#20197;&#26816;&#27979;NLP&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
"That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#26377;&#24847;&#21046;&#20316;&#30340;&#36755;&#20837;&#29978;&#33267;&#21487;&#20197;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#26080;&#27861;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#20197;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#25991;&#26412;&#31034;&#20363;&#30340;&#27169;&#22411;&#26080;&#20851;&#26816;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#25991;&#26412;&#26102;&#22312;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#35782;&#21035;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#22312;&#35782;&#21035;&#23545;&#25239;&#36755;&#20837;&#26041;&#38754;&#25552;&#39640;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;NLP&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35789;&#32423;&#25915;&#20987;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#21387;&#27861;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#30005;&#38459;&#21487;&#20197;&#26377;&#25928;&#24471;&#21040;&#30005;&#21387;&#21644;&#26377;&#25928;&#30005;&#38459;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;"&#22320;&#38754;"&#33410;&#28857;&#21487;&#20197;&#31616;&#21333;&#33258;&#28982;&#22320;&#35745;&#31639;&#25152;&#26377;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2203.00063</link><description>&lt;p&gt;
&#30005;&#21387;&#27861;&#32467;&#26500;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Structure from Voltage. (arXiv:2203.00063v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#21387;&#27861;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#30005;&#38459;&#21487;&#20197;&#26377;&#25928;&#24471;&#21040;&#30005;&#21387;&#21644;&#26377;&#25928;&#30005;&#38459;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;"&#22320;&#38754;"&#33410;&#28857;&#21487;&#20197;&#31616;&#21333;&#33258;&#28982;&#22320;&#35745;&#31639;&#25152;&#26377;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30005;&#38459;&#65288;ER&#65289;&#26159;&#19968;&#31181;&#25506;&#31350;&#22270;&#32467;&#26500;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#35745;&#31639;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#30340;&#26367;&#20195;&#26041;&#24335;&#12290;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#29992;&#20110;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#25214;&#21040;&#20302;&#32500;&#32467;&#26500;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22522;&#20110;ER&#30340;&#20998;&#26512;&#27604;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;Von Luxburg&#31561;&#20154;&#65288;2010&#65289;&#34920;&#26126;&#65292;&#24403;&#39030;&#28857;&#23545;&#24212;&#20110;&#20174;&#24230;&#37327;&#31354;&#38388;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#26679;&#26412;&#26102;&#65292;&#36828;&#31163;&#28857;&#20043;&#38388;&#30340;ER&#25910;&#25947;&#21040;&#19968;&#31181;&#26080;&#20851;&#20110;&#22270;&#32467;&#26500;&#30340;&#24179;&#20961;&#37327;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#20855;&#26377;$n$&#20010;&#39030;&#28857;&#21644;$n^2$&#30340;&#22270;&#20013;&#20351;&#29992;&#32553;&#25918;&#30005;&#38459;&#65292;&#21487;&#20197;&#24471;&#21040;&#30005;&#21387;&#21644;&#26377;&#25928;&#30005;&#38459;&#30340;&#26377;&#24847;&#20041;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#21521;&#24230;&#37327;&#22270;&#28155;&#21152;&#19968;&#20010;&#8220;&#22320;&#38754;&#8221;&#33410;&#28857;&#65292;&#21487;&#20197;&#31616;&#21333;&#33258;&#28982;&#22320;&#35745;&#31639;&#20174;&#19968;&#20010;&#36873;&#23450;&#28857;&#21040;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#25152;&#26377;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective resistance (ER) is an attractive way to interrogate the structure of graphs. It is an alternative to computing the eigen-vectors of the graph Laplacian. Graph laplacians are used to find low dimensional structures in high dimensional data. Here too, ER based analysis has advantages over eign-vector based methods. Unfortunately Von Luxburg et al. (2010) show that, when vertices correspond to a sample from a distribution over a metric space, the limit of the ER between distant points converges to a trivial quantity that holds no information about the structure of the graph. We show that by using scaling resistances in a graph with $n$ vertices by $n^2$, one gets a meaningful limit of the voltages and of effective resistances. We also show that by adding a "ground" node to a metric graph one gets a simple and natural way to compute all of the distances from a chosen point to all other points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#31185;&#23398;&#25991;&#31456;&#26356;&#20542;&#21521;&#20110;&#31215;&#26497;&#30340;&#31435;&#22330;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#25345;&#28040;&#26497;&#31435;&#22330;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2202.13610</link><description>&lt;p&gt;
AI&#26368;&#36817;&#21464;&#24471;&#26356;&#28040;&#26497;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Did AI get more negative recently?. (arXiv:2202.13610v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#31185;&#23398;&#25991;&#31456;&#26356;&#20542;&#21521;&#20110;&#31215;&#26497;&#30340;&#31435;&#22330;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#25345;&#28040;&#26497;&#31435;&#22330;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26680;&#24515;&#23376;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#31185;&#23398;&#25991;&#31456;&#20998;&#31867;&#20026;&#20004;&#31181;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#24341;&#20837;&#26032;&#25216;&#26415;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25991;&#31456;&#65292;&#34987;&#31216;&#20026;&#8220;&#31215;&#26497;&#31435;&#22330;&#8221;&#65307;&#21478;&#19968;&#31181;&#26159;&#20027;&#35201;&#25209;&#35780;&#29616;&#26377;&#25216;&#26415;&#19981;&#36275;&#30340;&#25991;&#31456;&#65292;&#34987;&#31216;&#20026;&#8220;&#28040;&#26497;&#31435;&#22330;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;1500&#31687;NLP&#21644;ML&#35770;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#20351;&#29992;&#22522;&#20110;SciBERT&#30340;&#27169;&#22411;&#33258;&#21160;&#39044;&#27979;&#35770;&#25991;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36817;35&#24180;&#26469;NLP&#21644;ML&#39046;&#22495;&#30340;&#36229;&#36807;41000&#31687;&#35770;&#25991;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#35770;&#25991;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#24471;&#26356;&#31215;&#26497;&#65292;&#20294;&#20063;&#26377;&#19968;&#20123;&#28040;&#26497;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we classify scientific articles in the domain of natural language processing (NLP) and machine learning (ML), as core subfields of artificial intelligence (AI), into whether (i) they extend the current state-of-the-art by the introduction of novel techniques which beat existing models or whether (ii) they mainly criticize the existing state-of-the-art, i.e. that it is deficient with respect to some property (e.g. wrong evaluation, wrong datasets, misleading task specification). We refer to contributions under (i) as having a 'positive stance' and contributions under (ii) as having a 'negative stance' (to related work). We annotate over 1.5 k papers from NLP and ML to train a SciBERT-based model to automatically predict the stance of a paper based on its title and abstract. We then analyse large-scale trends on over 41 k papers from the last approximately 35 years in NLP and ML, finding that papers have become substantially more positive over time, but negative papers als
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#19978;&#30340;&#26799;&#24230;&#27969;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#22823;&#22270;&#30340;&#36793;&#26435;&#37325;&#36866;&#24403;&#20989;&#25968;&#30340;&#27431;&#20960;&#37324;&#24471;&#26799;&#24230;&#27969;&#25910;&#25947;&#21040;&#22270;&#20989;&#25968;&#31354;&#38388;&#19978;&#19968;&#26465;&#26032;&#22411;&#36830;&#32493;&#26497;&#38480;&#12290;&#35768;&#22810;&#33258;&#28982;&#20989;&#25968;&#22312;&#35813;&#35774;&#32622;&#19979;&#37117;&#24471;&#21040;&#20102;&#28085;&#30422;&#65292;&#20363;&#22914;&#21516;&#24577;&#20989;&#25968;&#21644;&#26631;&#37327;&#29109;&#12290;</title><link>http://arxiv.org/abs/2111.09459</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#26799;&#24230;&#27969;&#65306;&#23384;&#22312;&#24615;&#12289;&#25910;&#25947;&#24615;&#12289;&#36830;&#32493;&#24615;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Gradient flows on graphons: existence, convergence, continuity equations. (arXiv:2111.09459v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#19978;&#30340;&#26799;&#24230;&#27969;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#22823;&#22270;&#30340;&#36793;&#26435;&#37325;&#36866;&#24403;&#20989;&#25968;&#30340;&#27431;&#20960;&#37324;&#24471;&#26799;&#24230;&#27969;&#25910;&#25947;&#21040;&#22270;&#20989;&#25968;&#31354;&#38388;&#19978;&#19968;&#26465;&#26032;&#22411;&#36830;&#32493;&#26497;&#38480;&#12290;&#35768;&#22810;&#33258;&#28982;&#20989;&#25968;&#22312;&#35813;&#35774;&#32622;&#19979;&#37117;&#24471;&#21040;&#20102;&#28085;&#30422;&#65292;&#20363;&#22914;&#21516;&#24577;&#20989;&#25968;&#21644;&#26631;&#37327;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;Wasserstein&#26799;&#24230;&#27969;&#22312;&#27010;&#29575;&#27979;&#24230;&#19978;&#21457;&#29616;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#23427;&#20204;&#36890;&#24120;&#20986;&#29616;&#20026;&#20132;&#25442;&#31890;&#23376;&#31995;&#32479;&#30340;&#36830;&#32493;&#26497;&#38480;&#65292;&#36825;&#20123;&#31890;&#23376;&#31995;&#32479;&#36890;&#36807;&#26576;&#31181;&#28041;&#21450;&#26799;&#24230;&#22411;&#21183;&#33021;&#30340;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#35832;&#22914;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25152;&#35859;&#30340;&#31890;&#23376;&#26159;&#22823;&#22270;&#19978;&#30340;&#36793;&#26435;&#37325;&#65292;&#20854;&#33410;&#28857;&#26159;&#21487;&#20132;&#25442;&#30340;&#12290;&#36825;&#26679;&#30340;&#22823;&#22270;&#24050;&#30693;&#22312;&#20854;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;&#31216;&#20026;&#22270;&#20989;&#25968;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36866;&#24403;&#20989;&#25968;&#30340;&#27431;&#20960;&#37324;&#24471;&#26799;&#24230;&#27969;&#25910;&#25947;&#21040;&#22270;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#19968;&#26465;&#26032;&#22411;&#36830;&#32493;&#26497;&#38480;&#65292;&#21487;&#36866;&#24403;&#25551;&#36848;&#20026;&#26799;&#24230;&#27969;&#25110;&#26356;&#25216;&#26415;&#24615;&#22320;&#35828;&#65292;&#26159;&#19968;&#26465;&#26368;&#22823;&#26012;&#29575;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#28085;&#30422;&#20102;&#22270;&#20989;&#25968;&#19978;&#30340;&#20960;&#20010;&#33258;&#28982;&#20989;&#25968;&#65292;&#20363;&#22914;&#21516;&#24577;&#20989;&#25968;&#21644;&#26631;&#37327;&#29109;&#65292;&#24182;&#19988;&#24050;&#32463;&#35814;&#32454;&#35745;&#31639;&#20102;&#36825;&#20123;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein gradient flows on probability measures have found a host of applications in various optimization problems. They typically arise as the continuum limit of exchangeable particle systems evolving by some mean-field interaction involving a gradient-type potential. However, in many problems, such as in multi-layer neural networks, the so-called particles are edge weights on large graphs whose nodes are exchangeable. Such large graphs are known to converge to continuum limits called graphons as their size grow to infinity. We show that the Euclidean gradient flow of a suitable function of the edge-weights converges to a novel continuum limit given by a curve on the space of graphons that can be appropriately described as a gradient flow or, more technically, a curve of maximal slope. Several natural functions on graphons, such as homomorphism functions and the scalar entropy, are covered by our set-up, and the examples have been worked out in detail.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26356;&#22810;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.04270</link><description>&lt;p&gt;
&#36208;&#21521;&#39044;&#27979;&#35745;&#31639;&#30340;&#36879;&#35270;&#20027;&#20041;&#30495;&#23454;&#24615;&#39564;&#35777;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Toward a Perspectivist Turn in Ground Truthing for Predictive Computing. (arXiv:2109.04270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26356;&#22810;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22522;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#32780;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26368;&#32456;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#20197;&#22810;&#25968;&#31080;&#20026;&#22522;&#30784;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#20513;&#23548;&#19968;&#31181;&#19981;&#21516;&#30340;&#33539;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#23427;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#36716;&#21521;&#37319;&#29992;&#25972;&#21512;&#20154;&#31867;&#21442;&#19982;&#30340;&#35282;&#24230;&#21644;&#35266;&#28857;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#20511;&#37492;&#21551;&#21457;&#25105;&#20204;&#25552;&#35758;&#30340;&#21069;&#20154;&#20316;&#21697;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#23545;&#20110;&#19981;&#20165;&#26159;&#26356;&#20027;&#35266;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#19982;&#20154;&#31867;&#35821;&#35328;&#26377;&#20851;&#30340;&#20219;&#21153;&#65289;&#65292;&#32780;&#19988;&#23545;&#20110;&#36890;&#24120;&#34987;&#35270;&#20026;&#23458;&#35266;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#21307;&#30103;&#20915;&#31574;&#65289;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#37319;&#29992;&#36879;&#35270;&#20027;&#20041;&#31435;&#22330;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Most Artificial Intelligence applications are based on supervised machine learning (ML), which ultimately grounds on manually annotated data. The annotation process is often performed in terms of a majority vote and this has been proved to be often problematic, as highlighted by recent studies on the evaluation of ML models. In this article we describe and advocate for a different paradigm, which we call data perspectivism, which moves away from traditional gold standard datasets, towards the adoption of methods that integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of ML processes. Drawing on previous works which inspired our proposal we describe the potential of our proposal for not only the more subjective tasks (e.g. those related to human language) but also to tasks commonly understood as objective (e.g. medical decision making), and present the main advantages of adopting a perspectivist stance in ML, as well as possible d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDMA&#30340;&#20132;&#21449;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#26497;&#23567;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#8220;&#19968;&#26086;&#26377;&#36275;&#22815;&#30340;&#21709;&#24212;&#65292;&#31435;&#21363;&#24320;&#22987;&#8221;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#22312;&#28041;&#21450;&#21040;&#19981;&#21487;&#38752;&#31227;&#21160;/IoT&#35774;&#22791;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20855;&#26377;&#23454;&#38469;&#21487;&#34892;&#24615;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2105.14216</link><description>&lt;p&gt;
CDMA&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#23454;&#29992;&#20132;&#21449;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CDMA: A Practical Cross-Device Federated Learning Algorithm for General Minimax Problems. (arXiv:2105.14216v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDMA&#30340;&#20132;&#21449;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#26497;&#23567;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#8220;&#19968;&#26086;&#26377;&#36275;&#22815;&#30340;&#21709;&#24212;&#65292;&#31435;&#21363;&#24320;&#22987;&#8221;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#22312;&#28041;&#21450;&#21040;&#19981;&#21487;&#38752;&#31227;&#21160;/IoT&#35774;&#22791;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20855;&#26377;&#23454;&#38469;&#21487;&#34892;&#24615;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#23567;&#21270;&#38382;&#39064;&#28041;&#21450;&#21040;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#65292;&#21253;&#25324;&#40065;&#26834;&#23545;&#25239;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#24335;&#19979;&#35299;&#20915;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#19968;&#33324;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#32852;&#21512;&#31639;&#27861;&#38656;&#35201;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36827;&#34892;&#23436;&#20840;&#32858;&#21512;&#65288;&#21363;&#23545;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#28041;&#21450;&#21040;&#35768;&#22810;&#19981;&#21487;&#38752;&#30340;&#31227;&#21160;/IoT&#35774;&#22791;&#30340;&#37325;&#35201;&#30340;&#20132;&#21449;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CDMA&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20132;&#21449;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#19968;&#33324;&#26497;&#23567;&#21270;&#38382;&#39064;&#12290;CDMA&#22522;&#20110;&#8220;&#19968;&#26086;&#26377;&#36275;&#22815;&#30340;&#21709;&#24212;&#65292;&#31435;&#21363;&#24320;&#22987;&#8221;&#30340;&#26426;&#21046;&#65292;&#21363;&#26381;&#21153;&#22120;&#39318;&#20808;&#21521;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#21457;&#20986;&#20449;&#21495;&#25191;&#34892;&#26412;&#22320;&#35745;&#31639;&#65292;&#28982;&#21518;&#19968;&#26086;&#25509;&#25910;&#21040;&#36275;&#22815;&#22810;&#30340;&#23458;&#25143;&#31471;&#21709;&#24212;&#65292;&#24320;&#22987;&#32858;&#21512;&#23458;&#25143;&#31471;&#25253;&#21578;&#30340;&#26412;&#22320;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimax problems arise in a wide range of important applications including robust adversarial learning and Generative Adversarial Network (GAN) training. Recently, algorithms for minimax problems in the Federated Learning (FL) paradigm have received considerable interest. Existing federated algorithms for general minimax problems require the full aggregation (i.e., aggregation of local model information from all clients) in each training round. Thus, they are inapplicable to an important setting of FL known as the cross-device setting, which involves numerous unreliable mobile/IoT devices. In this paper, we develop the first practical algorithm named CDMA for general minimax problems in the cross-device FL setting. CDMA is based on a Start-Immediately-With-Enough-Responses mechanism, in which the server first signals a subset of clients to perform local computation and then starts to aggregate the local results reported by clients once it receives responses from enough clients in each 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863;&#30340;&#26694;&#26550;&#65292;&#20026;&#20154;&#31867;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#19988;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35266;&#23519;&#26426;&#22120;&#24773;&#24863;&#21644;&#21160;&#26426;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2011.02151</link><description>&lt;p&gt;
&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863; (SHArE)&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Simulation of Human and Artificial Emotion (SHArE). (arXiv:2011.02151v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.02151
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863;&#30340;&#26694;&#26550;&#65292;&#20026;&#20154;&#31867;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#19988;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35266;&#23519;&#26426;&#22120;&#24773;&#24863;&#21644;&#21160;&#26426;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863; (SHArE) &#30340;&#26694;&#26550;&#25551;&#36848;&#20102;&#24773;&#24863;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#36827;&#34892;&#21442;&#25968;&#36716;&#31227;&#12290;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#34987;&#23450;&#20041;&#20026;&#25277;&#35937;&#27010;&#24565;&#65292;&#20063;&#21487;&#20197;&#32454;&#21270;&#21040;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#30005;&#21387;&#27700;&#24179;&#12290;&#35813;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20154;&#31867;&#30340;&#24773;&#24863;&#36712;&#36857;&#65292;&#20174;&#32780;&#21487;&#33021;&#20026;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#35266;&#23519;&#26426;&#22120;&#30340;&#24773;&#24863;&#21644;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
The framework for Simulation of Human and Artificial Emotion (SHArE) describes the architecture of emotion in terms of parameters transferable between psychology, neuroscience, and artificial intelligence. These parameters can be defined as abstract concepts or granularized down to the voltage levels of individual neurons. This model enables emotional trajectory design for humans which may lead to novel therapeutic solutions for various mental health concerns. For artificial intelligence, this work provides a compact notation which can be applied to neural networks as a means to observe the emotions and motivations of machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2009.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#25945;&#23398;&#30740;&#31350;&#25945;&#25480;&#24378;&#21270;&#23398;&#20064;&#32773;&#26102;&#20154;&#31867;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.02476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25104;&#21151;&#25945;&#23398;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#23398;&#20064;&#26041;&#24335;&#36827;&#34892;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#32773;&#22914;&#20309;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#30340;&#32463;&#39564;&#26469;&#26356;&#26032;&#20854;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861; Q-learning&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#32771;&#23519;&#20154;&#20204;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#26631;&#20934;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#26469;&#27169;&#25311;&#23398;&#20064;&#32773;&#22312;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23398;&#20064;&#39044;&#27979;&#21453;&#39304;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#22312;&#25945;&#25480;&#29702;&#24819;&#21270;&#30340;&#25506;&#32034;&#21033;&#29992;&#20219;&#21153;&#26102;&#65292;&#20154;&#20204;&#23545;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#21644;&#25240;&#25187;&#29575;&#26377;&#21738;&#20123;&#20551;&#35774;&#65311;&#22312;&#34892;&#20026;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#30456;&#23545;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25945;&#23548; Q-&#23398;&#20064;&#32773;&#36825;&#39033;&#20219;&#21153;&#12290;&#20154;&#20204;&#20542;&#21521;&#20110;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#65292;&#24182;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#20250;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36827;&#23637;&#35843;&#25972;&#33258;&#24049;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#38750;&#23545;&#31216;&#23567;&#27874;&#65292;&#23427;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/1911.06253</link><description>&lt;p&gt;
&#20102;&#35299;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms. (arXiv:1911.06253v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.06253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#38750;&#23545;&#31216;&#23567;&#27874;&#65292;&#23427;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#31687;&#24037;&#20316;&#24341;&#20837;&#20102;&#25955;&#23556;&#21464;&#25442;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#35774;&#32622;&#65288;&#22914;&#22270;&#24418;&#65289;&#20013;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#26500;&#36896;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#38750;&#24120;&#19968;&#33324;&#30340;&#38750;&#23545;&#31216;&#23567;&#27874;&#31867;&#30340;&#22270;&#24418;&#31383;&#21475;&#21270;&#21644;&#38750;&#31383;&#21475;&#21270;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#38750;&#23545;&#31216;&#22270;&#24418;&#25955;&#23556;&#21464;&#25442;&#19982;&#23545;&#31216;&#25955;&#23556;&#21464;&#25442;&#26377;&#35768;&#22810;&#30456;&#21516;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#30340;&#26500;&#36896;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#24050;&#30693;&#29702;&#35770;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#22823;&#37327;&#24102;&#26377;&#21487;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#19981;&#21464;&#24615;&#20445;&#35777;&#30340;&#32593;&#32476;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#20960;&#20309;&#25955;&#23556;&#21644;&#20854;&#20182;&#22270;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed geometric scattering transforms for graphs based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. In doing so, this work helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for g
&lt;/p&gt;</description></item></channel></rss>