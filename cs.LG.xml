<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01896</link><description>&lt;p&gt;
&#24212;&#23545;&#25345;&#32493;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22806;&#25512;&#19988;&#24378;&#28872;&#20381;&#36182;&#21608;&#26399;&#24615;&#37325;&#32622;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#20195;&#29702;&#39318;&#20808;&#23398;&#20064;&#31283;&#23450;&#24615;&#65288;&#21363;&#23454;&#29616;&#26377;&#30028;&#25104;&#26412;&#65289;&#65292;&#28982;&#21518;&#20877;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#20943;&#23569;&#20102;&#20195;&#29702;&#22120;&#30340;&#21457;&#25955;&#29575;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#22810;&#36890;&#36947;&#25968;&#25454;&#20013;&#30340;&#22810;&#20998;&#36776;&#29575;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#23618;&#20108;&#27425;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12290;&#35813;&#20998;&#31867;&#22120;&#37319;&#29992;&#20102;&#24809;&#32602;&#30340;&#22810;&#20803;&#32447;&#24615;&#21028;&#21035;&#24314;&#31435;&#20915;&#31574;&#33410;&#28857;&#65292;&#24182;&#19988;&#20351;&#29992;&#32452; Lasso &#27491;&#21017;&#21270;&#22120;&#23545;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#12290;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#25110;&#32773;&#36816;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.01893</link><description>&lt;p&gt;
&#20998;&#23618;&#20108;&#27425;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Quadratic Random Forest Classifier. (arXiv:2306.01893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#22810;&#36890;&#36947;&#25968;&#25454;&#20013;&#30340;&#22810;&#20998;&#36776;&#29575;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#23618;&#20108;&#27425;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12290;&#35813;&#20998;&#31867;&#22120;&#37319;&#29992;&#20102;&#24809;&#32602;&#30340;&#22810;&#20803;&#32447;&#24615;&#21028;&#21035;&#24314;&#31435;&#20915;&#31574;&#33410;&#28857;&#65292;&#24182;&#19988;&#20351;&#29992;&#32452; Lasso &#27491;&#21017;&#21270;&#22120;&#23545;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#12290;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#25110;&#32773;&#36816;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#22810;&#20998;&#36776;&#29575;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#23618;&#20108;&#27425;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#24212;&#29992;&#20110;&#22810;&#36890;&#36947;&#25968;&#25454;&#12290;&#35813;&#26862;&#26519;&#22312;&#27599;&#20010;&#20915;&#31574;&#33410;&#28857;&#20013;&#38598;&#25104;&#20102;&#19968;&#31181;&#24809;&#32602;&#30340;&#22810;&#20803;&#32447;&#24615;&#21028;&#21035;&#24335;&#65292;&#24182;&#20351;&#29992;&#24179;&#26041;&#29305;&#24449;&#22788;&#29702;&#23454;&#29616;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20108;&#27425;&#20915;&#31574;&#36793;&#30028;&#12290;&#24809;&#32602;&#21028;&#21035;&#24335;&#22522;&#20110;&#22810;&#31867;&#21035;&#31232;&#30095;&#21028;&#21035;&#20998;&#26512;&#65292;&#24182;&#22522;&#20110;&#32452; Lasso &#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#24809;&#32602;&#65292;&#20854;&#20301;&#20110; Lasso &#21644;&#23725;&#22238;&#24402;&#27491;&#21017;&#21270;&#22120;&#20043;&#38388;&#12290;&#35813;&#26862;&#26519;&#20272;&#35745;&#30340;&#20998;&#31867;&#27010;&#29575;&#21644;&#20854;&#20915;&#31574;&#33410;&#28857;&#23398;&#20064;&#30340;&#29305;&#24449;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#25110;&#20419;&#36827;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we proposed a hierarchical quadratic random forest classifier for classifying multiresolution samples extracted from multichannel data. This forest incorporated a penalized multivariate linear discriminant in each of its decision nodes and processed squared features to realize quadratic decision boundaries in the original feature space. The penalized discriminant was based on a multiclass sparse discriminant analysis and the penalization was based on a group Lasso regularizer which was an intermediate between the Lasso and the ridge regularizer. The classification probabilities estimated by this forest and the features learned by its decision nodes could be used standalone or foster graph-based classifiers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01890</link><description>&lt;p&gt;
&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#26680;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Metric Learning for Clustering Mixed-type Data. (arXiv:2306.01890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#21644;&#20998;&#31867;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#23558;&#28151;&#21512;&#25968;&#20540;&#21644;&#20998;&#31867;&#25968;&#25454;&#20998;&#32452;&#12290;&#39044;&#23450;&#20041;&#30340;&#36317;&#31163;&#27979;&#37327;&#29992;&#20110;&#26681;&#25454;&#23427;&#20204;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#32858;&#31867;&#25968;&#25454;&#28857;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36866;&#29992;&#20110;&#20855;&#26377;&#32431;&#25968;&#23383;&#23646;&#24615;&#21644;&#20960;&#20010;&#26377;&#24207;&#21644;&#26080;&#24207;&#20998;&#31867;&#25351;&#26631;&#30340;&#25968;&#25454;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#28151;&#21512;&#22411;&#25968;&#25454;&#30340;&#26368;&#20339;&#36317;&#31163;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#24230;&#37327;&#23558;&#25968;&#23383;&#23646;&#24615;&#36716;&#25442;&#20026;&#20998;&#31867;&#23646;&#24615;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#20182;&#20204;&#23558;&#25968;&#25454;&#28857;&#22788;&#29702;&#20026;&#21333;&#20010;&#23646;&#24615;&#31867;&#22411;&#65292;&#25110;&#32773;&#20998;&#21035;&#35745;&#31639;&#27599;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#65292;&#24182;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26469;&#23547;&#25214;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21253;&#21547;&#32431;&#36830;&#32493;&#65292;&#20998;&#31867;&#21644;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#26102;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. A predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an optimal distance for mixed-type data is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric that uses mixed kernels to measure dissimilarity, with cross-validated optimal kernel bandwidths. Our approach improves clustering accuracy when utilized for existing distance-based clustering algorithms on simulated and real-world datasets containing pure continuous, categorical, and mixed-type data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#26524;&#34631;&#20391;&#35282;&#30340;&#36830;&#25509;&#32452;&#23398;&#31227;&#26893;&#21040;&#20648;&#23618;&#35745;&#31639;&#26426;&#20013;&#65292;&#25506;&#31350;&#20854;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24182;&#22312;&#8220;&#30475;&#35265;&#20004;&#20493;&#8221;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#21644;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01885</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#25509;&#32452;&#23398;&#30340;&#20648;&#23618;&#35745;&#31639;&#26426;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multifunctionality in a Connectome-Based Reservoir Computer. (arXiv:2306.01885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#26524;&#34631;&#20391;&#35282;&#30340;&#36830;&#25509;&#32452;&#23398;&#31227;&#26893;&#21040;&#20648;&#23618;&#35745;&#31639;&#26426;&#20013;&#65292;&#25506;&#31350;&#20854;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24182;&#22312;&#8220;&#30475;&#35265;&#20004;&#20493;&#8221;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#21644;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21151;&#33021;&#24615;&#26159;&#25351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#20854;&#32593;&#32476;&#36830;&#25509;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#20010;&#20114;&#30456;&#25490;&#26021;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#65307;&#26159;&#22260;&#32469;&#30528;&#20648;&#23618;&#35745;&#31639;&#26426;&#23398;&#20064;&#33539;&#24335;&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#30340;&#22823;&#33041;&#20013;&#35266;&#23519;&#21040;&#65306;&#29305;&#21035;&#26159;&#22312;&#26524;&#34631;&#30340;&#20391;&#35282;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#26524;&#34631;&#20391;&#35282;&#30340;&#36830;&#25509;&#32452;&#23398;&#31227;&#26893;&#21040;&#20648;&#23618;&#35745;&#31639;&#26426;&#65288;RC&#65289;&#20013;&#65292;&#24182;&#20351;&#29992;&#8220;&#30475;&#35265;&#20004;&#20493;&#8221;&#30340;&#38382;&#39064;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#26469;&#35843;&#26597;&#36825;&#20010;&#8220;&#26524;&#34631;RC&#8221;&#65288;FFRC&#65289;&#23637;&#31034;&#22810;&#21151;&#33021;&#24615;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;FFRC&#22312;&#21464;&#21270;&#32593;&#32476;&#30340;&#35889;&#21322;&#24452;&#26102;&#22914;&#20309;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#30340;&#21160;&#24577;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;Erdos-Renyi&#20648;&#23618;&#35745;&#31639;&#26426;&#65288;ERRC&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;FFRC&#23637;&#31034;&#20102;&#26356;&#22823;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65307;&#22312;&#26356;&#24191;&#30340;&#36229;&#21442;&#25968;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#24182;&#36828;&#36828;&#36229;&#36234;&#20102;&#35299;&#20915;&#8220;&#30475;&#35265;&#20004;&#20493;&#8221;&#30340;ERRC&#12290;
&lt;/p&gt;
&lt;p&gt;
Multifunctionality describes the capacity for a neural network to perform multiple mutually exclusive tasks without altering its network connections; and is an emerging area of interest in the reservoir computing machine learning paradigm. Multifunctionality has been observed in the brains of humans and other animals: particularly, in the lateral horn of the fruit fly. In this work, we transplant the connectome of the fruit fly lateral horn to a reservoir computer (RC), and investigate the extent to which this 'fruit fly RC' (FFRC) exhibits multifunctionality using the 'seeing double' problem as a benchmark test. We furthermore explore the dynamics of how this FFRC achieves multifunctionality while varying the network's spectral radius. Compared to the widely-used Erd\"os-Renyi Reservoir Computer (ERRC), we report that the FFRC exhibits a greater capacity for multifunctionality; is multifunctional across a broader hyperparameter range; and solves the seeing double problem far beyond th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01875</link><description>&lt;p&gt;
DiffECG&#65306;ECG&#20449;&#21495;&#21512;&#25104;&#30340;&#19968;&#33324;&#21270;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ECG&#20449;&#21495;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#20013;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ECG&#21512;&#25104;&#26041;&#27861;,&#35206;&#30422;&#20102;&#19977;&#31181;&#24773;&#24418;&#65306;&#24515;&#36339;&#29983;&#25104;&#12289;&#37096;&#20998;&#20449;&#21495;&#23436;&#25104;&#21644;&#23436;&#25972;&#24515;&#36339;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#21508;&#31181;ECG&#30456;&#20851;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep generative models have gained attention as a promising data augmentation solution for heart disease detection using deep learning approaches applied to ECG signals. In this paper, we introduce a novel approach based on denoising diffusion probabilistic models for ECG synthesis that covers three scenarios: heartbeat generation, partial signal completion, and full heartbeat forecasting. Our approach represents the first generalized conditional approach for ECG synthesis, and our experimental results demonstrate its effectiveness for various ECG-related tasks. Moreover, we show that our approach outperforms other state-of-the-art ECG generative models and can enhance the performance of state-of-the-art classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01874</link><description>&lt;p&gt;
SACSoN&#65306;&#38754;&#21521;&#31038;&#20132;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#33258;&#20027;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SACSoN: Scalable Autonomous Data Collection for Social Navigation. (arXiv:2306.01874v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#26500;&#24314;&#31526;&#21512;&#31038;&#20132;&#35268;&#33539;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#36229;&#36234;&#20102;&#23545;&#20154;&#31867;&#34892;&#20026;&#30340;&#31616;&#21333;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#29702;&#35299;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#23398;&#20064;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31038;&#20132;&#23548;&#33322;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29615;&#22659;&#20013;&#25910;&#38598;&#23548;&#33322;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#36828;&#31243;&#25805;&#20316;&#25110;&#25345;&#32493;&#30417;&#35270;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#35270;&#35273;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;SACSoN&#65292;&#21487;&#20197;&#33258;&#20027;&#23548;&#33322;&#20110;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#34892;&#20154;&#21608;&#22260;&#65292;&#24182;&#40723;&#21169;&#20016;&#23500;&#30340;&#20132;&#20114;&#12290;SACSoN&#20351;&#29992;&#35270;&#35273;&#35266;&#23519;&#26469;&#35266;&#23519;&#21644;&#22238;&#24212;&#20854;&#38468;&#36817;&#30340;&#20154;&#31867;&#12290;&#23427;&#23558;&#36825;&#31181;&#35270;&#35273;&#29702;&#35299;&#19982;&#25345;&#32493;&#30340;&#23398;&#20064;&#21644;&#33258;&#20027;&#30896;&#25758;&#24674;&#22797;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20154;&#25805;&#20316;&#21592;&#30340;&#21442;&#19982;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25193;&#23637;&#20102;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#26469;&#25910;&#38598;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. However, collecting navigation data in human-occupied environments may require teleoperation or continuous monitoring, making the process prohibitively expensive to scale. In this paper, we present a scalable data collection system for vision-based navigation, SACSoN, that can autonomously navigate around pedestrians in challenging real-world environments while encouraging rich interactions. SACSoN uses visual observations to observe and react to humans in its vicinity. It couples this visual understanding with continual learning and an autonomous collision recovery system that limits the involvement of a human operator, allowing for better dataset scaling. We use a this system to collect th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#21457;&#29616;FA&#19982;GD&#20043;&#38388;&#23384;&#22312;&#38544;&#24335;&#20559;&#24046;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.01870</link><description>&lt;p&gt;
&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;
&lt;/p&gt;
&lt;p&gt;
Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks. (arXiv:2306.01870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#21457;&#29616;FA&#19982;GD&#20043;&#38388;&#23384;&#22312;&#38544;&#24335;&#20559;&#24046;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#29983;&#29289;&#21487;&#22609;&#24615;&#65292;&#21453;&#39304;&#23545;&#40784;&#65288;FA&#65289;&#20316;&#20026;&#20256;&#32479;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#23427;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21453;&#21521;&#20256;&#36755;&#26435;&#37325;&#26367;&#25442;&#20026;&#38543;&#26426;&#30697;&#38453;&#12290;&#34429;&#28982;FA&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#33021;&#22815;&#32469;&#36807;&#35745;&#31639;&#25361;&#25112;&#21644;&#20854;&#21487;&#20449;&#30340;&#29983;&#29289;&#23545;&#40784;&#24615;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#23398;&#20064;&#35268;&#21017;&#30340;&#29702;&#35299;&#36824;&#26159;&#26377;&#25152;&#27424;&#32570;&#30340;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25903;&#25745;FA&#23398;&#20064;&#21160;&#24577;&#30340;&#19968;&#32452;&#23432;&#24658;&#23450;&#24459;&#65292;&#25581;&#31034;&#20102;FA&#21644;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;FA&#20855;&#26377;&#19982;GD&#34920;&#29616;&#20986;&#30340;&#38544;&#24335;&#20559;&#24046;&#30456;&#20284;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#26681;&#26412;&#19981;&#21516;&#30340;&#27969;&#34892;&#35828;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#36825;&#24847;&#21619;&#30528;&#36807;&#21442;&#25968;&#21270;&#30340;&#21452;&#32447;&#24615;&#32593;&#32476;&#20013;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#22320;&#20195;&#26367;&#21518;&#21521;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest to enhance the efficiency and bio-plausibility of training deep neural networks, Feedback Alignment (FA), which replaces the backward pass weights with random matrices in the training process, has emerged as an alternative to traditional backpropagation. While the appeal of FA lies in its circumvention of computational challenges and its plausible biological alignment, the theoretical understanding of this learning rule remains partial. This paper uncovers a set of conservation laws underpinning the learning dynamics of FA, revealing intriguing parallels between FA and Gradient Descent (GD). Our analysis reveals that FA harbors implicit biases akin to those exhibited by GD, challenging the prevailing narrative that these learning algorithms are fundamentally different. Moreover, we demonstrate that these conservation laws elucidate sufficient conditions for layer-wise alignment with feedback matrices in ReLU networks. We further show that this implies over-parameterized tw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312; running time &#26041;&#38754;&#26159;&#21333;&#25351;&#25968;&#32423;&#21035;&#30340;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#22810;&#20010;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.01869</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#30340;&#24555;&#36895; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast $(1+\varepsilon)$-Approximation Algorithms for Binary Matrix Factorization. (arXiv:2306.01869v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312; running time &#26041;&#38754;&#26159;&#21333;&#25351;&#25968;&#32423;&#21035;&#30340;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#22810;&#20010;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;(BMF)&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#30697;&#38453; $\mathbf{A}\in\{0,1\}^{n\times d}$&#65292;&#25490;&#21517;&#21442;&#25968; $k&gt;0$&#65292;&#31934;&#24230;&#21442;&#25968; $\varepsilon&gt;0$&#65292;&#30446;&#30340;&#26159;&#23558; $\mathbf{A}$ &#36817;&#20284;&#20026;&#20302;&#31209;&#22240;&#23376; $\mathbf{U}\in\{0,1\}^{n\times k}$ &#21644; $\mathbf{V}\in\{0,1\}^{k\times d}$ &#30340;&#20056;&#31215;&#12290;&#31561;&#20215;&#22320;&#65292;&#25105;&#20204;&#35201;&#25214;&#21040; $\mathbf{U}$ &#21644; $\mathbf{V}$ &#20197;&#26368;&#23567;&#21270; Frobenius &#25439;&#22833; $\|\mathbf{U}\mathbf{V}\mathbf{A}\|_F^2$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20043;&#21069;&#65292;&#35813;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#36817;&#20284;&#31639;&#27861;&#26159; Kumar &#31561;&#20154;&#30340;&#31639;&#27861; [ICML 2019]&#65292;&#35813;&#31639;&#27861;&#23545;&#20110;&#26576;&#20010;&#24120;&#25968; $C \ge 576$ &#21487;&#20197;&#23454;&#29616; $C$ &#36817;&#20284;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992; $k$ &#30340;&#25351;&#25968;&#26102;&#38388;&#30340; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#20013; $k$ &#36890;&#24120;&#26159;&#23567;&#25972;&#25968;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#25512;&#24191;&#21040; BMF &#38382;&#39064;&#30340;&#20854;&#20182;&#24120;&#35265;&#21464;&#20307;&#65292;&#20174;&#32780;&#23548;&#33268;&#20108;&#26631;&#20934; $(1+\varepsilon)$ &#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce efficient $(1+\varepsilon)$-approximation algorithms for the binary matrix factorization (BMF) problem, where the inputs are a matrix $\mathbf{A}\in\{0,1\}^{n\times d}$, a rank parameter $k&gt;0$, as well as an accuracy parameter $\varepsilon&gt;0$, and the goal is to approximate $\mathbf{A}$ as a product of low-rank factors $\mathbf{U}\in\{0,1\}^{n\times k}$ and $\mathbf{V}\in\{0,1\}^{k\times d}$. Equivalently, we want to find $\mathbf{U}$ and $\mathbf{V}$ that minimize the Frobenius loss $\|\mathbf{U}\mathbf{V} \mathbf{A}\|_F^2$. Before this work, the state-of-the-art for this problem was the approximation algorithm of Kumar et. al. [ICML 2019], which achieves a $C$-approximation for some constant $C\ge 576$. We give the first $(1+\varepsilon)$-approximation algorithm using running time singly exponential in $k$, where $k$ is typically a small integer. Our techniques generalize to other common variants of the BMF problem, admitting bicriteria $(1+\varepsilon)$-approximation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;COVID&#21683;&#22013;&#20013;&#21457;&#29616;COVID-19&#30340;&#21683;&#22013;&#21644;&#21628;&#21560;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;0.81&#21644;0.86&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#23613;&#26089;&#21457;&#29616;&#26032;&#30142;&#30149;&#30340;&#29190;&#21457;&#12290;</title><link>http://arxiv.org/abs/2306.01864</link><description>&lt;p&gt;
&#21033;&#29992;&#21464;&#24322;&#39044;&#35757;&#32451;&#22495;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#21457;&#29616;COVID-19&#30340;&#21683;&#22013;&#21644;&#21628;&#21560;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains. (arXiv:2306.01864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;COVID&#21683;&#22013;&#20013;&#21457;&#29616;COVID-19&#30340;&#21683;&#22013;&#21644;&#21628;&#21560;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;0.81&#21644;0.86&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#23613;&#26089;&#21457;&#29616;&#26032;&#30142;&#30149;&#30340;&#29190;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;COVID-19&#36825;&#26679;&#30340;&#26032;&#30142;&#30149;&#30340;&#24555;&#36895;&#21457;&#29616;&#21487;&#20197;&#20419;&#36827;&#21450;&#26102;&#30340;&#30123;&#24773;&#24212;&#23545;&#65292;&#38450;&#27490;&#22823;&#35268;&#27169;&#20256;&#25773;&#24182;&#20445;&#25252;&#20844;&#20849;&#21355;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;COVID&#21683;&#22013;&#20013;&#21457;&#29616;COVID-19&#30340;&#21683;&#22013;&#21644;&#21628;&#21560;&#27169;&#24335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#20010;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#65288;&#22914;&#39046;&#22495;&#30456;&#20851;&#24615;&#21644;&#25968;&#25454;&#22686;&#24378;&#39034;&#24207;&#65289;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#26631;&#35760;&#30340;&#38750;COVID&#21683;&#22013;&#20013;&#21306;&#20998;COVID-19&#30340;&#21683;&#22013;&#21644;&#21628;&#21560;&#65292;&#20934;&#30830;&#24230;&#20998;&#21035;&#39640;&#36798;0.81&#21644;0.86&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#23613;&#26089;&#21457;&#29616;&#26032;&#30142;&#30149;&#30340;&#29190;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid discovery of new diseases, such as COVID-19 can enable a timely epidemic response, preventing the large-scale spread and protecting public health. However, limited research efforts have been taken on this problem. In this paper, we propose a contrastive learning-based modeling approach for COVID-19 coughing and breathing pattern discovery from non-COVID coughs. To validate our models, extensive experiments have been conducted using four large audio datasets and one image dataset. We further explore the effects of different factors, such as domain relevance and augmentation order on the pre-trained models. Our results show that the proposed model can effectively distinguish COVID-19 coughing and breathing from unlabeled data and labeled non-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively. Findings from this work will guide future research to detect an outbreak of a new disease early.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#34987;&#35777;&#26126;&#20026;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#30340;&#21830;&#21697;&#25293;&#21334;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01860</link><description>&lt;p&gt;
&#26080;&#31454;&#26631;&#65292;&#26080;&#36951;&#25022;&#65306;&#38024;&#23545;&#25968;&#23383;&#21830;&#21697;&#21644;&#25968;&#25454;&#25293;&#21334;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods and Data Auctions. (arXiv:2306.01860v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#34987;&#35777;&#26126;&#20026;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#30340;&#21830;&#21697;&#25293;&#21334;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21644; AI &#29983;&#25104;&#25968;&#23383;&#21830;&#21697;&#65288;&#22914;&#20010;&#24615;&#21270;&#20070;&#38754;&#20869;&#23481;&#21644;&#33402;&#26415;&#21697;&#65289;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#38656;&#35201;&#26377;&#25928;&#23450;&#20215;&#21644;&#21453;&#39304;&#26426;&#21046;&#26469;&#32771;&#34385;&#19981;&#30830;&#23450;&#30340;&#25928;&#29992;&#21644;&#26114;&#36149;&#30340;&#29983;&#20135;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#36866;&#29992;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#65292;&#20854;&#20013;&#21806;&#20986;&#21830;&#21697;&#30340;&#25928;&#29992;&#22312;&#38144;&#21806;&#21518;&#25581;&#31034;&#12290;&#35813;&#26426;&#21046;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#25351;&#23450;&#25968;&#37327;&#20540;&#26469;&#35828;&#23545;&#20154;&#31867;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992; epsilon-greedy &#31574;&#30053;&#36873;&#25321;&#20998;&#37197;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24050;&#20998;&#37197;&#21830;&#21697;&#30340;&#23454;&#29616;&#25928;&#29992;&#21644;&#20219;&#24847;&#20540;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#26159;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#12290;&#35813;&#26426;&#21046;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#12289;&#27809;&#26377;&#22266;&#23450;&#20215;&#26684;&#30340;&#21830;&#21697;&#25110;&#26381;&#21153;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing demand for data and AI-generated digital goods, such as personalized written content and artwork, necessitates effective pricing and feedback mechanisms that account for uncertain utility and costly production. Motivated by these developments, this study presents a novel mechanism design addressing a general repeated-auction setting where the utility derived from a sold good is revealed post-sale. The mechanism's novelty lies in using pairwise comparisons for eliciting information from the bidder, arguably easier for humans than assigning a numerical value. Our mechanism chooses allocations using an epsilon-greedy strategy and relies on pairwise comparisons between realized utility from allocated goods and an arbitrary value, avoiding the learning-to-bid problem explored in previous work. We prove this mechanism to be asymptotically truthful, individually rational, and welfare and revenue maximizing. The mechanism's relevance is broad, applying to any setting with made-to-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.01855</link><description>&lt;p&gt;
5IDER: &#32479;&#19968;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#29992;&#20110;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21450;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#35821;&#38899;&#21161;&#25163;&#23548;&#33322;&#22810;&#36718;&#23545;&#35805;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#22788;&#29702;&#22810;&#36718;&#20114;&#21160;&#38656;&#35201;&#31995;&#32479;&#29702;&#35299;&#21508;&#31181;&#20250;&#35805;&#29992;&#20363;&#65292;&#22914;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#12290; &#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21152;&#21095;&#20102;&#36825;&#20123;&#29992;&#20363;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#20107;&#23454;&#65292;&#36890;&#24120;&#21516;&#26102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#20116;&#20010;&#20219;&#21153;&#20197;&#21450;&#36825;&#20123;&#29992;&#20363;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#21333;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29992;&#20363;&#32452;&#21512;&#26041;&#38754;&#29978;&#33267;&#20248;&#20110;&#32463;&#36807;&#35843;&#20248;&#30340;T5&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#21442;&#25968;&#19978;&#23567;15&#20493;&#65292;&#22312;&#24310;&#36831;&#19978;&#24555;25&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#12289;&#26080;&#21442;&#25968;&#30340;&#24402;&#19968;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#31639;&#27861;&#21253;&#25324;&#36882;&#24402;&#21160;&#37327;&#26041;&#24046;&#32553;&#20943;&#26426;&#21046;&#65292;&#38024;&#23545;&#22823;&#26377;&#38480;&#29366;&#24577;&#34892;&#21160;&#31354;&#38388;&#25552;&#20986;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01854</link><description>&lt;p&gt;
&#24102;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#31616;&#21333;&#30340;&#26041;&#24046;&#32553;&#20943;&#21644;&#22823;&#29366;&#24577;&#34892;&#21160;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space. (arXiv:2306.01854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#12289;&#26080;&#21442;&#25968;&#30340;&#24402;&#19968;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#31639;&#27861;&#21253;&#25324;&#36882;&#24402;&#21160;&#37327;&#26041;&#24046;&#32553;&#20943;&#26426;&#21046;&#65292;&#38024;&#23545;&#22823;&#26377;&#38480;&#29366;&#24577;&#34892;&#21160;&#31354;&#38388;&#25552;&#20986;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#22823;&#21270;&#29366;&#24577;-&#21160;&#20316;&#21344;&#29992;&#24230;&#37327;&#20989;&#25968;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#32047;&#31215;&#22870;&#21169;RL&#35774;&#32622;&#22806;&#65292;&#36825;&#20010;&#38382;&#39064;&#21253;&#25324;&#32422;&#26463;RL&#65292;&#32431;&#25506;&#32034;&#21644;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#31561;&#29305;&#23450;&#24773;&#20917;&#12290;&#20026;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#21333;&#24490;&#29615;&#12289;&#26080;&#21442;&#25968;&#30340;&#24402;&#19968;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36882;&#24402;&#21160;&#37327;&#26041;&#24046;&#32553;&#20943;&#26426;&#21046;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}(\epsilon^{-3})$&#21644;$\tilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;$\epsilon$-&#19968;&#38454;&#31283;&#23450;&#24615;&#21644;$\epsilon$-&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21344;&#29992;&#24230;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#35299;&#20915;&#20102;&#22823;&#26377;&#38480;&#29366;&#24577;&#34892;&#21160;&#31354;&#38388;&#30340;&#35774;&#32622;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#23376;&#31243;&#24207;&#30340;&#31616;&#21333;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;$\tilde{\mathcal{O}}(\epsilon^{-4})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the reinforcement learning (RL) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward RL setting, this problem includes as particular cases constrained RL, pure exploration and learning from demonstrations among others. For this problem, we propose a simpler single-loop parameter-free normalized policy gradient algorithm. Implementing a recursive momentum variance reduction mechanism, our algorithm achieves $\tilde{\mathcal{O}}(\epsilon^{-3})$ and $\tilde{\mathcal{O}}(\epsilon^{-2})$ sample complexities for $\epsilon$-first-order stationarity and $\epsilon$-global optimality respectively, under adequate assumptions. We further address the setting of large finite state action spaces via linear function approximation of the occupancy measure and show a $\tilde{\mathcal{O}}(\epsilon^{-4})$ sample complexity for a simple policy gradient method with a linear regression subroutine.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01843</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#26377;&#26395;&#27604;&#24402;&#19968;&#21270;&#27969;&#26356;&#39640;&#25928;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#30340;&#25104;&#21151;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23558;&#36825;&#20004;&#31181;&#33539;&#24335;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#33258;&#30001;&#26684;&#24335;&#32593;&#32476;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#36807;&#20110;&#32531;&#24930;&#65292;&#20381;&#36182;&#20110;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#25104;&#26412;&#38543;&#28508;&#22312;&#32500;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#36845;&#20195;&#65292;&#20174;&#32780;&#20351;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#65288;&#27599;&#20010;&#25209;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#22823;&#32422;&#26159;&#26222;&#36890;&#33258;&#32534;&#30721;&#22120;&#30340;&#20004;&#20493;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#26420;&#32032;&#22320;&#23558;&#26368;&#22823;&#20284;&#28982;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#23548;&#33268;&#21457;&#25955;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#24819;&#27861;&#26469;&#25512;&#21160;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#29983;&#25104;&#22270;&#20687;&#12289;&#25554;&#20540;&#21644;&#21464;&#25442;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#32452;&#21512;&#30340;&#36716;&#31227;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#38454;&#27573;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01839</link><description>&lt;p&gt;
&#21442;&#25968;&#32452;&#21512;&#26694;&#26550;&#19979;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#21644;&#36801;&#31227;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-Task and Transfer Reinforcement Learning with Parameter-Compositional Framework. (arXiv:2306.01839v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#32452;&#21512;&#30340;&#36716;&#31227;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#38454;&#27573;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#22810;&#20219;&#21153;&#35757;&#32451;&#24182;&#21033;&#29992;&#20854;&#36827;&#34892;&#36716;&#31227;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#32452;&#21512;&#30340;&#36716;&#31227;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25913;&#36827;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#36716;&#31227;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#36716;&#31227;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#38454;&#27573;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the potential of improving multi-task training and also leveraging it for transferring in the reinforcement learning setting. We identify several challenges towards this goal and propose a transferring approach with a parameter-compositional formulation. We investigate ways to improve the training of multi-task reinforcement learning which serves as the foundation for transferring. Then we conduct a number of transferring experiments on various manipulation tasks. Experimental results demonstrate that the proposed approach can have improved performance in the multi-task training stage, and further show effective transferring in terms of both sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#22810;&#24207;&#21015;&#27604;&#23545;&#29983;&#25104;&#26032;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34917;&#20805;&#27809;&#26377;&#20016;&#23500;&#21516;&#28304;&#23478;&#26063;&#25968;&#25454;&#24211;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#22686;&#24378;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01824</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#24207;&#21015;&#27604;&#23545;&#29983;&#25104;&#22686;&#24378;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation. (arXiv:2306.01824v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#22810;&#24207;&#21015;&#27604;&#23545;&#29983;&#25104;&#26032;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34917;&#20805;&#27809;&#26377;&#20016;&#23500;&#21516;&#28304;&#23478;&#26063;&#25968;&#25454;&#24211;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#22686;&#24378;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22823;&#22823;&#25512;&#36827;&#20102;&#34507;&#30333;&#36136;&#25240;&#21472;&#30740;&#31350;&#39046;&#22495;&#65292;AlphaFold2&#65288;AF2&#65289;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#21407;&#23376;&#32423;&#31934;&#24230;&#12290;&#30001;&#20110;&#20849;&#21516;&#28436;&#21270;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#22810;&#24207;&#21015;&#27604;&#23545;&#65288;MSA&#65289;&#30340;&#28145;&#24230;&#20250;&#23545;AF2&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;MSA&#38656;&#35201;&#23545;&#22823;&#37327;&#34507;&#30333;&#36136;&#25968;&#25454;&#24211;&#36827;&#34892;&#24191;&#27867;&#25506;&#32034;&#20197;&#26597;&#25214;&#30456;&#20284;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#34507;&#30333;&#36136;&#24207;&#21015;&#37117;&#20855;&#26377;&#20016;&#23500;&#30340;&#21516;&#28304;&#23478;&#26063;&#65292;&#22240;&#27492;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#65292;AF2&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#38477;&#20302;&#65292;&#24182;&#19988;&#26377;&#26102;&#26080;&#27861;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;MSA-Augmenter&#65292;&#23427;&#21033;&#29992;&#34507;&#30333;&#36136;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22823;&#35268;&#27169;MSA&#29983;&#25104;&#30446;&#21069;&#23578;&#26410;&#22312;&#25968;&#25454;&#24211;&#20013;&#21457;&#29616;&#30340;&#26377;&#29992;&#30340;&#26032;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#36825;&#20123;&#24207;&#21015;&#34917;&#20805;&#20102;&#27973;&#23618;MSA&#65292;&#22686;&#24378;&#20102;&#32467;&#26500;&#23646;&#24615;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;CASP14&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MSA-Augmenter&#21487;&#20197;&#22686;&#24378;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of protein folding research has been greatly advanced by deep learning methods, with AlphaFold2 (AF2) demonstrating exceptional performance and atomic-level precision. As co-evolution is integral to protein structure prediction, AF2's accuracy is significantly influenced by the depth of multiple sequence alignment (MSA), which requires extensive exploration of a large protein database for similar sequences. However, not all protein sequences possess abundant homologous families, and consequently, AF2's performance can degrade on such queries, at times failing to produce meaningful results. To address this, we introduce a novel generative language model, MSA-Augmenter, which leverages protein-specific attention mechanisms and large-scale MSAs to generate useful, novel protein sequences not currently found in databases. These sequences supplement shallow MSAs, enhancing the accuracy of structural property predictions. Our experiments on CASP14 demonstrate that MSA-Augmenter can
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35823;&#24046;&#20989;&#25968;&#21644;ReLU&#30340;&#26032;&#22411;&#28608;&#27963;&#20989;&#25968;'ErfReLU'&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#38543;&#30528;&#23398;&#20064;&#32780;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#39033;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.01822</link><description>&lt;p&gt;
ErfReLU: &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
ErfReLU: Adaptive Activation Function for Deep Neural Network. (arXiv:2306.01822v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35823;&#24046;&#20989;&#25968;&#21644;ReLU&#30340;&#26032;&#22411;&#28608;&#27963;&#20989;&#25968;'ErfReLU'&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#38543;&#30528;&#23398;&#20064;&#32780;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#39033;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36873;&#25321;&#29992;&#20110;&#22686;&#21152;&#38750;&#32447;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;AF&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#24320;&#21457;&#21516;&#26102;&#33021;&#22815;&#38543;&#30528;&#23398;&#20064;&#32780;&#33258;&#36866;&#24212;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#24403;&#21069;&#30340;&#38656;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#24320;&#21457;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#31216;&#20026;&#21487;&#35757;&#32451;&#25110;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65288;AAF&#65289;&#12290;&#22686;&#24378;&#32467;&#26524;&#30340;AAF&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35823;&#24046;&#20989;&#25968;&#21644;ReLU&#30340;&#26032;&#22411;&#28608;&#27963;&#20989;&#25968;'ErfReLU'&#12290;&#31616;&#35201;&#20171;&#32461;&#20102;Sigmoid&#12289;ReLU&#12289;Tanh&#31561;&#26368;&#26032;&#28608;&#27963;&#20989;&#25968;&#21450;&#20854;&#30456;&#20851;&#29305;&#24615;&#12290;&#36824;&#20171;&#32461;&#20102;Tanhsoft1&#12289;Tanhsoft2&#12289;Tanhsoft3&#12289;TanhLU&#12289;SAAF&#12289;ErfAct&#12289;Pserf&#12289;Smish&#21644;Serf&#31561;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#20998;&#26512;&#20102;9&#31181;&#21487;&#35757;&#32451;AAF&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function 'ErfReLU' has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 traina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24182;&#21457;&#20998;&#31867;&#22120;&#38169;&#35823;&#26816;&#27979; (CCED) &#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#24182;&#21457;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#38169;&#35823;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#25110;&#36719;&#20214;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2306.01820</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24182;&#21457;&#20998;&#31867;&#22120;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems. (arXiv:2306.01820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24182;&#21457;&#20998;&#31867;&#22120;&#38169;&#35823;&#26816;&#27979; (CCED) &#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#24182;&#21457;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#38169;&#35823;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#25110;&#36719;&#20214;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#27599;&#24180;&#37117;&#22312;&#22686;&#21152;&#65292;&#24403;&#21069;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#25191;&#34892;&#25968;&#21313;&#20159;&#27425;&#31639;&#26415;&#36816;&#31639;&#12290;&#30001;&#20110;&#36825;&#20123;&#31995;&#32479;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#21487;&#38752;&#36816;&#34892;&#27491;&#22312;&#25104;&#20026;&#35774;&#35745;&#35201;&#27714;&#12290;&#20256;&#32479;&#30340;&#38169;&#35823;&#26816;&#27979;&#26426;&#21046;&#24341;&#20837;&#20102;&#30005;&#36335;&#25110;&#26102;&#38388;&#20887;&#20313;&#65292;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#24182;&#21457;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#19982;&#31995;&#32479;&#24182;&#34892;&#36816;&#34892;&#65292;&#24182;&#21033;&#29992;&#20854;&#23646;&#24615;&#26469;&#26816;&#27979;&#38169;&#35823;&#12290;&#24182;&#21457;&#38169;&#35823;&#26816;&#27979;&#23545;&#20110;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#28508;&#22312;&#22320;&#38477;&#20302;&#38169;&#35823;&#26816;&#27979;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#24182;&#21457;&#20998;&#31867;&#22120;&#38169;&#35823;&#26816;&#27979; (CCED) &#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#24182;&#21457;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#38169;&#35823;&#24182;&#22312;&#20027;&#35201;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#35782;&#21035;&#19968;&#32452;&#26816;&#26597;&#20449;&#21495;&#65292;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;&#24182;&#21457;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#20998;&#31867;&#22120;&#32463;&#36807;&#35757;&#32451;&#21487;&#35782;&#21035;&#20027;&#31995;&#32479;&#30340;&#27491;&#30830;&#36755;&#20986;&#12290;CCED&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#25110;&#36719;&#20214;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20027;&#31995;&#32479;&#30340;&#38169;&#35823;&#65292;&#20351;&#20854;&#25104;&#20026;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#38169;&#35823;&#26816;&#27979;&#30340;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;</title><link>http://arxiv.org/abs/2306.01818</link><description>&lt;p&gt;
Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#26816;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#19968;&#32452;&#36951;&#20256;&#24615;&#34880;&#28082;&#30142;&#30149;&#65292;&#24403;&#25658;&#24102;&#36755;&#27687;&#33267;&#36523;&#20307;&#21508;&#22788;&#30340;&#32418;&#32454;&#32990;&#20013;&#30340;&#34507;&#30333;&#36136;&#34880;&#32418;&#34507;&#30333;&#19981;&#36275;&#26102;&#20250;&#21457;&#29983;&#12290;&#22914;&#26524;&#29238;&#27597;&#21452;&#26041;&#37117;&#25658;&#24102;&#22320;&#20013;&#28023;&#36139;&#34880;&#22522;&#22240;&#65292;&#23401;&#23376;&#24739;&#30149;&#30340;&#20960;&#29575;&#20250;&#22686;&#21152;&#12290;&#30830;&#35786;&#21644;&#27835;&#30103;&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#38450;&#27490;&#20854;&#20256;&#36882;&#32473;&#19979;&#19968;&#20195;&#30340;&#20851;&#38190;&#12290;&#30446;&#21069;&#30340;&#34880;&#28082;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#36807;&#20110;&#26114;&#36149;&#12289;&#32791;&#26102;&#65292;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#31579;&#26597;&#35774;&#22791;&#12290;&#39640;&#25928;&#28082;&#30456;&#33394;&#35889;&#26159;&#26631;&#20934;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#25104;&#26412;&#39640;&#12289;&#26102;&#38388;&#38271;&#12289;&#38656;&#35201;&#29305;&#27530;&#35774;&#22791;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#19968;&#31181;&#24555;&#36895;&#12289;&#20415;&#23452;&#30340;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#30340;&#26041;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thalassemia is a group of inherited blood disorders that happen when hemoglobin, the protein in red blood cells that carries oxygen, is not made enough. It is found all over the body and is needed for survival. If both parents have thalassemia, a child's chance of getting it increases. Genetic counselling and early diagnosis are essential for treating thalassemia and stopping it from being passed on to future generations. It may be hard for healthcare professionals to differentiate between people with thalassemia carriers and those without. The current blood tests for beta thalassemia carriers are too expensive, take too long, and require too much screening equipment. The World Health Organization says there is a high death rate for people with thalassemia. Therefore, it is essential to find thalassemia carriers to act quickly. High-performance liquid chromatography (HPLC), the standard test method, has problems such as cost, time, and equipment needs. So, there must be a quick and che
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26550;&#26500;&#19979;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#19981;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#26377;&#26395;&#20248;&#21270;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;&#24515;&#33039;&#30149;&#26089;&#26399;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01817</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Heart Diseases Prediction Using Block-chain and Machine Learning. (arXiv:2306.01817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26550;&#26500;&#19979;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#19981;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#26377;&#26395;&#20248;&#21270;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;&#24515;&#33039;&#30149;&#26089;&#26399;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22823;&#37096;&#20998;&#20154;&#27515;&#20110;&#24515;&#33039;&#30142;&#30149;&#12290;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#21307;&#30103;&#37096;&#38376;&#23578;&#26410;&#24314;&#31435;&#23433;&#20840;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#24456;&#38590;&#26089;&#26399;&#39044;&#27979;&#30142;&#30149;&#12290;&#36825;&#31181;&#24555;&#36895;&#22686;&#21152;&#30340;&#27515;&#20129;&#29575;&#21487;&#20197;&#36890;&#36807;&#30417;&#27979;&#21644;&#28040;&#38500;&#26089;&#26399;&#20986;&#29616;&#30340;&#19968;&#20123;&#20851;&#38190;&#22240;&#32032;&#65288;&#20363;&#22914;&#34880;&#21387;&#12289;&#32966;&#22266;&#37255;&#27700;&#24179;&#12289;&#20307;&#37325;&#21644;&#21560;&#28895;&#25104;&#30270;&#65289;&#26469;&#25511;&#21046;&#12290;&#22312;&#21307;&#30103;&#37096;&#38376;&#20013;&#65292;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#65288;Cp&#65289;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#30417;&#25511;&#24739;&#32773;&#25968;&#25454;&#65292;&#20854;&#20013;&#21306;&#22359;&#38142;&#26159;&#26368;&#21487;&#38752;&#30340;&#25552;&#20379;&#32773;&#20043;&#19968;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22788;&#29702;&#30142;&#30149;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#27491;&#24358;&#20313;&#24358;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;Sine-Cosine SVM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most people around the globe are dying due to heart disease. The main reason behind the rapid increase in the death rate due to heart disease is that there is no infrastructure developed for the healthcare department that can provide a secure way of data storage and transmission. Due to redundancy in the patient data, it is difficult for cardiac Professionals to predict the disease early on. This rapid increase in the death rate due to heart disease can be controlled by monitoring and eliminating some of the key attributes in the early stages such as blood pressure, cholesterol level, body weight, and addiction to smoking. Patient data can be monitored by cardiac Professionals (Cp) by using the advanced framework in the healthcare departments. Blockchain is the world's most reliable provider. The use of advanced systems in the healthcare departments providing new ways of dealing with diseases has been developed as well. In this article Machine Learning (ML) algorithm known as a sine-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#38450;&#27835;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01816</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#65306;&#20998;&#31867;&#22120;&#12289;&#27169;&#22411;&#21644;SLR
&lt;/p&gt;
&lt;p&gt;
Prediction of Citrus Diseases Using Machine Learning And Deep Learning: Classifier, Models SLR. (arXiv:2306.01816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#38450;&#27835;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#26577;&#27224;&#30149;&#23475;&#19968;&#30452;&#26159;&#20840;&#29699;&#26577;&#27224;&#31181;&#26893;&#19994;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#23427;&#20204;&#20250;&#26174;&#33879;&#38477;&#20302;&#27700;&#26524;&#30340;&#36136;&#37327;&#12290;&#26368;&#21361;&#38505;&#30340;&#26577;&#27224;&#30149;&#23475;&#21253;&#25324;&#26577;&#27224;&#28291;&#30113;&#30149;&#12289;&#26577;&#27224;&#40644;&#40857;&#30149;&#12289;&#26577;&#27224;&#40657;&#26001;&#30149;&#12289;&#26577;&#27224;&#21494;&#34558;&#65292;&#36825;&#20123;&#30149;&#23475;&#21487;&#23548;&#33268;&#20840;&#29699;&#26577;&#27224;&#20135;&#19994;&#20986;&#29616;&#26174;&#33879;&#32463;&#27982;&#25439;&#22833;&#65292;&#38450;&#27835;&#31574;&#30053;&#21253;&#25324;&#21270;&#23398;&#27835;&#30103;&#31561;&#12290;&#26577;&#27224;&#30149;&#23475;&#20998;&#24067;&#22312;&#20840;&#29699;&#25152;&#26377;&#26577;&#27224;&#31181;&#26893;&#22320;&#21306;&#65292;&#24433;&#21709;&#26577;&#27224;&#26641;&#26681;&#12289;&#26577;&#27224;&#26641;&#21494;&#12289;&#26577;&#27224;&#31561;&#27700;&#26524;&#12290;&#26577;&#27224;&#30149;&#23475;&#30340;&#23384;&#22312;&#23545;&#32463;&#27982;&#22240;&#32032;&#20135;&#29983;&#39640;&#24230;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#20250;&#20135;&#29983;&#20302;&#21697;&#36136;&#27700;&#26524;&#65292;&#22686;&#21152;&#30149;&#23475;&#31649;&#29702;&#36153;&#29992;&#12290;&#21355;&#29983;&#21644;&#23450;&#26399;&#30417;&#27979;&#21487;&#20197;&#26377;&#25928;&#31649;&#29702;&#26576;&#20123;&#26577;&#27224;&#30149;&#23475;&#65292;&#20294;&#20854;&#20182;&#30149;&#23475;&#21487;&#33021;&#38656;&#35201;&#26356;&#21152;&#23494;&#38598;&#30340;&#27835;&#30103;&#65292;&#20363;&#22914;&#21270;&#23398;&#25110;&#29983;&#29289;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citrus diseases have been major issues for citrus growing worldwide for many years they can lead significantly reduce fruit quality. the most harmful citrus diseases are citrus canker, citrus greening, citrus black spot, citrus leaf miner which can have significant economic losses of citrus industry in worldwide prevention and management strategies like chemical treatments. Citrus diseases existing in all over the world where citrus is growing its effects the citrus tree root, citrus tree leaf, citrus tree orange etc. Existing of citrus diseases is highly impact on economic factor that can also produce low quality fruits and increased the rate for diseases management. Sanitation and routine monitoring can be effective in managing certain citrus diseases, but others may require more intensive treatments like chemical or biological control methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01814</link><description>&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast Interactive Search with a Scale-Free Comparison Oracle. (arXiv:2306.01814v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#36890;&#36807;&#22238;&#31572;&#8220;&#39033; $i$ &#21644; $j$ &#21738;&#19968;&#20010;&#26356;&#25509;&#36817; $t$&#65311;&#8221;&#30340;&#26597;&#35810;&#26469;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#30446;&#26631;&#39033; $t$ &#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#31034;&#36825;&#31181;&#30456;&#20284;&#24615;&#19977;&#20803;&#32452; $(i,j;t)$&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#25511;&#21046;&#39044;&#35328;&#30340;&#21306;&#20998;&#33021;&#21147;&#21644;&#21253;&#21547;&#39033;&#30340;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#26041;&#38754;&#20855;&#26377;&#29420;&#31435;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25628;&#32034;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312; $\gamma$-CKL &#27169;&#22411;&#19979;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#31639;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#19977;&#20803;&#32452;&#25968;&#25454;&#24211;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comparison-based search algorithm lets a user find a target item $t$ in a database by answering queries of the form, ``Which of items $i$ and $j$ is closer to $t$?'' Instead of formulating an explicit query (such as one or several keywords), the user navigates towards the target via a sequence of such (typically noisy) queries.  We propose a scale-free probabilistic oracle model called $\gamma$-CKL for such similarity triplets $(i,j;t)$, which generalizes the CKL triplet model proposed in the literature. The generalization affords independent control over the discriminating power of the oracle and the dimension of the feature space containing the items.  We develop a search algorithm with provably exponential rate of convergence under the $\gamma$-CKL oracle, thanks to a backtracking strategy that deals with the unavoidable errors in updating the belief region around the target.  We evaluate the performance of the algorithm both over the posited oracle and over several real-world tri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#30830;&#23450;&#36229;&#22270;&#21160;&#21147;&#31995;&#32479;&#30340;&#26368;&#23567;&#39034;&#24207;&#65292;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#30456;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#30452;&#25509;&#23398;&#20064;&#21160;&#21147;&#23398;&#21644;&#36229;&#22270;&#30340;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2306.01813</link><description>&lt;p&gt;
&#23398;&#20064;&#36229;&#22270;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#25928;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning the effective order of a hypergraph dynamical system. (arXiv:2306.01813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#30830;&#23450;&#36229;&#22270;&#21160;&#21147;&#31995;&#32479;&#30340;&#26368;&#23567;&#39034;&#24207;&#65292;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#30456;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#30452;&#25509;&#23398;&#20064;&#21160;&#21147;&#23398;&#21644;&#36229;&#22270;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21160;&#21147;&#31995;&#32479;&#21487;&#20197;&#23637;&#29616;&#20986;&#19982;&#25104;&#23545;&#20132;&#20114;&#31995;&#32479;&#19981;&#21487;&#35266;&#23519;&#21040;&#30340;&#20016;&#23500;&#30340;&#34892;&#20026;&#12290;&#23545;&#20110;&#20855;&#26377;&#20551;&#35774;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#24335;&#21160;&#21147;&#31995;&#32479;&#26469;&#35828;&#65292;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#21363;&#23454;&#38469;&#19978;&#26377;&#22810;&#23569;&#36229;&#22270;&#32467;&#26500;&#25165;&#26159;&#38656;&#35201;&#30340;&#65292;&#25165;&#33021;&#24544;&#23454;&#22320;&#22797;&#21046;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#34892;&#20026;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30830;&#23450;&#24517;&#35201;&#30340;&#26368;&#23567;&#36229;&#22270;&#39034;&#24207;&#65292;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#30456;&#24212;&#30340;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#24050;&#30693;&#21160;&#21147;&#23398;&#31867;&#22411;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#30830;&#23450;&#27492;&#39034;&#24207;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26500;&#24819;&#19982;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#65292;&#30452;&#25509;&#20174;&#21253;&#21547;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#36712;&#36857;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#26412;&#36523;&#21644;&#36229;&#22270;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical systems on hypergraphs can display a rich set of behaviours not observable for systems with pairwise interactions. Given a distributed dynamical system with a putative hypergraph structure, an interesting question is thus how much of this hypergraph structure is actually necessary to faithfully replicate the observed dynamical behaviour. To answer this question, we propose a method to determine the minimum order of a hypergraph necessary to approximate the corresponding dynamics accurately. Specifically, we develop an analytical framework that allows us to determine this order when the type of dynamics is known. We utilize these ideas in conjunction with a hypergraph neural network to directly learn the dynamics itself and the resulting order of the hypergraph from both synthetic and real data sets consisting of observed system trajectories.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAPI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#65292;&#36890;&#36807;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#12290;SAPI&#33021;&#22815;&#22312;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01812</link><description>&lt;p&gt;
SAPI:&#29615;&#22659;&#24863;&#30693;&#36710;&#36742;&#22312;&#36335;&#21475;&#30340;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SAPI: Surroundings-Aware Vehicle Trajectory Prediction at Intersections. (arXiv:2306.01812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAPI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#65292;&#36890;&#36807;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#12290;SAPI&#33021;&#22815;&#22312;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;SAPI&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;SAPI&#20351;&#29992;&#25277;&#35937;&#30340;&#26041;&#24335;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#21367;&#31215;&#32593;&#32476;(CNN)&#21644;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#32534;&#30721;&#22120;&#20197;&#21450;&#19968;&#20010;&#35299;&#30721;&#22120;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21407;&#22987;&#21382;&#21490;&#36712;&#36857;&#20449;&#24687;&#65292;&#22312;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#21270;&#22120;&#26469;&#36827;&#34892;&#22238;&#28335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#20027;&#36710;&#36742;&#22312;&#23454;&#38469;&#36335;&#21475;&#37319;&#38598;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;SAPI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#27979;&#36335;&#21475;&#36710;&#36742;&#36712;&#36857;&#26102;&#65292;SAPI&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;6&#31186;&#39044;&#27979;&#30340;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;(ADE)&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;(FDE)&#20998;&#21035;&#20026;1.84&#31859;&#21644;4.32&#31859;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a deep learning model, i.e., SAPI, to predict vehicle trajectories at intersections. SAPI uses an abstract way to represent and encode surrounding environment by utilizing information from real-time map, right-of-way, and surrounding traffic. The proposed model consists of two convolutional network (CNN) and recurrent neural network (RNN)-based encoders and one decoder. A refiner is proposed to conduct a look-back operation inside the model, in order to make full use of raw history trajectory information. We evaluate SAPI on a proprietary dataset collected in real-world intersections through autonomous vehicles. It is demonstrated that SAPI shows promising performance when predicting vehicle trajectories at intersection, and outperforms benchmark methods. The average displacement error(ADE) and final displacement error(FDE) for 6-second prediction are 1.84m and 4.32m respectively. We also show that the proposed model can accurately predict vehicle trajectories i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01811</link><description>&lt;p&gt;
DVFO&#65306;DNN&#36793;&#32536;&#25512;&#29702;&#30340;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#19981;&#21516;&#29305;&#24615;&#65292;&#20248;&#21270;&#36793;&#32536;&#35774;&#22791;&#19978;DNN&#25512;&#29702;&#24615;&#33021;&#65288;&#22312;&#33021;&#28304;&#28040;&#32791;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#65289;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#38500;&#20102;&#21160;&#24577;&#30005;&#21387;&#39057;&#29575;&#32553;&#25918;&#65288;DVFS&#65289;&#25216;&#26415;&#65292;&#36793;&#32536;&#20113;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;DNN&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#23578;&#26410;&#23545;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21508;&#31181;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DVFO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;DVFS&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DVFO&#33258;&#21160;&#20849;&#21516;&#20248;&#21270;&#20102;1&#65289;&#36793;&#32536;&#35774;&#22791;&#30340;CPU&#12289;GPU&#21644;&#20869;&#23384;&#39057;&#29575;&#65292;&#20197;&#21450;2&#65289;&#35201;&#21368;&#36733;&#21040;&#20113;&#26381;&#21153;&#22120;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#24605;&#32771;&#21363;&#34892;&#21160;&#30340;&#24182;&#21457;&#26426;&#21046;&#21152;&#36895;DRL&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#36827;&#19968;&#27493;&#38477;&#20302;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DVFO&#22312;&#25512;&#29702;&#20934;&#30830;&#24230;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01809</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attack Based on Prediction-Correction. (arXiv:2306.01809v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#23558;&#24494;&#23567;&#30340;&#25200;&#21160;&#28155;&#21152;&#21040;&#21407;&#26412;&#30340;&#26679;&#26412;&#20013;&#12290;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#20013;&#65292;&#28155;&#21152;&#30340;&#25200;&#21160;&#20027;&#35201;&#30001;&#25439;&#22833;&#20989;&#25968;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#20915;&#23450;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#26799;&#24230;&#25915;&#20987;&#19982;&#27714;&#35299;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243; (ODE) &#25968;&#20540;&#26041;&#27861;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#21463;ODE&#25968;&#20540;&#35299;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;(PC)&#30340;&#26032;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;PC-based&#25915;&#20987;&#20013;&#65292;&#21487;&#20197;&#20808;&#36873;&#25321;&#19968;&#20123;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#19968;&#20010;&#39044;&#27979;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#26679;&#26412;&#21644;&#24403;&#21069;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#23450;&#25152;&#28155;&#21152;&#30340;&#25200;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#25152;&#26377;&#21487;&#29992;&#30340;&#26799;&#24230;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#23545;&#25239;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#27604;&#36739;&#36827;&#34892;&#21387;&#32553;&#21644;&#21152;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial examples obtained by adding small perturbations to original examples. The added perturbations in existing attacks are mainly determined by the gradient of the loss function with respect to the inputs. In this paper, the close relationship between gradient-based attacks and the numerical methods for solving ordinary differential equation (ODE) is studied for the first time. Inspired by the numerical solution of ODE, a new prediction-correction (PC) based adversarial attack is proposed. In our proposed PC-based attack, some existing attack can be selected to produce a predicted example first, and then the predicted example and the current example are combined together to determine the added perturbations. The proposed method possesses good extensibility and can be applied to all available gradient-based attacks easily. Extensive experiments demonstrate that compared with the state-of-the-art gradient-based adversarial attacks, our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.01804</link><description>&lt;p&gt;
&#20174;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diffusion&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20063;&#34987;&#29992;&#20110;&#23398;&#20064;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#39640;&#24615;&#33021;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#27604;&#36739;&#24314;&#27169;&#20302;&#22870;&#21169;&#34892;&#20026;&#21644;&#24314;&#27169;&#39640;&#22870;&#21169;&#34892;&#20026;&#30340;&#20915;&#31574;&#20256;&#25773;&#27169;&#22411;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65307;&#36825;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26799;&#24230;&#19982;&#20004;&#20010;Diffusion&#27169;&#22411;&#30340;&#36755;&#20986;&#24046;&#24322;&#23545;&#40784;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;Diffusion&#27169;&#22411;&#26469;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cvHM&#65292;&#19968;&#31181;&#20351;&#29992;Hida-Mat'ern&#26680;&#21644;&#20849;&#36717;&#35745;&#31639;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#30340;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36890;&#29992;&#25512;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#25191;&#34892;&#28508;&#22312;&#31070;&#32463;&#36712;&#36857;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20197;&#36866;&#24212;&#20219;&#24847;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.01802</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#39640;&#26031;&#36807;&#31243;&#25512;&#26029;&#31070;&#32463;&#33033;&#20914;&#24207;&#21015;&#20013;&#30340;&#28508;&#22312;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Linear Time GPs for Inferring Latent Trajectories from Neural Spike Trains. (arXiv:2306.01802v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cvHM&#65292;&#19968;&#31181;&#20351;&#29992;Hida-Mat'ern&#26680;&#21644;&#20849;&#36717;&#35745;&#31639;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#30340;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36890;&#29992;&#25512;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#25191;&#34892;&#28508;&#22312;&#31070;&#32463;&#36712;&#36857;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20197;&#36866;&#24212;&#20219;&#24847;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#20197;&#20174;&#39034;&#24207;&#35266;&#27979;&#20013;&#25581;&#31034;&#38544;&#34255;&#29366;&#24577;&#30340;&#28436;&#21270;&#65292;&#20027;&#35201;&#29992;&#20110;&#31070;&#32463;&#27963;&#21160;&#35760;&#24405;&#12290;&#34429;&#28982;&#28508;&#22312;GP&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#21644;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#38750;&#20849;&#36717;&#35774;&#32622;&#20013;&#30340;&#19981;&#21487;&#34892;&#21518;&#39564;&#38656;&#35201;&#36817;&#20284;&#25512;&#26029;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#21487;&#33021;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cvHM&#65292;&#19968;&#31181;&#20351;&#29992;Hida-Mat'ern&#26680;&#21644;&#20849;&#36717;&#35745;&#31639;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#30340;&#28508;&#22312;GP&#27169;&#22411;&#30340;&#36890;&#29992;&#25512;&#29702;&#26694;&#26550;&#12290;&#20351;&#29992;cvHM&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#25191;&#34892;&#28508;&#22312;&#31070;&#32463;&#36712;&#36857;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20197;&#36866;&#24212;&#20219;&#24847;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#20351;&#29992;Hida-Mat'ern GPs&#23545;&#24179;&#31283;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24110;&#21161;&#25105;&#20204;&#23558;&#32534;&#30721;&#20808;&#21069;&#20551;&#35774;&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#19982;&#32534;&#30721;&#36712;&#36857;&#20551;&#35774;&#30340;GP&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#21160;&#21147;&#31995;&#32479;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#20449;&#24687;&#36807;&#28388;&#65292;&#23548;&#33268;&#27169;&#22411;&#25512;&#26029;&#26356;&#28165;&#26224;&#65292;&#20272;&#35745;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Mat\'ern kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Mat\'ern GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs. In contrast to previous work, we use bidirectional information filtering, leading to a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;&#27809;&#26377;&#20551;&#35774;eCPM&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01799</link><description>&lt;p&gt;
&#22312;&#24191;&#21578;&#25293;&#21334;&#20013;&#36890;&#36807;&#28857;&#20987;&#29575;&#39044;&#27979;&#26469;&#23454;&#29616;&#31119;&#21033;&#26368;&#22823;&#21270;&#30340;&#25104;&#23545;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions. (arXiv:2306.01799v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;&#27809;&#26377;&#20551;&#35774;eCPM&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#25439;&#22833;&#20989;&#25968;&#20197;&#22312;&#24191;&#21578;&#25293;&#21334;&#20013;&#20248;&#21270;&#65288;&#31038;&#20250;&#65289;&#31119;&#21033;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;CTR&#39044;&#27979;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#19994;&#21153;&#30446;&#26631;&#65288;&#20363;&#22914;&#31119;&#21033;&#65289;&#65292;&#35201;&#20040;&#20551;&#35774;&#21442;&#19982;&#32773;&#26399;&#26395;&#27599;&#27425;&#23637;&#31034;&#36153;&#29992;&#65288;eCPM&#65289;&#30340;&#20998;&#24067;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#28982;&#21518;&#20351;&#29992;&#21508;&#31181;&#38468;&#21152;&#20551;&#35774;&#26469;&#25512;&#23548;&#29992;&#20110;&#39044;&#27979;CTR&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24191;&#21578;&#25293;&#21334;&#30340;&#31119;&#21033;&#30446;&#26631;&#24102;&#22238;CTR&#39044;&#27979;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;eCPM&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#23545;&#31119;&#21033;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#26102;&#30340;&#26840;&#25163;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#8220;&#32769;&#24072;&#32593;&#32476;&#8221;&#29983;&#25104;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26657;&#20934;&#25439;&#22833;&#30340;&#29702;&#35770;&#21487;&#35777;&#26126;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the design of loss functions for click-through rates (CTR) to optimize (social) welfare in advertising auctions. Existing works either only focus on CTR predictions without consideration of business objectives (e.g., welfare) in auctions or assume that the distribution over the participants' expected cost-per-impression (eCPM) is known a priori, then use various additional assumptions on the parametric form of the distribution to derive loss functions for predicting CTRs. In this work, we bring back the welfare objectives of ad auctions into CTR predictions and propose a novel weighted rankloss to train the CTR model. Compared to existing literature, our approach provides a provable guarantee on welfare but without assumptions on the eCPMs' distribution while also avoiding the intractability of naively applying existing learning-to-rank methods. Further, we propose a theoretically justifiable technique for calibrating the losses using labels generated from a teacher network, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.01794</link><description>&lt;p&gt;
DiffPack&#65306;&#33258;&#22238;&#24402;&#34507;&#30333;&#36136;&#20391;&#38142;&#21253;&#35013;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing. (arXiv:2306.01794v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22312;&#29983;&#29289;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#19977;&#32500;&#32467;&#26500;&#23545;&#20110;&#20915;&#23450;&#23427;&#20204;&#30340;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;&#39592;&#26550;&#30340;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#23545;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#20934;&#30830;&#24230;&#26377;&#38480;&#65292;&#32780;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#38382;&#39064;&#35270;&#20026;&#22238;&#24402;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#22240;&#24120;&#37327;&#20849;&#20215;&#38190;&#38271;&#24230;&#21644;&#35282;&#24230;&#25152;&#38480;&#21046;&#32780;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#65292;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#36825;&#26159;&#20391;&#38142;&#21253;&#35013;&#20013;&#21807;&#19968;&#30340;&#33258;&#30001;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21516;&#26102;&#25200;&#21160;&#25152;&#26377;&#22235;&#20010;&#25197;&#36716;&#35282;&#24230;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;\c {hi}1&#21040;\c {hi}4&#33258;&#22238;&#24402;&#29983;&#25104;&#22235;&#20010;&#25197;&#36716;&#35282;&#24230;&#65292;&#24182;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins play a critical role in carrying out biological functions, and their 3D structures are essential in determining their functions. Accurately predicting the conformation of protein side-chains given their backbones is important for applications in protein structure prediction, design and protein-protein interactions. Traditional methods are computationally intensive and have limited accuracy, while existing machine learning methods treat the problem as a regression task and overlook the restrictions imposed by the constant covalent bond lengths and angles. In this work, we present DiffPack, a torsional diffusion model that learns the joint distribution of side-chain torsional angles, the only degrees of freedom in side-chain packing, by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, we propose autoregressively generating the four torsional angles from \c{hi}1 to \c{hi}4 and training diffusion m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SERP&#29305;&#24449;&#23545;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;SERP&#29305;&#24449;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#65292;&#33021;&#22815;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01785</link><description>&lt;p&gt;
&#36229;&#36234;&#25490;&#21517;&#65306;&#25506;&#32034;SERP&#29305;&#24449;&#23545;&#26377;&#26426;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Rankings: Exploring the Impact of SERP Features on Organic Click-through Rates. (arXiv:2306.01785v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SERP&#29305;&#24449;&#23545;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;SERP&#29305;&#24449;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#65292;&#33021;&#22815;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#20316;&#20026;&#36890;&#24448;&#24191;&#38420;&#20114;&#32852;&#32593;&#19990;&#30028;&#30340;&#25968;&#23383;&#38376;&#25143;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20197;&#32593;&#31449;&#25490;&#21517;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36825;&#20123;&#39029;&#38754;&#19978;&#65292;&#20197;&#30830;&#23450;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#65292;SERP&#30340;&#26223;&#35266;&#21457;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#28436;&#21464;&#65306;SERP&#29305;&#24449;&#65292;&#21253;&#25324;&#30693;&#35782;&#38754;&#26495;&#12289;&#23186;&#20307;&#30011;&#24266;&#12289;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#31561;&#20803;&#32032;&#65292;&#24050;&#32463;&#25104;&#20026;&#36825;&#20123;&#32467;&#26524;&#39029;&#38754;&#20013;&#36234;&#26469;&#36234;&#31361;&#20986;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25581;&#31034;&#23427;&#20204;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#65292;&#26080;&#35770;&#26159;&#25918;&#22823;&#36824;&#26159;&#20943;&#24369;&#23427;&#12290;&#25105;&#20204;&#20351;&#29992;&#28085;&#30422;40&#20010;&#19981;&#21516;&#32654;&#22269;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;67,000&#20010;&#20851;&#38190;&#23383;&#21450;&#20854;&#30456;&#24212;&#30340;Google SERPs&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22797;&#26434;&#30340;&#20132;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search Engine Result Pages (SERPs) serve as the digital gateways to the vast expanse of the internet. Past decades have witnessed a surge in research primarily centered on the influence of website ranking on these pages, to determine the click-through rate (CTR). However, during this period, the landscape of SERPs has undergone a dramatic evolution: SERP features, encompassing elements such as knowledge panels, media galleries, FAQs, and more, have emerged as an increasingly prominent facet of these result pages. Our study examines the crucial role of these features, revealing them to be not merely aesthetic components, but strongly influence CTR and the associated behavior of internet users. We demonstrate how these features can significantly modulate web traffic, either amplifying or attenuating it. We dissect these intricate interaction effects leveraging a unique dataset of 67,000 keywords and their respective Google SERPs, spanning over 40 distinct US-based e-commerce domains, gen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;</title><link>http://arxiv.org/abs/2306.01776</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#22312;&#19977;&#32500;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Diffusion for 3D Turbulent Flows. (arXiv:2306.01776v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#27969;&#21160;&#36890;&#24120;&#38590;&#20197;&#39044;&#27979;&#65292;&#20294;&#20108;&#32500;&#21644;&#19977;&#32500;&#30340;&#28237;&#27969;&#27969;&#21160;&#24615;&#36136;&#19981;&#21516;&#12290;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#65292;&#28237;&#27969;&#20250;&#24418;&#25104;&#22823;&#30340;&#12289;&#36830;&#32493;&#30340;&#32467;&#26500;&#65292;&#32780;&#22312;&#19977;&#32500;&#24773;&#20917;&#19979;&#65292;&#26059;&#28065;&#32423;&#32852;&#25104;&#36234;&#26469;&#36234;&#23567;&#30340;&#23610;&#24230;&#65292;&#24418;&#25104;&#35768;&#22810;&#24555;&#36895;&#21464;&#21270;&#30340;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21152;&#21095;&#20102;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#38590;&#20197;&#20351;&#29992;&#22238;&#24402;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#27969;&#22330;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21482;&#20381;&#38752;&#20960;&#20309;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27604;&#24037;&#19994;&#32423;&#25968;&#20540;&#27714;&#35299;&#22120;&#26356;&#24555;&#22320;&#29983;&#25104;&#28237;&#27969;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent flows are well known to be chaotic and hard to predict; however, their dynamics differ between two and three dimensions. While 2D turbulence tends to form large, coherent structures, in three dimensions vortices cascade to smaller and smaller scales. This cascade creates many fast-changing, small-scale structures and amplifies the unpredictability, making regression-based methods infeasible. We propose the first generative model for forced turbulence in arbitrary 3D geometries and introduce a sample quality metric for turbulent flows based on the Wasserstein distance of the generated velocity-vorticity distribution. In several experiments, we show that our generative diffusion model circumvents the unpredictability of turbulent flows and produces high-quality samples based solely on geometric information. Furthermore, we demonstrate that our model beats an industrial-grade numerical solver in the time to generate a turbulent flow field from scratch by an order of magnitude.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#23450;&#37327;&#26041;&#38754;&#23545;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#24635;&#32467;&#20102;&#22914;&#20309;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01768</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30740;&#31350;&#30340;&#23450;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#23450;&#37327;&#26041;&#38754;&#23545;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#24635;&#32467;&#20102;&#22914;&#20309;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#34987;&#25193;&#23637;&#29992;&#20110;&#22788;&#29702;&#19968;&#20123;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#24037;&#20855;&#20043;&#19968;&#12290;&#26412;&#31687;&#35770;&#25991;&#28085;&#30422;&#20102;&#23545;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#36825;&#19968;&#26680;&#24515;&#38382;&#39064;&#30340;&#30740;&#31350;&#27010;&#36848;&#65292;&#24182;&#23581;&#35797;&#23545;&#26368;&#36817;&#30340;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#30740;&#31350;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of "On Evaluation", they left an open question "which fundamental efficient Transformer one should consider," answered by "still a mystery" because "many research papers select their own benchmarks." Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency researc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01761</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#19968;&#21592;&#65292;&#26159;&#19968;&#31181;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#29983;&#25104;&#30475;&#20284;&#30001;&#33258;&#28982;&#26234;&#33021;&#25776;&#20889;&#30340;&#25991;&#26412;&#25991;&#20214;&#12290;&#34429;&#28982;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#21512;&#29702;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20849;&#35745;11&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Kaggle&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;&#25991;&#26412;&#65292;&#20854;&#20013;5,204&#20010;&#25991;&#26412;&#26159;&#20154;&#31867;&#20174;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#20889;&#20316;&#12290;&#22312;&#30001;GPT-3.5&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21576;&#29616;&#20986;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a conversational artificial intelligence that is a member of the generative pre-trained transformer of the large language model family. This text generative model was fine-tuned by both supervised learning and reinforcement learning so that it can produce text documents that seem to be written by natural intelligence. Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well. This paper presents a machine learning-based solution that can identify the ChatGPT delivered text from the human written text along with the comparative analysis of a total of 11 machine learning and deep learning algorithms in the classification process. We have tested the proposed model on a Kaggle dataset consisting of 10,000 texts out of which 5,204 texts were written by humans and collected from news and social media. On the corpus generated by GPT-3.5, the proposed algorithm presents an accuracy of 77%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#36753;&#20195;&#30721;&#30340;&#21516;&#26102;&#26816;&#27979;&#28431;&#27934;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#36229;&#36807;250&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22797;&#26434;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.01754</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#32534;&#36753;&#26102;&#28431;&#27934;&#26816;&#27979;&#65306;&#38646;&#26679;&#26412;&#12289;&#23567;&#26679;&#26412;&#36824;&#26159;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?. (arXiv:2306.01754v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#36753;&#20195;&#30721;&#30340;&#21516;&#26102;&#26816;&#27979;&#28431;&#27934;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#36229;&#36807;250&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22797;&#26434;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28431;&#27934;&#20250;&#32473;&#20225;&#19994;&#24102;&#26469;&#37325;&#22823;&#25439;&#22833;&#12290;&#23613;&#31649;&#38024;&#23545;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21162;&#21147;&#65292;&#20294;&#26410;&#21457;&#29616;&#30340;&#28431;&#27934;&#20173;&#20250;&#23545;&#36719;&#20214;&#25152;&#26377;&#32773;&#21644;&#29992;&#25143;&#36896;&#25104;&#39118;&#38505;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#35201;&#27714;&#20195;&#30721;&#29255;&#27573;&#33021;&#22815;&#22312;&#23581;&#35797;&#26816;&#27979;&#20043;&#21069;&#32534;&#35793;&#21644;&#26500;&#24314;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20250;&#22312;&#27880;&#20837;&#28431;&#27934;&#21040;&#21024;&#38500;&#28431;&#27934;&#30340;&#26102;&#38388;&#20043;&#38388;&#24341;&#20837;&#24456;&#38271;&#30340;&#24310;&#36831;&#65292;&#36825;&#21487;&#33021;&#20250;&#22823;&#22823;&#22686;&#21152;&#20462;&#22797;&#28431;&#27934;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#21547;250&#22810;&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22797;&#26434;&#30340;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#24182;&#22312;&#20195;&#30721;&#32534;&#36753;&#26102;&#26816;&#27979;&#28431;&#27934;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#38646;&#26679;&#26412;&#12289;&#23567;&#26679;&#26412;&#21644;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#28431;&#27934;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulnerability detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build before attempting detection. This, unfortunately, introduces a long latency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine learning can be used to detect vulnerable code patterns on syntactically incomplete code snippets as the developer is writing the code at EditTime. In this paper we present a practical system that leverages deep learning on a large-scale data set of vulnerable code patterns to learn complex manifestations of more than 250 vulnerability types and detect vulnerable code patterns at EditTime. We discus
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20896;&#29366;&#21160;&#33033; CT &#34880;&#31649;&#36896;&#24433;&#20013;&#33258;&#21160;&#26816;&#27979;&#29287;&#32650;&#20154;&#38057;&#24418;&#21491;&#20896;&#29366;&#21160;&#33033;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22788;&#29702;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01752</link><description>&lt;p&gt;
&#22312;&#20896;&#29366;&#21160;&#33033; CT &#34880;&#31649;&#36896;&#24433;&#20013;&#22788;&#29702;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#30740;&#31350;&#8212;&#8212;&#20197;&#29287;&#32650;&#20154;&#38057;&#24418;&#21491;&#20896;&#29366;&#21160;&#33033;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Handling Label Uncertainty on the Example of Automatic Detection of Shepherd's Crook RCA in Coronary CT Angiography. (arXiv:2306.01752v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20896;&#29366;&#21160;&#33033; CT &#34880;&#31649;&#36896;&#24433;&#20013;&#33258;&#21160;&#26816;&#27979;&#29287;&#32650;&#20154;&#38057;&#24418;&#21491;&#20896;&#29366;&#21160;&#33033;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22788;&#29702;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#36890;&#24120;&#36890;&#36807;&#21521;&#24739;&#30149;&#20896;&#29366;&#21160;&#33033;&#25554;&#20837;&#23548;&#31649;&#36827;&#34892;&#24494;&#21019;&#27835;&#30103;&#12290;&#22914;&#26524;&#24739;&#32773;&#20986;&#29616;&#29287;&#32650;&#20154;&#38057;&#24418;&#65288;SC&#65289;&#21491;&#20896;&#29366;&#21160;&#33033;&#65288;RCA&#65289;&#8212;&#8212;&#36825;&#26159;&#20896;&#29366;&#34880;&#31649;&#30340;&#19968;&#31181;&#35299;&#21078;&#27491;&#24120;&#21464;&#24322;&#8212;&#8212;&#21017;&#20250;&#22686;&#21152;&#36825;&#31181;&#25163;&#26415;&#30340;&#22797;&#26434;&#24615;&#12290;&#20174;&#20896;&#29366;&#21160;&#33033; CT &#34880;&#31649;&#36896;&#24433;&#31579;&#26597;&#20013;&#33258;&#21160;&#25253;&#21578;&#27492;&#21464;&#24322;&#26377;&#21161;&#20110;&#39118;&#38505;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#27531;&#24046;&#25193;&#24352;&#21367;&#31215;&#26469;&#33258;&#21160;&#30830;&#23450;&#36825;&#31181;&#27491;&#24120;&#21464;&#24322;&#65292;&#24182;&#20174;&#20808;&#21069;&#25552;&#21462;&#30340;&#34880;&#31649;&#20013;&#24515;&#32447;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110; SC RCA &#30456;&#23545;&#20110;&#20855;&#20307;&#27979;&#37327;&#24182;&#19981;&#28165;&#26224;&#65292;&#26631;&#31614;&#36824;&#21253;&#25324;&#23450;&#24615;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377; 4.23% &#30340;&#26679;&#26412;&#26631;&#35760;&#20026;&#19981;&#30830;&#23450;&#30340; SC RCA&#65292;5.97% &#30340;&#26679;&#26412;&#26631;&#35760;&#20026;&#30830;&#23450;&#30340; SC RCA&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22788;&#29702;&#36825;&#31181;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#25514;&#26045;&#65292;&#21253;&#25324;&#20840;&#23616;/&#27169;&#22411;&#38543;&#26426;&#20998;&#37197;&#12289;&#25490;&#38500;&#21644;&#36719;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronary artery disease (CAD) is often treated minimally invasively with a catheter being inserted into the diseased coronary vessel. If a patient exhibits a Shepherd's Crook (SC) Right Coronary Artery (RCA) - an anatomical norm variant of the coronary vasculature - the complexity of this procedure is increased. Automated reporting of this variant from coronary CT angiography screening would ease prior risk assessment. We propose a 1D convolutional neural network which leverages a sequence of residual dilated convolutions to automatically determine this norm variant from a prior extracted vessel centerline. As the SC RCA is not clearly defined with respect to concrete measurements, labeling also includes qualitative aspects. Therefore, 4.23% samples in our dataset of 519 RCA centerlines were labeled as unsure SC RCAs, with 5.97% being labeled as sure SC RCAs. We explore measures to handle this label uncertainty, namely global/model-wise random assignment, exclusion, and soft label assi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20854;&#20013;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#25928;&#26524;&#26174;&#33879;&#65292;DP-SignOPORP&#31639;&#27861;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;iDP&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01751</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#21644;&#31526;&#21495;&#38543;&#26426;&#25237;&#24433;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy with Random Projections and Sign Random Projections. (arXiv:2306.01751v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20854;&#20013;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#25928;&#26524;&#26174;&#33879;&#65292;DP-SignOPORP&#31639;&#27861;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;iDP&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#65288;RP&#65289;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#31526;&#21495;&#38543;&#26426;&#25237;&#24433;&#65288;SignRP&#65289;&#30340;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#65288;iDP&#65289;&#35774;&#32622;&#19979;&#38750;&#24120;&#26377;&#25928;&#65292;&#32780;DP-SignOPORP&#31639;&#27861;&#22312;&#26631;&#20934;DP&#35774;&#32622;&#19979;&#21033;&#29992;&#8220;&#19968;&#27425;&#25490;&#21015;+&#19968;&#27425;&#38543;&#26426;&#25237;&#24433;&#8221;&#65288;OPORP&#65289;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#38500;&#19981;&#32771;&#34385;&#31526;&#21495;&#20043;&#22806;&#65292;&#22312;DP-RP&#23478;&#26063;&#20013;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;iDP&#65288;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#65289;&#30340;&#27010;&#24565;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;iDP&#19981;&#26159;&#20005;&#26684;&#30340;DP&#65292;&#20294;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65288;&#22914;&#21521;&#23567;&#32452;&#29992;&#25143;&#21457;&#24067;&#21253;&#25324;&#23884;&#20837;&#20449;&#24687;&#25110;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20869;&#23481;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#27844;&#38706;&#19981;&#23646;&#20110;&#35813;&#32452;&#30340;&#20010;&#20154;&#30340;&#20219;&#20309;&#31169;&#20154;&#20449;&#24687;&#65289;&#21487;&#33021;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a series of differential privacy (DP) algorithms from a family of random projections (RP), for general applications in machine learning, data mining, and information retrieval. Among the presented algorithms, \textbf{iDP-SignRP} is remarkably effective under the setting of ``individual differential privacy'' (iDP), based on sign random projections (SignRP). Also, \textbf{DP-SignOPORP} considerably improves existing algorithms in the literature under the standard DP setting, using ``one permutation + one random projection'' (OPORP), where OPORP is a variant of the celebrated count-sketch method with fixed-length binning and normalization. Without taking signs, among the DP-RP family, \textbf{DP-OPORP} achieves the best performance.  The concept of iDP (individual differential privacy) is defined only on a particular dataset of interest. While iDP is not strictly DP, iDP might be useful in certain applications, such as releasing a dataset (including sharing embe
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#20005;&#37325;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25171;&#30772;&#36825;&#20123;&#35884;&#35265;&#21644;&#24320;&#21457;&#26356;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01744</link><description>&lt;p&gt;
&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#25171;&#30772;XAI&#31070;&#35805;-&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Disproving XAI Myths with Formal Methods -- Initial Results. (arXiv:2306.01744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01744
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#20005;&#37325;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25171;&#30772;&#36825;&#20123;&#35884;&#35265;&#21644;&#24320;&#21457;&#26356;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#23637;&#26082;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21448;&#28145;&#36828;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#20154;&#20204;&#23545;&#26368;&#20339;&#34920;&#29616;&#30340;ML&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#20449;&#20219;&#32570;&#20047;&#30340;&#24433;&#21709;&#12290;&#22312;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20351;&#29992;ML&#27169;&#22411;&#26102;&#65292;&#32570;&#20047;&#20449;&#20219;&#30340;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#20026;&#25552;&#20379;&#21487;&#20449;&#36182;AI&#32780;&#36827;&#34892;&#30340;&#19981;&#26029;&#21162;&#21147;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;XAI&#20805;&#26021;&#30528;&#20851;&#38190;&#35823;&#35299;&#65292;&#36825;&#20123;&#35823;&#35299;&#21161;&#38271;&#20102;&#19981;&#20449;&#20219;&#32780;&#19981;&#26159;&#24314;&#31435;&#20449;&#20219;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#26126;&#26174;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25512;&#32763;&#36825;&#20123;&#35823;&#35299;&#65292;&#20197;&#21450;&#35774;&#35745;&#23454;&#38469;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in Machine Learning (ML) in recent years have been both impressive and far-reaching. However, the deployment of ML models is still impaired by a lack of trust in how the best-performing ML models make predictions. The issue of lack of trust is even more acute in the uses of ML models in high-risk or safety-critical domains. eXplainable artificial intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy AI. Unfortunately, XAI is riddled with critical misconceptions, that foster distrust instead of building trust. This paper details some of the most visible misconceptions in XAI, and shows how formal methods have been used, both to disprove those misconceptions, but also to devise practically effective alternatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.01742</link><description>&lt;p&gt;
&#36229;&#36234;&#28040;&#26497;&#24773;&#32490;&#65306;&#20851;&#20110;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#30340;&#37325;&#26032;&#20998;&#26512;&#21644;&#21518;&#32493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection. (arXiv:2306.01742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#19987;&#23478;&#20204;&#35748;&#20026;&#65292;&#24076;&#26395;&#22312;&#22686;&#24378;&#20010;&#20154;&#30340;&#36523;&#24515;&#20581;&#24247;&#12289;&#20419;&#36827;&#24247;&#22797;&#21644;&#24674;&#22797;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24076;&#26395;&#35328;&#35770;&#26159;&#25351;&#22312;&#35780;&#35770;&#12289;&#24086;&#23376;&#21644;&#20854;&#20182;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20013;&#25552;&#20379;&#25903;&#25345;&#12289;&#23433;&#24944;&#12289;&#24314;&#35758;&#12289;&#21551;&#31034;&#21644;&#35265;&#35299;&#30340;&#35328;&#35770;&#12290;&#24076;&#26395;&#35328;&#35770;&#30340;&#26816;&#27979;&#28041;&#21450;&#36825;&#31181;&#25991;&#26412;&#20869;&#23481;&#30340;&#20998;&#26512;&#65292;&#26088;&#22312;&#35782;&#21035;&#33021;&#22815;&#21796;&#36215;&#20154;&#20204;&#31215;&#26497;&#24773;&#32490;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#20844;&#24320;&#22312; https://github.com/aflah02/Hope_Speech_Detection &#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health experts assert that hope plays a crucial role in enhancing individuals' physical and mental well-being, facilitating their recovery, and promoting restoration. Hope speech refers to comments, posts and other social media messages that offer support, reassurance, suggestions, inspiration, and insight. The detection of hope speech involves the analysis of such textual content, with the aim of identifying messages that invoke positive emotions in people. Our study aims to find computationally efficient yet comparable/superior methods for hope speech detection. We also make our codebase public at https://github.com/aflah02/Hope_Speech_Detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24490;&#29615;&#22270;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01494</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Local Message Passing on Frustrated Systems. (arXiv:2306.01494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24490;&#29615;&#22270;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#22270;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26159;&#27010;&#29575;&#25512;&#29702;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26159;&#27714;&#21644;-&#31215;&#31639;&#27861;&#65288;SPA&#65289;&#65292;&#23427;&#22312;&#26641;&#19978;&#21487;&#24471;&#21040;&#31934;&#30830;&#32467;&#26524;&#65292;&#20294;&#22312;&#20855;&#26377;&#35768;&#22810;&#23567;&#24490;&#29615;&#30340;&#22270;&#19978;&#24448;&#24448;&#22833;&#36133;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26367;&#20195;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36825;&#31181;&#24490;&#29615;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;SPA&#30340;&#22806;&#37096;&#21407;&#21017;&#65292;&#36825;&#22833;&#21435;&#20102;&#22312;&#24490;&#29615;&#22270;&#19978;&#30340;&#23458;&#35266;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23558;&#24213;&#23618;&#22270;&#30340;&#22240;&#23376;&#33410;&#28857;&#22788;&#30340;&#26412;&#22320;SPA&#28040;&#24687;&#26356;&#26032;&#35268;&#21017;&#26367;&#25442;&#20026;&#36890;&#29992;&#26144;&#23556;&#12290;&#36825;&#20123;&#20462;&#25913;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;SPA&#30340;&#31616;&#21333;&#24615;&#12290;&#25105;&#20204;&#23545;&#20004;&#31867;&#24490;&#29615;&#22270;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;2x2&#23436;&#20840;&#36830;&#25509;&#30340;Ising&#32593;&#26684;&#21644;&#32447;&#24615;&#36890;&#20449;&#36890;&#36947;&#19978;&#30340;&#31526;&#21495;&#26816;&#27979;&#22240;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing on factor graphs is a powerful framework for probabilistic inference, which finds important applications in various scientific domains. The most wide-spread message passing scheme is the sum-product algorithm (SPA) which gives exact results on trees but often fails on graphs with many small cycles. We search for an alternative message passing algorithm that works particularly well on such cyclic graphs. Therefore, we challenge the extrinsic principle of the SPA, which loses its objective on graphs with cycles. We further replace the local SPA message update rule at the factor nodes of the underlying graph with a generic mapping, which is optimized in a data-driven fashion. These modifications lead to a considerable improvement in performance while preserving the simplicity of the SPA. We evaluate our method for two classes of cyclic graphs: the 2x2 fully connected Ising grid and factor graphs for symbol detection on linear communication channels with inter-symbol interf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01474</link><description>&lt;p&gt;
&#36890;&#29992;&#31561;&#21464;Transformer&#65306;&#29992;&#20110;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#35768;&#22810;&#36807;&#31243;&#28041;&#21450;&#19981;&#21516;&#20998;&#23376;&#20043;&#38388;&#30340;&#21508;&#31181;3D&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#19982;&#34507;&#30333;&#36136;&#65292;&#34507;&#30333;&#36136;&#19982;&#23567;&#20998;&#23376;&#31561;&#12290;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26469;&#23398;&#20064;&#26222;&#36866;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20998;&#23376;&#36890;&#24120;&#20197;&#19981;&#21516;&#31890;&#24230;&#34920;&#31034;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#23558;3D&#20998;&#23376;&#36890;&#29992;&#34920;&#31034;&#20026;&#38598;&#21512;&#30340;&#20960;&#20309;&#22270;&#24418;&#22270;&#65292;&#19982;&#20256;&#32479;&#21333;&#23618;&#34920;&#31034;&#24418;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#34920;&#31034;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#31561;&#21464;Transformer&#65288;GET&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#31232;&#30095;&#22359;&#32423;&#21644;&#23494;&#38598;&#21407;&#23376;&#32423;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GET&#30001;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#32452;&#25104;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#20197;&#28385;&#36275;3D&#19990;&#30028;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;GET&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#32454;&#24494;&#30340;&#30315;&#30187;&#30149;&#28790;&#36827;&#34892;&#35782;&#21035;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01375</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#29992;&#20110;&#32454;&#24494;&#30315;&#30187;&#30149;&#28790;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach. (arXiv:2306.01375v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#32454;&#24494;&#30340;&#30315;&#30187;&#30149;&#28790;&#36827;&#34892;&#35782;&#21035;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#32454;&#24494;&#30340;&#23616;&#38480;&#24615;&#30382;&#36136;&#21457;&#32946;&#19981;&#33391;&#65288;FCD&#65289;&#30149;&#21464;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#28155;&#21152;&#36741;&#21161;&#25439;&#22833;&#21644;&#24369;&#30417;&#30563;&#20998;&#31867;&#25439;&#22833;&#30340;&#26041;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. "Ground truth" manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex- or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based feature
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.01121</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24046;&#20998;&#38544;&#31169;&#24335;&#24773;&#33410;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;(DP)&#38480;&#21046;&#19979;&#30340;&#37325;&#23614;&#22870;&#21169;&#30340;&#65288;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#31169;&#26377;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22870;&#21169;&#26469;&#33258;&#19968;&#20123;&#26377;&#30028;&#25110;&#27425;&#39640;&#26031;&#20998;&#24067;&#20197;&#30830;&#20445;DP&#30456;&#27604;&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#20998;&#24067;&#21482;&#26377;&#26377;&#38480;&#30340;$(1+v)$&#38454;&#30697;&#30340;&#24773;&#20917;&#65292;$v \in (0,1]$&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#30340;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#37325;&#23614;MDP&#30340;&#26694;&#26550;&#65292;&#21363;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;(JDP)&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;(LDP)&#27169;&#22411;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;JDP&#21644;LDP&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#65292;&#24182;&#34920;&#26126;&#20998;&#24067;&#30340;&#30697;&#21644;&#38544;&#31169;&#39044;&#31639;&#37117;&#23545;&#36951;&#25022;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.00980</link><description>&lt;p&gt;
SnapFusion&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#20004;&#31186;&#20869;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. (arXiv:2306.00980v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21019;&#24314;&#20986;&#24778;&#20154;&#30340;&#22270;&#20687;&#65292;&#19981;&#20122;&#20110;&#19987;&#19994;&#33402;&#26415;&#23478;&#21644;&#25668;&#24433;&#24072;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36739;&#22823;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#21313;&#20010;&#21435;&#22122;&#36845;&#20195;&#65292;&#20351;&#20854;&#35745;&#31639;&#26114;&#36149;&#19988;&#36816;&#34892;&#32531;&#24930;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#39640;&#31471;GPU&#21644;&#22522;&#20110;&#20113;&#30340;&#25512;&#29702;&#26469;&#25353;&#27604;&#20363;&#36816;&#34892;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#25968;&#25454;&#21457;&#36865;&#21040;&#31532;&#19977;&#26041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#19981;&#21040;2&#31186;&#38047;&#20869;&#35299;&#38145;&#20102;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by explori
&lt;/p&gt;</description></item><item><title>STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.00937</link><description>&lt;p&gt;
STEVE-1: &#19968;&#20010;&#29992;&#20110;Minecraft&#20013;&#25991;&#26412;-&#34892;&#20026;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00937
&lt;/p&gt;
&lt;p&gt;
STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#25991;&#26412;&#25351;&#20196;&#20570;&#20986;&#21709;&#24212;&#30340;AI&#27169;&#22411;&#23545;&#20110;&#36830;&#32493;&#24615;&#20915;&#31574;&#20219;&#21153;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STEVE-1&#30340;Minecraft&#25351;&#20196;&#35843;&#25972;&#22411;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;DALL-E 2&#20013;&#20351;&#29992;&#30340;unCLIP&#26041;&#27861;&#20063;&#23545;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#38750;&#24120;&#26377;&#25928;&#12290;STEVE-1&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#35757;&#32451;&#65306;&#39318;&#20808;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;VPT&#27169;&#22411;&#36866;&#24212;MineCLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25351;&#20196;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#20197;&#20174;&#25991;&#26412;&#39044;&#27979;&#28508;&#22312;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;VPT&#65292;&#36991;&#20813;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25991;&#26412;&#27880;&#37322;&#12290;&#36890;&#36807;&#21033;&#29992;VPT&#21644;MineCLIP&#31561;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;STEVE-1&#30340;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;60&#32654;&#20803;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Minecraft&#20013;&#36981;&#24490;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#20026;&#24320;&#25918;&#30340;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.00630</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#31867;&#38170;&#28857;&#36793;&#36317;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Class Anchor Margin Loss for Content-Based Image Retrieval. (arXiv:2306.00630v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23481;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#26816;&#32034;&#65288;CBIR&#65289;&#20013;&#30340;&#24615;&#33021;&#21463;&#25152;&#36873;&#30340;&#25439;&#22833;&#65288;&#30446;&#26631;&#65289;&#20989;&#25968;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#31070;&#32463;&#27169;&#22411;&#30340;&#22823;&#22810;&#25968;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20998;&#20026;&#24230;&#37327;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20064;&#20004;&#31867;&#12290;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#25104;&#23545;&#25366;&#25496;&#31574;&#30053;&#65292;&#36825;&#24448;&#24448;&#32570;&#20047;&#25928;&#29575;&#65292;&#32780;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20854;&#38388;&#25509;&#29305;&#24449;&#20248;&#21270;&#32780;&#26080;&#27861;&#29983;&#25104;&#39640;&#24230;&#21387;&#32553;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21364;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#30001;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34987;&#21560;&#24341;&#21040;&#21508;&#33258;&#25351;&#23450;&#30340;&#21487;&#23398;&#20064;&#31867;&#38170;&#28857;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#32452;&#20998;&#23545;&#38170;&#28857;&#36827;&#34892;&#35843;&#33410;&#65292;&#24378;&#21046;&#23427;&#20204;&#30456;&#20114;&#20043;&#38388;&#26377;&#19968;&#23450;&#38388;&#38548;&#65292;&#32780;&#31532;&#19977;&#20010;&#30446;&#26631;&#30830;&#20445;&#38170;&#28857;&#19981;&#20250;&#23849;&#28291;&#20026;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;&#65292;&#23427;&#19981;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#25104;&#23545;&#36317;&#31163;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of neural networks in content-based image retrieval (CBIR) is highly influenced by the chosen loss (objective) function. The majority of objective functions for neural models can be divided into metric learning and statistical learning. Metric learning approaches require a pair mining strategy that often lacks efficiency, while statistical learning approaches are not generating highly compact features due to their indirect feature optimization. To this end, we propose a novel repeller-attractor loss that falls in the metric learning paradigm, yet directly optimizes for the L2 metric without the need of generating pairs. Our loss is formed of three components. One leading objective ensures that the learned features are attracted to each designated learnable class anchor. The second loss component regulates the anchors and forces them to be separable by a margin, while the third objective ensures that the anchors do not collapse to zero. Furthermore, we develop a more eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.00575</link><description>&lt;p&gt;
&#22312;&#38654;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#22797;&#21046;&#30340;&#26102;&#38388;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#38654;&#29615;&#22659;&#30340;&#22909;&#22788;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#20301;&#32622;&#38750;&#24120;&#37325;&#35201;&#12290;&#30450;&#30446;&#25110;&#21453;&#24212;&#24335;&#30340;&#25968;&#25454;&#22797;&#21046;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#38654;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#39044;&#27979;&#23458;&#25143;&#31471;&#20309;&#26102;&#20309;&#22320;&#36830;&#25509;&#12290;&#34429;&#28982;&#31354;&#38388;&#39044;&#27979;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26102;&#38388;&#39044;&#27979;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23558;&#26102;&#38388;&#39044;&#27979;&#32435;&#20837;&#29616;&#26377;&#31354;&#38388;&#39044;&#27979;&#27169;&#22411;&#30340;&#20248;&#21183;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#22312;&#39044;&#27979;&#22797;&#21046;&#30340;&#32972;&#26223;&#19979;&#23545;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39034;&#24207;&#21644;&#21608;&#26399;&#24615;&#29992;&#25143;&#31227;&#21160;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#36712;&#36857;&#30340;&#38654;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20313;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;15&#65285;&#30340;&#38477;&#20302;&#65292;&#32780;&#25968;&#25454;&#21487;&#29992;&#24615;&#21482;&#26377;1&#65285;&#30340;&#24494;&#23567;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00488</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#22312;&#30830;&#23450;&#21160;&#24577;&#27169;&#24335;&#12289;&#21453;&#24605;&#39044;&#38450;&#25514;&#26045;&#21644;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#37325;&#35201;&#65292;&#20294;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#30001;&#20110;&#30149;&#24577;&#12289;&#29190;&#28856;&#24615;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#32780;&#20855;&#26377;&#26497;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#29992;&#20110;&#25193;&#25955;&#21382;&#21490;&#37325;&#24314;&#12290;&#23427;&#20204;&#20165;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20844;&#24335;&#65292;&#38656;&#35201;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26356;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#21333;&#20010;&#24555;&#29031;&#65288;DASH&#65289;&#20013;&#37325;&#24314;&#25193;&#25955;&#21382;&#21490;&#65292;&#25105;&#20204;&#35797;&#22270;&#20165;&#20174;&#26368;&#32456;&#24555;&#29031;&#37325;&#24314;&#21382;&#21490;&#65292;&#32780;&#19981;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;MLE&#20844;&#24335;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00114</link><description>&lt;p&gt;
&#21152;&#25343;&#22823;&#20892;&#30000;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#20892;&#19994;&#22810;&#26102;&#30456;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#26032;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#30417;&#27979;&#22303;&#22320;&#35206;&#30422;&#26159;&#30740;&#31350;&#29615;&#22659;&#21464;&#21270;&#21644;&#36890;&#36807;&#31918;&#39135;&#20135;&#37327;&#39044;&#27979;&#30830;&#20445;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#23588;&#20854;&#26159;&#65292;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#21160;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#21487;&#38752;&#12289;&#32454;&#31890;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26679;&#26412;&#25903;&#25345;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21463;&#30410;&#20110;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;10&#20010;&#20892;&#20316;&#29289;&#31867;&#21035;&#30340;78,536&#20010;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;(10&#31859;/&#20687;&#32032;&#65292;640 x 640&#31859;)&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#20102;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;(2017-2020)&#21644;&#20116;&#20010;&#26376;&#20221;(&#20845;&#26376;-&#21313;&#26376;)&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#21253;&#21547;12&#20010;&#20809;&#35889;&#27874;&#27573;&#12289;&#19968;&#24352;RGB&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#26893;&#34987;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20934;&#20984;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#20984;&#21487;&#34892;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#21629;&#20013;&#29575;&#12290; &#22312;&#38050;&#38081;&#29983;&#20135;&#30340;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#20351;&#21629;&#20013;&#29575;&#25552;&#39640;&#33267;&#23569;41.11&#65285;&#21644;31.01&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.20003</link><description>&lt;p&gt;
&#22522;&#20110;&#21629;&#20013;&#29575;&#30340;&#40657;&#30418;&#36807;&#31243;&#36136;&#37327;&#20248;&#21270;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Black Box Process Quality Optimization Approach based on Hit Rate. (arXiv:2305.20003v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20934;&#20984;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#20984;&#21487;&#34892;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#21629;&#20013;&#29575;&#12290; &#22312;&#38050;&#38081;&#29983;&#20135;&#30340;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#20351;&#21629;&#20013;&#29575;&#25552;&#39640;&#33267;&#23569;41.11&#65285;&#21644;31.01&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#20013;&#29575;&#26159;&#32508;&#21512;&#24037;&#19994;&#36807;&#31243;&#20013;&#39044;&#27979;&#20135;&#21697;&#36136;&#37327;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65292;&#23427;&#34920;&#31034;&#22312;&#36136;&#37327;&#25511;&#21046;&#33539;&#22260;&#20869;&#34987;&#19979;&#28216;&#22788;&#29702;&#36807;&#31243;&#25509;&#21463;&#30340;&#20135;&#21697;&#30340;&#30334;&#20998;&#27604;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#21629;&#20013;&#29575;&#26159;&#19968;&#20010;&#38750;&#20984;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20934;&#20984;&#26041;&#27861;&#65292;&#23558;&#38454;&#20056;&#38544;&#24335;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#12289;&#22810;&#20219;&#21153;&#24377;&#24615;&#32593;&#32476;&#21644;&#20934;&#20984;&#20248;&#21270;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#20984;&#21487;&#34892;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#21629;&#20013;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;Monte Carlo&#27169;&#25311;&#21644;&#22312;&#38050;&#38081;&#29983;&#20135;&#20013;&#36827;&#34892;&#30340;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#20102;&#20984;&#20248;&#21270;&#29305;&#24615;&#21644;&#20934;&#20984;&#21069;&#27839;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20351;&#21629;&#20013;&#29575;&#25552;&#39640;&#20102;&#33267;&#23569;41.11%&#21644;31.01%&#12290;&#27492;&#22806;&#65292;&#20934;&#20984;&#21069;&#27839;&#20026;&#35299;&#20915;&#26041;&#26696;&#30340;&#24694;&#21270;&#25552;&#20379;&#20102;&#21442;&#32771;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hit rate is a key performance metric in predicting process product quality in integrated industrial processes. It represents the percentage of products accepted by downstream processes within a controlled range of quality. However, optimizing hit rate is a non-convex and challenging problem. To address this issue, we propose a data-driven quasi-convex approach that combines factorial hidden Markov models, multitask elastic net, and quasi-convex optimization. Our approach converts the original non-convex problem into a set of convex feasible problems, achieving an optimal hit rate. We verify the convex optimization property and quasi-convex frontier through Monte Carlo simulations and real-world experiments in steel production. Results demonstrate that our approach outperforms classical models, improving hit rates by at least 41.11% and 31.01% on two real datasets. Furthermore, the quasi-convex frontier provides a reference explanation and visualization for the deterioration of solution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19663</link><description>&lt;p&gt;
Vandermonde&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#21463;&#27426;&#36814;&#30340;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#25805;&#20316;&#31526;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;PDE&#20013;&#20986;&#29616;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;FNO&#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#20197;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#65292;&#25152;&#20197;&#35813;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20165;&#38480;&#20110;&#31515;&#21345;&#23572;&#32593;&#26684;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;FNO&#25512;&#24191;&#21040;&#22788;&#29702;&#20998;&#24067;&#22312;&#38750;&#22343;&#21248;&#28857;&#20998;&#24067;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;Vandermonde&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;VNO&#65289;&#65292;&#21033;&#29992;Vandermonde&#32467;&#26500;&#30697;&#38453;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21363;&#20351;&#22312;&#20219;&#24847;&#20998;&#24067;&#30340;&#28857;&#19978;&#20063;&#21487;&#20197;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;VNO&#21487;&#20197;&#27604;FNO&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#27604;&#30340;&#38750;&#22343;&#21248;&#26041;&#27861;&#65288;&#22914;Geo-FNO&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.19591</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#65306;&#36817;&#26399;&#36827;&#23637;&#19982;&#26032;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#32531;&#35299;&#20840;&#29699;&#24615;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#36127;&#38754;&#24433;&#21709;&#21253;&#25324;&#39069;&#22806;&#26053;&#34892;&#26102;&#38388;&#30340;&#25439;&#22833;&#21644;&#29123;&#26009;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#20132;&#36890;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20132;&#36890;&#39044;&#27979;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#36817;&#26399;&#36827;&#23637;&#21644;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#65292;&#36825;&#26159;&#30001;&#20110;&#36817;&#24180;&#26469;&#36825;&#31867;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#25104;&#21151;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.19550</link><description>&lt;p&gt;
Spotlight Attention: &#20855;&#22791;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#30340;&#40065;&#26834;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior. (arXiv:2305.19550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#30340;&#30446;&#30340;&#26159;&#26500;&#24314;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#26174;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#26159;&#36890;&#36807;&#19968;&#32452;&#21487;&#20114;&#25442;&#30340;&#27169;&#22359;(&#31216;&#20026;slot&#25110;&#23545;&#35937;&#25991;&#20214;)&#33719;&#24471;&#30340;&#65292;&#23427;&#20204;&#31454;&#20105;&#22270;&#20687;&#30340;&#23616;&#37096;&#34917;&#19969;&#12290;&#35813;&#31454;&#20105;&#20855;&#26377;&#24369;&#24863;&#24615;&#20559;&#24046;&#65292;&#20197;&#20445;&#25345;&#31354;&#38388;&#36830;&#32493;&#24615;;&#22240;&#27492;&#65292;&#19968;&#20010;slot&#21487;&#33021;&#20250;&#23459;&#31216;&#22312;&#25972;&#20010;&#22270;&#20687;&#20013;&#25955;&#24067;&#30340;&#34917;&#19969;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20154;&#31867;&#35270;&#35273;&#30340;&#24863;&#24615;&#20559;&#24046;&#24456;&#24378;&#65292;&#21040;&#20102;&#27880;&#24847;&#21147;&#32463;&#20856;&#29992;&#32858;&#20809;&#28783;&#27604;&#21947;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#34701;&#20837;&#29616;&#20195;&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26174;&#30528;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#12290;&#31867;&#20284;&#20110;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#22270;&#20687;&#20869;&#23481;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#32452;&#21512;&#20135;&#29983;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26080;&#30417;&#30563;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#65292;&#21253;&#25324;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of object-centric vision is to construct an explicit representation of the objects in a scene. This representation is obtained via a set of interchangeable modules called \emph{slots} or \emph{object files} that compete for local patches of an image. The competition has a weak inductive bias to preserve spatial continuity; consequently, one slot may claim patches scattered diffusely throughout the image. In contrast, the inductive bias of human vision is strong, to the degree that attention has classically been described with a spotlight metaphor. We incorporate a spatial-locality prior into state-of-the-art object-centric vision models and obtain significant improvements in segmenting objects in both synthetic and real-world datasets. Similar to human visual attention, the combination of image content and spatial constraints yield robust unsupervised object-centric learning, including less sensitivity to model hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20303;&#23429;&#33021;&#37327;&#28789;&#27963;&#24615;&#30340;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#38598;&#20013;&#21270;&#20294;&#20998;&#35299;&#30340;&#35780;&#35770;&#23478;&#8221;&#22312;&#25191;&#34892;&#21069;&#36827;&#34892;&#21327;&#35843;&#25490;&#32451;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.18875</link><description>&lt;p&gt;
&#38598;&#20013;&#21270;&#21327;&#20316;&#30340;&#20998;&#25955;&#24335;&#25490;&#32451;&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20303;&#23429;&#33021;&#37327;&#28789;&#27963;&#24615;&#21487;&#25193;&#23637;&#21327;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility. (arXiv:2305.18875v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20303;&#23429;&#33021;&#37327;&#28789;&#27963;&#24615;&#30340;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#38598;&#20013;&#21270;&#20294;&#20998;&#35299;&#30340;&#35780;&#35770;&#23478;&#8221;&#22312;&#25191;&#34892;&#21069;&#36827;&#34892;&#21327;&#35843;&#25490;&#32451;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#23454;&#29616;&#20303;&#23429;&#33021;&#37327;&#28789;&#27963;&#24615;&#30340;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21327;&#35843;&#12290;&#20998;&#24067;&#24335;&#36164;&#28304;&#30340;&#21327;&#35843;&#65292;&#22914;&#30005;&#21160;&#27773;&#36710;&#21644;&#20379;&#26262;&#65292;&#23558;&#23545;&#25104;&#21151;&#25972;&#21512;&#22823;&#35268;&#27169;&#21487;&#20877;&#29983;&#33021;&#28304;&#21040;&#25105;&#20204;&#30340;&#30005;&#21147;&#32593;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20943;&#32531;&#27668;&#20505;&#21464;&#21270;&#12290;&#39044;&#20808;&#23398;&#20064;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#20998;&#24067;&#24335;&#25511;&#21046;&#65292;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#20849;&#20139;&#20010;&#20154;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#26029;&#22686;&#21152;&#35757;&#32451;&#35745;&#31639;&#36127;&#25285;&#65292;&#38543;&#30528;&#31995;&#32479;&#35268;&#27169;&#30340;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#38598;&#20013;&#21270;&#20294;&#20998;&#35299;&#30340;&#35780;&#35770;&#23478;&#8221;&#22312;&#25191;&#34892;&#21069;&#36827;&#34892;&#21327;&#35843;&#25490;&#32451;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21327;&#35843;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#23569;&#30340;&#20449;&#24687;&#21644;&#36890;&#35759;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how deep multi-agent reinforcement learning can enable the scalable and privacy-preserving coordination of residential energy flexibility. The coordination of distributed resources such as electric vehicles and heating will be critical to the successful integration of large shares of renewable energy in our electricity grid and, thus, to help mitigate climate change. The pre-learning of individual reinforcement learning policies can enable distributed control with no sharing of personal data required during execution. However, previous approaches for multi-agent reinforcement learning-based distributed energy resources coordination impose an ever greater training computational burden as the size of the system increases. We therefore adopt a deep multi-agent actor-critic method which uses a \emph{centralised but factored critic} to rehearse coordination ahead of execution. Results show that coordination is achieved at scale, with minimal information and communica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.18444</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#25552;&#31034;&#30340;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#30340;&#25345;&#32493;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#19981;&#26029;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26469;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#30340;&#20803;&#31574;&#30053;&#65292;&#26159;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36830;&#32493;&#20219;&#21153;&#20998;&#37197;&#30340;&#31232;&#30095;&#25552;&#31034;&#65288;CoTASP&#65289;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#23376;&#32593;&#32476;&#21644;&#25552;&#31034;&#65292;CoTASP&#26356;&#26032;&#20102;&#20803;&#31574;&#30053;&#65292;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#26356;&#26032;&#23383;&#20856;&#65292;&#20197;&#20351;&#20248;&#21270;&#21518;&#30340;&#25552;&#31034;&#19982;&#20219;&#21153;&#23884;&#20837;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#25429;&#25417;&#20854;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#20219;&#21153;&#36890;&#36807;&#30456;&#20284;&#30340;&#25552;&#31034;&#22312;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#20849;&#20139;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36328;&#20219;&#21153;&#24178;&#25200;&#23548;&#33268;&#36951;&#24536;&#34987;&#26377;&#25928;&#22320;&#32422;&#26463;&#12290;&#32473;&#23450;&#32463;&#36807;&#35757;&#32451;&#30340;&#20803;&#31574;&#30053;&#21644;&#26356;&#26032;&#21518;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#25552;&#31034;&#26469;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#20013;&#25552;&#21462;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23548;&#33322;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CoTASP&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20219;&#21153;&#23436;&#25104;&#24230;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18402</link><description>&lt;p&gt;
&#31070;&#32463;&#38613;&#22609;&#65306;&#36890;&#36807;&#20462;&#21098;&#21644;&#32593;&#32476;&#20998;&#26512;&#25581;&#31034;&#20998;&#23618;&#27169;&#22359;&#21270;&#20219;&#21153;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30446;&#26631;&#20989;&#25968;&#21644;&#20219;&#21153;&#36890;&#24120;&#34920;&#29616;&#20026;&#20998;&#23618;&#27169;&#22359;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20989;&#25968;&#20197;&#20998;&#23618;&#32452;&#32455;&#12290;&#36825;&#20123;&#23376;&#20989;&#25968;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#29305;&#24449;&#65306;&#23427;&#20204;&#26377;&#19968;&#32452;&#19981;&#21516;&#30340;&#36755;&#20837;&#65288;&#36755;&#20837;&#21487;&#20998;&#31163;&#24615;&#65289;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#23618;&#27425;&#20013;&#20316;&#20026;&#36755;&#20837;&#34987;&#37325;&#29992;&#65288;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#31435;&#20102;&#20998;&#23618;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23398;&#20064;&#25928;&#29575;&#12289;&#27867;&#21270;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#22914;&#20309;&#35782;&#21035;&#28508;&#22312;&#30340;&#23376;&#20989;&#25968;&#21450;&#20854;&#20998;&#23618;&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20197;&#25581;&#31034;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#24067;&#23572;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#31070;&#32463;&#38613;&#22609;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20219;&#21153;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26356;&#23481;&#26131;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18394</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#23398;&#20064;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#24120;&#29992;&#20110;&#35299;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20808;&#39564;&#20449;&#24687;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#21442;&#25968;&#21152;&#20197;&#26435;&#34913;&#65292;&#32780;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#20363;&#22914;&#24046;&#24322;&#21407;&#21017;&#21644;L-&#26354;&#32447;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#21512;&#36866;&#30340;&#21442;&#25968;&#20540;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#21483;&#20570;&#21452;&#23618;&#23398;&#20064;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#20110;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#31574;&#30053;&#26377;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21452;&#23618;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#26465;&#20214;&#26469;&#34920;&#24449;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#27491;&#20540;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.17626</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#33021;&#21147;&#20043;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#26032;&#30340;&#24773;&#20917;&#19982;&#36807;&#21435;&#30340;&#32463;&#39564;&#20851;&#32852;&#26469;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#12290;&#34429;&#28982;&#23427;&#34987;&#35748;&#20026;&#23545;&#20110;AI&#31995;&#32479;&#30340;&#24378;&#22823;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;/&#25110;&#22266;&#21270;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#25165;&#33021;&#24212;&#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#21457;&#29616;&#20154;&#31867;&#35821;&#35328;&#19982;&#31867;&#27604;&#21046;&#20316;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#30452;&#35266;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#25277;&#35937;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#31867;&#27604;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#35270;&#35273;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#31616;&#21333;&#22320;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;PLMs&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#24182;&#25509;&#36817;&#20110;&#21463;&#30417;&#30563;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#21464;&#21270;&#25277;&#35937;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17553</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#22833;&#36133;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#25215;&#35834;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#36731;&#35760;&#24518;&#38169;&#35823;&#25110;&#36807;&#26102;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#24341;&#20837;&#22823;&#37327;&#26410;&#34987;&#29616;&#26377;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#26816;&#27979;&#21040;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;CounterFact&#22522;&#20934;&#27979;&#35797;&#20197;&#21253;&#25324;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#31216;&#20026;CounterFact+&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#26412;&#36136;&#25351;&#26631;&#25193;&#23637;&#20102;&#29992;&#20110;&#34913;&#37327;&#29305;&#24322;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#29305;&#24322;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20984;&#26174;&#20102;&#38656;&#35201;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#20197;&#35782;&#21035;&#21644;&#39044;&#38450;&#19981;&#33391;&#21103;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17535</link><description>&lt;p&gt;
PFN&#26159;&#36866;&#29992;&#20110;&#23454;&#38469;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;(PFNs)&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#30340;&#28789;&#27963;&#20195;&#29702;&#12290;PFN&#26159;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#65292;&#34987;&#35757;&#32451;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;(PPD)&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#26377;&#25928;&#37319;&#26679;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#36827;&#34892;BO&#30340;&#20195;&#29702;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;PFN&#26469;&#27169;&#25311;&#19968;&#20010;&#26420;&#32032;&#39640;&#26031;&#36807;&#31243;(GP)&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;GP&#21644;&#19968;&#20010;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNN)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36827;&#19968;&#27493;&#30340;&#20449;&#24687;&#32435;&#20837;&#20808;&#39564;&#65292;&#20363;&#22914;&#20801;&#35768;&#26377;&#20851;&#26368;&#20248;&#20301;&#32622;&#30340;&#25552;&#31034;(&#29992;&#25143;&#20808;&#39564;)&#65292;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#26469;&#25191;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#28789;&#27963;&#24615;&#20026;&#20351;&#29992;PFN&#36827;&#34892;BO&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#27979;&#35797;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;PFN&#23545;BO&#30340;&#26377;&#29992;&#24615;&#65306;HPO-B&#12289;Bayesmark&#21644;PD1&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#24615;mixup&#36890;&#36807;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#25552;&#39640;&#35757;&#32451;&#20998;&#24067;&#65292;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16817</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#28151;&#21512;&#26377;&#21161;&#20110;&#24212;&#23545;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#19981;&#20165;&#20165;&#26159;&#22240;&#20026;&#28151;&#21512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16817
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;mixup&#36890;&#36807;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#25552;&#39640;&#35757;&#32451;&#20998;&#24067;&#65292;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#39640;&#24230;&#25104;&#21151;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#37197;&#23545;&#30340;&#32452;&#21512;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#36873;&#25321;&#24615;mixup&#26159;&#19968;&#31995;&#21015;&#23558;mixup&#24212;&#29992;&#20110;&#29305;&#23450;&#23545;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#20165;&#22312;&#31867;&#21035;&#25110;&#39046;&#22495;&#20043;&#38388;&#32452;&#21512;&#31034;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#22768;&#31216;&#22312;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#30340;&#26426;&#21046;&#21644;&#38480;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36873;&#25321;&#24615;mixup&#30340;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20174;&#19968;&#20010;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#20250;&#24433;&#21709;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#19982;&#28151;&#21512;&#25216;&#26415;&#23436;&#20840;&#26080;&#20851;&#30340;&#26041;&#24335;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#31867;&#21035;&#20043;&#38388;&#30340;mixup&#38544;&#21547;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#37325;&#37319;&#26679;&#65292;&#20197;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;&#36825;&#31181;&#38544;&#21547;&#37325;&#37319;&#26679;&#35299;&#37322;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#22823;&#37096;&#20998;&#25913;&#36827;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#20381;&#36182;&#20110;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#38656;&#35201;&#21306;&#20998;&#30495;&#27491;&#30340;&#37325;&#37319;&#26679;&#21644;&#28151;&#21512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.16583</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#22238;&#24402;&#27169;&#22411;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22256;&#25200;&#30528;&#35768;&#22810;&#25968;&#20540;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25968;&#25454;&#35760;&#24405;&#30340;&#20540;&#21487;&#33021;&#30001;&#20110;&#38169;&#35823;&#30340;&#20256;&#24863;&#22120;&#12289;&#25968;&#25454;&#36755;&#20837;/&#22788;&#29702;&#38169;&#35823;&#25110;&#19981;&#23436;&#32654;&#30340;&#20154;&#31867;&#20272;&#35745;&#31561;&#21407;&#22240;&#32780;&#26080;&#27861;&#21305;&#37197;&#30495;&#23454;&#30340;&#24213;&#23618;&#20540;&#12290;&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#27839;&#25968;&#20540;&#21015;&#21738;&#20123;&#25968;&#25454;&#20540;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#65288;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#20854;&#20182;&#21464;&#37327;&#26469;&#39044;&#27979;&#35813;&#21015;&#20540;&#30340;&#32479;&#35745;&#23398;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#26465;&#20214;&#26159;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#31526;&#21512;&#24615;&#25512;&#26029;&#65289;&#38590;&#20197;&#26816;&#27979;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35823;&#24046;&#26816;&#27979;&#22522;&#20934;&#65292;&#28041;&#21450; 5 &#20010;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#38169;&#35823;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#65288;&#23545;&#20110;&#20854;&#20013;&#30340;&#30495;&#23454;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16304</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26088;&#22312;&#25214;&#21040;&#26368;&#21305;&#37197;&#32473;&#23450;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;(&#21253;&#25324;&#21442;&#32771;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;)&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#39044;&#20808;&#35745;&#31639;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#32463;&#36807;&#26597;&#35810;&#25991;&#26412;&#20462;&#25913;&#30340;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#30701;&#25991;&#26412;&#25551;&#36848;&#24341;&#23548;&#20462;&#25913;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#29420;&#31435;&#20110;&#28508;&#22312;&#30340;&#20505;&#36873;&#39033;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#20801;&#35768;&#26597;&#35810;&#21644;&#27599;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21363;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#65292;&#24182;&#20174;&#25972;&#20010;&#38598;&#21512;&#20013;&#36873;&#25321;&#26368;&#20339;&#21305;&#37197;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26356;&#20855;&#26377;&#21028;&#21035;&#24615;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#19981;&#33021;&#39044;&#20808;&#35745;&#31639;&#20505;&#36873;&#23884;&#20837;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#26159;&#31105;&#27490;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#26696;&#30340;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;</title><link>http://arxiv.org/abs/2305.15877</link><description>&lt;p&gt;
&#25351;&#25968;&#24179;&#28369;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#26469;&#23547;&#25214;&#25913;&#36827;&#30340;&#31574;&#30053;&#65292;&#36890;&#24120;&#20351;&#29992;&#35760;&#24405;&#30340;&#36172;&#21338;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;IPS&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#26159;&#21487;&#22788;&#29702;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#26631;&#20934;IPS&#65292;&#22240;&#27492;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#20309;&#26102;&#27491;&#21017;&#21270;IPS&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#21363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#12290;&#36825;&#19982;&#22312;&#23454;&#36341;&#20013;&#65292;&#21098;&#36753;IPS&#24120;&#24120;&#27604;OPL&#20013;&#30340;&#26631;&#20934;IPS&#34920;&#29616;&#26356;&#22909;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#32467;&#26500;&#28857;&#20113;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#26684;&#28857;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#20854;&#26356;&#39640;&#32500;&#24230;&#34920;&#36798;&#65292;&#20197;&#25552;&#39640;&#21307;&#30103;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15426</link><description>&lt;p&gt;
&#36229;&#36234;&#26684;&#28857;&#65306;&#22522;&#20110;&#28857;&#20113;&#21644;&#34920;&#38754;&#34920;&#31034;&#30340;&#31070;&#32463;&#23398;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing. (arXiv:2305.15426v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#32467;&#26500;&#28857;&#20113;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#26684;&#28857;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#20854;&#26356;&#39640;&#32500;&#24230;&#34920;&#36798;&#65292;&#20197;&#25552;&#39640;&#21307;&#30103;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#20934;&#30830;&#20998;&#31867;&#21307;&#23398;&#22270;&#20687;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20855;&#26377;&#19968;&#33268;&#32593;&#26684;&#32467;&#26500;&#30340;&#21307;&#23398;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#36817;&#26399;&#21307;&#23398;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#35843;&#25972;&#20307;&#31995;&#32467;&#26500;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#25968;&#25454;&#30340;&#34920;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#26684;&#28857;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#20854;&#26356;&#39640;&#32500;&#24230;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26080;&#32467;&#26500;&#28857;&#20113;&#25968;&#25454;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#20687;&#32032;&#39068;&#33394;&#20449;&#24687;&#20316;&#20026;&#31354;&#38388;&#22352;&#26631;&#26469;&#20174;&#22270;&#20687;&#29983;&#25104;&#31232;&#30095;&#28857;&#20113;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#28857;&#32452;&#25104;&#30340;&#36229;&#34920;&#38754;&#65292;&#22522;&#20110;&#22270;&#20687;&#23610;&#23544;&#65292;&#36229;&#34920;&#38754;&#20869;&#30340;&#27599;&#20010;&#24179;&#28369;&#37096;&#20998;&#37117;&#34920;&#31034;&#29305;&#23450;&#30340;&#20687;&#32032;&#20301;&#32622;&#12290;&#22810;&#36793;&#24418;&#38754;&#26500;&#36896;&#26159;&#36890;&#36807;&#37051;&#25509;&#24352;&#37327;&#23436;&#25104;&#30340;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23494;&#38598;&#37319;&#26679;&#26500;&#36896;&#30340;&#36229;&#34920;&#38754;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;&#28857;&#20113;&#65292;&#37325;&#28857;&#26159;&#37325;&#26032;&#37319;&#26679;&#20197;&#23454;&#29616;&#20998;&#31867;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, accurately classifying medical images is vital, but conventional methods often hinge on medical data with a consistent grid structure, which may restrict their overall performance. Recent medical research has been focused on tweaking the architectures to attain better performance without giving due consideration to the representation of data. In this paper, we present a novel approach for transforming grid based data into its higher dimensional representations, leveraging unstructured point cloud data structures. We first generate a sparse point cloud from an image by integrating pixel color information as spatial coordinates. Next, we construct a hypersurface composed of points based on the image dimensions, with each smooth section within this hypersurface symbolizing a specific pixel location. Polygonal face construction is achieved using an adjacency tensor. Finally, a dense point cloud is generated by densely sampling the constructed hypersurface, with a focus on re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#20013;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;&#12290;&#31639;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#24615;&#36136;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#21512;&#23398;&#20064;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15148</link><description>&lt;p&gt;
&#29702;&#35770;&#25351;&#23548;&#30340;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Theoretically Principled Federated Learning for Balancing Privacy and Utility. (arXiv:2305.15148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#20013;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;&#12290;&#31639;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#24615;&#36136;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#21512;&#23398;&#20064;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#23454;&#29616;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#23558;&#25197;&#26354;&#26144;&#23556;&#21040;&#23454;&#20540;&#30340;&#38544;&#31169;&#27979;&#37327;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#23427;&#21487;&#20197;&#20026;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#21644;&#32454;&#31890;&#24230;&#30340;&#20445;&#25252;&#21487;&#20197;&#25552;&#39640;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#21512;&#23398;&#20064;&#30340;&#25928;&#21147;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#20445;&#25252;&#36229;&#21442;&#25968;&#30340;&#25928;&#29992;&#25439;&#22833;&#19982;&#26368;&#20248;&#20445;&#25252;&#36229;&#21442;&#25968;&#30340;&#25928;&#29992;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#27425;&#32447;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#27425;&#32447;&#24615;&#30340;&#29305;&#28857;&#34920;&#26126;&#65292;&#24403;&#36845;&#20195;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#31639;&#27861;&#24615;&#33021;&#21644;&#26368;&#20248;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#22343;&#24046;&#36317;&#36235;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general learning framework for the protection mechanisms that protects privacy via distorting model parameters, which facilitates the trade-off between privacy and utility. The algorithm is applicable to arbitrary privacy measurements that maps from the distortion to a real value. It can achieve personalized utility-privacy trade-off for each model parameter, on each client, at each communication round in federated learning. Such adaptive and fine-grained protection can improve the effectiveness of privacy-preserved federated learning.  Theoretically, we show that gap between the utility loss of the protection hyperparameter output by our algorithm and that of the optimal protection hyperparameter is sub-linear in the total number of iterations. The sublinearity of our algorithm indicates that the average gap between the performance of our algorithm and that of the optimal performance goes to zero when the number of iterations goes to infinity. Further, we provide the conv
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27973;&#23618;&#30340;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#20026;&#27599;&#19968;&#23618;&#35745;&#31639;&#33258;&#36866;&#24212;&#27493;&#38271;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#35813;&#31574;&#30053;&#30340;&#31639;&#27861;&#22312;DNN&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13664</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning. (arXiv:2305.13664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27973;&#23618;&#30340;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#20026;&#27599;&#19968;&#23618;&#35745;&#31639;&#33258;&#36866;&#24212;&#27493;&#38271;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#35813;&#31574;&#30053;&#30340;&#31639;&#27861;&#22312;DNN&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#29992;&#20110;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289; &#27973;&#23618;&#20013;&#21253;&#21547;&#30340;&#23545;&#35282;&#32447;&#22359;&#30340;&#23618;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#26469;&#35745;&#31639;&#27599;&#19968;&#23618;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#65288;&#21363;&#23398;&#20064;&#29575;&#65289;&#12290;&#35813;&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#24403;&#65292;&#32780;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20165;&#22686;&#21152;&#20102;&#32422;&#31561;&#20110;&#21478;&#19968;&#20010;&#26799;&#24230;&#35745;&#31639;&#37327;&#30340;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#25152;&#25552;&#20986;&#30340;&#20998;&#23618;&#27493;&#24133;&#22823;&#23567;&#30340;SGD&#21160;&#37327;&#27861;&#21644;AdamW&#33021;&#22815;&#36873;&#25321;&#26377;&#25928;&#30340;&#23398;&#20064;&#29575;&#36827;&#24230;&#65292;&#24182;&#22312;Autoencoder&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20219;&#21153;&#30340;DNN&#35757;&#32451;&#20013;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new per-layer adaptive step-size procedure for stochastic first-order optimization methods for minimizing empirical loss functions in deep learning, eliminating the need for the user to tune the learning rate (LR). The proposed approach exploits the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian in deep neural networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular first-order and second-order algorithms for training DNNs on Autoencoder, Convolutional Neural Network (CN
&lt;/p&gt;</description></item><item><title>INVICTUS&#26159;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#25628;&#32034;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#32508;&#21512;&#37197;&#26041;&#20197;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13164</link><description>&lt;p&gt;
INVICTUS: &#36890;&#36807;&#21327;&#21516;&#23398;&#20064;&#21644;&#25628;&#32034;&#20248;&#21270;&#24067;&#23572;&#36923;&#36753;&#30005;&#36335;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search. (arXiv:2305.13164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13164
&lt;/p&gt;
&lt;p&gt;
INVICTUS&#26159;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#25628;&#32034;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#32508;&#21512;&#37197;&#26041;&#20197;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#32508;&#21512;&#26159;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20063;&#26159;&#26368;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;INVICTUS&#65292;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#24050;&#26377;&#30340;&#35774;&#35745;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#8220;&#32508;&#21512;&#37197;&#26041;&#8221;&#65289;&#65292;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic synthesis is the first and most vital step in chip design. This steps converts a chip specification written in a hardware description language (such as Verilog) into an optimized implementation using Boolean logic gates. State-of-the-art logic synthesis algorithms have a large number of logic minimization heuristics, typically applied sequentially based on human experience and intuition. The choice of the order greatly impacts the quality (e.g., area and delay) of the synthesized circuit. In this paper, we propose INVICTUS, a model-based offline reinforcement learning (RL) solution that automatically generates a sequence of logic minimization heuristics ("synthesis recipe") based on a training dataset of previously seen designs. A key challenge is that new designs can range from being very similar to past designs (e.g., adders and multipliers) to being completely novel (e.g., new processor instructions). %Compared to prior work, INVICTUS is the first solution that uses a mix of R
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#36801;&#31227;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#26041;&#27861;&#22312;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;DL&#26041;&#27861;&#21487;&#20197;&#25509;&#36817;&#22522;&#20110;&#26641;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.12432</link><description>&lt;p&gt;
&#22810;&#36824;&#26159;&#23569;&#26679;&#26412;&#65311;&#22312;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#20013;&#27604;&#36739;&#36801;&#31227;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification. (arXiv:2305.12432v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#36801;&#31227;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#26041;&#27861;&#22312;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;DL&#26041;&#27861;&#21487;&#20197;&#25509;&#36817;&#22522;&#20110;&#26641;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27969;&#34892;&#65292;&#21152;&#19978;&#30001;&#20110;HTTPS&#12289;QUIC&#21644;DNS-SEC&#30340;&#24191;&#27867;&#37319;&#29992;&#23548;&#33268;&#32593;&#32476;&#27969;&#37327;&#21487;&#35265;&#24615;&#38477;&#20302;&#65292;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#27969;&#37327;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25511;&#21046;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#23547;&#25214;&#26356;&#22909;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36328;&#20219;&#21153;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36801;&#31227;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#19982;&#21442;&#32771;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22522;&#20110;&#26641;&#21644;&#21333;&#29255;&#24335;DL&#27169;&#22411;&#65288;&#24635;&#20849;16&#31181;&#26041;&#27861;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;MIRAGE19&#65288;40&#31867;&#65289;&#21644;AppClassNet&#65288;500&#31867;&#65289;&#65292;&#25105;&#20204;&#34920;&#26126;&#65288;i&#65289;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#26356;&#36890;&#29992;&#30340;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#22909;&#30340;&#26041;&#27861;&#65292;&#65288;iii&#65289;&#20803;&#23398;&#20064;&#26159;&#26368;&#24046;&#30340;&#26041;&#27861;&#65292;&#65288;iv&#65289;&#34429;&#28982;ML&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#19981;&#33021;&#22788;&#29702;&#22823;&#22411;&#20219;&#21153;&#65292;&#20294;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;DL&#26041;&#27861;&#27491;&#22312;&#25509;&#36817;&#22522;&#20110;&#26641;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;2D&#21644;3D&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21407;&#23376;&#31867;&#22411;&#12289;&#21270;&#23398;&#38190;&#20449;&#24687;&#21644;3D&#22352;&#26631;&#30340;&#23436;&#25972;&#20998;&#23376;&#65292;&#24182;&#33021;&#25429;&#25417;2D&#38190;&#21512;&#22270;&#21644;3D&#20998;&#23376;&#20960;&#20309;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.12347</link><description>&lt;p&gt;
&#23398;&#20064;&#32852;&#21512;2D&#21644;3D&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23436;&#25972;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Joint 2D &amp; 3D Diffusion Models for Complete Molecule Generation. (arXiv:2305.12347v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;2D&#21644;3D&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21407;&#23376;&#31867;&#22411;&#12289;&#21270;&#23398;&#38190;&#20449;&#24687;&#21644;3D&#22352;&#26631;&#30340;&#23436;&#25972;&#20998;&#23376;&#65292;&#24182;&#33021;&#25429;&#25417;2D&#38190;&#21512;&#22270;&#21644;3D&#20998;&#23376;&#20960;&#20309;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26032;&#20998;&#23376;&#23545;&#20110;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26088;&#22312;&#27169;&#25311;&#20998;&#23376;&#20998;&#24067;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#32553;&#23567;&#21270;&#23398;&#30740;&#31350;&#39046;&#22495;&#21644;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#20998;&#23376;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#27169;&#25311;2D&#38190;&#21512;&#22270;&#25110;3D&#20960;&#20309;&#65292;&#36825;&#26159;&#20998;&#23376;&#30340;&#20004;&#20010;&#20114;&#34917;&#25551;&#36848;&#31526;&#12290;&#32852;&#21512;&#24314;&#27169;&#33021;&#21147;&#30340;&#19981;&#36275;&#38480;&#21046;&#20102;&#29983;&#25104;&#36136;&#37327;&#30340;&#25552;&#39640;&#21644;&#36827;&#19968;&#27493;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;2D&#21644;3D&#25193;&#25955;&#27169;&#22411;&#65288;JODO&#65289;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21407;&#23376;&#31867;&#22411;&#12289;&#21270;&#23398;&#38190;&#20449;&#24687;&#21644;3D&#22352;&#26631;&#30340;&#23436;&#25972;&#20998;&#23376;&#12290;&#20026;&#20102;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25429;&#33719;&#20998;&#23376;&#22270;&#21644;&#20960;&#20309;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25193;&#25955;&#22270;&#21464;&#25442;&#22120;&#26469;&#21442;&#25968;&#21270;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#24674;&#22797;&#21407;&#22987;&#25968;&#25454;&#12290;&#25193;&#25955;&#22270;&#21464;&#25442;&#22120;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30456;&#20114;&#20316;&#29992;&#33410;&#28857;&#21644;&#36793;&#32536;&#29305;&#24449;&#65292;&#20351;&#24471;JODO&#33021;&#22815;&#25429;&#25417;2D&#38190;&#21512;&#22270;&#21644;3D&#20998;&#23376;&#20960;&#20309;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing new molecules is essential for drug discovery and material science. Recently, deep generative models that aim to model molecule distribution have made promising progress in narrowing down the chemical research space and generating high-fidelity molecules. However, current generative models only focus on modeling either 2D bonding graphs or 3D geometries, which are two complementary descriptors for molecules. The lack of ability to jointly model both limits the improvement of generation quality and further downstream applications. In this paper, we propose a new joint 2D and 3D diffusion model (JODO) that generates complete molecules with atom types, formal charges, bond information, and 3D coordinates. To capture the correlation between molecular graphs and geometries in the diffusion process, we develop a Diffusion Graph Transformer to parameterize the data prediction model that recovers the original data from noisy data. The Diffusion Graph Transformer interacts node and ed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#26159;&#23548;&#33268;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#25552;&#20986;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#24182;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10964</link><description>&lt;p&gt;
&#23398;&#20064;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#26159;&#23548;&#33268;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#25552;&#20986;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#24182;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#25512;&#26029;&#26102;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#33021;&#37327;&#21644;&#20869;&#23384;&#65292;&#21516;&#26102;&#21487;&#20197;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#22312;&#39640;&#20462;&#21098;&#27604;&#29575;&#19979;SNN&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#21487;&#33021;&#22312;&#20851;&#38190;&#37096;&#32626;&#26465;&#20214;&#19979;&#25104;&#20026;&#38382;&#39064;&#12290; &#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#20462;&#21098;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#20851;&#27880;&#34987;&#24573;&#30053;&#30340;&#22240;&#32032;&#65306;&#36229;&#21442;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#21487;&#20197;&#39069;&#22806;&#24402;&#22240;&#20110;&#65288;i&#65289;&#26222;&#36941;&#20351;&#29992;ReLU&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#40664;&#35748;&#36873;&#25321;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;&#19982;&#23494;&#38598;&#32593;&#32476;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#26469;&#24494;&#35843;SNN&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#31232;&#30095;&#32593;&#32476;&#30340;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290; &#36890;&#36807;&#23545;&#22312;MNIST&#19978;&#35757;&#32451;&#30340;&#27969;&#34892;DNN&#27169;&#22411;&#65288;LeNet-5&#65292;VGG-16&#65292;ResNet-18&#21644;EfficientNet-B0&#65289;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#23376;&#33258;&#21160;&#21270;&#35774;&#35745;&#20013;&#39640;&#32423;&#32508;&#21512;&#24037;&#20855;&#30340;&#20248;&#21270;&#65292;&#24182;&#20419;&#36827;&#39046;&#22495;&#29305;&#23450;&#21152;&#36895;&#22120;&#65288;DSAs&#65289;&#30340;&#35774;&#35745;&#33258;&#21160;&#21270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.10838</link><description>&lt;p&gt;
ProgSG&#65306;&#29992;&#20110;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#31243;&#24207;&#30340;&#36328;&#27169;&#24577;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation. (arXiv:2305.10838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#23376;&#33258;&#21160;&#21270;&#35774;&#35745;&#20013;&#39640;&#32423;&#32508;&#21512;&#24037;&#20855;&#30340;&#20248;&#21270;&#65292;&#24182;&#20419;&#36827;&#39046;&#22495;&#29305;&#23450;&#21152;&#36895;&#22120;&#65288;DSAs&#65289;&#30340;&#35774;&#35745;&#33258;&#21160;&#21270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39046;&#22495;&#29305;&#23450;&#21152;&#36895;&#22120;&#65288;DSAs&#65289;&#65288;&#20363;&#22914;Google&#30340;TPUs&#65289;&#22312;&#21152;&#36895;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#12289;&#25628;&#32034;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#65289;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20026;&#20102;&#20419;&#36827;DSA&#35774;&#35745;&#65292;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#65288;HLS&#65289;&#65292;&#23427;&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#23558;C&#21644;C ++&#36719;&#20214;&#20195;&#30721;&#20013;&#30340;&#39640;&#32423;&#25551;&#36848;&#32534;&#35793;&#20026;&#20302;&#32423;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;&#20363;&#22914;VHDL&#25110;Verilog&#65289;&#19978;&#30340;&#35774;&#35745;&#65292;&#24182;&#26368;&#32456;&#21512;&#25104;&#20026;ASIC&#25110;FPGA&#19978;&#30340;DSA&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HLS&#24037;&#20855;&#20173;&#38656;&#35201;&#24494;&#26550;&#26500;&#20915;&#31574;&#65292;&#20197;pragma&#65288;&#20363;&#22914;&#24182;&#34892;&#21270;&#21644;&#27969;&#27700;&#32447;&#25351;&#20196;&#65289;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#26356;&#22810;&#20154;&#35774;&#35745;DSA&#65292;&#24076;&#26395;&#33021;&#22815;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#21270;&#20570;&#20986;&#36825;&#20123;&#20915;&#31574;&#20197;&#39044;&#27979;HLS&#35774;&#35745;&#30340;&#36136;&#37327;&#12290;&#36825;&#38656;&#35201;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#31243;&#24207;&#65292;&#21363;&#21407;&#22987;&#20195;&#30721;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the growing popularity of domain-specific accelerators (DSAs), such as Google's TPUs, for accelerating various applications such as deep learning, search, autonomous driving, etc. To facilitate DSA designs, high-level synthesis (HLS) is used, which allows a developer to compile a high-level description in the form of software code in C and C++ into a design in low-level hardware description languages (such as VHDL or Verilog) and eventually synthesized into a DSA on an ASIC (application-specific integrated circuit) or FPGA (field-programmable gate arrays). However, existing HLS tools still require microarchitecture decisions, expressed in terms of pragmas (such as directives for parallelization and pipelining). To enable more people to design DSAs, it is desirable to automate such decisions with the help of deep learning for predicting the quality of HLS designs. This requires us a deeper understanding of the program, which is a combination of original code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;GNNs&#20013;&#30340;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#12290;&#36890;&#36807;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#25512;&#23548;&#23427;&#20204;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#35299;&#20915;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#20013;&#30340;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10758</link><description>&lt;p&gt;
&#20174;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;GNNs&#20013;&#30340;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#12290;&#36890;&#36807;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#25512;&#23548;&#23427;&#20204;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#35299;&#20915;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#20013;&#30340;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22788;&#29702;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21487;&#23454;&#29616;&#30340;&#25512;&#26029;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;MLPs&#20173;&#28982;&#26159;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#30340;&#20027;&#21147;&#20891;&#12290;&#20026;&#20102;&#32553;&#23567;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#31934;&#24515;&#35774;&#35745;&#30340;&#25945;&#24072;GNN&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#23398;&#29983;MLP&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;GNN-to-MLP&#33976;&#39311;&#12290;&#20294;&#26159;&#65292;&#33976;&#39311;&#30340;&#36807;&#31243;&#36890;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#25439;&#22833;&#65292;&#8220;&#21738;&#20123;GNN&#30340;&#30693;&#35782;&#27169;&#24335;&#26356;&#21487;&#33021;&#20250;&#34987;&#20445;&#30041;&#24182;&#33976;&#39311;&#21040;MLP&#20013;&#65311;&#8221;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#39057;&#35889;&#22495;&#20013;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#28982;&#21518;&#25512;&#23548;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#23384;&#22312;&#28508;&#22312;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;GNNs&#30340;&#39640;&#39057;&#30693;&#35782;&#21487;&#33021;&#34987;&#20302;&#39057;&#30693;&#35782;&#25152;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.10133</link><description>&lt;p&gt;
Lingo3DMol:&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25512;&#21160;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#22791;&#21463;&#30633;&#30446;&#12290; &#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#20108;&#32500;&#32467;&#26500;&#20013;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21017;&#21487;&#20197;&#30452;&#25509;&#20135;&#29983;&#20855;&#26377;&#20934;&#30830;&#19977;&#32500;&#22352;&#26631;&#30340;&#20998;&#23376;&#12290;&#21463;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#19977;&#32500;&#22352;&#26631;&#30340;&#33021;&#21147;&#12290; &#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#25968;&#25454;&#19981;&#36275;&#65292;&#22240;&#27492;&#35774;&#35745;&#20102;&#19968;&#31181;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#30340;&#23567;&#20998;&#23376;&#25968;&#25454;&#12290; &#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#24102;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#22352;&#26631;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;SMILES&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#21644;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#12290;&#26368;&#32456;&#65292;CrossDocked&#21644;DUD-E&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.09938</link><description>&lt;p&gt;
&#22270;&#20013;&#38271;&#23614;&#31867;&#21035;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21253;&#25324;&#37329;&#34701;&#20132;&#26131;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#21512;&#20316;&#32593;&#32476;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#22686;&#24378;&#25110;&#30446;&#26631;&#37325;&#26032;&#21152;&#26435;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#25991;&#29486;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#26469;&#34920;&#24449;&#22270;&#19978;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#29702;&#35299;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21363;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#39044;&#27979;&#19968;&#20010;&#29305;&#23450;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#38271;&#23614;&#20998;&#31867;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#25152;&#26377;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#33539;&#22260;&#21644;&#20219;&#21153;&#24635;&#25968;&#30340;&#25903;&#37197;&#12290;&#22312;&#29702;&#35770;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08062</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#24314;&#27169;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20256;&#32479;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#26041;&#24040;&#30340;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26041;&#24040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;OffCEM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#27169;&#22411;&#65288;CEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25928;&#24212;&#20998;&#20026;&#32676;&#38598;&#25928;&#24212;&#21644;&#27531;&#24046;&#25928;&#24212;&#12290;OffCEM&#20165;&#23545;&#34892;&#21160;&#32676;&#38598;&#24212;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#20272;&#35745;&#22120;&#26159;&#26080;&#20559;&#30340;&#65292;&#35813;&#26465;&#20214;&#20165;&#35201;&#27714;&#27531;&#24046;&#25928;&#24212;&#27169;&#22411;&#20445;&#30041;&#27599;&#20010;&#32676;&#38598;&#20013;&#34892;&#21160;&#30340;&#30456;&#23545;&#26399;&#26395;&#22870;&#21169;&#24046;&#24322;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;CEM&#21644;&#26412;&#22320;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#36807;&#31243;&#65292;&#29992;&#20110;&#25191;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#31532;&#19968;&#27493;&#26368;&#23567;&#21270;&#20559;&#24046;&#65292;&#31532;&#20108;&#27493;&#26368;&#23567;&#21270;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;OPE&#20272;&#35745;&#22120;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07247</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#30340;Schr\"odinger bridge&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Schr\"odinger bridge&#38382;&#39064;&#65288;SBP&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#30340;&#25237;&#24433;&#26159;&#21807;&#19968;&#21487;&#29992;&#30340;&#65292;&#20854;&#25910;&#25947;&#24615;&#36824;&#19981;&#26159;&#21313;&#20998;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;SBP&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#23637;&#31034;&#20102;&#20248;&#21270;&#20256;&#36755;&#25104;&#26412;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06395</link><description>&lt;p&gt;
ACTC: &#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#20381;&#36182;&#20110;&#20272;&#35745;&#24471;&#20998;&#27169;&#22411;(&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;)-&#20803;&#32452;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#23884;&#20837;&#21021;&#22987;&#30693;&#35782;&#22270;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#38408;&#20540;(&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#31034;&#20363;)&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#36136;&#37327;&#12290;&#26412;&#25991;&#23581;&#35797;&#39318;&#27425;&#38024;&#23545;KGC&#36827;&#34892;&#20919;&#21551;&#21160;&#26657;&#20934;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#21021;&#22987;&#27809;&#26377;&#27880;&#37322;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;ACTC&#22522;&#20110;&#26377;&#38480;&#30340;&#27880;&#37322;&#20803;&#32452;&#26377;&#25928;&#22320;&#25214;&#21040;&#22909;&#30340;&#27599;&#20010;&#20851;&#31995;&#30340;&#38408;&#20540;&#12290;&#38500;&#20102;&#19968;&#20123;&#27880;&#37322;&#30340;&#20803;&#32452;&#22806;&#65292;ACTC&#36824;&#21033;&#29992;Logistic&#22238;&#24402;&#25110;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#22120;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20803;&#32452;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23494;&#24230;&#21644;&#38543;&#26426;&#36873;&#25321;&#31561;&#19981;&#21516;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#35780;&#20998;&#27169;&#22411;&#21644;&#19968;&#20010;oracle&#27880;&#37322;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-SpecPart&#30340;&#36229;&#22270;&#21010;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#25429;&#25417;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#22312;&#22810;&#20803;&#21010;&#20998;&#20013;&#25552;&#39640;&#31639;&#27861;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.06167</link><description>&lt;p&gt;
K-SpecPart: &#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#36229;&#22270;&#21010;&#20998;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#30340;&#30417;&#30563;&#35889;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement. (arXiv:2305.06167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-SpecPart&#30340;&#36229;&#22270;&#21010;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#25429;&#25417;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#22312;&#22810;&#20803;&#21010;&#20998;&#20013;&#25552;&#39640;&#31639;&#27861;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36229;&#22270;&#21010;&#20998;&#22120;&#37319;&#29992;&#22810;&#23618;&#27425;&#31574;&#30053;&#65292;&#26500;&#24314;&#22810;&#20010;&#26356;&#31895;&#31961;&#30340;&#36229;&#22270;&#26469;&#36827;&#34892;&#20999;&#21106;&#23610;&#23544;&#30340;&#20248;&#21270;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65306;&#65288;&#19968;&#65289;&#31895;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#23616;&#37096;&#37051;&#22495;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#23384;&#22312;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#35889;&#26694;&#26550;&#8212;&#8212;K-SpecPart&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#25429;&#25417;&#20102;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#23618;&#27425;&#21010;&#20998;&#26041;&#26696;&#20316;&#20026;&#25552;&#31034;&#12290;&#22312;&#22810;&#20803;&#21010;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;K-SpecPart&#20174;&#22810;&#20803;&#25552;&#31034;&#21010;&#20998;&#26041;&#26696;&#20013;&#33719;&#24471;&#22810;&#20010;&#21452;&#21521;&#21010;&#20998;&#26041;&#26696;&#12290;&#23558;&#36825;&#20123;&#26041;&#26696;&#25972;&#21512;&#21040;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#20013;&#20197;&#35745;&#31639;&#29305;&#24449;&#21521;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#22823;&#32500;&#24230;&#23884;&#20837;&#12290;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#29992;&#20110;&#23558;&#20854;&#36716;&#25442;&#20026;&#20302;&#32500;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art hypergraph partitioners follow the multilevel paradigm, constructing multiple levels of coarser hypergraphs to drive cutsize refinement. These partitioners face limitations: (i) coarsening processes depend on local neighborhood structure, ignoring global hypergraph structure; (ii) refinement heuristics risk entrapment in local minima. We introduce K-SpecPart, a supervised spectral framework addressing these limitations by solving a generalized eigenvalue problem, capturing balanced partitioning objectives and global hypergraph structure in a low-dimensional vertex embedding while leveraging high-quality multilevel partitioning solutions as hints. In multi-way partitioning, K-SpecPart derives multiple bipartitioning solutions from a multi-way hint partitioning solution. It integrates these solutions into the generalized eigenvalue problem to compute eigenvectors, creating a large-dimensional embedding. Linear Discriminant Analysis (LDA) is used to transform this into a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.04837</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;Scalable Optimal Margin Distribution Machine&#65289;
&lt;/p&gt;
&lt;p&gt;
Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#26681;&#25454;&#26032;&#30340;&#36793;&#32536;&#29702;&#35770;&#24314;&#31435;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22823;&#38388;&#38548;&#30340;&#23545;&#24212;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#26680;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#26222;&#36941;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;ODM&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#24403;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;&#22823;&#37327;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26497;&#39640;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#24694;&#21270;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#65292;&#24182;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04560</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#27969;&#24418;&#30340;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65306;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach. (arXiv:2305.04560v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#65292;&#24182;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#65292;&#22914;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#21644;Grassmann&#27969;&#24418;&#65292;&#20986;&#29616;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#24212;&#29992;&#38464;&#34746;&#32676;&#21644;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#30340;&#29702;&#35770;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#21452;&#26354;&#20960;&#20309;&#30340;&#24378;&#22823;&#26694;&#26550;&#8212;&#8212;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#26500;&#24314;&#27431;&#20960;&#37324;&#24503;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32771;&#34385;&#27969;&#24418;&#30340;&#20869;&#31215;&#21644;&#38464;&#34746;&#35282;&#31561;&#27010;&#24565;&#30340;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#65292;&#30456;&#27604;&#20110;&#29992;&#20110;&#30740;&#31350;&#21452;&#26354;&#20960;&#20309;&#30340;&#37027;&#20123;&#27010;&#24565;&#65292;&#36825;&#20123;&#24037;&#20316;&#25552;&#20379;&#30340;&#25216;&#26415;&#21644;&#25968;&#23398;&#24037;&#20855;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;D-MFDAL&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#20445;&#30495;&#24230;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04392</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#35299;&#32544;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Disentangled Multi-Fidelity Deep Bayesian Active Learning. (arXiv:2305.04392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;D-MFDAL&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#20445;&#30495;&#24230;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24179;&#34913;&#36136;&#37327;&#21644;&#25104;&#26412;&#65292;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#20250;&#22312;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#19978;&#36816;&#34892;&#27169;&#25311;&#12290;&#22810;&#20445;&#30495;&#24230;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20174;&#22810;&#20010;&#20445;&#30495;&#24230;&#27700;&#24179;&#31215;&#26497;&#22320;&#33719;&#21462;&#25968;&#25454;&#26469;&#23398;&#20064;&#20174;&#36755;&#20837;&#21442;&#25968;&#21040;&#27169;&#25311;&#36755;&#20986;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38544;&#34255;&#34920;&#31034;&#20013;&#24378;&#21046;&#23454;&#26045;&#20998;&#23618;&#32467;&#26500;&#65292;&#20165;&#25903;&#25345;&#20174;&#20302;&#20445;&#30495;&#24230;&#21040;&#39640;&#20445;&#30495;&#24230;&#20256;&#36882;&#20449;&#24687;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#20174;&#20302;&#20445;&#30495;&#24230;&#34920;&#31034;&#21040;&#39640;&#20445;&#30495;&#24230;&#34920;&#31034;&#20013;&#19981;&#33391;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32544;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#65288;D-MFDAL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#20102;&#22312;&#22810;&#20010;&#20445;&#30495;&#24230;&#19978;&#20989;&#25968;&#20998;&#24067;&#30340;&#26465;&#20214;&#19979;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To balance quality and cost, various domain areas of science and engineering run simulations at multiple levels of sophistication. Multi-fidelity active learning aims to learn a direct mapping from input parameters to simulation outputs at the highest fidelity by actively acquiring data from multiple fidelity levels. However, existing approaches based on Gaussian processes are hardly scalable to high-dimensional data. Deep learning-based methods often impose a hierarchical structure in hidden representations, which only supports passing information from low-fidelity to high-fidelity. These approaches can lead to the undesirable propagation of errors from low-fidelity representations to high-fidelity ones. We propose a novel framework called Disentangled Multi-fidelity Deep Bayesian Active Learning (D-MFDAL), that learns the surrogate models conditioned on the distribution of functions at multiple fidelities. On benchmark tasks of learning deep surrogates of partial differential equatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2305.03063</link><description>&lt;p&gt;
&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25439;&#20260;&#26816;&#27979;&#26041;&#27861;&#36805;&#36895;&#20174;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#12289;&#38750;&#20405;&#20837;&#24615;&#22320;&#20272;&#35745;&#26753;&#32467;&#26500;&#29366;&#24577;&#12290;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#24005;&#23792;&#34920;&#29616;&#65292;&#20154;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#23427;&#20204;&#36866;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#32570;&#22833;&#65292;&#30001;&#20110;&#30693;&#35782;&#32534;&#30721;&#22312;&#24352;&#37327;&#20540;&#20013;&#32780;&#27809;&#26377;&#21253;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#22522;&#20110;&#26032;&#39062;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#30452;&#25509;&#23558;&#23454;&#38469;&#36923;&#36753;&#21253;&#21547;&#21040;&#27169;&#22411;&#20013;&#30340;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#12290;&#35813;&#28151;&#21512;&#21028;&#21035;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#31934;&#30830;&#22320;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#25439;&#20260;&#30340;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25928;&#29575;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#30340;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#31639;&#27861;&#65292;&#29992;&#20110;&#20805;&#20998;&#21457;&#25381;GPU&#19978;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02522</link><description>&lt;p&gt;
BitGNN&#65306;&#37322;&#25918;&#20108;&#36827;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;GPU&#19978;&#30340;&#24615;&#33021;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs. (arXiv:2305.02522v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25928;&#29575;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#30340;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#31639;&#27861;&#65292;&#29992;&#20110;&#20805;&#20998;&#21457;&#25381;GPU&#19978;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#24352;&#37327;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#20108;&#36827;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#33410;&#30465;GNN&#35745;&#31639;&#30340;&#35745;&#31639;&#37327;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#35774;&#35745;&#25110;&#35757;&#32451;&#25216;&#26415;&#19978;&#65292;&#27809;&#26377;&#23436;&#20840;&#23454;&#29616;&#23558;&#24615;&#33021;&#28508;&#21147;&#26174;&#29616;&#21040;&#21152;&#36895;&#22120;&#30828;&#20214;&#19978;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20174;&#25928;&#29575;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#20102;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25277;&#35937;&#21644;&#25216;&#26415;&#65292;&#20197;&#26368;&#20339;&#22320;&#26144;&#23556;&#20108;&#36827;&#21046;GNN&#21450;&#20854;&#35745;&#31639;&#65292;&#20197;&#36866;&#24212;GPU&#19978;&#30340;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GCNs&#12289;GraphSAGE&#21644;GraphSAINT&#30340;&#30495;&#23454;&#22270;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#12290;BitGNN&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that Binary Graph Neural Networks (GNNs) are promising for saving computations of GNNs through binarized tensors. Prior work, however, mainly focused on algorithm designs or training techniques, leaving it open to how to materialize the performance potential on accelerator hardware fully. This work redesigns the binary GNN inference backend from the efficiency perspective. It fills the gap by proposing a series of abstractions and techniques to map binary GNNs and their computations best to fit the nature of bit manipulations on GPUs. Results on real-world graphs with GCNs, GraphSAGE, and GraphSAINT show that the proposed techniques outperform state-of-the-art binary GNN implementations by 8-22X with the same accuracy maintained. BitGNN code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01610</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#25506;&#27979;&#20013;&#23547;&#25214;&#28023;&#37327;&#31070;&#32463;&#20803;: &#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24212;&#29992;&#21644;&#37096;&#32626;&#36805;&#36895;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#35745;&#31639;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39640;&#32423;&#21487;&#35299;&#37322;&#29305;&#24449;&#22312;LLM&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;(&#25506;&#38024;)&#26469;&#35757;&#32451;&#36825;&#20123;&#20869;&#37096;&#28608;&#27963;&#20540;&#65292;&#24182;&#39044;&#27979;&#36755;&#20837;&#30340;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65307;&#36890;&#36807;&#25913;&#21464;$k$&#20540;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#24403;$k=1$&#26102;&#65292;&#25105;&#20204;&#23450;&#20301;&#26576;&#20010;&#29305;&#23450;&#29305;&#24449;&#38750;&#24120;&#30456;&#20851;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;LLM&#30340;&#19968;&#33324;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#35768;&#22810;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#20284;&#20046;&#20855;&#26377;&#19987;&#38376;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#26356;&#39640;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#32780;&#22686;&#21152;&#30340;&#35268;&#27169;&#21017;&#23548;&#33268;&#34920;&#31034;&#31232;&#30095;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.01034</link><description>&lt;p&gt;
&#26080;&#29305;&#23450;&#27169;&#22411;&#27867;&#21270;&#38590;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24230;&#37327;&#26159;&#20854;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#38590;&#24230;&#65292;&#36275;&#22815;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#30340;&#27867;&#21270;&#38590;&#24230;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#20219;&#21153;&#22266;&#26377;&#27867;&#21270;&#38590;&#24230;&#30340;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#12290;&#36890;&#36807;&#27979;&#37327;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20551;&#35774;&#22312;&#20219;&#21153;&#20013;&#27867;&#21270;&#30340;&#20998;&#25968;&#21344;&#25454;&#30340;&#23481;&#31215;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23427;&#19982;&#27169;&#22411;&#24517;&#39035;&#27867;&#21270;&#30340;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#25968;&#25104;&#25351;&#25968;&#27604;&#20363;&#65292;&#20294;&#20165;&#22312;&#27599;&#20010;&#32500;&#24230;&#30340;&#20998;&#36776;&#29575;&#19978;&#21576;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#34920;&#26126;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#30340;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00664</link><description>&lt;p&gt;
&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#36328;&#22270;&#20256;&#36755;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#36816;&#36755;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#65292;&#32771;&#34385;&#24050;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#26631;&#31614;&#30340;&#28304;&#22270;&#21644;&#26631;&#31614;&#31232;&#30095;&#30340;&#30446;&#26631;&#22270;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#24449;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#30446;&#26631;&#22495;&#22312;&#19979;&#19968;&#20010;&#26102;&#38388;&#25139;&#30340;&#27867;&#21270;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#27867;&#21270;&#24615;&#33021;&#30001;&#39046;&#22495;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26694;&#26550;(DISC)&#26469;&#25233;&#21046;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20551;&#30456;&#20851;&#65292;&#36890;&#36807;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;DISC&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00650</link><description>&lt;p&gt;
&#21457;&#29616;&#24182;&#26657;&#27491;&#65306;&#27010;&#24565;&#24863;&#30693;&#30340;&#20551;&#30456;&#20851;&#25233;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discover and Cure: Concept-aware Mitigation of Spurious Correlation. (arXiv:2305.00650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26694;&#26550;(DISC)&#26469;&#25233;&#21046;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20551;&#30456;&#20851;&#65292;&#36890;&#36807;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;DISC&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20381;&#36182;&#20110;&#20551;&#30456;&#20851;&#26469;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#20250;&#23548;&#33268;&#26080;&#27861;&#36229;&#36234;&#35757;&#32451;&#29615;&#22659;&#30340;&#19968;&#33324;&#21270;&#12290;&#20363;&#22914;&#65292;&#23558;&#29483;&#19982;&#24202;&#20316;&#20026;&#32972;&#26223;&#32852;&#31995;&#30340;&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#24202;&#30340;&#20854;&#20182;&#29615;&#22659;&#20013;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#21040;&#29483;&#30340;&#23384;&#22312;&#12290;&#25233;&#21046;&#20551;&#30456;&#20851;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#25552;&#20379;&#26377;&#20851;&#25233;&#21046;&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;Discover and Cure (DISC)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;DISC&#36845;&#20195;&#22320; 1)&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;&#28982;&#21518; 2)&#20351;&#29992;&#21457;&#29616;&#30340;&#27010;&#24565;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#20197;&#20943;&#23569;&#20551;&#30456;&#20851;&#12290;&#22312;&#31995;&#32479;&#23454;&#39564;&#20013;&#65292;DISC&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task
&lt;/p&gt;</description></item><item><title>NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.00097</link><description>&lt;p&gt;
NNSplitter&#65306;&#22522;&#20110;&#33258;&#21160;&#26435;&#37325;&#28151;&#28102;&#30340;DNN&#27169;&#22411;&#20027;&#21160;&#38450;&#24481;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00097
&lt;/p&gt;
&lt;p&gt;
NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#24050;&#32463;&#36890;&#36807;&#25968;&#23383;&#27700;&#21360;&#31561;&#25216;&#26415;&#36827;&#34892;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34987;&#21160;&#27169;&#22411;&#20445;&#25252;&#24182;&#19981;&#33021;&#23436;&#20840;&#38450;&#27490;&#27169;&#22411;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#26696;&#65292;&#21363;NNSplitter&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#20004;&#37096;&#20998;&#26469;&#20027;&#21160;&#20445;&#25252;&#27169;&#22411;&#65306;&#19968;&#20010;&#34920;&#29616;&#36739;&#24046;&#30340;&#28151;&#28102;&#27169;&#22411;&#21644;&#30001;&#28151;&#28102;&#26435;&#37325;&#30340;&#32034;&#24341;&#21644;&#21407;&#22987;&#20540;&#32452;&#25104;&#30340;&#27169;&#22411;&#31192;&#23494;&#65292;&#21482;&#26377;&#25480;&#26435;&#29992;&#25143;&#25165;&#33021;&#35775;&#38382;&#12290;NNSplitter&#21033;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#20445;&#25252;&#31192;&#23494;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20462;&#25913;&#36229;&#36807;2800&#19975;&#20010;&#26435;&#37325;&#30340;313&#20010;&#65288;&#21363;0.001&#65285;&#65289;&#65292;&#28151;&#28102;VGG-11&#27169;&#22411;&#22312;Fashion-MNIST&#19978;&#30340;&#31934;&#24230;&#21487;&#20197;&#38477;&#20302;&#21040;10&#65285;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;NNSplitter&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.14762</link><description>&lt;p&gt;
&#21033;&#29992;&#25200;&#21160;&#26469;&#25913;&#21892;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#21270;&#26031;&#22374;&#36317;&#65288;KSD&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;&#21363;&#20351;&#30446;&#26631;&#20998;&#24067;&#20855;&#26377;&#26410;&#30693;&#30340;&#26631;&#20934;&#21270;&#22240;&#23376;&#65292;&#20363;&#22914;&#22312;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#30446;&#26631;&#20998;&#24067;&#21644;&#26367;&#20195;&#20998;&#24067;&#20855;&#26377;&#30456;&#21516;&#19988;&#30456;&#36317;&#36739;&#36828;&#30340;&#27169;&#24335;&#20294;&#22312;&#28151;&#21512;&#27604;&#20363;&#19978;&#26377;&#25152;&#19981;&#21516;&#26102;&#65292;KSD&#26816;&#39564;&#21487;&#33021;&#20250;&#20986;&#29616;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#26680;&#23545;&#35266;&#27979;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#65292;&#20351;&#20854;&#30456;&#23545;&#20110;&#30446;&#26631;&#20998;&#24067;&#19981;&#21464;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#25200;&#21160;&#26679;&#26412;&#19978;&#20351;&#29992;KSD&#26816;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#25968;&#20540;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;&#36866;&#24403;&#36873;&#25321;&#30340;&#26680;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;KSD&#26816;&#39564;&#20855;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.13972</link><description>&lt;p&gt;
&#26494;&#24347;&#20551;&#35774;&#19979;Adam&#25910;&#25947;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#24191;&#27867;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#23545;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;Adam&#65289;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#12290;&#34429;&#28982;Adam&#31639;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27969;&#34892;&#24230;&#21644;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#38656;&#35201;&#36807;&#20110;&#24378;&#30340;&#20551;&#35774;&#65292;&#22914;&#20840;&#23616;&#26799;&#24230;&#26377;&#30028;&#65292;&#20197;&#35777;&#26126;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26356;&#20026;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#20197;$\mathcal{O}(\epsilon^{-4})$&#26799;&#24230;&#22797;&#26434;&#24230;&#25910;&#25947;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#26159;&#26681;&#25454;&#19968;&#31181;&#24191;&#20041;&#20809;&#28369;&#24615;&#20551;&#35774;&#32473;&#20986;&#30340;&#65292;&#27839;&#30528;&#20248;&#21270;&#36712;&#36857;&#30340;&#26799;&#24230;&#26377;&#30028;&#30340;&#26032;&#35777;&#26126;&#12290;&#26681;&#25454;&#35813;&#20551;&#35774;&#65292;&#23616;&#37096;&#20809;&#28369;&#24615;(&#21363;&#23384;&#22312;&#26102;&#30340;Hessian norm)&#21463;&#26799;&#24230;&#33539;&#25968;&#30340;&#27425;&#24179;&#26041;&#20989;&#25968;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#32422;&#20943;&#29256;&#26412;&#30340;Adam&#19982;&#21152;&#36895;Gradient&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08996</link><description>&lt;p&gt;
&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;NOMA&#32593;&#32476;&#19979;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#21487;&#20197;&#20351;&#24471;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;FL&#37096;&#32626;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26102;&#65292;&#30001;&#20110;&#36890;&#20449;&#38142;&#36335;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;FL&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#65292;&#20934;&#30830;&#36873;&#21462;&#23458;&#25143;&#31471;&#21644;&#25511;&#21046;&#36164;&#28304;&#20998;&#37197;&#23545;&#20110;&#25552;&#39640;FL&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22312;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;NOMA&#65289;&#26080;&#32447;&#32593;&#32476;&#19978;&#27599;&#36718;FL&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#30340;&#26032;&#26087;&#31243;&#24230;&#26469;&#35774;&#35745;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24180;&#40836;&#26356;&#26032;&#65288;AoU&#65289;&#25351;&#26631;&#33719;&#24471;&#36164;&#28304;&#20998;&#37197;&#30340;&#38381;&#21512;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#26080;&#26631;&#31614;CBM&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;CBM&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06129</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Label-Free Concept Bottleneck Models. (arXiv:2304.06129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06129
&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#31614;CBM&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;CBM&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBM)&#26159;&#19968;&#31181;&#21019;&#24314;&#26356;&#26131;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#23545;&#24212;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CBM&#21450;&#20854;&#21464;&#20307;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#20010;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#25910;&#38598;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#65307;&#20854;&#27425;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;CBM&#30340;&#20934;&#30830;&#24615;&#36890;&#24120;&#26126;&#26174;&#20302;&#20110;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#26679;&#30340;&#34920;&#29616;&#20026;&#20854;&#22312;&#23454;&#38469;&#19990;&#30028;&#24212;&#29992;&#20013;&#36896;&#25104;&#19968;&#23450;&#30340;&#38556;&#30861;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#26631;&#31614;CBM&#65292;&#23427;&#26159;&#19968;&#31181;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;CBM&#30340;&#26032;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#27010;&#24565;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26080;&#26631;&#31614;CBM&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#23427;&#26159;&#21487;&#25193;&#23637;&#30340;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#25193;&#23637;&#21040;ImageNet&#30340;CBM&#65292;&#39640;&#25928;&#30340;&#8212;&#8212;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;CBM&#20165;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#65292;&#32780;&#19988;&#21487;&#20197;&#33258;&#21160;&#21270;&#36827;&#34892;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.04033</link><description>&lt;p&gt;
&#25506;&#31350;&#40065;&#26834;&#24615;&#27169;&#22411;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36890;&#36807;&#20998;&#35299;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#33021;&#37327;&#22522;&#27169;&#22411;(EBM)&#24418;&#24335;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#23545;&#25239;&#28857;&#31163;&#24320;&#20102;&#36755;&#20837;&#25968;&#25454;&#30340;&#27969;&#24418;&#65292;&#20294;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35777;&#25454;:&#38750;&#23450;&#21521;&#25915;&#20987;&#30340;&#27010;&#29575;&#29978;&#33267;&#27604;&#33258;&#28982;&#25968;&#25454;&#36824;&#35201;&#39640;&#65292;&#24182;&#19988;&#38543;&#30528;&#25915;&#20987;&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#20854;&#27010;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#26816;&#27979;&#23427;&#20204;&#24182;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#65292;&#33021;&#22815;&#27450;&#39575;&#20998;&#31867;&#22120;&#20294;&#20855;&#26377;&#19982;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.17727</link><description>&lt;p&gt;
BOLT&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17727
&lt;/p&gt;
&lt;p&gt;
BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#23545;&#20110;&#27665;&#20027;&#21270;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#30446;&#21069;&#65292;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#24191;&#27867;&#20351;&#29992;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#20363;&#22914;GPU&#65289;&#65292;&#36825;&#20123;&#21152;&#36895;&#22120;&#20165;&#38480;&#20110;&#23569;&#25968;&#20855;&#26377;&#30456;&#24403;&#36130;&#21153;&#36164;&#28304;&#30340;&#26426;&#26500;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#24102;&#26469;&#24778;&#20154;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BOLT&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;BOLT&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#29992;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#35813;API&#23545;&#20110;&#29616;&#26377;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#29087;&#24713;&#30340;&#12290;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#19987;&#29992;&#36229;&#21442;&#25968;&#65292;BOLT&#20063;&#25277;&#35937;&#25481;&#20102;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;ResNets&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23478;&#26063;&#38480;&#21046;&#26680;&#65292;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#22312;&#26080;&#38480;&#28145;&#24230;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#33267;&#31070;&#32463;CDE&#12290;</title><link>http://arxiv.org/abs/2303.17671</link><description>&lt;p&gt;
&#31070;&#32463;&#31614;&#21517;&#26680;&#20316;&#20026;&#21463;&#25511;ResNets&#30340;&#26080;&#38480;&#23485;&#24230;-&#28145;&#24230;&#26497;&#38480;&#12290;(arXiv:2303.17671v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
Neural signature kernels as infinite-width-depth-limits of controlled ResNets. (arXiv:2303.17671v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17671
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;ResNets&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23478;&#26063;&#38480;&#21046;&#26680;&#65292;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#22312;&#26080;&#38480;&#28145;&#24230;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#33267;&#31070;&#32463;CDE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27785;&#31215;&#35745;&#31639;&#33539;&#20363;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#30001;&#31070;&#32463;&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;CDE&#65289;&#30340;&#27431;&#25289;&#31163;&#25955;&#21270;&#23450;&#20041;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#21463;&#25511;ResNets&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#23485;&#24230;-&#28145;&#24230;&#38480;&#21046;&#21644;&#36866;&#24403;&#30340;&#32553;&#25918;&#19979;&#65292;&#36825;&#20123;&#26550;&#26500;&#24369;&#25910;&#25947;&#21040;&#19968;&#20123;&#36830;&#32493;&#36335;&#24452;&#31354;&#38388;&#19978;&#32034;&#24341;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#28385;&#36275;&#26681;&#25454;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#21464;&#21270;&#30340;&#26576;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26680;&#12290;&#22312;&#28608;&#27963;&#20026;&#24658;&#31561;&#24335;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#35813;&#26041;&#31243;&#24335;&#31616;&#21270;&#20026;&#32447;&#24615;PDE&#65292;&#26497;&#38480;&#26680;&#19982;Salvi&#31561;&#20154;&#30340;&#31614;&#21517;&#26680;&#19968;&#33268;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#23485;&#24230;-&#28145;&#24230;&#26497;&#38480;&#26159;&#21487;&#20132;&#25442;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#30340;&#38480;&#21046;&#26680;&#23478;&#26063;&#31216;&#20026;&#31070;&#32463;&#31614;&#21517;&#26680;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#28145;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#21463;&#25511;ResNets&#25353;&#20998;&#24067;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;CDE&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;w&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs). We show that in the infinite-width-then-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function. In the special case where the activation is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021). In this setting, we also show that the width-depth limits commute. We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SDES&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#31561;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#31354;&#38388;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15821</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#31163;&#25955;&#21270;&#28436;&#21270;&#25628;&#32034;&#30340;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search. (arXiv:2303.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SDES&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#31561;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#31354;&#38388;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#39046;&#22495;&#65292;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;(MOSGs)&#20801;&#35768;&#38450;&#24481;&#32773;&#21516;&#26102;&#20445;&#25252;&#22810;&#20010;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#12290;MOSGs&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#25152;&#26377;&#24322;&#36136;&#24615;&#22238;&#25253;&#65292;&#20363;&#22914;&#29983;&#21629;&#12289;&#37329;&#38065;&#21644;&#29359;&#32618;&#29575;&#65292;&#32780;&#19981;&#21512;&#24182;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#20445;&#25252;&#30340;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#33021;&#36229;&#20986;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;MOSGs&#21463;&#21040;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#31216;&#20026;SDES&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#25628;&#32034;&#26469;&#25193;&#23637;MOSGs&#30340;&#22823;&#35268;&#27169;&#30446;&#26631;&#21644;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#12290;SDES&#30001;&#22235;&#20010;&#36830;&#32493;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SDES&#20351;&#29992;&#21338;&#24328;&#29702;&#35770;&#20013;&#30340;&#26368;&#22823;&#19981;&#24179;&#31561;&#24615;&#21407;&#29702;&#23558;&#21407;&#22987;&#30340;&#39640;&#32500;&#36830;&#32493;&#35299;&#31354;&#38388;&#31163;&#25955;&#21270;&#20026;&#20302;&#32500;&#31163;&#25955;&#35299;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of security, multi-objective security games (MOSGs) allow defenders to simultaneously protect targets from multiple heterogeneous attackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs, e.g., life, money, and crime rate, without merging heterogeneous attackers. In real-world scenarios, the number of heterogeneous attackers and targets to be protected may exceed the capability of most existing state-of-the-art methods, i.e., MOSGs are limited by the issue of scalability. To this end, this paper proposes a general framework called SDES based on many-objective evolutionary search to scale up MOSGs to large-scale targets and heterogeneous attackers. SDES consists of four consecutive key components, i.e., discretization, optimization, restoration and evaluation, and refinement. Specifically, SDES first discretizes the originally high-dimensional continuous solution space to the low-dimensional discrete one by the maximal indifference property in game theo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31995;&#32479;&#35282;&#24230;&#23545;&#29616;&#26377;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65288;MOMs&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36873;&#25321;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#24182;&#23545;&#21508;&#31181;MOMs&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#26041;&#26696;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#36873;&#25321;&#21644;&#24212;&#29992;MOMs&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.14633</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Memory Optimization Methods for Training Neural Networks. (arXiv:2303.14633v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31995;&#32479;&#35282;&#24230;&#23545;&#29616;&#26377;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65288;MOMs&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36873;&#25321;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#24182;&#23545;&#21508;&#31181;MOMs&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#26041;&#26696;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#36873;&#25321;&#21644;&#24212;&#29992;MOMs&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65288;MOMs&#65289;&#30340;&#24320;&#21457;&#24050;&#25104;&#20026;&#35299;&#20915;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#20869;&#23384;&#29942;&#39048;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20840;&#38754;&#26816;&#26597;&#21508;&#31181;MOM&#30340;&#23454;&#38469;&#20215;&#20540;&#65292;&#25105;&#20204;&#20174;&#31995;&#32479;&#35282;&#24230;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#30740;&#31350;&#30028;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;:&#32570;&#23569;&#26631;&#20934;&#24230;&#37327;&#26469;&#26377;&#25928;&#35780;&#20272;MOM&#30340;&#21151;&#25928;&#12290;&#20449;&#24687;&#26377;&#38480;&#30340;&#35780;&#20272;&#25351;&#26631;&#22952;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21487;&#38752;&#22320;&#27604;&#36739;&#21644;&#22522;&#20934;&#19981;&#21516;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#24471;&#20986;&#26126;&#30830;&#30340;&#32467;&#35770;&#24182;&#20570;&#20986;&#20851;&#20110;&#36873;&#25321;&#21644;&#24212;&#29992;MOMs&#30340;&#30693;&#24773;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;MOM&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#35777;&#26126;&#20248;&#21183;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#35758;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21508;&#31181;MOM&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#20854;&#30456;&#23545;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;MOM&#22312;&#20943;&#23569;&#27169;&#22411;&#35757;&#32451;&#35760;&#24518;&#38656;&#27714;&#30340;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#36873;&#25321;&#21644;&#24212;&#29992;MOMs&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models continue to grow in size, the development of memory optimization methods (MOMs) has emerged as a solution to address the memory bottleneck encountered when training large models. To comprehensively examine the practical value of various MOMs, we have conducted a thorough analysis of existing literature from a systems perspective. Our analysis has revealed a notable challenge within the research community: the absence of standardized metrics for effectively evaluating the efficacy of MOMs. The scarcity of informative evaluation metrics hinders the ability of researchers and practitioners to compare and benchmark different approaches reliably. Consequently, drawing definitive conclusions and making informed decisions regarding the selection and application of MOMs becomes a challenging endeavor. To address the challenge, this paper summarizes the scenarios in which MOMs prove advantageous for model training. We propose the use of distinct evaluation metrics under different scen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#36138;&#23146;&#21098;&#26525;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;&#21487;&#20197;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.11453</link><description>&lt;p&gt;
&#22522;&#20110;Group Lasso&#30340;&#36138;&#23146;&#21098;&#26525;&#22312;&#30697;&#38453;&#24863;&#30693;&#21644;&#20108;&#27425;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#35777;&#22320;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#36138;&#23146;&#21098;&#26525;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;&#21487;&#20197;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26041;&#26696;&#24191;&#27867;&#29992;&#20110;&#38477;&#20302;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#36341;&#30740;&#31350;&#34920;&#26126;&#65292;&#20462;&#21098;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#24182;&#24494;&#35843;&#21487;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;&#34429;&#28982;&#20197;&#19978;&#34987;&#31216;&#20026;&#21098;&#26525;+&#24494;&#35843;&#30340;&#27969;&#31243;&#22312;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#29702;&#35770;&#20173;&#28982;&#19981;&#29978;&#20102;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36229;&#21442;&#25968;&#21270;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#19978;&#30340;&#21098;&#26525;+&#24494;&#35843;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#30495;&#23454;&#32467;&#26524;&#34920;&#31034;&#20026;$U_\star \in \mathbb{R}^{d \times r}$&#65292;&#32780;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#34920;&#31034;&#20026;$U \in \mathbb{R}^{d \times k}$&#65292;&#20854;&#20013;$k \gg r$&#12290;&#25105;&#20204;&#30740;&#31350;&#21152;&#19978;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#24179;&#28369;&#29256;&#26412;$\sum_{i=1}^k \| U e_i \|_2$&#30340;&#24179;&#22343;&#35823;&#24046;&#30340;&#36817;&#20284;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35777;&#26126;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;$U_{
&lt;/p&gt;
&lt;p&gt;
Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10472</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#23454;&#29992;&#21305;&#37197;&#26799;&#24230;&#26041;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#30340;&#26799;&#24230;&#26041;&#24046;&#26159;&#24314;&#31435;&#20854;&#25910;&#25947;&#24615;&#21644;&#31639;&#27861;&#25913;&#36827;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#34920;&#26126;BBVI&#30340;&#26799;&#24230;&#26041;&#24046;&#28385;&#36275;&#29992;&#20110;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26102;&#65292;BBVI&#28385;&#36275;&#19982;SGD&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;ABC&#26465;&#20214;&#30456;&#21305;&#37197;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24179;&#22343;&#22330;&#21442;&#25968;&#21270;&#30340;&#26041;&#24046;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#30340;&#20248;&#36234;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.08272</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#19987;&#21033;&#25552;&#21462;&#25903;&#25345;&#32858;&#28966;&#21270;&#23398;&#31354;&#38388;&#20869;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#21453;&#21521;&#20998;&#23376;&#35774;&#35745;&#30340;&#19968;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#25163;&#27573;&#65292;&#20854;&#36827;&#23637;&#26469;&#33258;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#20998;&#23376;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24212;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#26102;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#20855;&#26377;&#23646;&#24615;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#24050;&#21457;&#24067;&#30340;&#19987;&#21033;&#21253;&#21547;&#22312;&#20854;&#22312;&#26399;&#21002;&#19978;&#21457;&#34920;&#20043;&#21069;&#25259;&#38706;&#26032;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#24191;&#27867;&#26469;&#28304;&#12290;&#30001;&#20110;&#19987;&#21033;&#34987;&#25552;&#20132;&#26159;&#20026;&#20102;&#20445;&#25252;&#29305;&#23450;&#29992;&#36884;&#65292;&#22240;&#27492;&#19987;&#21033;&#20013;&#30340;&#20998;&#23376;&#21487;&#20197;&#34987;&#35270;&#20026;&#24369;&#26631;&#35760;&#30340;&#24212;&#29992;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#30001;&#32654;&#22269;&#19987;&#21033;&#19982;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#21457;&#24067;&#30340;&#19987;&#21033;&#20855;&#26377;&#21487;&#19979;&#36733;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26412;&#21644;&#20998;&#23376;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06034</link><description>&lt;p&gt;
Tactile-Filter: &#29992;&#20110;&#38646;&#37096;&#20214;&#35013;&#37197;&#30340;&#20132;&#20114;&#24335;&#35302;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20381;&#36182;&#35302;&#24863;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#23436;&#25104;&#24456;&#22810;&#24039;&#22937;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25509;&#35302;&#24418;&#24335;&#20197;&#21450;&#20219;&#20309;&#20132;&#20114;&#20013;&#29289;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#38646;&#37096;&#20214;&#65288;&#38144;&#23376;&#21644;&#23380;&#65289;&#30340;&#25311;&#21512;&#20272;&#35745;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32032;&#21253;&#23481;&#30340;&#20013;&#24515;&#23545;&#31216;&#20989;&#25968;&#26469;&#24320;&#21457;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#26032;&#31995;&#32479;&#65292;&#24182;&#20026;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#30340;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#30340;&#21457;&#23637;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;</title><link>http://arxiv.org/abs/2303.05911</link><description>&lt;p&gt;
&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lifelong Machine Learning Potentials. (arXiv:2303.05911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32032;&#21253;&#23481;&#30340;&#20013;&#24515;&#23545;&#31216;&#20989;&#25968;&#26469;&#24320;&#21457;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#26032;&#31995;&#32479;&#65292;&#24182;&#20026;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#30340;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#30340;&#21457;&#23637;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20934;&#30830;&#30340;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;(Machine learning potentials, MLPs)&#21487;&#20197;&#20445;&#25345;&#39640;&#31934;&#24230;&#19988;&#35745;&#31639;&#38656;&#27714;&#23567;&#12290;&#20294;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#20010;&#31995;&#32479;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;&#22240;&#20026;&#23398;&#20064;&#38468;&#21152;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#20197;&#19981;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#25152;&#20197;&#24050;&#32463;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#22823;&#37327;&#30340;MLP&#12290;&#27492;&#22806;&#65292;MLP&#30340;&#22823;&#22810;&#25968;&#24120;&#35265;&#32467;&#26500;&#25551;&#36848;&#31526;&#26080;&#27861;&#26377;&#25928;&#22320;&#34920;&#31034;&#35768;&#22810;&#19981;&#21516;&#30340;&#21270;&#23398;&#20803;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20803;&#32032;&#21253;&#23481;&#30340;&#20013;&#24515;&#23545;&#31216;&#20989;&#25968; (eeACSFs)&#65292;&#32467;&#21512;&#26469;&#33258;&#21608;&#26399;&#34920;&#30340;&#32467;&#26500;&#23646;&#24615;&#21644;&#20803;&#32032;&#20449;&#24687;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;eeACSFs&#26159;&#25105;&#20204;&#21457;&#23637;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#21183;(lMLP)&#30340;&#20851;&#38190;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21487;&#34987;&#21033;&#29992;&#26469;&#36229;&#36234;&#19968;&#20010;&#22266;&#23450;&#30340;&#12289;&#39044;&#20808;&#35757;&#32451;&#30340;MLP&#65292;&#20197;&#36798;&#21040;&#36830;&#32493;&#36866;&#24212;&#30340;lMLP&#65292;&#22240;&#20026;&#22987;&#32456;&#20250;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#31934;&#24230;&#27700;&#24179;&#21644;&#22686;&#24378;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#26032;&#31995;&#32479;&#65292;&#24182;&#20026;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#30340;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#30340;&#21457;&#23637;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning potentials (MLPs) trained on accurate quantum chemical data can retain the high accuracy, while inflicting little computational demands. On the downside, they need to be trained for each individual system. In recent years, a vast number of MLPs has been trained from scratch because learning additional data typically requires to train again on all data to not forget previously acquired knowledge. Additionally, most common structural descriptors of MLPs cannot represent efficiently a large number of different chemical elements. In this work, we tackle these problems by introducing element-embracing atom-centered symmetry functions (eeACSFs) which combine structural properties and element information from the periodic table. These eeACSFs are a key for our development of a lifelong machine learning potential (lMLP). Uncertainty quantification can be exploited to transgress a fixed, pre-trained MLP to arrive at a continuously adapting lMLP, because a predefined level of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#21457;&#20102;&#19968;&#31181;&#25972;&#21512;&#24739;&#32773;ICU&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#26469;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24739;&#32773;&#29366;&#20917;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2303.04630</link><description>&lt;p&gt;
&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#30340;&#20020;&#24202;&#36827;&#31243;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65306;&#20174;&#27431;&#27954;&#37325;&#30151;&#30417;&#25252;&#23460;&#25968;&#25454;&#20013;&#25366;&#25496;&#24739;&#32773;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data. (arXiv:2303.04630v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#21457;&#20102;&#19968;&#31181;&#25972;&#21512;&#24739;&#32773;ICU&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#26469;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24739;&#32773;&#29366;&#20917;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#24739;&#32773;&#29366;&#20917;&#34920;&#36848;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#20010;&#20307;&#21270;&#27835;&#30103;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24314;&#27169;&#31574;&#30053;&#65292;&#25972;&#21512;&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20303;&#38498;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#30740;&#31350;&#38543;&#26426;&#36873;&#25321;&#20102;1550&#21517;&#26469;&#33258;&#27431;&#27954;65&#20010;&#20013;&#24515;&#12289;19&#20010;&#22269;&#23478;&#30340;TBI&#24739;&#32773;&#65292;&#25552;&#21462;&#20102;&#22312;ICU&#20303;&#38498;&#26399;&#38388;&#25110;&#20043;&#21069;&#25910;&#38598;&#30340;&#25152;&#26377;1166&#20010;&#21464;&#37327;&#65292;&#20197;&#21450;6&#20010;&#26376;&#30340;Glasgow Outcome Scale-Extended&#65288;GOSE&#65289;&#21151;&#33021;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#25152;&#26377;&#21464;&#37327;&#30340;&#20196;&#29260;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65288;&#21253;&#25324;&#32570;&#22833;&#25968;&#25454;&#65289;&#26144;&#23556;&#21040;&#27599;2&#20010;&#23567;&#26102;&#30340;GOSE&#39044;&#21518;&#12290;&#36890;&#36807;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Somers' Dxy&#35780;&#20272;&#20102;GOSE&#30340;&#26085;&#24120;&#24046;&#24322;&#30340;&#26657;&#20934;&#21644;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;TimeSHAP&#26469;&#35745;&#31639;&#21464;&#37327;&#21450;&#20808;&#21069;&#26102;&#38388;&#23545;&#27599;&#20010;&#30149;&#20363;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods to characterise the evolving condition of traumatic brain injury (TBI) patients in the intensive care unit (ICU) do not capture the context necessary for individualising treatment. We aimed to develop a modelling strategy which integrates all data stored in medical records to produce an interpretable disease course for each TBI patient's ICU stay. From a prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI patients, we extracted all 1,166 variables collected before or during ICU stay as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended (GOSE). We trained recurrent neural network models to map a token-embedded time series representation of all variables (including missing data) to an ordinal GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated calibration and the explanation of ordinal variance in GOSE with Somers' Dxy. Furthermore, we applied TimeSHAP to calculate the contribution of variables and prior ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20248;&#21270;I-FENN&#24615;&#33021;&#65292;&#36890;&#36807;&#31995;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#25506;&#31350;PINN&#35757;&#32451;&#35823;&#24046;&#21644;&#20840;&#23616;&#35823;&#24046;&#30340;&#25910;&#25947;&#34892;&#20026;&#21644;&#36229;&#21442;&#25968;-&#24615;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.03918</link><description>&lt;p&gt;
PINNs&#30340;&#35823;&#24046;&#25910;&#25947;&#24615;&#21644;&#24037;&#31243;&#24341;&#23548;&#36229;&#21442;&#25968;&#25628;&#32034;&#65306;&#23454;&#29616;I-FENN&#24615;&#33021;&#26368;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Error convergence and engineering-guided hyperparameter search of PINNs: towards optimized I-FENN performance. (arXiv:2303.03918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20248;&#21270;I-FENN&#24615;&#33021;&#65292;&#36890;&#36807;&#31995;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#25506;&#31350;PINN&#35757;&#32451;&#35823;&#24046;&#21644;&#20840;&#23616;&#35823;&#24046;&#30340;&#25910;&#25947;&#34892;&#20026;&#21644;&#36229;&#21442;&#25968;-&#24615;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;I-FENN&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#26377;&#38480;&#20803;&#32423;&#21035;&#20351;&#29992;PINNs&#26469;&#24555;&#36895;&#36924;&#36817;&#24863;&#20852;&#36259;&#30340;&#29366;&#24577;&#21464;&#37327;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38750;&#23616;&#37096;&#26799;&#24230;&#22686;&#24378;&#25439;&#20260;&#21147;&#23398;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#20854;PINN&#32452;&#20214;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26469;&#22686;&#24378;I-FENN&#30340;&#20005;&#23494;&#24615;&#21644;&#24615;&#33021;&#65306;a&#65289;&#35823;&#24046;&#25910;&#25947;&#24615;&#20998;&#26512; &#21644; b&#65289;&#36229;&#21442;&#25968;-&#24615;&#33021;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#24615;&#33021;&#25351;&#26631;&#30340;&#26032;&#22411;&#31995;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#22238;&#31572;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;&#22312;&#31532;&#19968;&#20010;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;PINN&#35757;&#32451;&#35823;&#24046;&#21644;&#20840;&#23616;&#35823;&#24046;&#38543;&#32593;&#32476;&#22823;&#23567;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#21464;&#21270;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#35843;&#26597;&#30340;&#32593;&#32476;&#22823;&#23567;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#32452;&#21512;&#37117;&#20855;&#26377;&#19968;&#33268;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our recently proposed Integrated Finite Element Neural Network (I-FENN) framework (Pantidis and Mobasher, 2023) we showcased how PINNs can be deployed on a finite element-level basis to swiftly approximate a state variable of interest, and we applied it in the context of non-local gradient-enhanced damage mechanics. In this paper, we enhance the rigour and performance of I-FENN by focusing on two crucial aspects of its PINN component: a) the error convergence analysis and b) the hyperparameter-performance relationship. Guided by the available theoretical formulations in the field, we introduce a systematic numerical approach based on a novel set of holistic performance metrics to answer both objectives. In the first objective, we explore in detail the convergence of the PINN training error and the global error against the network size and the training sample size. We demonstrate a consistent converging behavior of the two error types for any investigated combination of network compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#35009;&#21098;&#26041;&#27861;SPIDER&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#36739;&#23569;&#30340;&#38543;&#26426;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968;&#19979;&#25214;&#21040;&#19968;&#20010;&#36739;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00883</link><description>&lt;p&gt;
&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Variance-reduced Clipping for Non-convex Optimization. (arXiv:2303.00883v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#35009;&#21098;&#26041;&#27861;SPIDER&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#36739;&#23569;&#30340;&#38543;&#26426;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968;&#19979;&#25214;&#21040;&#19968;&#20010;&#36739;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#35009;&#21098;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#26631;&#20934;&#35757;&#32451;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#26799;&#24230;&#29190;&#28856;&#31561;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#35009;&#21098;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#35757;&#32451;&#30446;&#26631;&#27839;&#30528;&#20854;&#36712;&#36857;&#30340;&#24179;&#28369;&#24615;&#20855;&#26377;&#19968;&#31181;&#29305;&#27530;&#30340;&#34892;&#20026;&#65292;&#21363;&#24179;&#28369;&#24615;&#38543;&#30528;&#26799;&#24230;&#33539;&#25968;&#22686;&#38271;&#32780;&#22686;&#38271;&#12290;&#36825;&#19982;&#27665;&#38388;&#38750;&#20984;&#20248;&#21270;&#20013;&#24191;&#27867;&#27969;&#20256;&#30340;$L$-&#24179;&#28369;&#20551;&#35774;&#24418;&#25104;&#26126;&#26174;&#23545;&#27604;&#65292;&#21363;&#20840;&#23616;&#24179;&#28369;&#24615;&#34987;&#20551;&#23450;&#20026;&#30001;&#24120;&#25968;$L$&#19978;&#30028;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;$(L_0,L_1)$-&#24179;&#28369;&#24230;&#26159;&#19968;&#20010;&#26356;&#25918;&#26494;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#21040;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#36825;&#31181;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36825;&#31181;&#25918;&#26494;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#22312;SGD&#35009;&#21098;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;$O(\epsilon^{-4})$&#38543;&#26426;&#26799;&#24230;&#35745;&#31639;&#25165;&#33021;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#31283;&#23450;&#35299;&#12290;&#26412;&#25991;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;SPIDER&#65292;&#24182;&#28436;&#31034;&#22914;&#20309;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a standard training technique used in deep learning applications such as large-scale language modeling to mitigate exploding gradients. Recent experimental studies have demonstrated a fairly special behavior in the smoothness of the training objective along its trajectory when trained with gradient clipping. That is, the smoothness grows with the gradient norm. This is in clear contrast to the well-established assumption in folklore non-convex optimization, a.k.a. $L$--smoothness, where the smoothness is assumed to be bounded by a constant $L$ globally. The recently introduced $(L_0,L_1)$--smoothness is a more relaxed notion that captures such behavior in non-convex optimization. In particular, it has been shown that under this relaxed smoothness assumption, SGD with clipping requires $O(\epsilon^{-4})$ stochastic gradient computations to find an $\epsilon$--stationary solution. In this paper, we employ a variance reduction technique, namely SPIDER, and demonstrate
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#26041;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#32553;&#26494;&#24347;&#24230;&#20801;&#35768;&#26356;&#28608;&#36827;&#30340;&#21098;&#26525;&#65292;&#26174;&#33879;&#38477;&#20302;&#27714;&#35299;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.14471</link><description>&lt;p&gt;
&#23433;&#20840;&#21093;&#31163;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Safe Peeling for L0-Regularized Least-Squares with supplementary material. (arXiv:2302.14471v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14471
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#26041;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#32553;&#26494;&#24347;&#24230;&#20801;&#35768;&#26356;&#28608;&#36827;&#30340;&#21098;&#26525;&#65292;&#26174;&#33879;&#38477;&#20302;&#27714;&#35299;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#23433;&#20840;&#21093;&#31163;&#8221;&#65292;&#36890;&#36807;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#21152;&#36895;&#35299;&#20915;L0&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31243;&#24207;&#20351;&#24471;&#22312;BnB&#20915;&#31574;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#22788;&#32771;&#34385;&#21040;&#25910;&#32553;&#26494;&#24347;&#24230;&#65292;&#22240;&#27492;&#21487;&#33021;&#20801;&#35768;&#26356;&#21152;&#28608;&#36827;&#30340;&#21098;&#26525;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25506;&#32034;&#33410;&#28857;&#25968;&#37327;&#21644;&#25972;&#20307;&#27714;&#35299;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new methodology dubbed ``safe peeling'' to accelerate the resolution of L0-regularized least-squares problems via a Branch-and-Bound (BnB) algorithm. Our procedure enables to tighten the convex relaxation considered at each node of the BnB decision tree and therefore potentially allows for more aggressive pruning. Numerical simulations show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.s show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#21450;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26126;&#26234;&#30340;&#20171;&#20837;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19988;&#36825;&#20010;&#24046;&#24322;&#21487;&#30456;&#24403;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2302.14260</link><description>&lt;p&gt;
&#23545;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Intervention Procedure of Concept Bottleneck Models. (arXiv:2302.14260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#21450;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26126;&#26234;&#30340;&#20171;&#20837;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19988;&#36825;&#20010;&#24046;&#24322;&#21487;&#30456;&#24403;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#26159;&#19968;&#31867;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22522;&#20110;&#20854;&#39640;&#32423;&#27010;&#24565;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#21709;&#24212;&#12290;&#19982;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#19981;&#21516;&#65292;CBMs&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#23545;&#39044;&#27979;&#30340;&#27010;&#24565;&#36827;&#34892;&#24178;&#39044;&#24182;&#32416;&#27491;&#20219;&#20309;&#38169;&#35823;&#65292;&#20197;&#20415;&#22312;&#26368;&#32456;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20219;&#21153;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#31181;&#21487;&#24178;&#39044;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#25511;&#21046;&#36884;&#24452;&#65292;&#20294;&#20171;&#20837;&#31243;&#24207;&#30340;&#35768;&#22810;&#26041;&#38754;&#20173;&#28982;&#30456;&#24403;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#24335;&#26469;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#28436;&#21464;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#26126;&#26234;&#20171;&#20837;&#30340;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19982;&#24403;&#21069;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#24178;&#39044;&#27425;&#25968;&#19979;&#65292;&#36825;&#20010;&#24046;&#24322;&#21487;&#20197;&#30456;&#24403;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end. While such intervenability provides a powerful avenue of control, many aspects of the intervention procedure remain rather unexplored. In this work, we develop various ways of selecting intervening concepts to improve the intervention effectiveness and conduct an array of in-depth analyses as to how they evolve under different circumstances. Specifically, we find that an informed intervention strategy can reduce the task error more than ten times compared to the current baseline under the same amount of intervention counts in realistic settings, and yet, this can vary quite significantly when taking into account diffe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13875</link><description>&lt;p&gt;
&#22312;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#19979;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#20998;&#24067;&#20559;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#25110;&#25552;&#20379;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#32423;&#38382;&#39064;&#20013;&#65292;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#23588;&#20026;&#22797;&#26434;&#65292;&#22240;&#20026;&#26679;&#26412;&#26159;&#30456;&#20114;&#20381;&#36182;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#21508;&#31181;&#26377;&#24847;&#20041;&#30340;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32771;&#34385;&#33410;&#28857;&#32423;&#20998;&#24067;&#20559;&#31227;&#30340;&#22270;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#65292;&#32780;&#32467;&#26500;&#23646;&#24615;&#23545;&#22270;&#38382;&#39064;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#24341;&#20986;&#22810;&#26679;&#21270;&#20998;&#24067;&#20559;&#31227;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26681;&#25454;&#20960;&#20010;&#33410;&#28857;&#30340;&#32467;&#26500;&#23646;&#24615;&#65306;&#27969;&#34892;&#24230;&#12289;&#23616;&#37096;&#24615;&#21644;&#23494;&#24230;&#26469;&#21019;&#24314;&#25968;&#25454;&#20998;&#21106;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36824;&#20462;&#35746;&#20102;&#19968;&#20123;&#20851;&#20110;&#22522;&#20934;&#27979;&#35797;&#22270;&#27169;&#22411;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32771;&#34385;&#20102;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13335</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#28436;&#31034;&#32780;&#27809;&#26377;&#35775;&#38382;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#30340;&#23398;&#20064;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#23558;&#19987;&#23478;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#27010;&#29575;p(a|s)&#65288;&#20363;&#22914;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;BC&#65289;&#65292;&#35201;&#20040;&#23558;&#32852;&#21512;&#27010;&#29575;p(s,a)&#24314;&#27169;&#65288;&#20363;&#22914;&#65292;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#65289;&#12290;&#23613;&#31649;&#34892;&#20026;&#20811;&#38534;&#23545;&#20110;&#24314;&#27169;&#26465;&#20214;&#27010;&#29575;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#12290;&#34429;&#28982;&#23545;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#36973;&#21463;&#27969;&#24418;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#37319;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#19987;&#23478;&#34892;&#20026;&#65292;&#24182;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26681;&#25454;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#22909;&#30340;&#25351;&#23548;GNN&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#20840;&#38754;&#21051;&#30011;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#26032;&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.11556</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#22810;&#39033;&#24335;
&lt;/p&gt;
&lt;p&gt;
Equivariant Polynomials for Graph Neural Networks. (arXiv:2302.11556v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#22909;&#30340;&#25351;&#23548;GNN&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#20840;&#38754;&#21051;&#30011;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#26032;&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22312;&#20854;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#37325;&#35201;&#24037;&#20316;(Xu&#31561;&#65292;2019&#65307;Morris&#31561;&#65292;2019b)&#24341;&#20837;&#20102;Weisfeiler-Lehman(WL)&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#34429;&#28982;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#25512;&#21160;&#20102;GNN&#20998;&#26512;&#21644;&#26550;&#26500;&#21457;&#23637;&#19978;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#23384;&#22312;&#30528;&#19968;&#20123;&#26174;&#33879;&#30340;&#38480;&#21046;&#12290;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22797;&#26434;&#30340;&#23450;&#20041;&#65292;&#32570;&#20047;&#25351;&#23548;&#27169;&#22411;&#25913;&#36827;&#30340;&#30452;&#25509;&#25351;&#23548;&#20197;&#21450;&#19968;&#20010;&#36807;&#20110;&#31895;&#31961;&#26080;&#27861;&#30740;&#31350;&#24403;&#21069;GNN&#30340;WL&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#33021;&#22815;&#35745;&#31639;&#29305;&#23450;&#27425;&#25968;&#30340;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#26174;&#33879;&#25512;&#24191;&#20102;&#20197;&#21069;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#30340;&#20840;&#38754;&#21051;&#30011;&#12290;&#27599;&#20010;&#22522;&#24213;&#20803;&#32032;&#23545;&#24212;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#22270;&#65292;&#20854;&#22312;&#26576;&#20123;&#22270;&#25968;&#25454;&#36755;&#20837;&#19978;&#30340;&#35745;&#31639;&#23545;&#24212;&#20110;&#19968;&#20010;&#24352;&#37327;&#25910;&#32553;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#31561;&#21464;&#22810;&#39033;&#24335;&#26469;&#23450;&#20041;&#26032;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#65292;&#25193;&#23637;&#20102;WL&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#26356;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;GNN&#26550;&#26500;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#34920;&#26126;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#23545;ICL&#21462;&#24471;&#30340;&#24615;&#33021;&#26377;&#39640;&#36798;16.3%&#30340;&#24433;&#21709;&#65292;&#26696;&#20363;&#30740;&#31350;&#20013;&#20063;&#21457;&#29616;&#20102;&#31034;&#20363;&#25490;&#24207;&#20013;&#30340;&#8220;&#26368;&#36817;&#24615;&#20559;&#24046;&#8221;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11042</link><description>&lt;p&gt;
&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#34920;&#26126;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#23545;ICL&#21462;&#24471;&#30340;&#24615;&#33021;&#26377;&#39640;&#36798;16.3%&#30340;&#24433;&#21709;&#65292;&#26696;&#20363;&#30740;&#31350;&#20013;&#20063;&#21457;&#29616;&#20102;&#31034;&#20363;&#25490;&#24207;&#20013;&#30340;&#8220;&#26368;&#36817;&#24615;&#20559;&#24046;&#8221;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26159;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20986;&#29616;&#30340;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#26377;&#30528;&#35768;&#22810;&#26377;&#21033;&#26041;&#38754;&#65292;ICL&#30340;&#24615;&#33021;&#20173;&#28982;&#23545;&#36755;&#20837;&#31034;&#20363;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;$\textit{&#19978;&#19979;&#25991;&#24433;&#21709;}$&#26469;&#30452;&#25509;&#20998;&#26512;&#23569;&#26679;&#26412;ICL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#22312;9&#20010;SuperGLUE&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#22312;&#20351;&#29992;&#26368;&#27491;&#38754;&#31034;&#20363;&#21644;&#26368;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#65292;&#24615;&#33021;&#24046;&#24322;&#21487;&#39640;&#36798;$16.3\%$&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#37327;&#21270;&#23569;&#26679;&#26412;ICL&#31034;&#20363;&#25490;&#24207;&#20013;&#26368;&#36817;&#24615;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25345;&#32493;&#21516;&#35843;&#25216;&#26415;&#22312;&#25429;&#25417;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#31243;&#22270;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#30340;&#39640;&#34920;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09826</link><description>&lt;p&gt;
&#25345;&#32493;&#21516;&#35843;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#34920;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Expressivity of Persistent Homology in Graph Learning. (arXiv:2302.09826v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25345;&#32493;&#21516;&#35843;&#25216;&#26415;&#22312;&#25429;&#25417;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#31243;&#22270;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#30340;&#39640;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35745;&#31639;&#25299;&#25169;&#23398;&#20013;&#30340;&#19968;&#39033;&#25216;&#26415;&#65292;&#25345;&#32493;&#21516;&#35843;&#23637;&#29616;&#20986;&#22312;&#22270;&#20998;&#31867;&#26041;&#38754;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#36890;&#36807;&#39640;&#38454;&#25299;&#25169;&#29305;&#24449;&#8212;&#8212;&#22914;&#20219;&#24847;&#38271;&#24230;&#30340;&#29615;&#8212;&#8212;&#20197;&#21450;&#22810;&#23610;&#24230;&#25299;&#25169;&#25551;&#36848;&#31526;&#25429;&#25417;&#38271;&#31243;&#22270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;&#22914;&#20998;&#23376;&#8212;&#8212;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25345;&#32493;&#21516;&#35843;&#30340;&#29702;&#35770;&#24615;&#36136;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25345;&#32493;&#21516;&#35843;&#22312;&#22270;&#20013;&#30340;&#31616;&#35201;&#20171;&#32461;&#20197;&#21450;&#23545;&#20854;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#24615;&#36827;&#34892;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#24357;&#21512;&#35745;&#31639;&#25299;&#25169;&#23398;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#39118;&#38505;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#40723;&#21169;&#26234;&#33021;&#20307;&#25506;&#32034;&#26410;&#30693;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20013;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.09339</link><description>&lt;p&gt;
&#36890;&#36807;&#35748;&#30693;&#39118;&#38505;&#23548;&#21521;&#31574;&#30053;&#20248;&#21270;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization. (arXiv:2302.09339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#39118;&#38505;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#40723;&#21169;&#26234;&#33021;&#20307;&#25506;&#32034;&#26410;&#30693;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20013;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#20048;&#35266;&#20027;&#20041;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#22914;&#20309;&#23558;&#35813;&#21407;&#21017;&#26368;&#22909;&#22320;&#36716;&#21270;&#21040;&#28041;&#21450;&#22312;&#32447;&#38543;&#26426;&#26799;&#24230;&#21644;&#28145;&#24230;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23578;&#26410;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20048;&#35266;&#30446;&#26631;&#65292;&#24403;&#20248;&#21270;&#26102;&#65292;&#20135;&#29983;&#19968;&#31181;&#21487;&#35777;&#26126;&#26377;&#25928;&#25506;&#32034;&#30340;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26032;&#30446;&#26631;&#26159;&#19968;&#31181;&#38646;&#21644;&#20108;&#20154;&#21338;&#24328;&#65292;&#28304;&#20110;&#36171;&#20104;&#20195;&#29702;&#19968;&#20010;&#35748;&#30693;&#39118;&#38505;&#23548;&#21521;&#25928;&#29992;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#20154;&#25506;&#32034;&#19981;&#30830;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#28216;&#25103;&#30340;&#35299;&#20915;&#26041;&#26696;&#26368;&#23567;&#21270;&#20102;&#24724;&#24680;&#30340;&#19968;&#20010;&#19978;&#30028;&#65292;&#20854;&#20013;&#8220;&#29609;&#23478;&#8221;&#21508;&#33258;&#23581;&#35797;&#26368;&#23567;&#21270;&#29305;&#23450;&#24724;&#24680;&#20998;&#35299;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. In this paper we propose a new, differentiable optimistic objective that when optimized yields a policy that provably explores efficiently, with guarantees even under function approximation. Our new objective is a zero-sum two-player game derived from endowing the agent with an epistemic-risk-seeking utility function, which converts uncertainty into value and encourages the agent to explore uncertain states. We show that the solution to this game minimizes an upper bound on the regret, with the 'players' each attempting to minimize one component of a particular regret decomposition. We derive a new model-free algorithm which
&lt;/p&gt;</description></item><item><title>MiDi&#26159;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#29983;&#25104;&#20998;&#23376;&#22270;&#21644;&#21407;&#23376;&#19977;&#32500;&#25490;&#21015;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;MiDi&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#31283;&#23450;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2302.09048</link><description>&lt;p&gt;
MiDi&#65306;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#28151;&#21512;&#22270;&#21644;&#19977;&#32500;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation. (arXiv:2302.09048v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09048
&lt;/p&gt;
&lt;p&gt;
MiDi&#26159;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#29983;&#25104;&#20998;&#23376;&#22270;&#21644;&#21407;&#23376;&#19977;&#32500;&#25490;&#21015;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;MiDi&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#31283;&#23450;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MiDi&#65292;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20849;&#21516;&#29983;&#25104;&#20998;&#23376;&#22270;&#21644;&#30456;&#24212;&#30340;&#21407;&#23376;&#19977;&#32500;&#25490;&#21015;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#20197;&#22522;&#20110;&#19977;&#32500;&#26500;&#35937;&#30830;&#23450;&#20998;&#23376;&#38190;&#19981;&#21516;&#65292;MiDi&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#26041;&#27861;&#26469;&#31616;&#21270;&#20998;&#23376;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;GEOM-DRUGS&#25968;&#25454;&#38598;&#20013;&#65292;MiDi&#29983;&#25104;&#20102;92&#65285;&#30340;&#31283;&#23450;&#20998;&#23376;&#65292;&#32780;&#20808;&#21069;&#20351;&#29992;&#21407;&#23376;&#38388;&#36317;&#36827;&#34892;&#38190;&#39044;&#27979;&#30340;EDM&#27169;&#22411;&#20165;&#29983;&#25104;&#20102;6&#65285;&#65292;&#32780;&#20351;&#29992;EDM&#21518;&#36319;&#30452;&#25509;&#20248;&#21270;&#38190;&#24207;&#20197;&#30830;&#20445;&#26377;&#25928;&#24615;&#30340;&#31639;&#27861;&#20165;&#29983;&#25104;&#20102;40&#65285;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;github.com/cvignac/MiDi&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces MiDi, a novel diffusion model for jointly generating molecular graphs and their corresponding 3D arrangement of atoms. Unlike existing methods that rely on predefined rules to determine molecular bonds based on the 3D conformation, MiDi offers an end-to-end differentiable approach that streamlines the molecule generation process. Our experimental results demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS dataset, MiDi generates 92% of stable molecules, against 6% for the previous EDM model that uses interatomic distances for bond prediction, and 40% using EDM followed by an algorithm that directly optimize bond orders for validity. Our code is available at github.com/cvignac/MiDi.
&lt;/p&gt;</description></item><item><title>GFlowNet-EM&#37319;&#29992;GFlowNets&#31639;&#27861;&#20316;&#20026;&#24314;&#27169;&#28508;&#21464;&#37327;&#21518;&#39564;&#27010;&#29575;&#30340;E&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31163;&#25955;&#32452;&#21512;&#28508;&#21464;&#37327;&#30340;&#34920;&#29616;&#21147;&#24378;&#30340;LVM&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06576</link><description>&lt;p&gt;
GFlowNet-EM&#29992;&#20110;&#23398;&#20064;&#32452;&#21512;&#38544;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GFlowNet-EM for learning compositional latent variable models. (arXiv:2302.06576v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06576
&lt;/p&gt;
&lt;p&gt;
GFlowNet-EM&#37319;&#29992;GFlowNets&#31639;&#27861;&#20316;&#20026;&#24314;&#27169;&#28508;&#21464;&#37327;&#21518;&#39564;&#27010;&#29575;&#30340;E&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31163;&#25955;&#32452;&#21512;&#28508;&#21464;&#37327;&#30340;&#34920;&#29616;&#21147;&#24378;&#30340;LVM&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31163;&#25955;&#32452;&#21512;&#28508;&#21464;&#37327;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;LVM&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#28508;&#21464;&#37327;&#30340;&#21487;&#33021;&#32452;&#21512;&#25968;&#37327;&#32452;&#21512;&#24456;&#22823;&#12290;&#22312;&#24314;&#27169;&#28508;&#21464;&#37327;&#30340;&#21518;&#39564;&#27010;&#29575;&#26102;&#65292;&#34920;&#29616;&#21644;&#21487;&#36319;&#36394;&#30340;&#20248;&#21270;&#20043;&#38388;&#20855;&#26377;&#20851;&#38190;&#30340;&#26435;&#34913;&#12290;&#23545;&#20110;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#31639;&#27861;&#65292;E-&#27493;&#39588;&#24448;&#24448;&#22312;&#27809;&#26377;&#23545;&#21518;&#39564;&#36827;&#34892;&#38480;&#21046;&#30340;&#36817;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#36319;&#36394;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;GFlowNets&#65292;&#19968;&#31181;&#23398;&#20064;&#20174;&#26410;&#35268;&#33539;&#21270;&#30340;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;&#31574;&#30053;&#20197;&#36827;&#34892;&#39034;&#24207;&#26679;&#26412;&#26500;&#24314;&#30340;&#31639;&#27861;&#65292;&#26469;&#22788;&#29702;&#36825;&#20010;&#19981;&#21487;&#36319;&#36394;&#30340;E-&#27493;&#39588;&#12290;&#36890;&#36807;&#35757;&#32451;GFlowNets&#20174;&#28508;&#21464;&#37327;&#21518;&#39564;&#20013;&#37319;&#26679;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#23427;&#20204;&#20316;&#20026;&#31163;&#25955;&#32467;&#26500;&#22797;&#26434;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;GFlowNet-EM&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20855;&#26377;&#31163;&#25955;&#32452;&#21512;&#28508;&#21464;&#37327;&#30340;&#34920;&#29616;&#21147;&#24378;&#30340;LVM&#36827;&#34892;&#35757;&#32451;&#65292;&#22914;&#22312;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#24402;&#32435;&#21644;&#22270;&#20687;&#32763;&#35793;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on image
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#30446;&#21069;&#20581;&#22766;&#31639;&#27861;&#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GAS&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766;&#31639;&#27861;&#29992;&#20110;&#38750;IID&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.06079</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#20998;&#21106;&#23454;&#29616;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting. (arXiv:2302.06079v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#30446;&#21069;&#20581;&#22766;&#31639;&#27861;&#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GAS&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766;&#31639;&#27861;&#29992;&#20110;&#38750;IID&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377; vulnerabilities&#65292;&#21363;&#25915;&#20987;&#32773;&#21487;&#20197;&#21521;&#20013;&#22830;&#26381;&#21153;&#22120;&#21457;&#36865;&#20219;&#24847;&#26799;&#24230;&#20197;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#19968;&#20123;&#20581;&#22766;&#30340;&#32858;&#21512;&#35268;&#21017;&#65288;AGRs&#65289;&#24050;&#34987;&#25552;&#20986;&#26469;&#20197;&#25269;&#24481;&#23545;&#25239;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#24403;&#25968;&#25454;&#19981;&#26381;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#26102;&#65292;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#20173;&#28982;&#21487;&#20197;&#35268;&#36991;&#20581;&#22766;&#30340; AGRs&#12290;&#26412;&#25991;&#39318;&#20808;&#25581;&#31034;&#20102;&#24403;&#21069;&#20581;&#22766; AGRs &#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26681;&#26412;&#21407;&#22240;&#65306;&#32500;&#24230;&#28798;&#38590;&#21644;&#26799;&#24230;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; GAS&#65292;&#19968;&#31181;&#32553;&#30701;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766; AGRs &#36866;&#24212;&#20110;&#38750;IID&#29615;&#22659;&#12290;&#24403;&#29616;&#26377;&#20581;&#22766; AGRs &#19982; GAS &#32452;&#21512;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; GAS &#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312; https://github.com/Y &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/Y
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#65292;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05103</link><description>&lt;p&gt;
&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Controllability-Aware Unsupervised Skill Discovery. (arXiv:2302.05103v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#65292;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#30340;&#20851;&#38190;&#33021;&#21147;&#20043;&#19968;&#26159;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#24448;&#24448;&#21482;&#33021;&#33719;&#24471;&#31616;&#21333;&#12289;&#26131;&#23398;&#30340;&#25216;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#21457;&#29616;&#26356;&#22797;&#26434;&#12289;&#26377;&#25361;&#25112;&#24615;&#34892;&#20026;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#25511;&#24615;&#24863;&#30693;&#25216;&#33021;&#21457;&#29616;&#65288;CSD&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#25214;&#22797;&#26434;&#12289;&#38590;&#20197;&#25511;&#21046;&#30340;&#25216;&#33021;&#12290;CSD&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21487;&#25511;&#24615;&#24863;&#30693;&#36317;&#31163;&#20989;&#25968;&#65292;&#23427;&#32473;&#24403;&#21069;&#25216;&#33021;&#23454;&#29616;&#26356;&#38590;&#30340;&#29366;&#24577;&#36716;&#25442;&#20998;&#37197;&#26356;&#22823;&#30340;&#20540;&#12290;&#19982;&#36317;&#31163;&#26368;&#22823;&#21270;&#30340;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;CSD&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#65292;&#22240;&#20026;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#20102;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#36816;&#21160;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CSD&#21487;&#20197;&#21457;&#29616;&#21508;&#31181;&#19981;&#21516;&#30340;&#22797;&#26434;&#25216;&#33021;&#65292;&#32988;&#36807;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key capabilities of intelligent agents is the ability to discover useful skills without external supervision. However, the current unsupervised skill discovery methods are often limited to acquiring simple, easy-to-learn skills due to the lack of incentives to discover more complex, challenging behaviors. We introduce a novel unsupervised skill discovery method, Controllability-aware Skill Discovery (CSD), which actively seeks complex, hard-to-control skills without supervision. The key component of CSD is a controllability-aware distance function, which assigns larger values to state transitions that are harder to achieve with the current skills. Combined with distance-maximizing skill discovery, CSD progressively learns more challenging skills over the course of training as our jointly trained distance function reduces rewards for easy-to-achieve skills. Our experimental results in six robotic manipulation and locomotion environments demonstrate that CSD can discover diver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#23558;&#21407;&#22987;MDP&#36716;&#25442;&#20026;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.03921</link><description>&lt;p&gt;
&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predictable MDP Abstraction for Unsupervised Model-Based RL. (arXiv:2302.03921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#23558;&#21407;&#22987;MDP&#36716;&#25442;&#20026;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#33021;&#39044;&#27979;&#34892;&#21160;&#32467;&#26524;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20250;&#38477;&#20302;&#27169;&#22411;&#21270;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#22797;&#26434;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21487;&#33021;&#20250;&#24102;&#26469;&#26497;&#20854;&#22256;&#38590;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#65288;PMA&#65289;&#65306;&#19981;&#26159;&#22312;&#21407;&#22987;MDP&#19978;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#26159;&#22312;&#19968;&#20010;&#36716;&#25442;&#21518;&#30340;&#20855;&#26377;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#30340;MDP&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#34892;&#21160;&#31354;&#38388;&#21482;&#20801;&#35768;&#21487;&#39044;&#27979;&#12289;&#26131;&#24314;&#27169;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#35206;&#30422;&#21407;&#22987;&#29366;&#24577;-&#34892;&#21160;&#31354;&#38388;&#12290;&#32467;&#26524;&#26159;&#65292;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#21644;&#20934;&#30830;&#65292;&#36825;&#20801;&#35768;&#40065;&#26834;&#12289;&#31283;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#25110;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#12290;&#36825;&#31181;&#36716;&#25442;&#26159;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#20219;&#20309;&#20219;&#21153;&#20043;&#21069;&#12290;&#38543;&#21518;&#65292;&#19979;&#28216;&#20219;&#21153;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#36890;&#36807;&#27169;&#22411;&#21270;&#25511;&#21046;&#35299;&#20915;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DIFF2 &#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20197;&#26799;&#24230;&#24046;&#24322;&#20026;&#22522;&#30784;&#26500;&#36896;&#20102;&#19968;&#20010;&#20855;&#26377;&#23567;&#26041;&#24046;&#30340;&#20840;&#23616;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#24179;&#28369;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#23567;&#35823;&#24046;&#19979;&#30028;&#65292;&#25552;&#39640;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03884</link><description>&lt;p&gt;
DIFF2: &#22522;&#20110;&#26799;&#24230;&#24046;&#24322;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#38750;&#20984;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIFF2: Differential Private Optimization via Gradient Differences for Nonconvex Distributed Learning. (arXiv:2302.03884v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DIFF2 &#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20197;&#26799;&#24230;&#24046;&#24322;&#20026;&#22522;&#30784;&#26500;&#36896;&#20102;&#19968;&#20010;&#20855;&#26377;&#23567;&#26041;&#24046;&#30340;&#20840;&#23616;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#24179;&#28369;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#23567;&#35823;&#24046;&#19979;&#30028;&#65292;&#25552;&#39640;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38750;&#20984;&#24179;&#28369;&#30446;&#26631;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#20197;&#24448;&#30340;&#24037;&#20316;&#20013;&#65292;&#22522;&#20110;&#20840;&#26799;&#24230;&#33539;&#25968;&#30340;&#26368;&#20339;&#25928;&#29992;&#36793;&#30028;&#24050;&#30693;&#20026; $\widetilde O(\sqrt{d}/(n\varepsilon_\mathrm{DP}))$&#65292;&#20854;&#20013; $n$ &#20026;&#26679;&#26412;&#22823;&#23567;&#65292;$d$ &#20026;&#38382;&#39064;&#32500;&#24230;&#65292;$\varepsilon_\mathrm{DP}$ &#20026;&#24046;&#20998;&#38544;&#31169;&#21442;&#25968;&#65292;DP-GD &#26159;&#23454;&#29616;&#35813;&#36793;&#30028;&#30340;&#19968;&#31181;&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#24050;&#30693;&#30340;&#25928;&#29992;&#36793;&#30028;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26694;&#26550; DIFF2&#65288;DIFFerential private optimization via gradient DIFFerences&#65289;&#65292;&#23427;&#26500;&#36896;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#20840;&#23616;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20855;&#26377;&#21487;&#33021;&#38750;&#24120;&#23567;&#30340;&#26041;&#24046;&#65292;&#22522;&#20110;&#36890;&#20449;&#30340;&#8220;&#26799;&#24230;&#24046;&#24322;&#8221;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#26412;&#36523;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23376;&#20363;&#31243;&#30340; DIFF2 &#23454;&#29616;&#30340;&#25928;&#29992;&#20026; $\widetilde O(d^{2/3}/(n\varepsilon_\mathrm{DP})^{4/3})$&#65292;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential private optimization for nonconvex smooth objective is considered. In the previous work, the best known utility bound is $\widetilde O(\sqrt{d}/(n\varepsilon_\mathrm{DP}))$ in terms of the squared full gradient norm, which is achieved by Differential Private Gradient Descent (DP-GD) as an instance, where $n$ is the sample size, $d$ is the problem dimensionality and $\varepsilon_\mathrm{DP}$ is the differential privacy parameter. To improve the best known utility bound, we propose a new differential private optimization framework called \emph{DIFF2 (DIFFerential private optimization via gradient DIFFerences)} that constructs a differential private global gradient estimator with possibly quite small variance based on communicated \emph{gradient differences} rather than gradients themselves. It is shown that DIFF2 with a gradient descent subroutine achieves the utility of $\widetilde O(d^{2/3}/(n\varepsilon_\mathrm{DP})^{4/3})$, which can be significantly better than the prev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#38543;&#26426;&#28857;&#31934;&#24230;&#27979;&#35797;&#8221;&#65288;TARP&#65289;&#35206;&#30422;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#29983;&#25104;&#21518;&#39564;&#20272;&#35745;&#22120;&#35206;&#30422;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#20013;&#32534;&#30721;&#21518;&#39564;&#31934;&#24230;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#27979;&#35797;&#39640;&#32500;&#31354;&#38388;&#20013;&#21518;&#39564;&#25512;&#26029;&#20998;&#26512;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.03026</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36890;&#29992;&#25512;&#26029;&#21518;&#39564;&#20272;&#35745;&#31934;&#24230;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Sampling-Based Accuracy Testing of Posterior Estimators for General Inference. (arXiv:2302.03026v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#38543;&#26426;&#28857;&#31934;&#24230;&#27979;&#35797;&#8221;&#65288;TARP&#65289;&#35206;&#30422;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#29983;&#25104;&#21518;&#39564;&#20272;&#35745;&#22120;&#35206;&#30422;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#20013;&#32534;&#30721;&#21518;&#39564;&#31934;&#24230;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#27979;&#35797;&#39640;&#32500;&#31354;&#38388;&#20013;&#21518;&#39564;&#25512;&#26029;&#20998;&#26512;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#25512;&#26029;&#26159;&#35768;&#22810;&#31185;&#23398;&#23398;&#31185;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#19968;&#20123;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#29992;&#20316;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#22522;&#20110;&#20284;&#28982;&#21644;&#22522;&#20110;&#27169;&#25311;&#30340;&#21518;&#39564;&#25512;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#21518;&#39564;&#31934;&#24230;&#24182;&#19981;&#31616;&#21333;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#38543;&#26426;&#28857;&#31934;&#24230;&#27979;&#35797;&#8221;&#65288;TARP&#65289;&#35206;&#30422;&#27979;&#35797;&#20316;&#20026;&#19968;&#31181;&#20272;&#35745;&#29983;&#25104;&#21518;&#39564;&#20272;&#35745;&#22120;&#35206;&#30422;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#20197;&#21069;&#23384;&#22312;&#30340;&#38656;&#35201;&#21518;&#39564;&#35780;&#20272;&#30340;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#30830;&#23450;&#21518;&#39564;&#20272;&#35745;&#22120;&#20934;&#30830;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#31034;&#20363;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;TARP&#21487;&#29992;&#20110;&#27979;&#35797;&#39640;&#32500;&#31354;&#38388;&#20013;&#21518;&#39564;&#25512;&#26029;&#20998;&#26512;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter inference, i.e. inferring the posterior distribution of the parameters of a statistical model given some data, is a central problem to many scientific disciplines. Generative models can be used as an alternative to Markov Chain Monte Carlo methods for conducting posterior inference, both in likelihood-based and simulation-based problems. However, assessing the accuracy of posteriors encoded in generative models is not straightforward. In this paper, we introduce `Tests of Accuracy with Random Points' (TARP) coverage testing as a method to estimate coverage probabilities of generative posterior estimators. Our method differs from previously-existing coverage-based methods, which require posterior evaluations. We prove that our approach is necessary and sufficient to show that a posterior estimator is accurate. We demonstrate the method on a variety of synthetic examples, and show that TARP can be used to test the results of posterior inference analyses in high-dimensional spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; RLSbench&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#29992;&#20110;&#23485;&#26494;&#26631;&#31614;&#20559;&#31227;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#26088;&#22312;&#35780;&#20272;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26631;&#31614;&#36793;&#38469;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03020</link><description>&lt;p&gt;
RLSbench: &#23485;&#26494;&#26631;&#31614;&#20559;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
RLSbench: Domain Adaptation Under Relaxed Label Shift. (arXiv:2302.03020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; RLSbench&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#29992;&#20110;&#23485;&#26494;&#26631;&#31614;&#20559;&#31227;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#26088;&#22312;&#35780;&#20272;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26631;&#31614;&#36793;&#38469;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20986;&#29616;&#20102;&#35299;&#20915;&#26631;&#31614;&#20559;&#31227;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#31867;&#26465;&#20214;&#20998;&#24067;&#30340;&#20559;&#31227;&#25935;&#24863;&#24615;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#27969;&#34892;&#30340;&#28145;&#24230;&#39046;&#22495;&#33258;&#36866;&#24212;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#38754;&#23545;&#26631;&#31614;&#27604;&#20363;&#20559;&#31227;&#26102;&#24448;&#24448;&#30130;&#36719;&#12290;&#34429;&#28982;&#26377;&#20960;&#31687;&#35770;&#25991;&#25913;&#36827;&#20102;&#36825;&#20123;&#21551;&#21457;&#26041;&#27861;&#20197;&#23581;&#35797;&#22788;&#29702;&#26631;&#31614;&#27604;&#20363;&#20559;&#31227;&#65292;&#20294;&#35780;&#20272;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#30340;&#19981;&#19968;&#33268;&#20351;&#24471;&#35780;&#20272;&#24403;&#21069;&#26368;&#20339;&#23454;&#36341;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837; RLSbench&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23485;&#26494;&#26631;&#31614;&#20559;&#31227;&#22522;&#20934;&#65292;&#28085;&#30422;500&#22810;&#20010;&#20998;&#24067;&#20559;&#31227;&#23545;&#65292;&#36328;&#35270;&#35273;&#12289;&#34920;&#26684;&#21644;&#35821;&#35328;&#27169;&#24335;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#27604;&#20363;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31867;&#26465;&#20214;$p(x|y)$&#20559;&#31227;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36824;&#20851;&#27880;&#26631;&#31614;&#36793;&#38469;&#20559;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;13&#31181;&#27969;&#34892;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35777;&#26126;&#22312;&#26631;&#31614;&#27604;&#20363;&#20559;&#31227;&#19979;&#26356;&#26222;&#36941;&#22320;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the emergence of principled methods for domain adaptation under label shift, their sensitivity to shifts in class conditional distributions is precariously under explored. Meanwhile, popular deep domain adaptation heuristics tend to falter when faced with label proportions shifts. While several papers modify these heuristics in attempts to handle label proportions shifts, inconsistencies in evaluation standards, datasets, and baselines make it difficult to gauge the current best practices. In this paper, we introduce RLSbench, a large-scale benchmark for relaxed label shift, consisting of $&gt;$500 distribution shift pairs spanning vision, tabular, and language modalities, with varying label proportions. Unlike existing benchmarks, which primarily focus on shifts in class-conditional $p(x|y)$, our benchmark also focuses on label marginal shifts. First, we assess 13 popular domain adaptation methods, demonstrating more widespread failures under label proportion shifts than were pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22312;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#26041;&#38754;&#27604;GD&#26356;&#24555;&#65292;&#20294;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#33021;&#22815;&#21462;&#24471;&#25104;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.02904</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22312;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#26041;&#38754;&#27604;GD&#26356;&#24555;&#65292;&#20294;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#33021;&#22815;&#21462;&#24471;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#65288;GN&#65289;&#23545;&#19968;&#23618;&#38544;&#34255;&#23618;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#26102;&#30340;&#20840;&#23616;&#25910;&#25947;&#21644;&#27867;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#19979;&#30830;&#23450;&#20102;GN&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#30001;&#20110;&#25913;&#21892;&#20102;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#27604;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#26356;&#24555;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#22238;&#24402;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;GN&#26041;&#27861;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22987;&#32456;&#27604;GD&#26356;&#24555;&#22320;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#20294;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21463;&#21040;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#32467;&#26524;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#20063;&#26159;GD&#30340;&#19968;&#31181;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#21487;&#20197;&#20351;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the global convergence and generalization properties of Gauss Newton's (GN) when optimizing one-hidden layer networks in the over-parameterized regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. We find that, while GN is consistently faster than GD in finding a global optimum, the performance of the learned model on a test dataset is heavily influenced by both the learning rate and the variance of the randomly initialized network's weights. Specifically, we find that initializing with a smaller variance results in a better generalization, a behavior also observed for GD. However, in contrast to GD where larger learning rates lead to the best generalization, we find that GN achieves an improved generalization when using smaller learning
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38450;&#24481;&#26041;&#27861;Simple-Tuning&#65292;&#21487;&#29992;&#20110;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.01677</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks. (arXiv:2302.01677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01677
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38450;&#24481;&#26041;&#27861;Simple-Tuning&#65292;&#21487;&#29992;&#20110;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20010;&#24615;&#21270;&#33021;&#21542;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;FEMNIST&#21644;CIFAR-10&#36825;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;6&#31181;pFL&#26041;&#27861;&#36827;&#34892;&#20102;4&#31181;&#21518;&#38376;&#25915;&#20987;&#30340;&#27979;&#35797;&#65292;&#36827;&#34892;&#20102;600&#27425;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;pFL&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20855;&#26377;&#23436;&#20840;&#27169;&#22411;&#20849;&#20139;&#30340;pFL&#26041;&#27861;&#24182;&#19981;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19981;&#21516;pFL&#26041;&#27861;&#30340;&#20840;&#38754;&#21078;&#26512;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#40065;&#26834;&#24615;&#34920;&#29616;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;&#22522;&#20110;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#38450;&#24481;&#26041;&#27861;Simple-Tuning&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#32463;&#39564;&#19978;&#25552;&#39640;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;pFL&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#25351;&#23548;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;FL&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future. W
&lt;/p&gt;</description></item><item><title>ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.01501</link><description>&lt;p&gt;
ANTM: &#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25506;&#32034;&#28436;&#21464;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics. (arXiv:2302.01501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01501
&lt;/p&gt;
&lt;p&gt;
ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#40784;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;ANTM&#65289;&#30340;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#31639;&#27861;&#23478;&#26063;&#65292;&#23427;&#32467;&#21512;&#20102;&#26032;&#39062;&#30340;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;&#28436;&#21464;&#30340;&#20027;&#39064;&#12290;ANTM&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#26102;&#38388;&#24863;&#30693;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#25991;&#26723;&#32858;&#31867;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#12290;&#36825;&#31181;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#22312;&#27599;&#20010;&#26102;&#38388;&#26694;&#26550;&#20869;&#26631;&#35782;&#19981;&#21516;&#25968;&#37327;&#30340;&#20027;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#27573;&#20869;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#32858;&#31867;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20010;&#36807;&#31243;&#25429;&#25417;&#20102;&#19981;&#21516;&#26102;&#26399;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#65292;&#24182;&#20801;&#35768;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21464;&#20027;&#39064;&#34920;&#31034;&#12290;&#38024;&#23545;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#27010;&#29575;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#25913;&#21892;&#20102;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an algorithmic family of dynamic topic models called Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms to provide a modular framework for discovering evolving topics. ANTM maintains the temporal continuity of evolving topics by extracting time-aware features from documents using advanced pre-trained Large Language Models (LLMs) and employing an overlapping sliding window algorithm for sequential document clustering. This overlapping sliding window algorithm identifies a different number of topics within each time frame and aligns semantically similar document clusters across time periods. This process captures emerging and fading trends across different periods and allows for a more interpretable representation of evolving topics. Experiments on four distinct datasets show that ANTM outperforms probabilistic dynamic topic models in terms of topic coherence and diversity metrics. Moreover, it improves the scalability and flexibility of dy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#21644;&#31232;&#30095;&#30340;Top-k&#36816;&#31639;&#31526;&#65292;&#23558;&#20854;&#35270;&#20026;&#25490;&#21015;&#20984;&#21253;&#19978;&#30340;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;p-&#33539;&#25968;&#27491;&#21017;&#21270;&#39033;&#20197;&#24179;&#28369;&#36816;&#31639;&#31526;&#12290;&#31639;&#27861;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#31471;&#31639;&#23376;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;Top-k&#36816;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.01425</link><description>&lt;p&gt;
&#24555;&#36895;&#65292;&#21487;&#24494;&#20998;&#21644;&#31232;&#30095;&#30340;Top-k: &#20984;&#20998;&#26512;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective. (arXiv:2302.01425v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#21644;&#31232;&#30095;&#30340;Top-k&#36816;&#31639;&#31526;&#65292;&#23558;&#20854;&#35270;&#20026;&#25490;&#21015;&#20984;&#21253;&#19978;&#30340;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;p-&#33539;&#25968;&#27491;&#21017;&#21270;&#39033;&#20197;&#24179;&#28369;&#36816;&#31639;&#31526;&#12290;&#31639;&#27861;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#31471;&#31639;&#23376;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;Top-k&#36816;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Top-k&#36816;&#31639;&#31526;&#36820;&#22238;&#19968;&#20010;&#31232;&#30095;&#21521;&#37327;&#65292;&#20854;&#20013;&#38750;&#38646;&#20540;&#23545;&#24212;&#20110;&#36755;&#20837;k&#20010;&#26368;&#22823;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#26159;&#19968;&#20010;&#19981;&#36830;&#32493;&#30340;&#20989;&#25968;&#65292;&#25152;&#20197;&#24456;&#38590;&#23558;&#20854;&#32435;&#20837;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;&#21487;&#24494;&#20998;&#21644;&#31232;&#30095;&#30340;Top-k&#36816;&#31639;&#31526;&#12290;&#25105;&#20204;&#23558;Top-k&#36816;&#31639;&#31526;&#35270;&#20026;&#25490;&#21015;&#20984;&#21253;&#19978;&#30340;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;p-&#33539;&#25968;&#27491;&#21017;&#21270;&#39033;&#20197;&#24179;&#28369;&#36816;&#31639;&#31526;&#65292;&#35777;&#26126;&#20854;&#35745;&#31639;&#21487;&#20197;&#20943;&#23569;&#21040;&#21516;&#21521;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#26174;&#33879;&#27604;&#29616;&#26377;&#26694;&#26550;&#26356;&#36890;&#29992;&#65292;&#21487;&#20197;&#34920;&#31034;&#36873;&#25321;&#22823;&#23567;&#20540;&#30340;Top-k&#36816;&#31639;&#31526;&#12290;&#22312;&#31639;&#27861;&#26041;&#38754;&#65292;&#38500;&#20102;&#27744;&#30456;&#37051;&#36829;&#35268;&#31639;&#27861;&#65288;PAV&#65289;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#31471;&#31639;&#23376;&#65292;&#20801;&#35768;&#26377;&#25928;&#30340;Top-k&#36816;&#31639;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#24207;&#21015;&#26631;&#35760;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The top-k operator returns a sparse vector, where the non-zero values correspond to the k largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-k operators. We view the top-k operator as a linear program over the permutahedron, the convex hull of permutations. We then introduce a p-norm regularization term to smooth out the operator, and show that its computation can be reduced to isotonic optimization. Our framework is significantly more general than the existing one and allows for example to express top-k operators that select values in magnitude. On the algorithmic side, in addition to pool adjacent violator (PAV) algorithms, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00491</link><description>&lt;p&gt;
&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Prototype Classifiers for Long-Tailed Recognition. (arXiv:2302.00491v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#29289;&#20307;&#30340;&#24130;&#24459;&#20998;&#24067;&#65292;&#38271;&#23614;&#35782;&#21035;(LTR)&#38382;&#39064;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;LTR&#20013;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#24037;&#20316;&#20351;&#29992;softmax&#20998;&#31867;&#22120;&#65292;&#20854;&#20855;&#26377;&#23558;&#20998;&#31867;&#22120;&#33539;&#25968;&#19982;&#32473;&#23450;&#31867;&#21035;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#30456;&#20851;&#32852;&#30340;&#20542;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21407;&#22411;&#20998;&#31867;&#22120;&#19981;&#21463;&#36825;&#31181;&#32570;&#28857;&#30340;&#22256;&#25200;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#26368;&#36817;&#31867;&#24179;&#22343;&#20540;&#65288;NCM&#65289;&#21363;&#21487;&#20132;&#20184;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#21407;&#22411;&#26159;&#32463;&#39564;&#36136;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;LTR&#20013;&#65292;&#21407;&#22411;&#20998;&#31867;&#22120;&#20316;&#20026;softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#30340;&#24615;&#36136;&#65292;&#36825;&#23548;&#33268;&#20102;&#31283;&#23450;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#12290;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#22312;LTR&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#22312;&#20960;&#31181;&#26368;&#26032;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that have a tendency to correlate classifier norm with the amount of training data for a given class. On the other hand, Prototype classifiers do not suffer from this shortcoming and can deliver promising results simply using Nearest-Class-Mean (NCM), a special case where prototypes are empirical centroids. However, the potential of Prototype classifiers as an alternative to softmax in LTR is relatively underexplored. In this work, we propose Prototype classifiers, which jointly learn prototypes that minimize average cross-entropy loss based on probability scores from distances to prototypes. We theoretically analyze the properties of Euclidean distance based prototype classifiers that leads to stable gradient-based optimization which is robust to outliers. We further enhance Prot
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LieGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.00236</link><description>&lt;p&gt;
&#23545;&#31216;&#29983;&#25104;&#23545;&#25239;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Symmetry Discovery. (arXiv:2302.00236v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00236
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LieGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20107;&#20808;&#30693;&#36947;&#23545;&#31216;&#32676;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#30693;&#36947;&#35201;&#20351;&#29992;&#21738;&#20010;&#23545;&#31216;&#32676;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#22320;&#24378;&#21046;&#20351;&#29992;&#23545;&#31216;&#32676;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;LieGAN&#26694;&#26550;&#65292;&#36890;&#36807;&#31867;&#20284;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#33539;&#24335;&#33258;&#21160;&#20174;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#31561;&#21464;&#24615;&#12290;&#29983;&#25104;&#22120;&#23398;&#20064;&#19968;&#32452;&#24212;&#29992;&#20110;&#25968;&#25454;&#30340;&#21464;&#25442;&#65292;&#36825;&#20123;&#21464;&#25442;&#20445;&#25345;&#21407;&#22987;&#20998;&#24067;&#24182;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;LieGAN&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26446;&#20195;&#25968;&#22522;&#34920;&#31034;&#23545;&#31216;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36712;&#36857;&#39044;&#27979;&#21644;&#39030;&#22840;&#20811;&#26631;&#35760;&#20219;&#21153;&#20013;&#21457;&#29616;&#21508;&#31181;&#23545;&#31216;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#32676;$\mathrm{SO}(n)$&#65292;&#38480;&#21046;Lorentz&#32676;$\mathrm{SO}(1,3)^+$&#12290;&#25152;&#23398;&#20064;&#30340;&#23545;&#31216;&#24615;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group $\mathrm{SO}(n)$, restricted Lorentz group $\mathrm{SO}(1,3)^+$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuxiNash&#30340;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24191;&#20041;&#35758;&#20215;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#38750;&#23545;&#31216;&#20219;&#21153;&#35848;&#21028;&#33021;&#21147;&#24179;&#34913;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13501</link><description>&lt;p&gt;
&#36741;&#21161;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#38750;&#23545;&#31216;&#21338;&#24328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Learning as an Asymmetric Bargaining Game. (arXiv:2301.13501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuxiNash&#30340;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24191;&#20041;&#35758;&#20215;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#38750;&#23545;&#31216;&#20219;&#21153;&#35848;&#21028;&#33021;&#21147;&#24179;&#34913;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#23398;&#20064;&#26159;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#23567;&#25968;&#25454;&#38598;&#26102;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#36935;&#21040;&#19968;&#20123;&#22256;&#38590;&#65306;&#65288;i&#65289;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21487;&#33021;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#65288;ii&#65289;&#22914;&#20309;&#24179;&#34913;&#36741;&#21161;&#20219;&#21153;&#20197;&#26368;&#22909;&#22320;&#24110;&#21161;&#20027;&#35201;&#20219;&#21153;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;AuxiNash&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#38750;&#23545;&#31216;&#20219;&#21153;&#35848;&#21028;&#33021;&#21147;&#30340;&#24191;&#20041;&#35758;&#20215;&#21338;&#24328;&#26469;&#24179;&#34913;&#36741;&#21161;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#23545;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#30340;&#36129;&#29486;&#26469;&#23398;&#20064;&#20219;&#21153;&#35848;&#21028;&#33021;&#21147;&#30340;&#26377;&#25928;&#31243;&#24207;&#65292;&#24182;&#20026;&#20854;&#25910;&#25947;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;AuxiNash&#65292;&#24182;&#21457;&#29616;&#23427;&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary learning is an effective method for enhancing the generalization capabilities of trained models, particularly when dealing with small datasets. However, this approach may present several difficulties: (i) optimizing multiple objectives can be more challenging, and (ii) how to balance the auxiliary tasks to best assist the main task is unclear. In this work, we propose a novel approach, named AuxiNash, for balancing tasks in auxiliary learning by formalizing the problem as generalized bargaining game with asymmetric task bargaining power. Furthermore, we describe an efficient procedure for learning the bargaining power of tasks based on their contribution to the performance of the main task and derive theoretical guarantees for its convergence. Finally, we evaluate AuxiNash on multiple multi-task benchmarks and find that it consistently outperforms competing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#65292;&#33021;&#22815;&#23454;&#29616;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13195</link><description>&lt;p&gt;
&#24377;&#24615;&#36755;&#20837;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Adaptive Computation with Elastic Input Sequence. (arXiv:2301.13195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#65292;&#33021;&#22815;&#23454;&#29616;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#12289;&#19981;&#21516;&#30340;&#22788;&#29702;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#26102;&#38388;&#33457;&#36153;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26080;&#35770;&#26679;&#26412;&#30340;&#24615;&#36136;&#25110;&#38590;&#24230;&#37117;&#26377;&#22266;&#23450;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;&#33258;&#36866;&#24212;&#35745;&#31639;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#36171;&#20104;&#20174;&#19994;&#32773;&#28789;&#27963;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#30340;&#24378;&#22823;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#12290;AdaTape&#21033;&#29992;&#24377;&#24615;&#36755;&#20837;&#24207;&#21015;&#65292;&#36890;&#36807;&#35013;&#22791;&#24102;&#26377;&#21160;&#24577;&#35835;&#20889;&#30913;&#24102;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#30913;&#24102;&#24211;&#30340;&#30913;&#24102;&#31526;&#21495;&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#36755;&#20837;&#24207;&#21015;&#65292;&#36825;&#20123;&#31526;&#21495;&#21487;&#35757;&#32451;&#25110;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#27966;&#29983;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33719;&#24471;&#21160;&#24577;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#30340;&#25361;&#25112;&#21644;&#35201;&#27714;&#65292;&#20197;&#21450;AdaTape&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdaTape&#33021;&#22815;&#23398;&#20064;&#33258;&#36866;&#24212;&#35745;&#31639;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#65289;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21106;&#30340;&#20840;&#36830;&#25509;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#31215;&#30340;&#24418;&#24335;&#34920;&#31034;&#32858;&#31867;&#30446;&#26631;&#65292;&#20351;&#24471;&#31639;&#27861;&#26356;&#39640;&#25928;&#65292;&#24182;&#22312;ImageNet&#21644;CIFAR&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12159</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#21106;&#30340;&#20840;&#36830;&#25509;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
ClusterFuG: Clustering Fully connected Graphs by Multicut. (arXiv:2301.12159v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21106;&#30340;&#20840;&#36830;&#25509;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#31215;&#30340;&#24418;&#24335;&#34920;&#31034;&#32858;&#31867;&#30446;&#26631;&#65292;&#20351;&#24471;&#31639;&#27861;&#26356;&#39640;&#25928;&#65292;&#24182;&#22312;ImageNet&#21644;CIFAR&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21106;&#65288;Multicut&#65292;&#21448;&#31216;&#21152;&#26435;&#30456;&#20851;&#32858;&#31867;&#65289;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20687;&#21407;&#22987;&#30340;&#31232;&#30095;&#22810;&#21106;&#26041;&#27861;&#19968;&#26679;&#25351;&#23450;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#65292;&#24615;&#33021;&#20063;&#26377;&#21487;&#33021;&#26356;&#22909;&#12290;&#19982;&#38750;&#21152;&#26435;&#30456;&#20851;&#32858;&#31867;&#19981;&#21516;&#65292;&#25105;&#20204;&#20801;&#35768;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21152;&#26435;&#25104;&#26412;&#32467;&#26500;&#12290;&#22312;&#23494;&#38598;&#22810;&#21106;&#20013;&#65292;&#32858;&#31867;&#30446;&#26631;&#20197;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#30340;&#20869;&#31215;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#30456;&#23545;&#20110;&#22810;&#21106;/&#21152;&#26435;&#30456;&#20851;&#32858;&#31867;&#26356;&#21152;&#39640;&#25928;&#65292;&#21518;&#32773;&#22312;&#23436;&#25972;&#22270;&#19978;&#30340;&#34920;&#31034;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#33267;&#23569;&#26159;&#20108;&#27425;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#25105;&#20204;&#30340;&#23494;&#38598;&#35774;&#32622;&#19979;&#37325;&#20889;&#32463;&#20856;&#36138;&#24515;&#31639;&#27861;&#30340;&#22810;&#21106;&#26041;&#27861;&#65292;&#24182;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#21040;&#25317;&#26377;&#25968;&#19975;&#20010;&#33410;&#28857;&#30340;&#22270;&#20013;&#12290;&#22312;ImageNet&#21644;CIFAR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25110;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#30456;&#19978;&#19979;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a graph clustering formulation based on multicut (a.k.a. weighted correlation clustering) on the complete graph. Our formulation does not need specification of the graph topology as in the original sparse formulation of multicut, making our approach simpler and potentially better performing. In contrast to unweighted correlation clustering we allow for a more expressive weighted cost structure. In dense multicut, the clustering objective is given in a factorized form as inner products of node feature vectors. This allows for an efficient formulation and inference in contrast to multicut/weighted correlation clustering, which has at least quadratic representation and computation complexity when working on the complete graph. We show how to rewrite classical greedy algorithms for multicut in our dense setting and how to modify them for greater efficiency and solution quality. In particular, our algorithms scale to graphs with tens of thousands of nodes. Empirical evidence on i
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.11990</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#25903;&#25345;&#40065;&#26834;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11990
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#20851;&#24515;AI&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#19990;&#30028;&#34920;&#24449;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#24314;&#35758;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#24230;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#19968;&#20010;U&#24418;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;491&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#20110;&#23545;&#25239;&#25915;&#20987;&#21644;&#22495;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20154;&#31867;&#23545;&#40784;&#24448;&#24448;&#26159;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#12289;&#40065;&#26834;&#24615; &#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#26368;&#32456;&#35780;&#20272;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.11989</link><description>&lt;p&gt;
&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Practical Differentially Private Hyperparameter Tuning with Subsampling. (arXiv:2301.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#26368;&#32456;&#35780;&#20272;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#36890;&#36807;&#36229;&#21442;&#25968;&#20540;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20165;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#38543;&#26426;&#23376;&#38598;&#24182;&#23558;&#26368;&#20339;&#20540;&#22806;&#25512;&#21040;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35813;&#26041;&#27861;&#30340; Renyi &#24046;&#20998;&#38544;&#31169;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#19981;&#25439;&#22833;&#25152;&#36873;&#36229;&#21442;&#25968;&#30340;&#26368;&#32456;&#35780;&#20272;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized itself. Commonly, these algorithms still considerably increase the DP privacy parameter $\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the computational cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by extrapolating the optimal values to a larger dataset. We provide a R\'enyi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#30410;&#35843;&#33410;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#31070;&#32463;&#30005;&#36335;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#33410;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#22686;&#30410;&#26469;&#36866;&#24212;&#22320;&#30333;&#21270;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#23545;&#30149;&#24577;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11955</link><description>&lt;p&gt;
&#24102;&#22686;&#30410;&#35843;&#33410;&#30340;&#31070;&#32463;&#20803;&#31181;&#32676;&#33258;&#36866;&#24212;&#30333;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive whitening in neural populations with gain-modulating interneurons. (arXiv:2301.11955v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#30410;&#35843;&#33410;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#31070;&#32463;&#30005;&#36335;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#33410;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#22686;&#30410;&#26469;&#36866;&#24212;&#22320;&#30333;&#21270;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#23545;&#30149;&#24577;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#35745;&#31639;&#31995;&#32479;&#20013;&#65292;&#32479;&#35745;&#30333;&#21270;&#21464;&#25442;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#20063;&#21487;&#33021;&#22312;&#29983;&#29289;&#24863;&#35273;&#31995;&#32479;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#31070;&#32463;&#30005;&#36335;&#27169;&#22411;&#36890;&#36807;&#20462;&#25913;&#31361;&#35302;&#30456;&#20114;&#20316;&#29992;&#26469;&#25805;&#20316;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#20462;&#25913;&#20284;&#20046;&#26082;&#22826;&#24930;&#21448;&#19981;&#22815;&#21487;&#36870;&#12290;&#21463;&#21040;&#22686;&#30410;&#35843;&#33410;&#30340;&#24191;&#27867;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#33410;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#22686;&#30410;&#26469;&#36866;&#24212;&#22320;&#30333;&#21270;&#20854;&#21709;&#24212;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#30333;&#21270;&#30446;&#26631;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#36807;&#23436;&#22791;&#30340;&#25237;&#24433;&#30340;&#36793;&#38469;&#26041;&#24046;&#26469;&#30333;&#21270;&#20854;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#26144;&#23556;&#21040;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#31361;&#35302;&#26435;&#37325;&#21644;&#22686;&#30410;&#35843;&#33410;&#20013;&#38388;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22686;&#30410;&#30340;&#31526;&#21495;&#32422;&#26463;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#30149;&#24577;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#30005;&#36335;&#30340;&#19968;&#31181;&#27867;&#21270;&#24418;&#24335;&#23454;&#29616;&#20102;&#19968;&#31181;&#26412;&#22320;&#30340;
&lt;/p&gt;
&lt;p&gt;
Statistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. We demonstrate numerically that sign-constraining the gains improves robustness of the network to ill-conditioned inputs, and a generalization of the circuit achieves a form of local 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11716</link><description>&lt;p&gt;
&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#38656;&#25913;&#21464;ST&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26469;&#20943;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#19982;&#26356;&#24120;&#35265;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#32456;ST&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#27809;&#26377;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#22343;&#33021;&#22815;&#25552;&#20379;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#40065;&#26834;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#26159;&#21542;&#20445;&#35777;&#32456;&#27490;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35268;&#21010;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20462;&#22797;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11342</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#30340;&#40065;&#26834;&#20248;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks. (arXiv:2301.11342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#40065;&#26834;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#26159;&#21542;&#20445;&#35777;&#32456;&#27490;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35268;&#21010;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20462;&#22797;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#26088;&#22312;&#21019;&#24314;&#20855;&#26377;&#25968;&#23398;&#23433;&#20840;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#26159;&#21542;&#20445;&#35777;&#32456;&#27490;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#26126;&#21453;&#20363;&#24341;&#23548;&#20462;&#22797;&#21487;&#20197;&#34987;&#35270;&#20026;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#26412;&#36523;&#30340;&#32456;&#27490;&#20445;&#35777;&#20173;&#28982;&#36229;&#20986;&#20102;&#25105;&#20204;&#30340;&#33021;&#21147;&#33539;&#22260;&#65292;&#20294;&#26159;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#21463;&#38480;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32456;&#27490;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#19981;&#32456;&#27490;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#29702;&#35770;&#32467;&#26524;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#24120;&#35265;&#39564;&#35777;&#22120;&#21644;&#36829;&#32972;&#32773;&#23545;&#20462;&#22797;&#30340;&#36866;&#29992;&#24615;&#65292;&#23613;&#31649;&#29702;&#35770;&#32467;&#26524;&#20855;&#26377;&#19981;&#21033;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#35265;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35268;&#21010;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20462;&#22797;&#26032;&#31639;&#27861;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterexample-guided repair aims at creating neural networks with mathematical safety guarantees, facilitating the application of neural networks in safety-critical domains. However, whether counterexample-guided repair is guaranteed to terminate remains an open question. We approach this question by showing that counterexample-guided repair can be viewed as a robust optimisation algorithm. While termination guarantees for neural network repair itself remain beyond our reach, we prove termination for more restrained machine learning models and disprove termination in a general setting. We empirically study the practical implications of our theoretical results, demonstrating the suitability of common verifiers and falsifiers for repair despite a disadvantageous theoretical result. Additionally, we use our theoretical insights to devise a novel algorithm for repairing linear regression models based on quadratic programming, surpassing existing approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#21453;&#28436;&#31639;&#23376;(NIOs)&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#21487;&#20197;&#31283;&#20581;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#65292;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2301.11167</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#30340;&#31070;&#32463;&#21453;&#28436;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Neural Inverse Operators for Solving PDE Inverse Problems. (arXiv:2301.11167v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#21453;&#28436;&#31639;&#23376;(NIOs)&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#21487;&#20197;&#31283;&#20581;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#65292;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PDE&#21453;&#38382;&#39064;&#30340;&#22823;&#37096;&#20998;&#21453;&#38382;&#39064;&#21482;&#26377;&#20316;&#20026;&#31639;&#23376;&#21040;&#20989;&#25968;&#26144;&#23556;&#25165;&#34987;&#24456;&#22909;&#22320;&#23450;&#20041;&#12290;&#29616;&#26377;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#23558;&#20989;&#25968;&#26144;&#23556;&#21040;&#20989;&#25968;&#65292;&#38656;&#36827;&#34892;&#20462;&#25913;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21453;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#65292;&#31216;&#20026;&#31070;&#32463;&#21453;&#28436;&#31639;&#23376;(NIOs)&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;PDE&#21453;&#38382;&#39064;&#12290;&#21463;&#22522;&#30784;&#25968;&#23398;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;NIO&#22522;&#20110;DeepONets&#21644;FNOs&#30340;&#36866;&#24403;&#32452;&#21512;&#26469;&#36924;&#36817;&#20174;&#31639;&#23376;&#21040;&#20989;&#25968;&#30340;&#26144;&#23556;&#12290;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#65292;&#34920;&#26126;NIO&#26174;&#33879;&#22320;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#31283;&#20581;&#65292;&#20934;&#30830;&#22320;&#35299;&#20915;PDE&#21453;&#38382;&#39064;&#65292;&#19988;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#30452;&#25509;&#21644;PDE&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large class of inverse problems for PDEs are only well-defined as mappings from operators to functions. Existing operator learning frameworks map functions to functions and need to be modified to learn inverse maps from data. We propose a novel architecture termed Neural Inverse Operators (NIOs) to solve these PDE inverse problems. Motivated by the underlying mathematical structure, NIO is based on a suitable composition of DeepONets and FNOs to approximate mappings from operators to functions. A variety of experiments are presented to demonstrate that NIOs significantly outperform baselines and solve PDE inverse problems robustly, accurately and are several orders of magnitude faster than existing direct and PDE-constrained optimization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.10448</link><description>&lt;p&gt;
&#39044;&#35745;&#31639;&#20869;&#23384;&#25110;&#23454;&#26102;&#32534;&#30721;&#65311;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#20351;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#26368;&#22823;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#35299;&#30721;&#22120;&#20013;&#30340;Fusion&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#35774;&#32622;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#23545;&#22823;&#37327;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#36827;&#34892;&#32534;&#30721;&#65292;&#23427;&#20204;&#20063;&#38750;&#24120;&#26114;&#36149;&#12290;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#23558;&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#32534;&#30721;&#20026;&#20869;&#23384;&#65292;&#24182;&#30452;&#25509;&#26816;&#32034;&#23494;&#38598;&#34920;&#31034;&#26469;&#36991;&#20813;&#36825;&#31181;&#25104;&#26412;&#12290;&#20294;&#26159;&#65292;&#39044;&#32534;&#30721;&#20869;&#23384;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36136;&#37327;&#24809;&#32602;&#65292;&#22240;&#20026;&#20869;&#23384;&#34920;&#31034;&#26410;&#38024;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUMEN&#65292;&#23427;&#26159;&#36825;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#30340;&#28151;&#21512;&#20307;&#65292;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#32534;&#30721;&#22120;&#23436;&#25104;&#32534;&#30721;&#65292;&#35813;&#23454;&#26102;&#32534;&#30721;&#22120;&#26159;&#22522;&#20110;&#38382;&#39064;&#36827;&#34892;&#26465;&#20214;&#21270;&#30340;&#65292;&#24182;&#20026;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;LUMEN&#26126;&#26174;&#20248;&#20110;&#32431;&#20869;&#23384;&#65292;&#21516;&#26102;&#27604;FiD&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;LUMEN&#30340;&#25928;&#26524;&#20248;&#20110;&#20004;&#32773;&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#26102;&#65292;LUMEN&#30456;&#23545;&#20110;FiD&#30340;&#20248;&#21183;&#20063;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20309;&#29256;&#26412;&#30340;Weisfeiler-Leman&#27979;&#35797;(GWL)&#65292;&#21487;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309;GNN&#30340;&#34920;&#29616;&#21147;</title><link>http://arxiv.org/abs/2301.09308</link><description>&lt;p&gt;
&#35770;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Geometric Graph Neural Networks. (arXiv:2301.09308v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20309;&#29256;&#26412;&#30340;Weisfeiler-Leman&#27979;&#35797;(GWL)&#65292;&#21487;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309;GNN&#30340;&#34920;&#29616;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; Weisfeiler-Leman (WL) &#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#30340;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340; GNNs &#21644; WL &#26694;&#26550;&#19981;&#36866;&#29992;&#20110;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20960;&#20309;&#22270;&#24418;&#65292;&#20363;&#22914;&#29983;&#29289;&#20998;&#23376;&#12289;&#26448;&#26009;&#21644;&#20854;&#20182;&#29289;&#29702;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; WL &#27979;&#35797;&#30340;&#20960;&#20309;&#29256;&#26412; (GWL)&#65292;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#21516;&#26102;&#23562;&#37325;&#24213;&#23618;&#29289;&#29702;&#23545;&#31216;&#24615;&#65306;&#25490;&#21015;&#12289;&#26059;&#36716;&#12289;&#21453;&#23556;&#21644;&#24179;&#31227;&#12290;&#25105;&#20204;&#20351;&#29992; GWL &#26469;&#34920;&#24449;&#20855;&#26377;&#19981;&#21464;&#25110;&#31561;&#21464;&#20110;&#29289;&#29702;&#23545;&#31216;&#24615;&#30340;&#20960;&#20309; GNN &#30340;&#34920;&#29616;&#21147;&#65292;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#12290;GWL &#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309; GNN &#30340;&#34920;&#29616;&#21147;&#65306;(1) &#19981;&#21464;&#23618;&#34920;&#29616;&#21147;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#19968;&#36339;&#30456;&#21516;&#30340;&#20960;&#20309;&#22270;&#24418;&#65307;(2) &#31561;&#21464;&#23618;&#36890;&#36807;&#20256;&#25773;&#23616;&#37096;&#37051;&#22495;&#20043;&#22806;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#21306;&#20998;&#26356;&#22823;&#31867;&#21035;&#30340;&#22270;&#24418;&#65307;(3)
&lt;/p&gt;
&lt;p&gt;
The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3)
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;SAM&#21450;&#20854;&#21464;&#20307;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;SAM&#26356;&#21916;&#27426;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#32780;&#38750;&#23574;&#23792;&#20540;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;SAM&#22312;&#26576;&#20123;&#29616;&#23454;&#26465;&#20214;&#19979;&#20250;&#34987;&#21560;&#24341;&#21040;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.08203</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;SAM&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65306;&#29702;&#35770;&#19982;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
An SDE for Modeling SAM: Theory and Insights. (arXiv:2301.08203v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08203
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;SAM&#21450;&#20854;&#21464;&#20307;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;SAM&#26356;&#21916;&#27426;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#32780;&#38750;&#23574;&#23792;&#20540;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;SAM&#22312;&#26576;&#20123;&#29616;&#23454;&#26465;&#20214;&#19979;&#20250;&#34987;&#21560;&#24341;&#21040;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#65288;Sharpness-Aware Minimization&#65289;&#20248;&#21270;&#22120;&#65292;&#30001;&#20110;&#20854;&#22312;&#27604;&#26356;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21464;&#20307;&#19978;&#34920;&#29616;&#20986;&#30340;&#26356;&#39640;&#24615;&#33021;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#24456;&#22810;&#20154;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20197;SDEs&#30340;&#24418;&#24335;&#65289;&#26469;&#22788;&#29702;SAM&#21450;&#20854;&#20004;&#20010;&#21464;&#20307;&#65292;&#21253;&#25324;&#20840;&#25209;&#37327;&#21644;&#23567;&#25209;&#37327;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SDE&#26159;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#30340;&#20005;&#26684;&#36924;&#36817;&#65288;&#20197;&#24369;&#24847;&#20041;&#65292;&#19982;&#23398;&#20064;&#36895;&#29575;&#32447;&#24615;&#32553;&#25918;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;SAM&#26356;&#21916;&#27426;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#32780;&#38750;&#23574;&#23792;&#20540;-&#8212;&#36890;&#36807;&#23637;&#31034;SAM&#22312;&#26368;&#23567;&#21270;&#20855;&#26377;Hessian&#30456;&#20851;&#22122;&#22768;&#32467;&#26500;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SAM&#22312;&#26576;&#20123;&#29616;&#23454;&#26465;&#20214;&#19979;&#20250;&#34987;&#21560;&#24341;&#21040;&#38797;&#28857;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#33719;&#24471;&#20102;&#35814;&#32454;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#32858;&#31867;&#25439;&#22833;&#32452;&#25104;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#21033;&#29992;&#32858;&#31867;&#25439;&#22833;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.03041</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#32858;&#31867;&#25439;&#22833;&#20043;&#38388;&#20851;&#31995;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning. (arXiv:2301.03041v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#32858;&#31867;&#25439;&#22833;&#32452;&#25104;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#21033;&#29992;&#32858;&#31867;&#25439;&#22833;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#20351;&#32593;&#32476;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#21147;&#30340;&#29305;&#24449;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22270;&#20687;&#20004;&#20010;&#25193;&#22686;&#29256;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#25193;&#22686;&#29256;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#35299;&#25918;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#23398;&#20064;&#21482;&#26159;&#21033;&#29992;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#65292;&#32780;&#23398;&#21040;&#30340;&#20449;&#24687;&#21487;&#33021;&#20165;&#23616;&#38480;&#20110;&#21516;&#19968;&#23454;&#20363;&#30340;&#19981;&#21516;&#35270;&#22270;&#12290;&#26412;&#25991;&#35797;&#22270;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#29305;&#24449;&#32423;&#20132;&#21449;&#29109;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20004;&#20010;&#25439;&#22833;&#23545;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#28165;&#26970;&#12290;&#30456;&#20284;&#24615;&#25439;&#22833;&#26377;&#21161;&#20110;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#24471;&#20855;&#26377;&#36776;&#21035;&#21147;&#21644;&#40065;&#26834;&#24615;&#30340;&#29305;&#24449;&#12290;&#32858;&#31867;&#25439;&#22833;&#24378;&#35843;&#22312;&#32858;&#31867;&#20013;&#20998;&#32452;&#30456;&#20284;&#25968;&#25454;&#24182;&#20998;&#31163;&#19981;&#21516;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#32858;&#31867;&#25439;&#22833;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#21033;&#29992;&#32858;&#31867;&#25439;&#22833;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning enables networks to learn discriminative features from massive data itself. Most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. By utilizing the consistency of two augmentations, the burden of manual annotations can be freed. Contrastive learning exploits instance-level information to learn robust features. However, the learned information is probably confined to different views of the same instance. In this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. In contrast to instance-level information, the similarity between two distinct images may provide more useful information. Besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. These two losses are essential for most deep learning methods. However, the relation between these two losses is not clear. Similarity loss helps o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Detector Explanation Toolkit (DExT)&#65292;&#20351;&#29992;&#26576;&#20123;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#23454;&#29616;&#20102;&#29983;&#25104;&#25152;&#26377;&#26816;&#27979;&#22120;&#20915;&#31574;&#30340;&#20840;&#38754;&#35299;&#37322;&#12290;&#23427;&#21487;&#20197;&#20026;&#36793;&#30028;&#26694;&#21644;&#20998;&#31867;&#20915;&#31574;&#20135;&#29983;&#35299;&#37322;&#65292;&#26159;&#19968;&#20010;&#24110;&#21161;&#20154;&#20204;&#20102;&#35299;&#29289;&#20307;&#26816;&#27979;&#22120;&#31995;&#32479;&#20915;&#31574;&#21407;&#22240;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.11409</link><description>&lt;p&gt;
DExT&#65306;&#26816;&#27979;&#22120;&#35828;&#26126;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
DExT: Detector Explanation Toolkit. (arXiv:2212.11409v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Detector Explanation Toolkit (DExT)&#65292;&#20351;&#29992;&#26576;&#20123;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#23454;&#29616;&#20102;&#29983;&#25104;&#25152;&#26377;&#26816;&#27979;&#22120;&#20915;&#31574;&#30340;&#20840;&#38754;&#35299;&#37322;&#12290;&#23427;&#21487;&#20197;&#20026;&#36793;&#30028;&#26694;&#21644;&#20998;&#31867;&#20915;&#31574;&#20135;&#29983;&#35299;&#37322;&#65292;&#26159;&#19968;&#20010;&#24110;&#21161;&#20154;&#20204;&#20102;&#35299;&#29289;&#20307;&#26816;&#27979;&#22120;&#31995;&#32479;&#20915;&#31574;&#21407;&#22240;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#30001;&#20110;&#20854;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#20869;&#37096;&#35745;&#31639;&#32780;&#34987;&#35270;&#20026;&#40657;&#30418;&#23376;&#12290;&#21363;&#20351;&#22312;&#26816;&#27979;&#22120;&#24615;&#33021;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#30001;&#20110;&#26080;&#27861;&#35299;&#37322;&#20854;&#36755;&#20986;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#26410;&#33021;&#20026;&#36793;&#30028;&#26694;&#21644;&#20998;&#31867;&#20915;&#31574;&#20135;&#29983;&#35299;&#37322;&#65292;&#24182;&#19988;&#36890;&#24120;&#20026;&#21508;&#31181;&#26816;&#27979;&#22120;&#21046;&#20316;&#21333;&#29420;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Detector Explanation Toolkit (DExT)&#65292;&#23427;&#20351;&#29992;&#26576;&#20123;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#23454;&#29616;&#20102;&#29983;&#25104;&#25152;&#26377;&#26816;&#27979;&#22120;&#20915;&#31574;&#30340;&#20840;&#38754;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#22810;&#23545;&#35937;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#21512;&#24182;&#22270;&#20687;&#20013;&#26816;&#27979;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#30340;&#35299;&#37322;&#20197;&#21450;&#21333;&#20010;&#22270;&#20687;&#20013;&#30456;&#24212;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26816;&#27979;&#22120;&#30456;&#27604;&#65292;Single Shot MultiBox Detector (SSD) &#26356;&#33021;&#24544;&#23454;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art object detectors are treated as black boxes due to their highly non-linear internal computations. Even with unprecedented advancements in detector performance, the inability to explain how their outputs are generated limits their use in safety-critical applications. Previous work fails to produce explanations for both bounding box and classification decisions, and generally make individual explanations for various detectors. In this paper, we propose an open-source Detector Explanation Toolkit (DExT) which implements the proposed approach to generate a holistic explanation for all detector decisions using certain gradient-based explanation methods. We suggests various multi-object visualization methods to merge the explanations of multiple objects detected in an image as well as the corresponding detections in a single image. The quantitative evaluation show that the Single Shot MultiBox Detector (SSD) is more faithfully explained compared to other detectors regardless
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10786</link><description>&lt;p&gt;
&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#24050;&#32463;&#25193;&#23637;&#21040;&#36328;&#25991;&#26723;&#22330;&#26223;&#20013;&#65292;&#22240;&#20026;&#35768;&#22810;&#20851;&#31995;&#19981;&#20165;&#20165;&#22312;&#19968;&#20010;&#25991;&#26723;&#20013;&#25551;&#36848;&#12290;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#20102;&#26377;&#25928;&#30340;&#24320;&#25918;&#31354;&#38388;&#35777;&#25454;&#26816;&#32034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#22810;&#36339;&#25512;&#29702;&#30340;&#25361;&#25112;&#65292;&#20197;&#22788;&#29702;&#25955;&#24067;&#22312;&#24320;&#25918;&#24335;&#25991;&#26723;&#38598;&#20013;&#30340;&#23454;&#20307;&#21644;&#35777;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MR.COD(&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#36335;&#24452;&#25366;&#25496;&#21644;&#25490;&#24207;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20010;&#26816;&#32034;&#22120;&#30340;&#21464;&#20307;&#65292;&#20197;&#26174;&#31034;&#35777;&#25454;&#26816;&#32034;&#22312;&#36328;&#25991;&#26723;RE&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#20026;&#27492;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#12290;&#22312;CodRED&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MR.COD&#30340;&#35777;&#25454;&#26816;&#32034;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#31471;&#21040;&#31471;RE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations, along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose MR.COD (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking. We explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE. We also propose a contextual dense retriever for this setting. Experiments on CodRED show that evidence retrieval with MR.COD effectively acquires crossdocument evidence and boosts end-to-end RE performance in both closed and open settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10726</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#25104;&#21151;&#29992;&#20110;&#26816;&#32034;&#35821;&#20041;&#23545;&#40784;&#30340;&#21477;&#23376;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#25209;&#37327;&#22788;&#29702;&#25110;&#31934;&#24515;&#30340;&#24037;&#31243;&#25165;&#33021;&#22863;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#35821;&#35328;&#25991;&#26412;&#23884;&#20837;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#32034;&#25110;&#35780;&#20998;&#21477;&#23376;&#23545;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;$N$&#31181;&#35821;&#35328;&#30340;&#24182;&#34892;&#25968;&#25454;&#19978;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#23558;&#32763;&#35793;&#20043;&#38388;&#20849;&#20139;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#35821;&#20307;&#25110;&#35821;&#35328;&#29305;&#23450;&#21464;&#21270;&#20998;&#24320;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22810;&#35821;&#35328;&#25991;&#26412;&#23884;&#20837;&#26041;&#38754;&#30340;&#22823;&#35268;&#27169;&#20180;&#32454;&#27604;&#36739;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21313;&#20998;&#27969;&#34892;&#21364;&#20174;&#26410;&#27604;&#36739;&#36807;&#30340;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#21452;&#35821;&#25366;&#25496;&#21644;&#36328;&#35821;&#35328;&#38382;&#39064;&#26816;&#32034;&#8212;&#8212;&#26368;&#21518;&#19968;&#20010;&#20219;&#21153;&#23558;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09282</link><description>&lt;p&gt;
APOLLO&#65306;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#36923;&#36753;&#25512;&#29702;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#12289;&#23427;&#20204;&#30340;&#30456;&#20114;&#32852;&#31995;&#65292;&#28982;&#21518;&#36890;&#36807;&#23427;&#20204;&#26469;&#25512;&#26029;&#26032;&#30340;&#32467;&#35770;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;Wikipedia&#30340;&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22522;&#20110;&#19968;&#32452;&#36923;&#36753;&#25512;&#29702;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65306;&#20462;&#25913;&#36807;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#21482;&#23545;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25512;&#29702;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#26412;&#35821;&#35328;&#29702;&#35299;&#30340;&#29305;&#23450;&#35789;&#24615;&#30340;&#21333;&#35789;&#36827;&#34892;&#25513;&#30721;&#65292;&#20197;&#21450;&#21477;&#23376;&#32423;&#20998;&#31867;&#25439;&#22833;&#65292;&#25945;&#23548;&#27169;&#22411;&#21306;&#20998;&#36923;&#36753;&#19978;&#36830;&#25509;&#21644;&#19981;&#36830;&#25509;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;APOLLO&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19981;&#25439;&#22833;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning of text is an important ability that requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills. In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20013;&#38388;&#27169;&#22411;&#21629;&#21517;&#20026; PMLP &#24182;&#22312;&#27979;&#35797;&#26102;&#37319;&#29992; GNNs &#30340;&#26550;&#26500;&#65292;&#21457;&#29616; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09034</link><description>&lt;p&gt;
GNN &#21644; MLP &#30456;&#20114;&#32852;&#31995;&#25581;&#31034; GNN &#22312;&#26412;&#36136;&#19978;&#26159;&#22909;&#30340;&#27867;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20013;&#38388;&#27169;&#22411;&#21629;&#21517;&#20026; PMLP &#24182;&#22312;&#27979;&#35797;&#26102;&#37319;&#29992; GNNs &#30340;&#26550;&#26500;&#65292;&#21457;&#29616; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20107;&#23454;&#19978;&#27169;&#22411;&#31867;&#21035;&#65292;&#23427;&#20204;&#24314;&#31435;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20307;&#31995;&#32467;&#26500;&#20043;&#19978;&#65292;&#24182;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#20197;&#20801;&#35768;&#29305;&#24449;&#22312;&#33410;&#28857;&#20043;&#38388;&#27969;&#21160;&#12290;&#26412;&#25991;&#29468;&#27979; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#21457;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299; GNNs &#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#24182;&#21487;&#20197;&#29992;&#20316;&#26356;&#28145;&#23618;&#27425;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.08153</link><description>&lt;p&gt;
FiDO&#65306;&#38024;&#23545;&#26356;&#24378;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#19978;&#26641;&#31435;&#20102;&#19994;&#30028;&#26631;&#26438;&#12290;&#20294;&#26159;&#65292;FiD&#25152;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#36890;&#36807;&#23545;&#26631;&#20934;T5&#27169;&#22411;&#20570;&#26368;&#23567;&#20462;&#25913;&#32780;&#36873;&#25321;&#30340;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#23545;&#20110;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#26469;&#35828;&#26159;&#39640;&#24230;&#19981;&#20248;&#21270;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;FiD&#23558;&#22823;&#37096;&#20998;FLOPs&#20998;&#37197;&#32473;&#20102;&#32534;&#30721;&#22120;&#65292;&#32780;&#22823;&#22810;&#25968;&#25512;&#29702;&#26102;&#38388;&#26159;&#30001;&#20110;&#35299;&#30721;&#22120;&#20013;&#30340;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#24182;&#20351;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;7&#20493;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25104;&#26412;&#20351;&#29992;&#26356;&#22823;&#30340;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#32463;&#36807;&#19978;&#36848;&#20462;&#25913;&#30340;FiD&#31216;&#20026;FiDO&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#24191;&#27867;&#30340;&#25512;&#29702;&#39044;&#31639;&#33539;&#22260;&#20869;&#27604;&#29616;&#26377;&#30340;FiD&#27169;&#22411;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;FiDO-Large-XXL&#27604;FiD-Base&#36827;&#34892;&#26356;&#24555;&#30340;&#25512;&#29702;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;FiD-Large&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.17091</link><description>&lt;p&gt;
&#21033;&#29992;&#37492;&#21035;&#22120;&#24341;&#23548;&#22312;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#23436;&#21892;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#26088;&#22312;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#65292;&#26126;&#30830;&#22320;&#30417;&#30563;&#21435;&#22122;&#26679;&#26412;&#36335;&#24452;&#26159;&#21542;&#30495;&#23454;&#12290;&#19982; GAN &#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#32852;&#21512;&#35757;&#32451;&#35780;&#20998;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#37492;&#21035;&#22120;&#35757;&#32451;&#31283;&#23450;&#19988;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#26679;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#21521;&#39044;&#35757;&#32451;&#30340;&#35780;&#20998;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#39033;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;&#35813;&#39033;&#23558;&#27169;&#22411;&#35780;&#20998;&#30699;&#27491;&#20026;&#26368;&#20248;&#37492;&#21035;&#22120;&#22788;&#30340;&#25968;&#25454;&#35780;&#20998;&#65292;&#36825;&#24847;&#21619;&#30528;&#37492;&#21035;&#22120;&#20197;&#34917;&#20805;&#30340;&#26041;&#24335;&#24110;&#21161;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#25968;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312; ImageNet 256x256 &#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID&#65288;1.68&#65289;&#21644;&#21484;&#22238;&#29575;&#65288;0.66&#65289;&#12290;&#25105;&#20204;&#22312; https://github.com/alsdudrla10/DG &#19978;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26041;&#27861;&#30340;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30446;&#21069;&#22256;&#38590;&#19988;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15368</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#20219;&#24847;&#22823;&#30340;&#26631;&#35760;&#38543;&#26426;&#21487;&#28385;&#36275;&#24615;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Arbitrarily Large Labelled Random Satisfiability Formulas for Machine Learning Training. (arXiv:2211.15368v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26041;&#27861;&#30340;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30446;&#21069;&#22256;&#38590;&#19988;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#20013;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#26041;&#21521;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#19978;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#30340;&#29702;&#35770;&#26680;&#24515;&#24615;&#21644;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#38556;&#30861;&#26159;&#65292;&#35757;&#32451;&#38598;&#20165;&#38480;&#20110;&#27604;&#23454;&#38469;&#24863;&#20852;&#36259;&#30340;&#20844;&#24335;&#23567;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#38543;&#26426;&#20844;&#24335;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#27867;&#21270;&#30340;&#20005;&#37325;&#25285;&#24551;&#65292;&#22240;&#20026;&#26631;&#35760;&#36234;&#26469;&#36234;&#22823;&#30340;&#38543;&#26426;&#20844;&#24335;&#21464;&#24471;&#19981;&#21487;&#35299;&#12290;&#36890;&#36807;&#22522;&#26412;&#24605;&#24819;&#20013;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#25105;&#20204;&#23436;&#20840;&#28040;&#38500;&#20102;&#36825;&#20010;&#38556;&#30861;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#20219;&#24847;&#25152;&#38656;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#65292;&#32780;&#26080;&#38656;&#35299;&#20915;&#24213;&#23618;&#20915;&#31574;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#21464;&#31616;&#21333;&#26631;&#37327;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#20844;&#24335;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#26159;&#21487;&#35843;&#30340;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential. Research in this direction has focused on the Boolean satisfiability (SAT) problem, both because of its theoretical centrality and practical importance. A major roadblock faced, though, is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest, raising serious concerns about generalization. This is because labeling random formulas of increasing size rapidly becomes intractable. By exploiting the probabilistic method in a fundamental way, we remove this roadblock entirely: we show how to generate correctly labeled random formulas of any desired size, without having to solve the underlying decision problem. Moreover, the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter. This opens up an entirely new level of sophistication fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;OpenFE&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.12507</link><description>&lt;p&gt;
OpenFE: &#20855;&#26377;&#19987;&#23478;&#32423;&#24615;&#33021;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
OpenFE: Automated Feature Generation with Expert-level Performance. (arXiv:2211.12507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;OpenFE&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25670;&#33073;&#25163;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#32321;&#29712;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20174;&#22823;&#37327;&#20505;&#36873;&#29305;&#24449;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#26377;&#25928;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OpenFE&#65292;&#19968;&#31181;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#12290;OpenFE&#36890;&#36807;&#20004;&#20010;&#32452;&#20214;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#65306;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#35780;&#20272;&#20505;&#36873;&#29305;&#24449;&#30340;&#22686;&#37327;&#24615;&#33021;&#65307;2&#65289;&#19968;&#31181;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#65292;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#36827;&#34892;&#29305;&#24449;&#20462;&#21098;&#12290;&#22312;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;OpenFE&#27604;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20004;&#20010;Kaggle&#27604;&#36187;&#20013;&#23545;OpenFE&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#27604;&#36187;&#26377;&#25968;&#21315;&#20010;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of automated feature generation is to liberate machine learning experts from the laborious task of manual feature generation, which is crucial for improving the learning performance of tabular data. The major challenge in automated feature generation is to efficiently and accurately identify effective features from a vast pool of candidate features. In this paper, we present OpenFE, an automated feature generation tool that provides competitive results against machine learning experts. OpenFE achieves high efficiency and accuracy with two components: 1) a novel feature boosting method for accurately evaluating the incremental performance of candidate features and 2) a two-stage pruning algorithm that performs feature pruning in a coarse-to-fine manner. Extensive experiments on ten benchmark datasets show that OpenFE outperforms existing baseline methods by a large margin. We further evaluate OpenFE in two Kaggle competitions with thousands of data science teams participating. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#39044;&#21518;&#29983;&#23384;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24739;&#32773;&#22312;&#25509;&#21463;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#33391;&#32467;&#26524;&#12290;&#26368;&#20339;&#27169;&#22411;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.11965</link><description>&lt;p&gt;
&#39044;&#27979;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#30340;&#19981;&#33391;&#32467;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting adverse outcomes following catheter ablation treatment for atrial fibrillation. (arXiv:2211.11965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#39044;&#21518;&#29983;&#23384;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24739;&#32773;&#22312;&#25509;&#21463;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#33391;&#32467;&#26524;&#12290;&#26368;&#20339;&#27169;&#22411;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20026;&#39044;&#27979;&#38750;&#29923;&#33180;&#24615;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#19981;&#33391;&#32467;&#26524;&#65292;&#24320;&#21457;&#39044;&#21518;&#29983;&#23384;&#27169;&#22411;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#32852;&#21512;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28595;&#22823;&#21033;&#20122;&#26032;&#21335;&#23041;&#23572;&#22763;&#24030;&#24739;&#32773;&#30340;&#21307;&#38498;&#34892;&#25919;&#25968;&#25454;&#12289;&#22788;&#26041;&#33647;&#29289;&#32034;&#36180;&#12289;&#24613;&#35786;&#31185;&#21576;&#29616;&#21644;&#27515;&#20129;&#30331;&#35760;&#12290;&#35813;&#38431;&#21015;&#21253;&#25324;&#25509;&#21463;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;&#20256;&#32479;&#21644;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#20027;&#35201;&#20986;&#34880;&#20107;&#20214;&#21644;&#24515;&#21147;&#34928;&#31469;&#12289;&#20013;&#39118;&#12289;&#24515;&#33039;&#39588;&#20572;&#21644;&#27515;&#20129;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;&#32467;&#26524;&#65306;&#22312;3285&#21517;&#38431;&#21015;&#20013;&#65292;177&#21517;&#65288;5.3&#65285;&#65289;&#32463;&#21382;&#20102;&#24515;&#21147;&#34928;&#31469;&#12289;&#20013;&#39118;&#12289;&#24515;&#33039;&#39588;&#20572;&#21644;&#27515;&#20129;&#30340;&#22797;&#21512;&#32467;&#26524;&#65292;167&#21517;&#65288;5.1&#65285;&#65289;&#22312;&#25509;&#21463;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#20986;&#29616;&#20102;&#37325;&#22823;&#20986;&#34880;&#20107;&#20214;&#12290;&#39044;&#27979;&#32452;&#21512;&#32467;&#26524;&#30340;&#27169;&#22411;&#20855;&#26377;&#39640;&#39118;&#38505;&#36776;&#21035;&#20934;&#30830;&#24615;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#25152;&#35780;&#20272;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#20855;&#26377;&#19968;&#33268;&#24615;&#25351;&#25968;&gt; 0.79&#12290;&#39044;&#27979;&#37325;&#22823;&#20986;&#34880;&#20107;&#20214;&#30340;&#27169;&#22411;&#31934;&#24230;&#36739;&#20302;&#65292;&#20294;&#20173;&#20248;&#20110;&#26420;&#32032;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20004;&#31181;&#32467;&#26524;&#30340;&#39044;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;&#32467;&#35770;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#39044;&#21518;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#25151;&#39076;&#21160;&#30005;&#23376;&#28040;&#34701;&#27835;&#30103;&#21518;&#30340;&#19981;&#33391;&#32467;&#26524;&#65292;&#22312;&#39044;&#27979;&#32452;&#21512;&#32467;&#26524;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#39044;&#27979;&#37325;&#22823;&#20986;&#34880;&#20107;&#20214;&#26041;&#38754;&#20855;&#26377;&#21512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To develop prognostic survival models for predicting adverse outcomes after catheter ablation treatment for non-valvular atrial fibrillation (AF).  Methods: We used a linked dataset including hospital administrative data, prescription medicine claims, emergency department presentations, and death registrations of patients in New South Wales, Australia. The cohort included patients who received catheter ablation for AF. Traditional and deep survival models were trained to predict major bleeding events and a composite of heart failure, stroke, cardiac arrest, and death.  Results: Out of a total of 3285 patients in the cohort, 177 (5.3%) experienced the composite outcome (heart failure, stroke, cardiac arrest, death) and 167 (5.1%) experienced major bleeding events after catheter ablation treatment. Models predicting the composite outcome had high risk discrimination accuracy, with the best model having a concordance index &gt; 0.79 at the evaluated time horizons. Models for predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38750;&#23545;&#31216;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#30693;&#22120;&#20559;&#32622;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection. (arXiv:2211.11255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38750;&#23545;&#31216;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#21035;&#22120;&#27169;&#22411;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#23481;&#26131;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#65292;&#30041;&#19979;&#19981;&#33391;&#24773;&#20917;&#21644;&#24694;&#24847;&#25915;&#20987;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24863;&#30693;&#22120;&#20559;&#32622;&#20551;&#35774;&#65292;&#23427;&#34920;&#26126;&#21028;&#21035;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26576;&#20123;&#29305;&#24449;&#26356;&#20026;&#25935;&#24863;&#65292;&#23548;&#33268;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23558;&#25193;&#25955;&#27169;&#22411;(DMs)&#38598;&#25104;&#21040;OOD&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#21435;&#22122;&#36807;&#31243;(DDP)&#20316;&#20026;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#38750;&#23545;&#31216;&#25554;&#20540;&#65292;&#24456;&#36866;&#21512;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#22312;DDP&#19979;&#65292;OOD&#25968;&#25454;&#30340;&#21028;&#21035;&#22120;&#27169;&#22411;&#29305;&#24449;&#34920;&#29616;&#20026;&#23574;&#38160;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#33539;&#25968;...
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial task for ensuring the reliability and safety of deep learning. Currently, discriminator models outperform other methods in this regard. However, the feature extraction process used by discriminator models suffers from the loss of critical information, leaving room for bad cases and malicious attacks. In this paper, we introduce a new perceptron bias assumption that suggests discriminator models are more sensitive to certain features of the input, leading to the overconfidence problem. To address this issue, we propose a novel framework that combines discriminator and generation models and integrates diffusion models (DMs) into OOD detection. We demonstrate that the diffusion denoising process (DDP) of DMs serves as a novel form of asymmetric interpolation, which is well-suited to enhance the input and mitigate the overconfidence problem. The discriminator model features of OOD data exhibit sharp changes under DDP, and we utilize the norm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10773</link><description>&lt;p&gt;
&#19968;&#31181; $k$ &#26368;&#36817;&#37051;&#30340;&#20004;&#38454;&#27573;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#33258;&#21160;&#36866;&#24212;&#20998;&#24067;&#27604;&#20363;&#21464;&#21270;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#38024;&#23545;&#33258;&#28982;&#20445;&#30041;&#36825;&#20123;&#20248;&#31168;&#29305;&#24615;&#30340;&#26412;&#22320;&#25237;&#31080;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#65292;&#35774;&#35745;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#27492; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#20026;&#36890;&#36807;&#25105;&#20204;&#26041;&#26696;&#33719;&#21462;&#30340;&#26679;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#24403;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968; $\mathbb{P}(Y=y|X=x)$ &#36275;&#22815;&#24179;&#28369;&#24182;&#19988; Tsybakov &#22122;&#22768;&#26465;&#20214;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
$k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20505;&#36873;&#27169;&#22411;&#31354;&#38388;&#20869;&#36827;&#34892;&#25554;&#20540;&#65292;&#21516;&#26102;&#20272;&#35745;&#29366;&#24577;&#22330;&#21644;&#21442;&#25968;&#20540;&#65292;&#35299;&#20915;&#20102;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#39044;&#27979;&#21160;&#24577;&#27169;&#22411;&#22240;&#25968;&#25454;&#31232;&#30095;&#21644;&#27169;&#22411;&#22810;&#26679;&#24615;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06714</link><description>&lt;p&gt;
&#32852;&#21512;&#29983;&#29289;&#22320;&#29699;&#21270;&#23398;-&#29289;&#29702;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Learning of Coupled Biogeochemical-Physical Models. (arXiv:2211.06714v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20505;&#36873;&#27169;&#22411;&#31354;&#38388;&#20869;&#36827;&#34892;&#25554;&#20540;&#65292;&#21516;&#26102;&#20272;&#35745;&#29366;&#24577;&#22330;&#21644;&#21442;&#25968;&#20540;&#65292;&#35299;&#20915;&#20102;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#39044;&#27979;&#21160;&#24577;&#27169;&#22411;&#22240;&#25968;&#25454;&#31232;&#30095;&#21644;&#27169;&#22411;&#22810;&#26679;&#24615;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#30340;&#39044;&#27979;&#21160;&#24577;&#27169;&#22411;&#21487;&#29992;&#20110;&#21508;&#31181;&#38656;&#27714;&#12290;&#30001;&#20110;&#27979;&#37327;&#31232;&#30095;&#21644;&#26377;&#38480;&#29702;&#35299;&#28023;&#27915;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#23384;&#22312;&#24456;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20027;&#35201;&#34920;&#29616;&#22312;&#27169;&#22411;&#21442;&#25968;&#20540;&#12289;&#19981;&#21516;&#21442;&#25968;&#21270;&#30340;&#21151;&#33021;&#24418;&#24335;&#12289;&#25152;&#38656;&#30340;&#22797;&#26434;&#31243;&#24230;&#20197;&#21450;&#29366;&#24577;&#22330;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20505;&#36873;&#27169;&#22411;&#31354;&#38388;&#20869;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20174;&#22122;&#22768;&#12289;&#31232;&#30095;&#21644;&#38388;&#25509;&#35266;&#27979;&#20013;&#21457;&#29616;&#26032;&#27169;&#22411;&#65292;&#21516;&#26102;&#20272;&#35745;&#29366;&#24577;&#22330;&#21644;&#21442;&#25968;&#20540;&#65292;&#20197;&#21450;&#25152;&#26377;&#23398;&#20064;&#25968;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#22686;&#24378;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;GMM-DO&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;&#30001;PDEs&#32479;&#27835;&#30340;&#39640;&#32500;&#24230;&#21644;&#22810;&#23398;&#31185;&#21160;&#21147;&#23398;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#21253;&#25324;&#38543;&#26426;&#20844;&#24335;&#21644;&#22797;&#26434;&#24230;&#21442;&#25968;&#65292;&#23558;&#20505;&#36873;&#27169;&#22411;&#32479;&#19968;&#25104;&#21333;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#38543;&#26426;&#25193;&#23637;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive dynamical models for marine ecosystems are used for a variety of needs. Due to sparse measurements and limited understanding of the myriad of ocean processes, there is however significant uncertainty. There is model uncertainty in the parameter values, functional forms with diverse parameterizations, level of complexity needed, and thus in the state fields. We develop a Bayesian model learning methodology that allows interpolation in the space of candidate models and discovery of new models from noisy, sparse, and indirect observations, all while estimating state fields and parameter values, as well as the joint PDFs of all learned quantities. We address the challenges of high-dimensional and multidisciplinary dynamics governed by PDEs by using state augmentation and the computationally efficient GMM-DO filter. Our innovations include stochastic formulation and complexity parameters to unify candidate models into a single general model as well as stochastic expansion paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CausalCF&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24050;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.05551</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Counterfactuals for Improving the Robustness of Reinforcement Learning. (arXiv:2211.05551v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CausalCF&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24050;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#33258;&#20027;&#22320;&#23398;&#20064;&#20219;&#21153;&#12290;&#20219;&#21153;&#36234;&#37325;&#35201;&#65292;&#23545;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#30340;&#38656;&#27714;&#23601;&#36234;&#39640;&#12290;&#22240;&#26524;RL&#23558;RL&#21644;&#22240;&#26524;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20351;RL&#26356;&#21152;&#40065;&#26834;&#12290;&#22240;&#26524;RL&#20195;&#29702;&#20351;&#29992;&#22240;&#26524;&#34920;&#31034;&#26469;&#25429;&#25417;&#21487;&#20197;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#19981;&#21464;&#22240;&#26524;&#26426;&#21046;&#12290;&#30446;&#21069;&#65292;&#22240;&#26524;RL&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#19981;&#23436;&#25972;&#25110;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalCF&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;Causal Curiosity&#21644;CoPhy&#30340;&#24605;&#24819;&#12290;Causal Curiosity&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#24178;&#39044;&#30340;&#26041;&#27861;&#65292;&#24182;&#20462;&#25913;&#20102;CoPhy&#65292;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;Causal Curiosity&#24050;&#24212;&#29992;&#20110;CausalWorld&#20013;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#12290;CausalWorld&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#20223;&#30495;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is used in various robotic applications. RL enables agents to learn tasks autonomously by interacting with the environment. The more critical the tasks are, the higher the demand for the robustness of the RL systems. Causal RL combines RL and causal inference to make RL more robust. Causal RL agents use a causal representation to capture the invariant causal mechanisms that can be transferred from one task to another. Currently, there is limited research in Causal RL, and existing solutions are usually not complete or feasible for real-world applications. In this work, we propose CausalCF, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy. Causal Curiosity provides an approach for using interventions, and CoPhy is modified to enable the RL agent to perform counterfactuals. Causal Curiosity has been applied to robotic grasping and manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#29992;&#20110;&#21452;&#23618;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming. (arXiv:2211.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#35268;&#21010;&#36817;&#26399;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#21333;&#26426;&#25110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#24182;&#23384;&#22312;&#36890;&#20449;&#25104;&#26412;&#39640;&#21644;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#20989;&#25968;&#30340;&#20998;&#25955;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel programming has recently received attention in the literature, due to a wide range of applications, including reinforcement learning and hyper-parameter optimization. However, it is widely assumed that the underlying bilevel optimization problem is solved either by a single machine or in the case of multiple machines connected in a star-shaped network, i.e., federated learning setting. The latter approach suffers from a high communication cost on the central node (e.g., parameter server) and exhibits privacy vulnerabilities. Hence, it is of interest to develop methods that solve bilevel optimization problems in a communication-efficient decentralized manner. To that end, this paper introduces a penalty function based decentralized algorithm with theoretical guarantees for this class of optimization problems. Specifically, a distributed alternating gradient-type algorithm for solving consensus bilevel programming over a decentralized network is developed. A key feature of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#21644;GAN&#30340;ECG&#20449;&#21495;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20351;&#29992;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#26356;&#20026;&#36924;&#30495;&#65292;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02626</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#24515;&#30005;&#22270;&#21512;&#25104;&#20013;&#21033;&#29992;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Statistical Shape Priors in GAN-based ECG Synthesis. (arXiv:2211.02626v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#21644;GAN&#30340;ECG&#20449;&#21495;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20351;&#29992;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#26356;&#20026;&#36924;&#30495;&#65292;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#36827;&#34892;&#24515;&#30005;&#22270;(ECG)&#25968;&#25454;&#25910;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;,&#22240;&#27492;&#24515;&#30005;&#22270;&#25968;&#25454;&#21512;&#25104;&#26159;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#21644;&#32479;&#35745;ECG&#25968;&#25454;&#24314;&#27169;&#26469;&#29983;&#25104;ECG&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;ECG&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#30340;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;ECG&#20449;&#21495;&#30340;&#26102;&#38388;&#21644;&#24133;&#24230;&#21464;&#21270;&#24314;&#27169;&#20026;2-D&#24418;&#29366;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#26356;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) data collection during emergency situations is challenging, making ECG data generation an efficient solution for dealing with highly imbalanced ECG training datasets. In this paper, we propose a novel approach for ECG signal generation using Generative Adversarial Networks (GANs) and statistical ECG data modeling. Our approach leverages prior knowledge about ECG dynamics to synthesize realistic signals, addressing the complex dynamics of ECG signals. To validate our approach, we conducted experiments using ECG signals from the MIT-BIH arrhythmia database. Our results demonstrate that our approach, which models temporal and amplitude variations of ECG signals as 2-D shapes, generates more realistic signals compared to state-of-the-art GAN based generation baselines. Our proposed approach has significant implications for improving the quality of ECG training datasets, which can ultimately lead to better performance of ECG classification algorithms. This research c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01842</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#20998;&#23618;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#20013;&#21457;&#29616;&#31070;&#32463;&#32467;&#26500;&#26159;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#25628;&#32034;&#19968;&#20123;&#38480;&#23450;&#26041;&#38754;&#30340;&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#32479;&#19968;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#32780;&#32039;&#20945;&#22320;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#27604;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#31354;&#38388;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#22686;&#24378;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#23618;&#26680;&#35774;&#35745;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#25628;&#32034;&#22914;&#27492;&#24222;&#22823;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#25628;&#32034;&#31574;&#30053;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#25512;&#23548;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36798;&#21040;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#32676;&#20307;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.01528</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#22788;&#29702;&#23454;&#29616;&#20844;&#24179;&#21644;&#26368;&#20248;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fair and Optimal Classification via Post-Processing. (arXiv:2211.01528v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#25512;&#23548;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36798;&#21040;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#32676;&#20307;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#21576;&#29616;&#30340;&#20559;&#35265;&#65292;&#20844;&#24179;&#24615;&#26631;&#20934;&#21487;&#20197;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;&#22312;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#20013;&#23454;&#29616;&#20844;&#24179;&#23545;&#24453;&#65292;&#28982;&#32780;&#36825;&#24448;&#24448;&#26159;&#20197;&#27169;&#22411;&#34920;&#29616;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36825;&#31181;&#26435;&#34913;&#26159;&#20844;&#24179;&#31639;&#27861;&#35774;&#35745;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#22312;&#26368;&#26222;&#36941;&#30340;&#22810;&#32452;&#12289;&#22810;&#31867;&#21035;&#21644;&#22024;&#26434;&#35774;&#32622;&#19979;&#65292;&#23436;&#25972;&#22320;&#34920;&#24449;&#20102;&#20844;&#24179;&#12289;&#36798;&#25705;&#23572;&#24179;&#31561;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20869;&#22312;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#21644;&#23646;&#24615;&#24863;&#30693;&#20844;&#24179;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#26368;&#23567;&#38169;&#35823;&#29575;&#26159;&#30001;&#27779;&#29791;&#26031;&#22374;&#37325;&#24515;&#38382;&#39064;&#30340;&#26368;&#20248;&#20540;&#32473;&#20986;&#30340;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#20174;&#35780;&#20998;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#35780;&#20998;&#20026;&#36125;&#21494;&#26031;&#26368;&#20248;&#26102;&#24471;&#21040;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#27425;&#20248;&#24615;&#20998;&#26512;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2210.15469</link><description>&lt;p&gt;
&#23398;&#20064;SDN&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#27979;&#35797;&#22833;&#36133;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Failure-Inducing Models for Testing Software-Defined Networks. (arXiv:2210.15469v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#33021;&#22815;&#23454;&#29616;&#30001;&#20013;&#22830;&#21270;&#30340;&#36719;&#20214;&#25511;&#21046;&#22120;&#31649;&#29702;&#30340;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#21487;&#33021;&#20250;&#30772;&#22351;&#22522;&#20110;SDN&#30340;&#31995;&#32479;&#30340;&#24213;&#23618;&#36890;&#20449;&#32593;&#32476;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#20180;&#32454;&#27979;&#35797;&#12290;&#24403;&#22522;&#20110;SDN&#30340;&#31995;&#32479;&#22833;&#36133;&#26102;&#65292;&#24037;&#31243;&#24072;&#38656;&#35201;&#31934;&#30830;&#22320;&#20102;&#35299;&#20854;&#21457;&#29983;&#26465;&#20214;&#20197;&#24212;&#23545;&#27492;&#31867;&#25925;&#38556;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#65288;1&#65289;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#21644;&#65288;2&#65289;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#65292;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FuzzSDN&#26159;&#39318;&#27425;&#23581;&#35797;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;SDN&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#24320;&#28304;SDN&#25511;&#21046;&#22120;&#25511;&#21046;&#30340;&#31995;&#32479;&#26469;&#35780;&#20272;FuzzSDN&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;FuzzSDN&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;SDN&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#21644;&#20004;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, FuzzSDN is the first attempt to simultaneously address these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Further, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22768;&#28304;&#23450;&#20301;&#22521;&#35757;&#31574;&#30053;&#65292;&#20351;&#29992;&#28369;&#21160;&#32622;&#25442;&#19981;&#21464;&#35757;&#32451;&#26469;&#36319;&#36394;&#22810;&#20010;&#21464;&#21270;&#30340;&#22768;&#28304;&#20301;&#32622;&#65292;&#26368;&#23567;&#21270;&#36523;&#20221;&#20999;&#25442;(IDS)&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.14536</link><description>&lt;p&gt;
&#29992;&#28369;&#21160;&#32622;&#25442;&#19981;&#21464;&#35757;&#32451;&#26469;&#36319;&#36394;&#22810;&#20010;&#21464;&#21270;&#30340;&#22768;&#28304;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Position tracking of a varying number of sound sources with sliding permutation invariant training. (arXiv:2210.14536v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22768;&#28304;&#23450;&#20301;&#22521;&#35757;&#31574;&#30053;&#65292;&#20351;&#29992;&#28369;&#21160;&#32622;&#25442;&#19981;&#21464;&#35757;&#32451;&#26469;&#36319;&#36394;&#22810;&#20010;&#21464;&#21270;&#30340;&#22768;&#28304;&#20301;&#32622;&#65292;&#26368;&#23567;&#21270;&#36523;&#20221;&#20999;&#25442;(IDS)&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#21644;&#23398;&#20064;&#30340;&#22768;&#28304;&#23450;&#20301;&#65288;SSL&#65289;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22768;&#23398;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#25345;&#32493;&#36319;&#36394;&#20986;&#29616;&#21644;&#28040;&#22833;&#30340;&#22810;&#20010;&#22768;&#28304;&#65292;&#23601;&#20687;&#22312;&#29616;&#23454;&#20013;&#19968;&#26679;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31574;&#30053;&#65292;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;SSL&#27169;&#22411;&#65292;&#20854;&#30452;&#25509;&#23454;&#29616;&#22522;&#20110;&#20808;&#21069;&#26102;&#38388;&#24103;&#20013;&#20272;&#35745;&#21644;&#21442;&#32771;&#20301;&#32622;&#20043;&#38388;&#30340;&#26368;&#20248;&#20851;&#32852;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#23427;&#20248;&#21270;&#20102;&#36319;&#36394;&#31995;&#32479;&#30340;&#26399;&#26395;&#29305;&#24615;&#65306;&#22788;&#29702;&#26102;&#21464;&#30340;&#22810;&#20010;&#22768;&#28304;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#30340;&#36712;&#36857;&#25490;&#24207;&#23450;&#20301;&#20272;&#35745;&#65292;&#26368;&#23567;&#21270;&#36523;&#20221;&#20999;&#25442;(IDS)&#12290;&#22312;&#27169;&#25311;&#22810;&#20010;&#28151;&#21709;&#31227;&#21160;&#26469;&#28304;&#30340;&#25968;&#25454;&#21644;&#20004;&#20010;&#27169;&#22411;&#26550;&#26500;&#30340;&#35780;&#20272;&#20013;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#20943;&#23569;&#36523;&#20221;&#20999;&#25442;&#30340;&#21516;&#26102;&#19981;&#25439;&#22833;&#36880;&#24103;&#23450;&#20301;&#31934;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data- and learning-based sound source localization (SSL) methods have shown strong performance in challenging acoustic scenarios. However, little work has been done on adapting such methods to track consistently multiple sources appearing and disappearing, as would occur in reality. In this paper, we present a new training strategy for deep learning SSL models with a straightforward implementation based on the mean squared error of the optimal association between estimated and reference positions in the preceding time frames. It optimizes the desired properties of a tracking system: handling a time-varying number of sources and ordering localization estimates according to their trajectories, minimizing identity switches (IDSs). Evaluation on simulated data of multiple reverberant moving sources and on two model architectures proves its effectiveness on reducing identity switches without compromising frame-wise localization accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#25299;&#25169;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;TO&#21644;ML&#20004;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;MLTO&#30740;&#31350;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.10782</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25299;&#25169;&#20248;&#21270;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Topology Optimization via Machine Learning and Deep Learning: A Review. (arXiv:2210.10782v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#25299;&#25169;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;TO&#21644;ML&#20004;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;MLTO&#30740;&#31350;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20248;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#28385;&#36275;&#19968;&#23450;&#30340;&#33655;&#36733;&#21644;&#36793;&#30028;&#26465;&#20214;&#65292;&#25512;&#23548;&#20986;&#26368;&#20339;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#21021;&#22987;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26377;&#25928;&#30340;&#35774;&#35745;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;21&#19990;&#32426;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#22240;&#27492;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#65292;&#20197;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#25299;&#25169;&#20248;&#21270;&#26469;&#23454;&#29616;&#26377;&#25928;&#21644;&#24555;&#36895;&#30340;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#22238;&#39038;&#21644;&#20998;&#26512;&#20102;&#20808;&#21069;&#20851;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25299;&#25169;&#20248;&#21270;&#65288;MLTO&#65289;&#30340;&#30740;&#31350;&#12290;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545; MLTO &#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21363;(1) TO&#21644;(2) ML&#35270;&#35282;&#12290;TO&#30340;&#35282;&#24230;&#35299;&#31572;&#20102;&#20026;&#20160;&#20040;&#35201;&#23558;ML&#29992;&#20110;TO&#65292;&#32780;ML&#30340;&#35282;&#24230;&#21017;&#35299;&#31572;&#20102;&#22914;&#20309;&#23558;ML&#24212;&#29992;&#20110;TO&#12290;&#27492;&#22806;&#65292;&#36824;&#26816;&#26597;&#20102;&#30446;&#21069;MLTO&#30740;&#31350;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization (TO) is a method of deriving an optimal design that satisfies a given load and boundary conditions within a design domain. This method enables effective design without initial design, but has been limited in use due to high computational costs. At the same time, machine learning (ML) methodology including deep learning has made great progress in the 21st century, and accordingly, many studies have been conducted to enable effective and rapid optimization by applying ML to TO. Therefore, this study reviews and analyzes previous research on ML-based TO (MLTO). Two different perspectives of MLTO are used to review studies: (1) TO and (2) ML perspectives. The TO perspective addresses "why" to use ML for TO, while the ML perspective addresses "how" to apply ML to TO. In addition, the limitations of current MLTO research and future research directions are examined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#30340;&#38543;&#26426;&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#39640;&#39118;&#38505;&#20915;&#31574;&#31995;&#32479;&#20013;&#38754;&#20020;&#30340;&#27495;&#35270;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.08781</link><description>&lt;p&gt;
&#38543;&#26426;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Differentially Private and Fair Learning. (arXiv:2210.08781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#30340;&#38543;&#26426;&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#39640;&#39118;&#38505;&#20915;&#31574;&#31995;&#32479;&#20013;&#38754;&#20020;&#30340;&#27495;&#35270;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#65292;&#27604;&#22914;&#26576;&#20123;&#31181;&#26063;&#12289;&#24615;&#21035;&#25110;&#24180;&#40836;&#30340;&#20010;&#20307;&#12290;&#36825;&#20123;&#24212;&#29992;&#20013;&#21478;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20405;&#29359;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#26469;&#20943;&#36731;&#27495;&#35270;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#20173;&#28982;&#21487;&#33021;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#30340;&#20581;&#24247;&#25110;&#36130;&#21153;&#35760;&#24405;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#27010;&#24565;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#26082;&#33021;&#22815;&#20445;&#25252;&#38544;&#31169;&#21448;&#33021;&#22815;&#36827;&#34892;&#20844;&#24179;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38543;&#26426; DP &#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#35201;&#20040;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#65292;&#35201;&#20040;&#38656;&#35201;&#27599;&#27425;&#31639;&#27861;&#36845;&#20195;&#20013;&#20351;&#29992;&#23436;&#25972;&#30340;&#25209;&#37327;&#25968;&#25454;&#26469;&#36827;&#34892;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#30340;&#38543;&#26426;&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#37324;&#30340;&#8220;&#38543;&#26426;&#8221;&#19968;&#35789;&#25351;&#30340;&#26159;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals' health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term "stochastic" refers
&lt;/p&gt;</description></item><item><title>MonoNeRF&#26159;&#19968;&#31181;&#26080;&#38656;&#28145;&#24230;&#21644;&#30456;&#26426;&#23039;&#24577;&#26631;&#27880;&#30340;&#36890;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27169;&#22411;&#65292;&#21487;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23398;&#24471;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#21644;&#21333;&#24352;&#22270;&#20687;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2210.07181</link><description>&lt;p&gt;
MonoNeRF&#65306;&#22312;&#26080;&#30456;&#26426;&#23039;&#24577;&#19979;&#65292;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23398;&#20064;&#36890;&#29992;&#30340;NeRF&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose. (arXiv:2210.07181v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07181
&lt;/p&gt;
&lt;p&gt;
MonoNeRF&#26159;&#19968;&#31181;&#26080;&#38656;&#28145;&#24230;&#21644;&#30456;&#26426;&#23039;&#24577;&#26631;&#27880;&#30340;&#36890;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27169;&#22411;&#65292;&#21487;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23398;&#24471;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#21644;&#21333;&#24352;&#22270;&#20687;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MonoNeRF&#30340;&#36890;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#30340;&#21333;&#30446;&#35270;&#39057;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#26469;&#29983;&#25104;&#38745;&#24577;&#22330;&#26223;&#30340;&#31227;&#21160;&#22270;&#20687;&#65292;&#26080;&#38656;&#28145;&#24230;&#21644;&#30456;&#26426;&#23039;&#24577;&#30340;&#26631;&#27880;&#12290;MonoNeRF&#30340;&#22522;&#30784;&#26159;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20272;&#35745;&#21333;&#30446;&#28145;&#24230;&#21644;&#30456;&#26426;&#23039;&#24577;&#65292;&#35299;&#30721;&#22120;&#26681;&#25454;&#28145;&#24230;&#32534;&#30721;&#22120;&#29305;&#24449;&#26500;&#24314;&#22810;&#24179;&#38754;NeRF&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#20272;&#35745;&#30340;&#30456;&#26426;&#28210;&#26579;&#36755;&#20837;&#24103;&#12290;&#23398;&#20064;&#30001;&#37325;&#26500;&#35823;&#24046;&#30417;&#30563;&#12290;&#23398;&#24471;&#27169;&#22411;&#21518;&#65292;&#21487;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#21644;&#21333;&#24352;&#22270;&#20687;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#26356;&#22810;&#23450;&#24615;&#32467;&#26524;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#26597;&#30475;&#65306;https://oasisyang.github.io/mononerf&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generalizable neural radiance fields - MonoNeRF, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane NeRF representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#33539;&#24335;&#65292;&#25552;&#20379;&#20102;MARO&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#25191;&#34892;&#26102;&#38388;&#20013;&#39044;&#27979;&#32570;&#22833;&#30340;&#26234;&#33021;&#20307;&#35266;&#27979;&#20540;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.06274</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28151;&#21512;&#25191;&#34892;&#38598;&#20013;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning. (arXiv:2210.06274v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#33539;&#24335;&#65292;&#25552;&#20379;&#20102;MARO&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#25191;&#34892;&#26102;&#38388;&#20013;&#39044;&#27979;&#32570;&#22833;&#30340;&#26234;&#33021;&#20307;&#35266;&#27979;&#20540;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#25191;&#34892;&#38598;&#20013;&#24335;&#35757;&#32451;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#33539;&#24335;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#65292;&#26234;&#33021;&#20307;&#20204;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#26469;&#23436;&#25104;&#21512;&#20316;&#20219;&#21153;&#65292;&#24182;&#22312;&#25191;&#34892;&#26102;&#38388;&#20013;&#23454;&#29616;&#36890;&#20449;&#32423;&#21035;&#30340;&#20219;&#24847;&#21464;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31867;&#21517;&#20026;&#8220;&#28151;&#21512;-POMDPs&#8221;&#30340;&#22810;&#26234;&#33021;&#20307;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MARO&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#22312;&#38598;&#20013;&#24335;&#35757;&#32451;&#19979;&#39044;&#27979;&#25191;&#34892;&#26102;&#38388;&#20013;&#32570;&#22833;&#30340;&#26234;&#33021;&#20307;&#35266;&#27979;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;MARO&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#21508;&#31181;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;MARO&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents' observations at execution time. We evaluate MARO on sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#33976;&#39311;&#30340;&#26694;&#26550;(LLP)&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21305;&#37197;&#20197;&#27599;&#20010;&#65288;&#38170;&#65289;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#20851;&#31995;&#30693;&#35782;&#26469;&#33976;&#39311;&#38142;&#25509;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#24555;&#25463;&#32780;&#26377;&#25928;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.05801</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#33976;&#39311;&#30340;&#26080;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Linkless Link Prediction via Relational Distillation. (arXiv:2210.05801v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#33976;&#39311;&#30340;&#26694;&#26550;(LLP)&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21305;&#37197;&#20197;&#27599;&#20010;&#65288;&#38170;&#65289;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#20851;&#31995;&#30693;&#35782;&#26469;&#33976;&#39311;&#38142;&#25509;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#24555;&#25463;&#32780;&#26377;&#25928;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#38142;&#25509;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#30001;&#38750;&#24179;&#20961;&#37051;&#22495;&#25968;&#25454;&#20381;&#36182;&#24615;&#24102;&#26469;&#30340;&#39640;&#24310;&#36831;&#38480;&#21046;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24050;&#30693;&#30340;&#39640;&#25928;&#22810;&#23618;&#24863;&#30693;&#22120;&#30001;&#20110;&#32570;&#20047;&#32852;&#31995;&#30693;&#35782;&#32780;&#27604;&#22270;&#31070;&#32463;&#32593;&#32476;&#19981;&#22815;&#26377;&#25928;&#12290;&#20026;&#20102;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20248;&#28857;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#30452;&#25509;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#39044;&#27979;&#30340;&#36923;&#36753;&#21305;&#37197;&#21644;&#33410;&#28857;&#34920;&#31034;&#21305;&#37197;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#30452;&#25509;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;Linkless Link Prediction&#65288;LLP&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#33976;&#39311;&#38142;&#25509;&#39044;&#27979;&#30340;&#30693;&#35782;&#12290;&#19982;&#31616;&#21333;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21305;&#37197;&#29420;&#31435;&#38142;&#25509;&#36923;&#36753;&#25110;&#33410;&#28857;&#34920;&#31034;&#19981;&#21516;&#65292;LLP&#33976;&#39311;&#20197;&#27599;&#20010;&#65288;&#38170;&#65289;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#20851;&#31995;&#30693;&#35782;&#21040;&#23398;&#29983;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25490;&#21517;&#30340;&#21305;&#37197;&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#21305;&#37197;&#26469;&#33976;&#39311;&#20851;&#31995;&#30693;&#35782;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLP&#22312;&#25512;&#29702;&#26102;&#36895;&#24230;&#24555;&#24471;&#22810;&#65292;&#32780;&#19988;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.04318</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36817;&#20284;&#21644;&#39044;&#27979;&#36235;&#21183;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PIs&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;PI&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#26435;&#37325;&#30001;&#21306;&#38388;&#23485;&#24230;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#31616;&#27905;&#25968;&#23398;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#25512;&#23548;PI&#65292;&#24182;&#35770;&#35777;&#20102;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#30456;&#20851;&#21464;&#37327;&#30340;PI&#32780;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#19979;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#30340;PI&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2210.04183</link><description>&lt;p&gt;
MAMO&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20027;&#35201;&#33268;&#21147;&#20110;&#24314;&#31435;&#20840;&#23616;&#32423;&#21035;&#30340;&#22270;&#20687;&#19982;&#35821;&#35328;&#23545;&#40784;&#65292;&#32570;&#20047;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#25513;&#33180;&#65292;&#24182;&#38598;&#25104;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#25513;&#33180;&#20449;&#21495;&#12290;&#20854;&#20013;&#65292;&#38544;&#24335;&#30446;&#26631;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#20379;&#32479;&#19968;&#19988;&#26080;&#20559;&#24046;&#30340;&#30446;&#26631;&#65292;&#27169;&#22411;&#39044;&#27979;&#26410;&#25513;&#33180;&#36755;&#20837;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#65307;&#26174;&#24335;&#30446;&#26631;&#21017;&#36890;&#36807;&#24674;&#22797;&#22270;&#20687;&#22359;&#30340;&#21160;&#37327;&#35270;&#35273;&#29305;&#24449;&#21644;&#21333;&#35789;&#26631;&#35760;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#25513;&#33180;&#24314;&#27169;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#36824;&#33021;&#23398;&#20064;&#21040;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#27604;&#20272;&#35745;&#25216;&#26415;&#26469;&#39044;&#27979;&#26032;&#20998;&#24067;&#30340;&#25130;&#27490;&#38408;&#20540;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#26631;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#26032;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04166</link><description>&lt;p&gt;
&#22522;&#20110;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples. (arXiv:2210.04166v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#27604;&#20272;&#35745;&#25216;&#26415;&#26469;&#39044;&#27979;&#26032;&#20998;&#24067;&#30340;&#25130;&#27490;&#38408;&#20540;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#26631;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#26032;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#20998;&#31867;&#22120;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#39044;&#27979;&#27809;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#22522;&#26412;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#36890;&#36807;&#35745;&#31639;&#19968;&#32452;&#21253;&#21547;&#20855;&#26377;&#29992;&#25143;&#25351;&#23450;&#27010;&#29575;&#30340;&#27491;&#30830;&#31867;&#30340;&#31867;&#26469;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#25552;&#20379;&#36825;&#26679;&#30340;&#38598;&#21512;&#65292;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#24120;&#24120;&#22522;&#20110;&#26657;&#20934;&#38598;&#21512;&#20272;&#35745;&#27010;&#29575;&#20272;&#35745;&#30340;&#25130;&#26029;&#38408;&#20540;&#12290;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#20165;&#22312;&#26657;&#20934;&#38598;&#21512;&#19982;&#27979;&#35797;&#38598;&#30456;&#21516;&#26102;&#20445;&#35777;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#32622;&#20449;&#24230;&#39044;&#27979;&#22120;&#38656;&#35201;&#20026;&#26032;&#20998;&#24067;&#37325;&#26032;&#26657;&#20934;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#24456;&#23569;&#26377;&#26469;&#33258;&#26032;&#20998;&#24067;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#20351;&#26657;&#20934;&#25104;&#20026;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#26410;&#26631;&#35760;&#26679;&#26412;&#39044;&#27979;&#26032;&#20998;&#24067;&#25130;&#27490;&#38408;&#20540;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#19968;&#33324;&#24773;&#20917;&#19979;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#26657;&#20934;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26032;&#20998;&#24067;&#30340;&#25130;&#27490;&#38408;&#20540;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#26631;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#26657;&#20934;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern image classifiers are very accurate, but the predictions come without uncertainty estimates. Conformal predictors provide uncertainty estimates by computing a set of classes containing the correct class with a user-specified probability based on the classifier's probability estimates. To provide such sets, conformal predictors often estimate a cutoff threshold for the probability estimates based on a calibration set. Conformal predictors guarantee reliability only when the calibration set is from the same distribution as the test set. Therefore, conformal predictors need to be recalibrated for new distributions. However, in practice, labeled data from new distributions is rarely available, making calibration infeasible. In this work, we consider the problem of predicting the cutoff threshold for a new distribution based on unlabeled examples. While it is impossible in general to guarantee reliability when calibrating based on unlabeled examples, we propose a method that provides
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#30828;&#32422;&#26463;&#26694;&#26550;&#26469;&#35299;&#20915;&#20960;&#20309;&#22797;&#26434;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#23558;&#36793;&#30028;&#26465;&#20214;&#36716;&#21270;&#20026;&#32447;&#24615;&#26041;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#25439;&#22833;&#39033;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#32593;&#32476;&#24182;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.03526</link><description>&lt;p&gt;
&#35299;&#20915;&#20960;&#20309;&#22797;&#26434;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32479;&#19968;&#30828;&#32422;&#26463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs. (arXiv:2210.03526v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#30828;&#32422;&#26463;&#26694;&#26550;&#26469;&#35299;&#20915;&#20960;&#20309;&#22797;&#26434;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#23558;&#36793;&#30028;&#26465;&#20214;&#36716;&#21270;&#20026;&#32447;&#24615;&#26041;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#25439;&#22833;&#39033;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#32593;&#32476;&#24182;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30828;&#32422;&#26463;&#26694;&#26550;&#65292;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20960;&#20309;&#22797;&#26434;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#32771;&#34385;&#20102;&#26368;&#24120;&#29992;&#30340;Dirichlet&#12289;Neumann&#21644;Robin&#36793;&#30028;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#28151;&#21512;&#26377;&#38480;&#20803;&#26041;&#27861;&#24341;&#20837;&#8220;&#39069;&#22806;&#22330;&#8221;&#65292;&#37325;&#26032;&#26500;&#36896;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#31561;&#20215;&#30340;&#26041;&#24335;&#23558;&#19977;&#31181;&#31867;&#22411;&#30340;&#36793;&#30028;&#26465;&#20214;&#36716;&#21270;&#20026;&#32447;&#24615;&#26041;&#31243;&#12290;&#22522;&#20110;&#37325;&#26032;&#26500;&#36896;&#65292;&#25105;&#20204;&#23545;&#36793;&#30028;&#26465;&#20214;&#25512;&#23548;&#20986;&#36890;&#29992;&#35299;&#26512;&#35299;&#65292;&#29992;&#20110;&#26500;&#36896;&#19968;&#20010;&#31526;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#35797;&#39564;&#20989;&#25968;&#12290;&#26377;&#20102;&#36825;&#26679;&#19968;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#28155;&#21152;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#20960;&#20309;&#22797;&#26434;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#32531;&#35299;&#19982;&#36793;&#30028;&#26465;&#20214;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#23545;&#24212;&#30340;&#25439;&#22833;&#39033;&#20043;&#38388;&#22833;&#34913;&#31454;&#20105;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#8220;&#39069;&#22806;&#22330;&#8221;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20960;&#20309;&#22797;&#26434;&#20559;&#24494;&#20998;&#26041;&#31243;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified hard-constraint framework for solving geometrically complex PDEs with neural networks, where the most commonly used Dirichlet, Neumann, and Robin boundary conditions (BCs) are considered. Specifically, we first introduce the "extra fields" from the mixed finite element method to reformulate the PDEs so as to equivalently transform the three types of BCs into linear equations. Based on the reformulation, we derive the general solutions of the BCs analytically, which are employed to construct an ansatz that automatically satisfies the BCs. With such a framework, we can train the neural networks without adding extra loss terms and thus efficiently handle geometrically complex PDEs, alleviating the unbalanced competition between the loss terms corresponding to the BCs and PDEs. We theoretically demonstrate that the "extra fields" can stabilize the training process. Experimental results on real-world geometrically complex PDEs showcase the effectiveness of our method co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GSDN&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#30340;&#22270;&#24418;&#33258;&#25105;&#33976;&#39311;&#65292;&#20197;&#20943;&#23567;GNN&#21644;MLP&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2210.02097</link><description>&lt;p&gt;
&#33258;&#25105;&#25945;&#23398;&#65306;&#22522;&#20110;&#37051;&#22495;&#30340;&#22270;&#24418;&#33258;&#25105;&#33976;&#39311;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Teaching Yourself: Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GSDN&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#30340;&#22270;&#24418;&#33258;&#25105;&#33976;&#39311;&#65292;&#20197;&#20943;&#23567;GNN&#21644;MLP&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#19982;&#22270;&#26377;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#30028;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20173;&#28982;&#26159;&#20027;&#35201;&#30340;&#24037;&#20316;&#39532;&#21147;&#12290;&#36896;&#25104;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24046;&#36317;&#20043;&#19968;&#26159;GNN&#20013;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#25152;&#24341;&#36215;&#30340;&#37051;&#22495;&#33719;&#21462;&#24310;&#36831;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#22312;&#38656;&#35201;&#24555;&#36895;&#25512;&#29702;&#30340;&#38656;&#35201;&#20302;&#24310;&#36831;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#28041;&#21450;&#20219;&#20309;&#29305;&#24449;&#32858;&#21512;&#65292;MLP&#27809;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#65292;&#25512;&#29702;&#36895;&#24230;&#27604;GNN&#35201;&#24555;&#24471;&#22810;&#65292;&#20294;&#20854;&#24615;&#33021;&#19981;&#22815;&#31454;&#20105;&#21147;&#12290;&#21463;&#36825;&#20123;&#20114;&#34917;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#30340;&#22270;&#24418;&#33258;&#25105;&#33976;&#39311;&#65288;GSDN&#65289;&#26694;&#26550;&#65292;&#20197;&#32553;&#23567;GNN&#21644;MLP&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#22320;&#65292;GSDN&#26694;&#26550;&#22522;&#20110;&#32431;MLP&#65292;&#32467;&#26500;&#20449;&#24687;&#20165;&#34987;&#38544;&#24335;&#22320;&#29992;&#20316;&#20808;&#39564;&#65292;&#20197;&#25351;&#23548;&#37051;&#22495;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#30693;&#35782;&#33258;&#25105;&#33976;&#39311;&#65292;&#26367;&#25442;&#25481;&#29305;&#24449;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for this academic-industrial gap is the neighborhood-fetching latency incurred by data dependency in GNNs, which make it hard to deploy for latency-sensitive applications that require fast inference. Conversely, without involving any feature aggregation, MLPs have no data dependency and infer much faster than GNNs, but their performance is less competitive. Motivated by these complementary strengths and weaknesses, we propose a Graph Self-Distillation on Neighborhood (GSDN) framework to reduce the gap between GNNs and MLPs. Specifically, the GSDN framework is based purely on MLPs, where structural information is only implicitly used as prior to guide knowledge self-distillation between the neighborhood and the target, substituting th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2210.01212</link><description>&lt;p&gt;
&#36890;&#36807;&#20887;&#20313;&#24615;&#23454;&#29616;&#31232;&#30095;&#24615;&#65306;&#29992;SGD&#27714;&#35299;$L_1$
&lt;/p&gt;
&lt;p&gt;
Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;$L_1$&#24809;&#32602;&#31561;&#20215;&#20110;&#24102;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#21487;&#24494;&#37325;&#21442;&#25968;&#21270;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#23545;&#20110;&#36890;&#29992;&#30340;&#38750;&#20984;&#20989;&#25968;&#65292;&#37325;&#21442;&#25968;&#21270;&#25216;&#24039;&#26159;&#23436;&#20840;&#8220;&#33391;&#24615;&#8221;&#30340;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;(1)&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#22312;&#38750;&#24120;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;(2)&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#20808;&#21069;&#23581;&#35797;&#24212;&#29992;$L_1$&#24809;&#32602;&#30340;&#26041;&#27861;&#22343;&#26410;&#25104;&#21151;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#36973;&#21463;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#27745;&#26579;&#30340;&#25915;&#20987;&#33021;&#22815;&#23454;&#29616;&#36739;&#22909;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.15266</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#36973;&#21463;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#36973;&#21463;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#27745;&#26579;&#30340;&#25915;&#20987;&#33021;&#22815;&#23454;&#29616;&#36739;&#22909;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#36827;&#34892;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20063;&#20250;&#20351;&#27169;&#22411;&#38754;&#20020;&#28508;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#26088;&#22312;&#25200;&#21160;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#35302;&#21457;&#24694;&#24847;&#34892;&#20026;&#12290;&#26412;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#36827;&#34892;&#20102;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#25506;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35821;&#35328;&#27169;&#24577;&#26159;&#21542;&#20063;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65311;&#65288;2&#65289;&#21738;&#31181;&#27169;&#24577;&#26368;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#25915;&#20987;&#31867;&#22411;&#12290;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#19977;&#31181;&#25915;&#20987;&#37117;&#33021;&#22815;&#22312;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the newly emerged multimodal models, which leverage both visual and linguistic modalities to train powerful encoders, have gained increasing attention. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training data to trigger malicious behaviors in it. In contrast to previous work, only poisoning visual modality, in this work, we take the first step to studying poisoning attacks against multimodal models in both visual and linguistic modalities. Specially, we focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we propose three types of poisoning attacks against multimodal models. Extensive evaluations on different datasets and model architectures show that all three attacks can achieve significant attack performance while maintaining 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; GFlowNets &#35757;&#32451;&#30446;&#26631;&#8212;&#8212;&#23376;&#36712;&#36857;&#24179;&#34913;(SubTB($\lambda$))&#65292;&#20174;&#37096;&#20998; episode &#23398;&#20064;&#30340;&#26041;&#24335;&#21487;&#20197;&#21152;&#36895;&#37319;&#26679;&#22120;&#22312;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20351;&#24471;&#22312;&#20043;&#21069;&#38590;&#20197;&#35757;&#32451;&#30340;&#38271;&#21160;&#20316;&#24207;&#21015;&#21644;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#22815;&#35757;&#32451; GFlowNets&#12290;</title><link>http://arxiv.org/abs/2209.12782</link><description>&lt;p&gt;
&#20174;&#37096;&#20998; episode &#20013;&#23398;&#20064; GFlowNets &#20197;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning GFlowNets from partial episodes for improved convergence and stability. (arXiv:2209.12782v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; GFlowNets &#35757;&#32451;&#30446;&#26631;&#8212;&#8212;&#23376;&#36712;&#36857;&#24179;&#34913;(SubTB($\lambda$))&#65292;&#20174;&#37096;&#20998; episode &#23398;&#20064;&#30340;&#26041;&#24335;&#21487;&#20197;&#21152;&#36895;&#37319;&#26679;&#22120;&#22312;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20351;&#24471;&#22312;&#20043;&#21069;&#38590;&#20197;&#35757;&#32451;&#30340;&#38271;&#21160;&#20316;&#24207;&#21015;&#21644;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#22815;&#35757;&#32451; GFlowNets&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26410;&#24402;&#19968;&#21270;&#30340;&#30446;&#26631;&#23494;&#24230;&#19979;&#23545;&#31163;&#25955;&#23545;&#35937;&#36827;&#34892;&#39034;&#24207;&#37319;&#26679;&#35757;&#32451;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#27010;&#29575;&#24314;&#27169;&#20219;&#21153;&#20013;&#12290;&#29616;&#26377;&#30340;GFlowNets&#35757;&#32451;&#30446;&#26631;&#26082;&#21487;&#20197;&#23616;&#37096;&#20851;&#27880;&#29366;&#24577;&#25110;&#36716;&#25442;&#65292;&#20063;&#21487;&#20197;&#22312;&#25972;&#20010;&#37319;&#26679;&#36712;&#36857;&#19978;&#20256;&#25773;&#22870;&#21169;&#20449;&#21495;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26367;&#20195;&#26041;&#26696;&#20195;&#34920;&#20102;&#26799;&#24230;&#20559;&#24046;&#8212;&#26041;&#24046;&#24179;&#34913;&#30340;&#20004;&#31471;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#31181;&#24179;&#34913;&#26469;&#20943;&#36731;&#20854;&#26377;&#23475;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#21463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340; TD($\lambda$)&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#36712;&#36857;&#24179;&#34913;(SubTB($\lambda$))&#65292;&#19968;&#31181; GFlowNets &#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#38271;&#24230;&#30340;&#37096;&#20998;&#21160;&#20316;&#23376;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; SubTB($\lambda$) &#21152;&#36895;&#20102;&#22312;&#20808;&#21069;&#30740;&#31350;&#36807;&#30340;&#21644;&#26032;&#30340;&#29615;&#22659;&#20013;&#30340;&#37319;&#26679;&#22120;&#25910;&#25947;&#65292;&#24182;&#20351;&#24471;&#22312;&#21160;&#20316;&#24207;&#21015;&#26356;&#38271;&#12289;&#22870;&#21169;&#26356;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451; GFlowNets &#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\lambda$) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB($\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was poss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20445;&#23432;&#24615;&#36136;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#28385;&#36275;&#20854;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#20445;&#23432;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#21407;&#26377;&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#38598;&#25104;&#35757;&#32451;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2209.12753</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20445;&#23432;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Investigating the Conservative Property of Score-Based Generative Models. (arXiv:2209.12753v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20445;&#23432;&#24615;&#36136;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#28385;&#36275;&#20854;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#20445;&#23432;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#21407;&#26377;&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#38598;&#25104;&#35757;&#32451;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20854;&#21442;&#25968;&#21270;&#26041;&#27861;&#20998;&#20026;&#32422;&#26463;&#22411;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#65288;CSBM&#65289;&#25110;&#38750;&#32422;&#26463;&#22411;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#65288;USBM&#65289;&#12290;CSBM&#23558;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24314;&#27169;&#20026;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#39044;&#27979;&#20316;&#20026;&#19968;&#20123;&#26631;&#37327;&#20540;&#33021;&#37327;&#20989;&#25968;&#30340;&#36127;&#26799;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;USBM&#37319;&#29992;&#28789;&#27963;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#30452;&#25509;&#20272;&#35745;&#20998;&#25968;&#32780;&#26080;&#38656;&#26174;&#24335;&#24314;&#27169;&#33021;&#37327;&#20989;&#25968;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;CSBM&#30340;&#26550;&#26500;&#38480;&#21046;&#21487;&#33021;&#20250;&#38480;&#21046;&#20854;&#24314;&#27169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;USBM&#26080;&#27861;&#20445;&#25345;&#20445;&#23432;&#24615;&#36136;&#21487;&#33021;&#20250;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20934;&#20445;&#23432;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#65288;QCSBM&#65289;&#20197;&#20445;&#25345;CSBM&#21644;USBM&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20102;QCSBM&#30340;&#35757;&#32451;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#21183;&#24182;&#39640;&#25928;&#22320;&#38598;&#25104;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Score-Based Models (SBMs) can be categorized into constrained SBMs (CSBMs) or unconstrained SBMs (USBMs) according to their parameterization approaches. CSBMs model probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USBMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSBMs may limit their modeling ability. In addition, we show that USBMs' inability to preserve the property of conservativeness may lead to degraded performance in practice. To address the above issues, we propose Quasi-Conservative Score-Based Models (QCSBMs) for keeping the advantages of both CSBMs and USBMs. Our theoretical derivations demonstrate that the training objective of QCSBMs can be efficiently integrated into the training processes by lever
&lt;/p&gt;</description></item><item><title>MoNNA&#31639;&#27861;&#20351;&#29992;Polyak&#30340;&#23616;&#37096;&#26799;&#24230;&#21160;&#37327;&#21644;&#26368;&#36817;&#37051;&#24179;&#22343;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#26799;&#24230;&#35745;&#31639;&#24320;&#38144;&#19982;&#38169;&#35823;&#33410;&#28857;&#30340;&#27604;&#20363;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2209.10931</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#26799;&#24230;&#24320;&#38144;&#30340;&#24378;&#20581;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Collaborative Learning with Linear Gradient Overhead. (arXiv:2209.10931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10931
&lt;/p&gt;
&lt;p&gt;
MoNNA&#31639;&#27861;&#20351;&#29992;Polyak&#30340;&#23616;&#37096;&#26799;&#24230;&#21160;&#37327;&#21644;&#26368;&#36817;&#37051;&#24179;&#22343;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#26799;&#24230;&#35745;&#31639;&#24320;&#38144;&#19982;&#38169;&#35823;&#33410;&#28857;&#30340;&#27604;&#20363;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;SGD&#65288;&#25110;D-SGD&#65289;&#65292;&#24456;&#23481;&#26131;&#21463;&#21040;&#23384;&#22312;&#36719;&#20214;&#25110;&#30828;&#20214;&#32570;&#38519;&#12289;&#26377;&#27602;&#25968;&#25454;&#25110;&#24694;&#24847;&#34892;&#20026;&#30340;&#25925;&#38556;&#33410;&#28857;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#20381;&#36182;&#20110;&#24378;&#20551;&#35774;&#65288;&#20449;&#20219;&#26381;&#21153;&#22120;&#65292;&#21516;&#36136;&#25968;&#25454;&#65292;&#29305;&#23450;&#22122;&#22768;&#27169;&#22411;&#65289;&#65292;&#35201;&#20040;&#26045;&#21152;&#30340;&#26799;&#24230;&#35745;&#31639;&#25104;&#26412;&#27604;D-SGD&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MoNNA&#31639;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;a&#65289;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65307;&#65288;b&#65289;&#26799;&#24230;&#35745;&#31639;&#24320;&#38144;&#19982;&#38169;&#35823;&#33410;&#28857;&#27604;&#20363;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#26368;&#32039;&#30340;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;Polyak&#30340;&#23616;&#37096;&#26799;&#24230;&#21160;&#37327;&#36827;&#34892;&#26412;&#22320;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#24179;&#22343;&#65288;NNA&#65289;&#36827;&#34892;&#20840;&#23616;&#28151;&#21512;&#12290;&#34429;&#28982;MoNNA&#31639;&#27861;&#30456;&#23545;&#31616;&#21333;&#26131;&#23454;&#29616;&#65292;&#20294;&#20854;&#20998;&#26512;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates and nearest-neighbor averaging (NNA) for global mixing, respectively. While MoNNA is rather simple to implement, its analysis has been more challenging and relies on two key 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.05135</link><description>&lt;p&gt;
&#35821;&#35328;&#31526;&#21495;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#31034;&#33539;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#20307;&#24863;&#25163;&#35821;&#25163;&#25351;&#25340;&#20889;&#30340;&#32763;&#35793;&#26426;&#22120;&#20154;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#20154;&#20013;&#32454;&#33268;&#30340;&#21160;&#20316;&#26159;&#19968;&#20010;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#26080;&#39069;&#22806;&#20449;&#24687;&#19979;&#30340;&#29087;&#32451;&#36816;&#21160;&#27169;&#20223;&#65292;&#20197;&#33719;&#24471;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;URDF&#27169;&#22411;&#65292;&#24182;&#20351;&#27599;&#20010;&#20851;&#33410;&#21482;&#26377;&#19968;&#20010;&#33268;&#21160;&#22120;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(&#21363;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;)&#26469;&#35757;&#32451;&#19968;&#31181;&#33021;&#22815;&#22797;&#21046;&#31034;&#33539;&#36816;&#21160;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#21442;&#32771;&#36816;&#21160;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#23545;&#24212;&#20110;&#25340;&#20889;&#23383;&#27597;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#24418;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24352;&#37327;&#31215;&#20197;&#21450;&#29699;&#24418;&#30721;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#21644;&#34920;&#24449;&#65292;&#22312;&#31232;&#30095;&#22270;&#34920;&#31034;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#25216;&#26415;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.10917</link><description>&lt;p&gt;
&#24352;&#37327;&#31215;&#19982;&#36817;&#20284;&#27491;&#20132;&#30721;&#30340;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Embeddings via Tensor Products and Approximately Orthonormal Codes. (arXiv:2208.10917v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#24418;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24352;&#37327;&#31215;&#20197;&#21450;&#29699;&#24418;&#30721;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#21644;&#34920;&#24449;&#65292;&#22312;&#31232;&#30095;&#22270;&#34920;&#31034;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#25216;&#26415;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#20197;&#20445;&#25345;&#32467;&#26500;&#26041;&#24335;&#26469;&#23884;&#20837;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20016;&#23500;&#30340;&#34920;&#24449;&#33021;&#21147;&#24182;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#23646;&#20110;&#32465;&#23450;&#21644;&#27714;&#21644;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#26174;&#31034;&#20102;&#24352;&#37327;&#31215;&#26159;&#23562;&#37325;&#21472;&#21152;&#21407;&#29702;&#30340;&#26368;&#19968;&#33324;&#30340;&#32465;&#23450;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20123;&#31934;&#30830;&#30340;&#32467;&#26524;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#20351;&#29992;&#30340;&#29699;&#24418;&#30721;&#23454;&#29616;&#20102;&#19968;&#20010;&#35013;&#31665;&#19978;&#38480;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#37051;&#25509;&#30697;&#38453;&#30340;&#32852;&#31995;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#19968;&#31181;&#37051;&#25509;&#30697;&#38453;&#30340;&#21387;&#32553;&#65292;&#20855;&#26377;&#31232;&#30095;&#22270;&#34920;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze a method for embedding graphs as vectors in a structure-preserving manner, showcasing its rich representational capacity and establishing some of its theoretical properties. Our procedure falls under the bind-and-sum approach, and we show that the tensor product is the most general binding operation that respects the superposition principle. We also establish some precise results characterizing the behavior of our method, and we show that our use of spherical codes achieves a packing upper bound. We establish a link to adjacency matrices, showing that our method is, in some sense, a compression of adjacency matrices with applications towards sparse graph representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.10230</link><description>&lt;p&gt;
&#20174;&#38745;&#24577;&#21040;&#21160;&#24577;&#30340;&#32467;&#26500;&#65306;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#32467;&#21512;&#20146;&#21644;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Static to Dynamic Structures: Improving Binding Affinity Prediction with a Graph-Based Deep Learning Model. (arXiv:2208.10230v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#26159;&#32467;&#26500;&#22522;&#30784;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#20013;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#20173;&#28982;&#21463;&#38480;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#21482;&#21033;&#29992;&#38745;&#24577;&#26230;&#20307;&#32467;&#26500;&#65292;&#32780;&#23454;&#38469;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#36890;&#24120;&#30001;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#25551;&#36848;&#12290;&#36924;&#36817;&#36825;&#26679;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#12290;&#26412;&#25991;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;3,218&#20010;&#19981;&#21516;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;MD&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Dynaformer&#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290; Dynaformer&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#20174;MD&#36712;&#36857;&#20013;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#20307;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CASF-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of the protein-ligand binding affinities is an essential challenge in the structure-based drug design. Despite recent advance in data-driven methods in affinity prediction, their accuracy is still limited, partially because they only take advantage of static crystal structures while the actual binding affinities are generally depicted by the thermodynamic ensembles between proteins and ligands. One effective way to approximate such a thermodynamic ensemble is to use molecular dynamics (MD) simulation. Here, we curated an MD dataset containing 3,218 different protein-ligand complexes, and further developed Dynaformer, which is a graph-based deep learning model. Dynaformer was able to accurately predict the binding affinities by learning the geometric characteristics of the protein-ligand interactions from the MD trajectories. In silico experiments demonstrated that our model exhibits state-of-the-art scoring and ranking power on the CASF-2016 benchmark dataset, outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23454;&#39564;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#24615;&#20020;&#24202;&#35797;&#39564;&#20013;&#21463;&#30410;&#20110;&#32473;&#23450;&#27835;&#30103;&#30340;&#24739;&#32773;&#20122;&#32676;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20135;&#29983;&#20102;&#26377;&#25928;&#21644;&#26377;&#29992;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.05844</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#20013;&#21463;&#30410;&#20154;&#32676;&#30340;&#33258;&#36866;&#24212;&#35782;&#21035;&#65306;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions. (arXiv:2208.05844v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23454;&#39564;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#24615;&#20020;&#24202;&#35797;&#39564;&#20013;&#21463;&#30410;&#20110;&#32473;&#23450;&#27835;&#30103;&#30340;&#24739;&#32773;&#20122;&#32676;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20135;&#29983;&#20102;&#26377;&#25928;&#21644;&#26377;&#29992;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30830;&#35748;&#24615;&#20020;&#24202;&#35797;&#39564;&#20013;&#33258;&#36866;&#24212;&#24615;&#22320;&#35782;&#21035;&#21463;&#30410;&#20110;&#32473;&#23450;&#27835;&#30103;&#30340;&#24739;&#32773;&#20122;&#32676;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#24615;&#20020;&#24202;&#35797;&#39564;&#24050;&#32463;&#22312;&#29983;&#29289;&#32479;&#35745;&#23398;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#20165;&#20801;&#35768;&#26377;&#38480;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25918;&#23485;&#36825;&#26679;&#30340;&#35774;&#35745;&#30340;&#32463;&#20856;&#38480;&#21046;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#34701;&#21512;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#22312;&#32447;&#23454;&#39564;&#30340;&#24605;&#24819;&#65292;&#20351;&#35797;&#39564;&#26356;&#21152;&#28789;&#27963;&#21644;&#39640;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20122;&#32676;&#36873;&#25321;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#8212;&#8212;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#65288;i&#65289;&#36890;&#24120;&#24863;&#20852;&#36259;&#30340;&#26159;&#22312;&#26377;&#38480;&#30340;&#39044;&#31639;&#19979;&#25214;&#21040;&#20219;&#20309;&#21463;&#30410;&#20110;&#27835;&#30103;&#30340;&#20122;&#32676;&#65288;&#32780;&#19981;&#19968;&#23450;&#26159;&#21333;&#20010;&#25928;&#26524;&#26368;&#22823;&#30340;&#20122;&#32452;&#65289;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21482;&#38656;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#35777;&#26126;&#26377;&#25928;&#24615;&#8212;&#8212;&#20652;&#29983;&#20102;&#26377;&#36259;&#30340;&#25361;&#25112;&#21644;&#35774;&#35745;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35201;&#27714;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#23376;&#32676;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23454;&#39564;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#19981;&#20165;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#19988;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20135;&#29983;&#20102;&#26377;&#25928;&#21644;&#26377;&#29992;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of adaptively identifying patient subpopulations that benefit from a given treatment during a confirmatory clinical trial. This type of adaptive clinical trial has been thoroughly studied in biostatistics, but has been allowed only limited adaptivity so far. Here, we aim to relax classical restrictions on such designs and investigate how to incorporate ideas from the recent machine learning literature on adaptive and online experimentation to make trials more flexible and efficient. We find that the unique characteristics of the subpopulation selection problem -- most importantly that (i) one is usually interested in finding subpopulations with any treatment benefit (and not necessarily the single subgroup with largest effect) given a limited budget and that (ii) effectiveness only has to be demonstrated across the subpopulation on average -- give rise to interesting challenges and new desiderata when designing algorithmic solutions. Building on these findings, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;D3Former&#65292;&#19968;&#31181;&#29992;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#30340;&#26080;&#20559;&#24046;&#21452;&#37325;&#33976;&#39311;Transformer&#12290;D3Former&#21033;&#29992;&#28151;&#21512;&#23884;&#22871;ViT&#35774;&#35745;&#65292;&#19981;&#20250;&#21160;&#24577;&#25193;&#23637;&#20854;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#20559;&#24046;&#26657;&#27491;&#27169;&#22359;&#21644;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#26469;&#25913;&#36827;&#20854;CIL&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.00777</link><description>&lt;p&gt;
D3Former: &#26080;&#20559;&#24046;&#21452;&#37325;&#33976;&#39311;&#21464;&#24418;&#22120;&#29992;&#20110;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
D3Former: Debiased Dual Distilled Transformer for Incremental Learning. (arXiv:2208.00777v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;D3Former&#65292;&#19968;&#31181;&#29992;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#30340;&#26080;&#20559;&#24046;&#21452;&#37325;&#33976;&#39311;Transformer&#12290;D3Former&#21033;&#29992;&#28151;&#21512;&#23884;&#22871;ViT&#35774;&#35745;&#65292;&#19981;&#20250;&#21160;&#24577;&#25193;&#23637;&#20854;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#20559;&#24046;&#26657;&#27491;&#27169;&#22359;&#21644;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#26469;&#25913;&#36827;&#20854;CIL&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#20013;&#65292;&#27599;&#27425;&#23398;&#20064;&#38454;&#27573;&#21521;&#27169;&#22411;&#24341;&#20837;&#19968;&#32452;&#31867;&#21035;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35266;&#27979;&#21040;&#30340;&#31867;&#21035;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#32771;&#34385;&#21040;Vision Transformers&#65288;ViTs&#65289;&#22312;&#20256;&#32479;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26222;&#21450;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;CIL&#30340;&#26080;&#20559;&#24046;&#21452;&#37325;&#33976;&#39311;Transformer&#65292;&#31216;&#20026;D3Former&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#28151;&#21512;&#23884;&#22871;ViT&#35774;&#35745;&#65292;&#20197;&#30830;&#20445;&#23545;&#23567;&#22411;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#19982;&#26368;&#36817;&#30340;&#22522;&#20110;ViT&#30340;CIL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;D3Former&#22312;&#23398;&#20064;&#26032;&#30340;&#20219;&#21153;&#26102;&#19981;&#20250;&#21160;&#24577;&#25193;&#23637;&#20854;&#26550;&#26500;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#37327;&#22686;&#37327;&#20219;&#21153;&#12290;D3Former&#34892;&#20026;&#30340;&#25913;&#36827;&#24402;&#21151;&#20110;ViT&#35774;&#35745;&#30340;&#20004;&#20010;&#22522;&#26412;&#21464;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22686;&#37327;&#23398;&#20064;&#35270;&#20026;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20559;&#24046;&#26657;&#27491;&#27169;&#22359;&#65292;&#32531;&#35299;&#20102;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#65292;&#21033;&#29992;&#24072;&#29983;ViT&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#30830;&#20445;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed $\textrm{D}^3\textrm{Former}$. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT based CIL approach, our $\textrm{D}^3\textrm{Former}$ does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of $\textrm{D}^3\textrm{Former}$ owes to two fundamental changes to the ViT design. First, we treat the incremental learning as a long-tail classi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.00755</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27493; Q-learning &#32531;&#35299; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#31163;&#31574;&#30053;&#20559;&#24046;&#65306;&#19968;&#31181;&#26032;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#31163;&#31574;&#30053;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#24046;&#22686;&#21152;&#26102;&#65292;&#31163;&#31574;&#30053;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#31163;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#26469;&#34917;&#20607;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#19968;&#31995;&#21015;&#38271;&#36712;&#36857;&#65292;&#24182;&#23548;&#33268;&#39069;&#22806;&#30340;&#38382;&#39064;&#65292;&#22914;&#28040;&#22833;/&#29190;&#28856;&#26799;&#24230;&#25110;&#25243;&#24323;&#35768;&#22810;&#26377;&#29992;&#30340;&#32463;&#39564;&#65292;&#26368;&#32456;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#36830;&#32493;&#21160;&#20316;&#22495;&#25110;&#30001;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#31574;&#30053;&#30340;&#27867;&#21270;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#36830;&#32493;&#25511;&#21046;&#20013;&#36825;&#31181;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#36731; Actor-Critic &#26041;&#27861;&#20013;&#31163;&#25919;&#31574;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65292;&#36825;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2207.12062</link><description>&lt;p&gt;
&#24212;&#29992;&#20803;&#23398;&#20064;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#33258;&#36866;&#24212;&#24322;&#27493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adaptive Asynchronous Control Using Meta-learned Neural Ordinary Differential Equations. (arXiv:2207.12062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12062
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65292;&#36825;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#22312;&#21253;&#25324;&#26426;&#22120;&#20154;&#29615;&#22659;&#22312;&#20869;&#30340;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24120;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#20197;&#21450;&#22312;&#19981;&#21516;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#36733;&#33655;&#24815;&#24615;&#29305;&#24615;&#65289;&#30340;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#31616;&#21333;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based Reinforcement Learning and Control have demonstrated great potential in various sequential decision making problem domains, including in robotics settings. However, real-world robotics systems often present challenges that limit the applicability of those methods. In particular, we note two problems that jointly happen in many industrial systems: 1) Irregular/asynchronous observations and actions and 2) Dramatic changes in environment dynamics from an episode to another (e.g. varying payload inertial properties). We propose a general framework that overcomes those difficulties by meta-learning adaptive dynamics models for continuous-time prediction and control. The proposed approach is task-agnostic and can be adapted to new tasks in a straight-forward manner. We present evaluations in two different robot simulations and on a real industrial robot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24352;&#37327;&#38477;&#32500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#21033;&#29992;&#22833;&#25928;&#26102;&#38388;&#30417;&#30563;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.11353</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24352;&#37327;&#38477;&#32500;&#30340;&#19981;&#23436;&#25972;&#25104;&#20687;&#25968;&#25454;&#30340;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data. (arXiv:2207.11353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24352;&#37327;&#38477;&#32500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#21033;&#29992;&#22833;&#25928;&#26102;&#38388;&#30417;&#30563;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24352;&#37327;&#25968;&#25454;&#30340;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#65292;&#19982;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#20004;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#35813;&#27169;&#22411;&#19981;&#35201;&#27714;&#24352;&#37327;&#25968;&#25454;&#23436;&#25972;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22833;&#25928;&#26102;&#38388;&#65288;TTF&#65289;&#26469;&#30417;&#30563;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#65292;&#20351;&#25552;&#21462;&#30340;&#29305;&#24449;&#26356;&#26377;&#25928;&#22320;&#29992;&#20110;&#21518;&#32493;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20026;&#21442;&#25968;&#20272;&#35745;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#29305;&#23450;&#20998;&#24067;&#19979;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a supervised dimension reduction methodology for tensor data which has two advantages over most image-based prognostic models. First, the model does not require tensor data to be complete which expands its application to incomplete data. Second, it utilizes time-to-failure (TTF) to supervise the extraction of low-dimensional features which makes the extracted features more effective for the subsequent prognostic. Besides, an optimization algorithm is proposed for parameter estimation and closed-form solutions are derived under certain distributions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LB-SGD&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#23545;&#21407;&#22987;&#38382;&#39064;&#30340;&#23545;&#25968;&#38556;&#30861;&#36817;&#20284;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#27492;&#26041;&#27861;&#21487;&#29992;&#20110;&#20351;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#38454;&#21644;&#38646;&#38454;&#21453;&#39304;&#30340;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#24179;&#28369;&#32422;&#26463;&#38382;&#39064;&#30340;&#23436;&#20840;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2207.10415</link><description>&lt;p&gt;
&#20855;&#26377;&#24212;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#40657;&#21283;&#23376;&#20248;&#21270;&#30340;&#23545;&#25968;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning. (arXiv:2207.10415v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10415
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LB-SGD&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#23545;&#21407;&#22987;&#38382;&#39064;&#30340;&#23545;&#25968;&#38556;&#30861;&#36817;&#20284;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#27492;&#26041;&#27861;&#21487;&#29992;&#20110;&#20351;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#38454;&#21644;&#38646;&#38454;&#21453;&#39304;&#30340;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#24179;&#28369;&#32422;&#26463;&#38382;&#39064;&#30340;&#23436;&#20840;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#19994;&#12289;&#26426;&#22120;&#20154;&#21644;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#65292;&#20248;&#21270;&#22024;&#26434;&#20989;&#25968;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#37096;&#32626;&#31995;&#32479;&#19978;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#38656;&#35201;&#36827;&#34892;&#23454;&#39564;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#20107;&#20808;&#19981;&#30693;&#36947;&#23433;&#20840;&#36755;&#20837;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#25105;&#20204;&#21482;&#33021;&#24471;&#21040;&#22024;&#26434;&#30340;&#20449;&#24687;&#65292;&#25351;&#31034;&#25105;&#20204;&#36317;&#31163;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#30340;&#36317;&#31163;&#26377;&#22810;&#36828;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#24517;&#39035;&#20445;&#35777;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#31639;&#27861;&#30340;&#26368;&#32456;&#36755;&#20986;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing noisy functions online, when evaluating the objective requires experiments on a deployed system, is a crucial task arising in manufacturing, robotics and many others. Often, constraints on safe inputs are unknown ahead of time, and we only obtain noisy information, indicating how close we are to violating the constraints. Yet, safety must be guaranteed at all times, not only for the final output of the algorithm.  We introduce a general approach for seeking a stationary point in high dimensional non-linear stochastic optimization problems in which maintaining safety during learning is crucial. Our approach called LB-SGD is based on applying stochastic gradient descent (SGD) with a carefully chosen adaptive step size to a logarithmic barrier approximation of the original problem. We provide a complete convergence analysis of non-convex, convex, and strongly-convex smooth constrained problems, with first-order and zeroth-order feedback. Our approach yields efficient updates an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15269</link><description>&lt;p&gt;
Swin Transformer &#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#23618;&#33258;&#25105;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#26377;&#20154;&#21162;&#21147;&#23558; Transformer &#36866;&#24212;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324; Vision Transformer &#21644; Swin Transformer&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#23558; Vision Transformer &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#20173;&#20572;&#30041;&#22312;&#23567;&#35268;&#27169;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#24517;&#39035;&#20381;&#36182;&#20110;&#25216;&#26415;&#26469;&#20943;&#23569; Vision Transformer &#30340;&#25104;&#26412;&#65292;&#36825;&#20063;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#31532;&#19968;&#20010;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65306;Swin DQN&#12290;Swin Transformer &#21487;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20027;&#24178;&#39592;&#24178;&#65292;&#23558;&#22270;&#20687;&#20687;&#32032;&#30340;&#32452;&#21512;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#65292;&#24182;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#24212;&#29992;&#23616;&#37096;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#23427;&#20204;&#22312; ImageNet &#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;Swin DQN &#22312; Atari &#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are neural network models that utilize multiple layers of self-attention heads and have exhibited enormous potential in natural language processing tasks. Meanwhile, there have been efforts to adapt transformers to visual tasks of machine learning, including Vision Transformers and Swin Transformers. Although some researchers use Vision Transformers for reinforcement learning tasks, their experiments remain at a small scale due to the high computational cost. Experiments conducted at a large scale, on the other hand, have to rely on techniques to cut the costs of Vision Transformers, which also yield inferior results.  To address this challenge, this article presents the first online reinforcement learning scheme that is based on Swin Transformers: Swin DQN. Swin Transformers are promising as a backbone in neural networks by splitting groups of image pixels into small patches and applying local self-attention operations inside the (shifted) windows of fixed sizes. They hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20154;&#31867;&#33410;&#30465;&#25353;&#38190;&#25968;&#37327;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.14578</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20154;&#31867;&#33410;&#30465;&#25353;&#38190;&#25968;&#37327;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#36741;&#21161;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19987;&#21033;&#39046;&#22495;&#30340;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20174;&#20154;&#31867;&#30340;&#35282;&#24230;&#35780;&#20272;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#24230;&#37327;&#22522;&#20110;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#34917;&#20840;&#25152;&#33021;&#33410;&#30465;&#30340;&#25353;&#38190;&#27604;&#29575;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#31181;&#20197;&#25353;&#38190;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26426;&#22120;-centric&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#26500;&#24314;&#30340;&#26368;&#22823;&#27169;&#22411;&#22823;&#23567;&#20026;6B&#65292;&#22312;&#19987;&#21033;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22522;&#20110;&#27492;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#26159;&#20154;&#31867;-centric&#24230;&#37327;&#26041;&#27861;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#36825;&#24847;&#21619;&#30528;&#22914;&#26524;&#30446;&#30340;&#26159;&#36890;&#36807;&#33258;&#21160;&#34917;&#20840;&#26469;&#36741;&#21161;&#20154;&#31867;&#20889;&#20316;&#65292;&#37027;&#20040;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are promising for assisting human writing in various domains. This manuscript aims to build generative language models in the patent domain and evaluate model performance from a human-centric perspective. The perspective is to measure the ratio of keystrokes that can be saved by autocompletion based on generative patent language models. A higher ratio means a more effective model which can save more keystrokes. This metric can be used to benchmark model performance. The metric is different from conventional machine-centric metrics that are token-based instead of keystroke-based. In terms of model size, the largest model built in this manuscript is 6B, which is state-of-the-art in the patent domain. Based on the metric, it is found that the largest model is not necessarily the best for the human-centric metric. The finding means that keeping increasing model sizes in the patent domain might be unnecessary if the purpose is to assist human writing with autocomp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#20110;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10032</link><description>&lt;p&gt;
&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning With Data and Client Heterogeneity. (arXiv:2206.10032v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#20110;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#21508;&#20010;&#33410;&#28857;&#20445;&#25345;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22823;&#35268;&#27169;FL&#26102;&#23384;&#22312;&#22266;&#26377;&#30340;&#23454;&#29992;&#25361;&#25112;&#65306;1&#65289;&#23616;&#37096;&#33410;&#28857;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;2&#65289;&#33410;&#28857;&#35745;&#31639;&#36895;&#24230;&#65288;&#24322;&#27493;&#24615;&#65289;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;3&#65289;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65288;FedAvg&#65289;&#30340;&#21464;&#20307;&#65292;&#21516;&#26102;&#25903;&#25345;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#37096;&#20998;&#23458;&#25143;&#31471;&#24322;&#27493;&#24615;&#21644;&#36890;&#20449;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#31995;&#32479;&#25918;&#23485;&#65292;&#23427;&#20173;&#28982;&#21487;&#20197;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#25552;&#20379;&#31867;&#20284;&#20110;FedAvg&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#22810;&#36798;300&#20010;&#33410;&#28857;&#30340;&#20005;&#26684;LEAF&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#65292;&#25552;&#39640;&#20102;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally.  However, executing FL at scale comes with inherent practical challenges:  1) heterogeneity of the local node data distributions,  2) heterogeneity of node computational speeds (asynchrony),  but also 3) constraints in the amount of communication between the clients and the server.  In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm  which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression.  Our algorithm comes with a rigorous analysis showing that, in spite of these system relaxations,  it can provide similar convergence to FedAvg in interesting parameter regimes.  Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improvin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;$\mathscr{L}_2(\mathbb{R})$&#30340;&#27491;&#20132;&#22522;&#20013;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#65292;&#23454;&#29616;&#20102;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2206.08648</link><description>&lt;p&gt;
&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Orthonormal Expansions for Translation-Invariant Kernels. (arXiv:2206.08648v3 [math.CA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08648
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;$\mathscr{L}_2(\mathbb{R})$&#30340;&#27491;&#20132;&#22522;&#20013;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#65292;&#23454;&#29616;&#20102;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;$\mathscr{L}_2(\mathbb{R})$&#19978;&#30340;&#27491;&#20132;&#22522;&#65292;&#24471;&#21040;&#20102;&#23454;&#36724;&#19978;&#25152;&#26377;&#21322;&#25972;&#25968;&#38454;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#20197;&#21450;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#65292;&#20998;&#21035;&#30001;&#30456;&#20851;&#30340;&#25289;&#30422;&#23572;&#20989;&#25968;&#12289;&#26377;&#29702;&#20989;&#25968;&#21644;&#21380;&#31859;&#20989;&#25968;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general Fourier analytic technique for constructing orthonormal basis expansions of translation-invariant kernels from orthonormal bases of $\mathscr{L}_2(\mathbb{R})$. This allows us to derive explicit expansions on the real line for (i) Mat\'ern kernels of all half-integer orders in terms of associated Laguerre functions, (ii) the Cauchy kernel in terms of rational functions, and (iii) the Gaussian kernel in terms of Hermite functions.
&lt;/p&gt;</description></item><item><title>MOOD&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2206.07632</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#29983;&#25104;&#25506;&#32034;&#21270;&#23398;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring Chemical Space with Score-based Out-of-distribution Generation. (arXiv:2206.07632v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07632
&lt;/p&gt;
&lt;p&gt;
MOOD&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#23616;&#38480;&#24615;&#26159;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#20998;&#23376;&#39640;&#24230;&#30456;&#20284;&#12290;&#20026;&#20102;&#29983;&#25104;&#20840;&#26032;&#30340;&#20998;&#23376;&#20197;&#23547;&#25214;&#26356;&#22909;&#30340;&#26032;&#39062;&#33647;&#29289;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#25506;&#32034;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25968;-based&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;(MOOD)&#65292;&#35813;&#31574;&#30053;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#30340;&#29983;&#25104;&#20013;&#32467;&#21512;&#20102;&#24322;&#20998;&#24067;(OOD)&#25511;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25511;&#21046;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#30001;&#20110;&#19968;&#20123;&#26032;&#39062;&#20998;&#23376;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#29616;&#23454;&#33647;&#29289;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;MOOD&#21033;&#29992;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#65288;&#22914;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#26679;&#24615;&#21644;&#21487;&#21512;&#25104;&#24615;&#65289;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;MOOD&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2206.05262</link><description>&lt;p&gt;
&#20803;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Meta Optimal Transport. (arXiv:2206.05262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20998;&#25674;&#20248;&#21270;&#26469;&#39044;&#27979;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;OT&#12290;&#36825;&#26377;&#21161;&#20110;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#20174;&#32780;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#12290;&#21542;&#21017;&#65292;&#26631;&#20934;&#26041;&#27861;&#20250;&#24573;&#30053;&#36807;&#21435;&#35299;&#20915;&#26041;&#26696;&#30340;&#30693;&#35782;&#65292;&#20174;&#22836;&#24320;&#22987;&#27425;&#20248;&#22320;&#37325;&#26032;&#35299;&#20915;&#27599;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#28784;&#24230;&#22270;&#20687;&#12289;&#29699;&#24418;&#25968;&#25454;&#12289;&#20998;&#31867;&#26631;&#31614;&#21644;&#39068;&#33394;&#35843;&#33394;&#26495;&#20043;&#38388;&#23454;&#20363;&#21270;&#20803;OT&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#25913;&#21892;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the use of amortized optimization to predict optimal transport (OT) maps from the input measures, which we call Meta OT. This helps repeatedly solve similar OT problems between different measures by leveraging the knowledge and information present from past problems to rapidly predict and solve new problems. Otherwise, standard methods ignore the knowledge of the past solutions and suboptimally re-solve each problem from scratch. We instantiate Meta OT models in discrete and continuous settings between grayscale images, spherical data, classification labels, and color palettes and use them to improve the computational time of standard OT solvers. Our source code is available at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#37322;&#26469;&#23457;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#35299;&#37322;&#30340;&#31639;&#27861;&#26469;&#23457;&#35745;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#23457;&#35745;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2206.04740</link><description>&lt;p&gt;
XAudit&#65306;&#23457;&#35745;&#19982;&#35299;&#37322;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
XAudit : A Theoretical Look at Auditing with Explanations. (arXiv:2206.04740v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#37322;&#26469;&#23457;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#35299;&#37322;&#30340;&#31639;&#27861;&#26469;&#23457;&#35745;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#23457;&#35745;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36127;&#36131;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#23457;&#35745;&#20197;&#36991;&#20813;&#19981;&#33391;&#21518;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#30456;&#20851;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#35299;&#37322;&#36827;&#34892;&#23457;&#35745;&#65292;&#20294;&#22914;&#20309;&#36827;&#34892;&#20197;&#21450;&#20026;&#20160;&#20040;&#36827;&#34892;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991; formalizes &#35299;&#37322;&#22312;&#23457;&#35745;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#30740;&#31350;&#27169;&#22411;&#35299;&#37322;&#22914;&#20309;&#24110;&#21161;&#23457;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;&#25512;&#26029;&#35299;&#37322;&#22312;&#23457;&#35745;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#34429;&#28982; Anchors &#21644; decision paths &#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#22826;&#26377;&#30410;&#65292;&#20294;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#23427;&#20204;&#30830;&#23454;&#24456;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Responsible use of machine learning requires models to be audited for undesirable properties. While a body of work has proposed using explanations for auditing, how to do so and why has remained relatively ill-understood. This work formalizes the role of explanations in auditing and investigates if and how model explanations can help audits. Specifically, we propose explanation-based algorithms for auditing linear classifiers and decision trees for feature sensitivity. Our results illustrate that Counterfactual explanations are extremely helpful for auditing. While Anchors and decision paths may not be as beneficial in the worst-case, in the average-case they do aid a lot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#20013;&#31574;&#30053;&#24615;&#33021;&#21644;&#25506;&#32034;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.03787</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#21160;&#20316;&#22122;&#22768;&#23545;&#25506;&#32034;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#20013;&#31574;&#30053;&#24615;&#33021;&#21644;&#25506;&#32034;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#25506;&#32034;&#24418;&#24335;&#65292;&#22914;&#36830;&#32493;&#25511;&#21046;&#39046;&#22495;&#24120;&#29992;&#30340;&#21152;&#24615;&#21160;&#20316;&#22122;&#22768;&#12290;&#36890;&#24120;&#65292;&#22312;&#22521;&#35757;&#26399;&#38388;&#20445;&#25345;&#21160;&#20316;&#22122;&#22768;&#30340;&#27604;&#20363;&#19981;&#21464;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#36830;&#32493;&#25511;&#21046;&#30340;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;&#22122;&#22768;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26368;&#24120;&#35265;&#30340;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#65292;&#39640;&#26031;&#22122;&#22768;&#21644; Ornstein-Uhlenbeck &#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#22122;&#22768;&#31867;&#22411;&#21644;&#27604;&#20363;&#21442;&#25968;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#21644;&#25506;&#32034;&#26399;&#38388;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#31561;&#26377;&#36259;&#30340;&#21464;&#37327;&#26469;&#35780;&#20272;&#32467;&#26524;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;X_&#119984;rel&#65292;&#35813;&#26041;&#27861;&#23545;&#20272;&#35745;&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimation artifacts caused by
&lt;/p&gt;</description></item><item><title>LICRA&#26159;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2205.15953</link><description>&lt;p&gt;
&#26102;&#26426;&#33267;&#20851;&#37325;&#35201;&#65306;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15953
&lt;/p&gt;
&lt;p&gt;
LICRA&#26159;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25191;&#34892;&#34892;&#21160;&#37117;&#20250;&#20135;&#29983;&#25104;&#26412;&#65307;&#37329;&#34701;&#31995;&#32479;&#20013;&#30340;&#20132;&#26131;&#25104;&#26412;&#21644;&#29123;&#27833;&#25104;&#26412;&#26159;&#24120;&#35265;&#30340;&#20363;&#23376;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25191;&#34892;&#34892;&#21160;&#36805;&#36895;&#31215;&#32047;&#25104;&#26412;&#65292;&#23548;&#33268;&#26497;&#20854;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21453;&#22797;&#34892;&#21160;&#20250;&#20135;&#29983;&#30952;&#25439;&#21644;&#26368;&#32456;&#25439;&#22351;&#12290;&#30830;&#23450;&#8220;&#20309;&#26102;&#34892;&#21160;&#8221;&#23545;&#20110;&#23454;&#29616;&#25104;&#21151;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#22312;&#34892;&#21160;&#20135;&#29983;&#26368;&#23567;&#38480;&#21046;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#8220;&#23398;&#20064;&#8221;&#34892;&#20026;&#26368;&#20248;&#31574;&#30053;&#30340;&#25361;&#25112;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#21517;&#20026;Learnable Impulse Control Reinforcement Algorithm&#65288;LICRA&#65289;&#65292;&#29992;&#20110;&#22312;&#34892;&#21160;&#20135;&#29983;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36873;&#25321;&#20309;&#26102;&#34892;&#21160;&#21644;&#37319;&#21462;&#21738;&#20123;&#34892;&#21160;&#20197;&#23454;&#29616;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \textbf{L}earnable \textbf{I}mpulse \textbf{C}ontrol \textbf{R}einforcement \textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \textit{impulse control} which learns to maximise objectives when actions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;&#65288;HRM&#65289;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#23558;&#20219;&#21153;&#36827;&#19968;&#27493;&#25277;&#35937;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#21487;&#20197;&#29420;&#31435;&#35299;&#20915;&#65307;&#20351;&#29992; HRM &#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#19988;&#22312;&#23398;&#20064;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2205.15752</link><description>&lt;p&gt;
&#22870;&#21169;&#29366;&#24577;&#26426;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hierarchies of Reward Machines. (arXiv:2205.15752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;&#65288;HRM&#65289;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#23558;&#20219;&#21153;&#36827;&#19968;&#27493;&#25277;&#35937;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#21487;&#20197;&#29420;&#31435;&#35299;&#20915;&#65307;&#20351;&#29992; HRM &#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#19988;&#22312;&#23398;&#20064;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#34920;&#31034;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#36793;&#32536;&#20351;&#29992;&#39640;&#32423;&#20107;&#20214;&#32534;&#30721;&#20219;&#21153;&#30340;&#23376;&#30446;&#26631;&#12290; RM&#30340;&#32467;&#26500;&#20351;&#24471;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#25104;&#31616;&#21333;&#21644;&#29420;&#31435;&#21487;&#35299;&#30340;&#23376;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#26377;&#21161;&#20110;&#22788;&#29702;&#38271;&#26399;&#35268;&#21010;&#21644;/&#25110;&#22870;&#21169;&#31232;&#30095;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#36171;&#20104;RM&#35843;&#29992;&#20854;&#20182;RM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#32452;&#21512;&#19968;&#20010;RM&#30340;&#23618;&#27425;&#32467;&#26500;&#65288;HRM&#65289;&#26469;&#36827;&#19968;&#27493;&#25277;&#35937;&#23376;&#20219;&#21153;&#32467;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;HRM&#36890;&#36807;&#23558;&#23545;RM&#30340;&#27599;&#20010;&#35843;&#29992;&#35270;&#20026;&#21333;&#29420;&#21487;&#35299;&#30340;&#23376;&#20219;&#21153;&#26469;&#20351;&#29992;&#36873;&#39033;&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#26041;&#27861;&#26469;&#20174;&#20195;&#29702;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20013;&#23398;&#20064;HRM&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;HRM&#27604;&#25153;&#24179;&#30340;HRM&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#22312;&#31561;&#20215;&#30340;&#25153;&#24179;&#34920;&#31034;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;HRM&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32454;&#33268;&#30740;&#31350;&#20102;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#38024;&#23545; $m\propto d^r$ &#39640;&#38454;&#26631;&#24230;&#20851;&#31995;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2205.14846</link><description>&lt;p&gt;
&#28857;&#31215;&#26680;&#22238;&#24402;&#30340;&#31934;&#30830;&#23398;&#20064;&#26354;&#32447;&#21644;&#39640;&#38454;&#26631;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32454;&#33268;&#30740;&#31350;&#20102;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#38024;&#23545; $m\propto d^r$ &#39640;&#38454;&#26631;&#24230;&#20851;&#31995;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#26029;&#25512;&#36827;&#35745;&#31639;&#21069;&#27839;&#65292;&#24320;&#21457;&#23545;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#32553;&#25918;&#26041;&#26696;&#19979;&#39044;&#26399;&#24615;&#33021;&#25552;&#39640;&#30340;&#31934;&#30830;&#20272;&#35745;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20851;&#20110;&#25551;&#36848;&#39044;&#27979;&#35823;&#24046;&#22914;&#20309;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#32780;&#21464;&#21270;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#29702;&#35770;&#29702;&#35299;&#21463;&#38480;&#20110;&#22823;&#26679;&#26412;&#28176;&#36817;&#24615; ($m\to\infty$) &#25110;&#23545;&#20110;&#26576;&#20123;&#31616;&#21333;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#32500;&#28176;&#36817;&#24615;&#65292;&#20854;&#20013;&#26679;&#26412;&#25968;&#37327;&#19982;&#32500;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363; ($m\propto d$)&#12290;&#36825;&#20004;&#20010;&#33539;&#30068;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#21253;&#25324;&#25152;&#26377;&#39640;&#38454;&#26631;&#24230;&#20851;&#31995; $m\propto d^r$&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#30740;&#31350;&#23545;&#35937;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312; $m/d\rightarrow2r$ &#30340; $r$ &#38454;&#28176;&#36817;&#26631;&#24230;&#19979;&#65288;&#20854;&#20013; $m\to\infty$&#65289;&#65292;&#23545;&#20110;&#20174;&#29699;&#38754;&#19978;&#22343;&#21248;&#25277;&#21462;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\to\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\to\infty$ with $m/d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20803;&#23398;&#20064;&#22120;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#65292;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2205.14714</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#20316;&#29992;&#20272;&#35745;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects. (arXiv:2205.14714v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20803;&#23398;&#20064;&#22120;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#65292;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#65292;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20272;&#35745;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#38500;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#22806;&#65292;&#36824;&#24320;&#21457;&#20986;&#20102;&#31216;&#20026;&#20803;&#23398;&#20064;&#22120;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#20197;&#20272;&#35745;CATE&#65292;&#20854;&#20027;&#35201;&#20248;&#28857;&#26159;&#19981;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#19981;&#26159;&#20108;&#36827;&#21046;&#30340;&#26102;&#65292;&#19968;&#20123;&#26420;&#32032;&#25193;&#23637;&#30340;&#38480;&#21046;&#20250;&#20986;&#29616;&#65292;&#36825;&#26679;&#30340;&#20219;&#21153;&#23601;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#29702;&#35770;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35823;&#24046;&#19978;&#30028;&#20316;&#20026;&#37325;&#35201;&#21442;&#25968;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22788;&#29702;&#27700;&#24179;&#30340;&#25968;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#25552;&#20379;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#21644;&#35752;&#35770;&#20102;&#19968;&#20123;&#20803;&#23398;&#20064;&#22120;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#25968;&#37327;&#22686;&#22810;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#19968;&#39033;&#20057;&#32925;&#27835;&#30103;&#30740;&#31350;&#30340;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#20803;&#23398;&#20064;&#22120;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multi-valued treatments. We consider different meta-learners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#19979;&#28216;&#24212;&#29992;&#21644;&#22242;&#20307;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.14358</link><description>&lt;p&gt;
&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fair Labeled Clustering. (arXiv:2205.14358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#19979;&#28216;&#24212;&#29992;&#21644;&#22242;&#20307;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32858;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#19981;&#21516;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#31639;&#27861;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#20844;&#24179;&#27010;&#24565;&#26159;&#22242;&#20307;&#20844;&#24179;&#24615;&#65292;&#21363;&#27599;&#20010;&#32858;&#31867;&#20013;&#37117;&#35201;&#20445;&#35777;&#22242;&#20307;&#27604;&#20363;&#30340;&#21512;&#29702;&#24615;&#12290;&#26412;&#25991;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#32771;&#34385;&#20102;&#32858;&#31867;&#20219;&#21153;&#30340;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#22312;&#35813;&#24212;&#29992;&#20013;&#22914;&#20309;&#23454;&#29616;&#22242;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#20915;&#31574;&#32773;&#36816;&#34892;&#32858;&#31867;&#31639;&#27861;&#65292;&#26816;&#26597;&#27599;&#20010;&#32858;&#31867;&#30340;&#20013;&#24515;&#24182;&#20026;&#30456;&#24212;&#30340;&#32858;&#31867;&#30830;&#23450;&#19968;&#20010;&#36866;&#24403;&#30340;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#65292;&#20363;&#22914;&#25307;&#32856;&#20013;&#30340;&#8220;&#24405;&#29992;&#8221;&#25110;&#8220;&#25298;&#32477;&#8221;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#30830;&#20445;&#22242;&#20307;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#24076;&#26395;&#27599;&#20010;&#26631;&#31614;&#37117;&#26377;&#31526;&#21512;&#27604;&#20363;&#30340;&#22242;&#20307;&#20195;&#34920;&#65292;&#20294;&#19981;&#19968;&#23450;&#35201;&#27714;&#27599;&#20010;&#32858;&#31867;&#37117;&#20855;&#26377;&#20004;&#20010;&#22242;&#20307;&#30340;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous algorithms have been produced for the fundamental problem of clustering under many different notions of fairness. Perhaps the most common family of notions currently studied is group fairness, in which proportional group representation is ensured in every cluster. We extend this direction by considering the downstream application of clustering and how group fairness should be ensured for such a setting. Specifically, we consider a common setting in which a decision-maker runs a clustering algorithm, inspects the center of each cluster, and decides an appropriate outcome (label) for its corresponding cluster. In hiring for example, there could be two outcomes, positive (hire) or negative (reject), and each cluster would be assigned one of these two outcomes. To ensure group fairness in such a setting, we would desire proportional group representation in every label but not necessarily in every cluster as is done in group fair clustering. We provide algorithms for such problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24418;&#24335;&#21270;&#20102;&#20559;&#22909;&#36816;&#34892;&#26102;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#25551;&#36848;&#31639;&#27861;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#19982;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21644;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2205.13028</link><description>&lt;p&gt;
&#24418;&#24335;&#21270;&#36816;&#34892;&#26102;&#20998;&#24067;&#19978;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Formalizing Preferences Over Runtime Distributions. (arXiv:2205.13028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24418;&#24335;&#21270;&#20102;&#20559;&#22909;&#36816;&#34892;&#26102;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#25551;&#36848;&#31639;&#27861;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#19982;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21644;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#35745;&#31639;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#22312;&#33021;&#22815;&#36820;&#22238;&#27491;&#30830;&#32467;&#26524;&#30340;&#31639;&#27861;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#20998;&#24067;&#19981;&#21516;&#65288;&#20363;&#22914;SAT&#27714;&#35299;&#22120;&#65292;&#25490;&#24207;&#31639;&#27861;&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24418;&#24335;&#21270;&#36816;&#34892;&#26102;&#20998;&#24067;&#19978;&#30340;&#20559;&#22909;&#20026;&#36825;&#20123;&#36873;&#25321;&#22880;&#23450;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#24448;&#24448;&#24076;&#26395;&#36873;&#25321;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#26368;&#30701;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20559;&#22909;&#23558;&#23436;&#20840;&#21463;&#21040;&#31639;&#27861;&#22312;&#22351;&#36755;&#20837;&#19978;&#34920;&#29616;&#22914;&#20309;&#32780;&#24433;&#21709;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#24895;&#24847;&#22312;&#38271;&#26102;&#38388;&#30340;&#36816;&#34892;&#27809;&#26377;&#32467;&#26463;&#20043;&#21069;&#23558;&#20854;&#20999;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#21487;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#25551;&#36848;&#31639;&#27861;&#20559;&#22909;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21462;&#20915;&#20110;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#38382;&#39064;&#35299;&#20915;&#30340;&#20215;&#20540;&#22914;&#20309;&#19979;&#38477;&#20197;&#21450;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#30495;&#23454;&#30340;&#25928;&#29992;&#20989;&#25968;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#20915;&#31574;&#31639;&#27861;&#30340;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#23545;&#36825;&#20123;&#20989;&#25968;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
When trying to solve a computational problem, we are often faced with a choice between algorithms that are guaranteed to return the right answer but differ in their runtime distributions (e.g., SAT solvers, sorting algorithms). This paper aims to lay theoretical foundations for such choices by formalizing preferences over runtime distributions. It might seem that we should simply prefer the algorithm that minimizes expected runtime. However, such preferences would be driven by exactly how slow our algorithm is on bad inputs, whereas in practice we are typically willing to cut off occasional, sufficiently long runs before they finish. We propose a principled alternative, taking a utility-theoretic approach to characterize the scoring functions that describe preferences over algorithms. These functions depend on the way our value for solving our problem decreases with time and on the distribution from which captimes are drawn. We describe examples of realistic utility functions and show 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Heterformer&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2205.10282</link><description>&lt;p&gt;
Heterformer&#65306;&#22522;&#20110;Transformer&#30340;&#24322;&#26500;&#25991;&#26412;&#32593;&#32476;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Heterformer&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20026;&#27599;&#20010;&#33410;&#28857;&#25512;&#23548;&#20986;&#26377;&#24847;&#20041;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#20174;&#32780;&#20419;&#36827;&#35832;&#22914;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#32858;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#24322;&#26500;&#25991;&#26412;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#23384;&#22312;&#19982;&#32570;&#22833;&#20197;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#24418;&#25104;&#30340;&#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#39033;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#33719;&#24471;&#24191;&#27867;&#36890;&#29992;&#24615;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25991;&#26412;&#20016;&#23500;&#30340;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#23558;PLMs&#21253;&#21547;&#21040;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21487;&#20197;&#26377;&#25928;&#22320;&#20849;&#21516;&#32771;&#34385;&#24322;&#26500;&#32467;&#26500;&#65288;&#32593;&#32476;&#65289;&#20449;&#24687;&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20016;&#23500;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Heterformer&#65292;&#19968;&#31181;&#24322;&#26500;&#32593;&#32476;&#24378;&#21270;&#30340;Transformer&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20016;&#23500;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs cont
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2205.09990</link><description>&lt;p&gt;
&#38024;&#23545;Few-Task Meta-Learning&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#22312;&#32473;&#23450;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#25968;&#37327;&#20173;&#28982;&#38656;&#35201;&#24456;&#22823;&#65292;&#25165;&#33021;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#36825;&#23545;&#20110;&#21482;&#26377;&#23569;&#37327;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#29942;&#39048;&#65292;&#21407;&#22240;&#21253;&#25324;&#26500;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;Meta-Interpolation&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes e
&lt;/p&gt;</description></item><item><title>CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.10965</link><description>&lt;p&gt;
CLIP-Dissect&#65306;&#28145;&#24230;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#33258;&#21160;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10965
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;CLIP-Dissect&#65292;&#21487;&#20197;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#21333;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;CLIP-Dissect&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CLIP-Dissect&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#22791;&#8220;&#22320;&#38754;&#30495;&#30456;&#8221;&#65288;ground-truth&#65289;&#30340;&#26368;&#21518;&#19968;&#23618;&#31070;&#32463;&#20803;&#20197;&#21450;&#20855;&#22791;&#23450;&#24615;&#22909;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65306;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#25193;&#23637;&#20197;&#21033;&#29992;&#26410;&#26469;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;CLIP-Dissect&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;4&#20998;&#38047;&#20869;&#26631;&#35760;ResNet-50&#30340;&#20116;&#23618;&#25152;&#26377;&#31070;&#32463;&#20803;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Trustworthy-ML-Lab/CLIP-dissect &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21152;&#36895;PDE&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#65292;&#20855;&#26377;&#20445;&#35777;&#30340;&#31934;&#24230;&#65292;&#24182;&#21487;&#29992;&#20110;&#20854;&#20182;&#35774;&#32622;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#25311;&#21442;&#25968;&#21521;&#21069;&#27169;&#22411;&#65292;&#21516;&#26102;&#30830;&#23450;Biot&#25968;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2204.02272</link><description>&lt;p&gt;
&#26059;&#36716;&#30424;&#31995;&#32479;&#20013;&#26102;&#31354;&#28909;&#27969;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#65292;&#37319;&#29992;&#28145;&#24230;&#20195;&#29702;&#21152;&#36895;&#24310;&#36831;&#25509;&#21463; HMC &#27861;
&lt;/p&gt;
&lt;p&gt;
Deep surrogate accelerated delayed-acceptance HMC for Bayesian inference of spatio-temporal heat fluxes in rotating disc systems. (arXiv:2204.02272v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21152;&#36895;PDE&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#65292;&#20855;&#26377;&#20445;&#35777;&#30340;&#31934;&#24230;&#65292;&#24182;&#21487;&#29992;&#20110;&#20854;&#20182;&#35774;&#32622;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#25311;&#21442;&#25968;&#21521;&#21069;&#27169;&#22411;&#65292;&#21516;&#26102;&#30830;&#23450;Biot&#25968;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915; PDE &#29615;&#22659;&#19979;&#30340;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20445;&#35777;&#30340;&#31934;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#38024;&#23545;&#19968;&#20010;&#34987;&#31216;&#20026; Biot &#25968;&#30340;&#26102;&#31354;&#28909;&#27969;&#21442;&#25968;&#30340;&#19981;&#36866;&#23450;&#38382;&#39064;&#36827;&#34892;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#25968;&#25454;&#26469;&#33258;&#36866;&#24212;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#65292;&#27169;&#25311;&#21442;&#25968;&#21521;&#21069;&#27169;&#22411;&#12290;&#36890;&#36807;&#21516;&#26102;&#30830;&#23450; Biot &#25968;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#23427;&#35843;&#25972;&#29289;&#29702;&#21551;&#21457;&#24335;&#35757;&#32451;&#25439;&#22833;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#22806;&#37096;&#27714;&#35299;&#30340;&#24773;&#20917;&#19979;&#36924;&#36817;&#21521;&#21069;&#21644;&#21453;&#21521;&#35299;&#12290;&#20351;&#29992;&#38543;&#26426;&#20999;&#27604;&#38634;&#22827;&#32423;&#25968;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#36924;&#36817;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#20351;&#29992;&#20195;&#29702;&#24212;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599; (HMC) &#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20195;&#29702;&#21518;&#39564;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a deep learning accelerated methodology to solve PDE-based Bayesian inverse problems with guaranteed accuracy. This is motivated by the ill-posed problem of inferring a spatio-temporal heat-flux parameter known as the Biot number given temperature data, however the methodology is generalisable to other settings. To accelerate Bayesian inference, we develop a novel training scheme that uses data to adaptively train a neural-network surrogate simulating the parametric forward model. By simultaneously identifying an approximate posterior distribution over the Biot number, and weighting a physics-informed training loss according to this, our approach approximates forward and inverse solution together without any need for external solves. Using a random Chebyshev series, we outline how to approximate a Gaussian process prior, and using the surrogate we apply Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution. We derive convergence of the surrogate posterior
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#31070;&#32463;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#37322;&#20854;&#20915;&#31574;&#12290;&#26041;&#27861;&#33021;&#22815;&#26631;&#35782;&#36129;&#29486;&#27599;&#20010;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2204.02241</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#24615;&#24182;&#35299;&#37322;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A Set Membership Approach to Discovering Feature Relevance and Explaining Neural Classifier Decisions. (arXiv:2204.02241v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#31070;&#32463;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#37322;&#20854;&#20915;&#31574;&#12290;&#26041;&#27861;&#33021;&#22815;&#26631;&#35782;&#36129;&#29486;&#27599;&#20010;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20998;&#31867;&#22120;&#26159;&#25552;&#20379;&#27169;&#24335;&#31867;&#21035;&#20915;&#31574;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26500;&#25104;&#20102;&#26576;&#20010;&#26410;&#30693;&#20989;&#25968;&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#65292;&#35813;&#20989;&#25968;&#23558;&#27169;&#24335;&#25968;&#25454;&#26144;&#23556;&#21040;&#20854;&#30456;&#24212;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35813;&#20989;&#25968;&#30340;&#30693;&#35782;&#20197;&#21450;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#24448;&#24448;&#26080;&#27861;&#33719;&#24471;&#26377;&#20851;&#22914;&#20309;&#36827;&#34892;&#20855;&#20307;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#24378;&#22823;&#30340;&#23398;&#20064;&#31995;&#32479;&#34987;&#35748;&#20026;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#23427;&#20204;&#24448;&#24448;&#34987;&#35748;&#20026;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20998;&#26512;&#65292;&#25105;&#20204;&#25226;&#36755;&#20837;&#27169;&#24335;&#20998;&#25104;&#23376;&#38598;&#65292;&#20197;&#20851;&#32852;&#19981;&#21516;&#30340;&#36755;&#20986;&#22522;&#31867;&#12290;&#36825;&#31181;&#20851;&#32852;&#26159;&#36890;&#36807;&#35745;&#31639;&#23558;&#20998;&#31867;&#22120;&#20915;&#31574;&#24341;&#23548;&#21040;&#26367;&#20195;&#31867;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#36755;&#20837;&#25668;&#21160;&#26469;&#25512;&#26029;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21738;&#20123;&#29305;&#24449;&#34987;&#35748;&#20026;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;&#26631;&#20934;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#21644;&#35299;&#37322;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20570;&#20986;&#30340;&#20998;&#31867;&#20915;&#31574;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural classifiers are non linear systems providing decisions on the classes of patterns, for a given problem they have learned. The output computed by a classifier for each pattern constitutes an approximation of the output of some unknown function, mapping pattern data to their respective classes. The lack of knowledge of such a function along with the complexity of neural classifiers, especially when these are deep learning architectures, do not permit to obtain information on how specific predictions have been made. Hence, these powerful learning systems are considered as black boxes and in critical applications their use tends to be considered inappropriate. Gaining insight on such a black box operation constitutes a one way approach in interpreting operation of neural classifiers and assessing the validity of their decisions. In this paper we tackle this problem introducing a novel methodology for discovering which features are considered relevant by a trained neural classifier a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#27010;&#25324;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#20116;&#20010;&#33879;&#21517;&#23376;&#22270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#23376;&#22270;&#21516;&#26500;&#65288;&#35745;&#25968;&#21644;&#21305;&#37197;&#65289;&#12289;&#26368;&#22823;&#20844;&#20849;&#23376;&#22270;&#12289;&#31038;&#21306;&#26816;&#27979;&#21644;&#31038;&#21306;&#25628;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.01057</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#32508;&#36848;&#65306;&#22270;&#27169;&#24335;&#25552;&#21462;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Survey on Machine Learning Solutions for Graph Pattern Extraction. (arXiv:2204.01057v3 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#27010;&#25324;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#20116;&#20010;&#33879;&#21517;&#23376;&#22270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#23376;&#22270;&#21516;&#26500;&#65288;&#35745;&#25968;&#21644;&#21305;&#37197;&#65289;&#12289;&#26368;&#22823;&#20844;&#20849;&#23376;&#22270;&#12289;&#31038;&#21306;&#26816;&#27979;&#21644;&#31038;&#21306;&#25628;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#32473;&#23450;&#22270;&#24418;&#30340;&#19968;&#37096;&#20998;&#39030;&#28857;&#21644;&#36793;&#26469;&#26500;&#36896;&#23376;&#22270;&#12290;&#23384;&#22312;&#35768;&#22810;&#23545;&#20110;&#23376;&#22270;&#20855;&#26377;&#36951;&#20256;&#24615;&#30340;&#22270;&#24418;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#26469;&#33258;&#19981;&#21516;&#31038;&#21306;&#30340;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#38750;&#24120;&#20851;&#27880;&#30740;&#31350;&#20247;&#22810;&#23376;&#22270;&#38382;&#39064;&#65292;&#32780;&#19981;&#20165;&#26159;&#26222;&#36890;&#30340;&#22270;&#38382;&#39064;&#12290;&#35768;&#22810;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#30740;&#31350;&#23376;&#22270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#25552;&#21462;&#32473;&#23450;&#22270;&#24418;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#30001;&#20110;&#26576;&#20123;&#31867;&#22411;&#30340;&#22270;&#24418;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#65292;&#20026;&#20102;&#25913;&#21892;&#29616;&#26377;&#26694;&#26550;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#26368;&#36817;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#22788;&#29702;&#21508;&#31181;&#23376;&#22270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#30340;&#20116;&#20010;&#33879;&#21517;&#23376;&#22270;&#38382;&#39064;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#23427;&#20204;&#26159;&#23376;&#22270;&#21516;&#26500;&#65288;&#35745;&#25968;&#21644;&#21305;&#37197;&#65289;&#12289;&#26368;&#22823;&#20844;&#20849;&#23376;&#22270;&#12289;&#31038;&#21306;&#26816;&#27979;&#21644;&#31038;&#21306;&#25628;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27599;&#20010;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
A subgraph is constructed by using a subset of vertices and edges of a given graph. There exist many graph properties that are hereditary for subgraphs. Hence, researchers from different communities have paid a great deal of attention in studying numerous subgraph problems, on top of the ordinary graph problems. Many algorithms are proposed in studying subgraph problems, where one common approach is by extracting the patterns and structures of a given graph. Due to the complex structures of certain types of graphs and to improve overall performances of the existing frameworks, machine learning techniques have recently been employed in dealing with various subgraph problems. In this article, we present a comprehensive review on five well known subgraph problems that have been tackled by using machine learning methods. They are subgraph isomorphism (both counting and matching), maximum common subgraph, community detection and community search problems. We provide an outline of each propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#20013;&#30340;&#23884;&#20837;&#31354;&#38388;&#23558;&#35821;&#38899;&#20449;&#21495;&#19982;&#22024;&#26434;&#29615;&#22659;&#20998;&#31163;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20174;&#22122;&#22768;&#25110;&#28151;&#21709;&#20013;&#20998;&#31163;&#35821;&#38899;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.15578</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#23884;&#20837;&#23558;&#35821;&#38899;&#19982;&#29615;&#22659;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling speech from surroundings with neural embeddings. (arXiv:2203.15578v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#20013;&#30340;&#23884;&#20837;&#31354;&#38388;&#23558;&#35821;&#38899;&#20449;&#21495;&#19982;&#22024;&#26434;&#29615;&#22659;&#20998;&#31163;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20174;&#22122;&#22768;&#25110;&#28151;&#21709;&#20013;&#20998;&#31163;&#35821;&#38899;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#20013;&#30340;&#23884;&#20837;&#31354;&#38388;&#23558;&#35821;&#38899;&#20449;&#21495;&#19982;&#22024;&#26434;&#29615;&#22659;&#20998;&#31163;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#32467;&#26500;&#21270;&#30340;&#38899;&#39057;&#27874;&#24418;&#32534;&#30721;&#21521;&#37327;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#32534;&#30721;&#21521;&#37327;&#34920;&#31034;&#35821;&#38899;&#20449;&#21495;&#65292;&#20854;&#20313;&#37096;&#20998;&#34920;&#31034;&#29615;&#22659;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#36755;&#20837;&#27874;&#24418;&#30340;&#23884;&#20837;&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#20174;&#28151;&#21512;&#20998;&#21306;&#20013;&#24544;&#23454;&#22320;&#37325;&#24314;&#38899;&#39057;&#65292;&#20174;&#32780;&#30830;&#20445;&#27599;&#20010;&#20998;&#21306;&#32534;&#30721;&#20102;&#19968;&#20010;&#21333;&#29420;&#30340;&#38899;&#39057;&#23646;&#24615;&#12290;&#20316;&#20026;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20174;&#22122;&#22768;&#25110;&#28151;&#21709;&#29305;&#24615;&#20013;&#20998;&#31163;&#35821;&#38899;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#26377;&#30446;&#30340;&#22320;&#35843;&#25972;&#38899;&#39057;&#36755;&#20986;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2203.12026</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;&#22312;ADAS&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;Deeper&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#23454;&#29616;&#30340;&#20223;&#30495;&#38598;&#25104;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#12290;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#32452;&#26032;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;-&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#65288;&#956;+&#955;&#65289;&#21644;&#65288;&#956;&#65292;&#955;&#65289;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#20197;&#21450;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#65292;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#36136;&#37327;&#31181;&#23376;&#31181;&#32676;&#20197;&#21450;&#20026;&#24314;&#27169;&#27979;&#35797;&#22330;&#26223;&#20351;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#20132;&#21449;&#21644;&#31361;&#21464;&#25805;&#20316;&#12290;&#20026;&#20102;&#23637;&#31034;Deeper&#20013;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#19982;SBST 2021&#30340;&#20116;&#20010;&#21442;&#36187;&#24037;&#20855;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;Deeper&#20013;&#30340;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#19981;&#20165;&#22312;&#20197;&#21069;&#30340;&#29256;&#26412;&#19978;&#26377;&#20102;&#24456;&#22823;&#25552;&#21319;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#22312;&#35780;&#20272;&#20013;&#38656;&#35201;&#32771;&#34385;&#20266;&#24433;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2203.02928</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#25200;&#21160;&#20266;&#24433;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#22312;&#35780;&#20272;&#20013;&#38656;&#35201;&#32771;&#34385;&#20266;&#24433;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22914;&#20309;&#35299;&#37322;&#20854;&#20915;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#20986;&#29616;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#29305;&#24449;&#26469;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#26159;&#25200;&#21160;&#26412;&#36523;&#21487;&#33021;&#20250;&#24341;&#20837;&#20266;&#24433;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20266;&#24433;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27492;&#26041;&#27861;&#35780;&#20272;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26102;&#32771;&#34385;&#20266;&#24433;&#23384;&#22312;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DIAD&#26694;&#26550;&#65292;&#20351;&#29992;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20316;&#20026;&#30333;&#30418;&#27169;&#22411;&#65292;&#36890;&#36807;&#37096;&#20998;&#35782;&#21035;&#30446;&#26631;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#24182;&#33021;&#22815;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.02034</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient and Interpretable Tabular Anomaly Detection. (arXiv:2203.02034v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DIAD&#26694;&#26550;&#65292;&#20351;&#29992;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20316;&#20026;&#30333;&#30418;&#27169;&#22411;&#65292;&#36890;&#36807;&#37096;&#20998;&#35782;&#21035;&#30446;&#26631;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#24182;&#33021;&#22815;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#20351;&#29992;&#24050;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#25968;&#37327;&#26377;&#38480;&#65292;&#20294;&#23545;&#20110;&#39640;&#31934;&#24230;&#30340;AD&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#19981;&#21487;&#35299;&#37322;&#65292;&#36825;&#20250;&#38459;&#30861;&#30456;&#20851;&#26041;&#20102;&#35299;&#24322;&#24120;&#20135;&#29983;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIAD&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20316;&#20026;&#30333;&#30418;&#27169;&#22411;&#65292;&#36890;&#36807;&#37096;&#20998;&#35782;&#21035;&#30446;&#26631;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#33258;&#28982;&#22320;&#22788;&#29702;&#22024;&#26434;&#25110;&#24322;&#36136;&#24615;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays an important role in numerous applications. We focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high AD accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. In addition, the proposed framework, DIAD, can incorporate a small amount of labeled data to further boost anomaly detection performances in semi-supervised settings. We demonstrate the superiority of our framework compared to previous work in both unsupervised and semi-supervised settings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.00922</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#21494;&#38754;&#65306;&#40065;&#26834;&#24615;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Canonical foliations of neural networks: application to robustness. (arXiv:2203.00922v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#32780;&#23545;&#25239;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#37319;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#12290;&#36890;&#36807;&#21019;&#24314;&#32771;&#34385;&#25968;&#25454;&#31354;&#38388;&#26354;&#29575;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363; two-step spectral attack&#65292;&#26469;&#35828;&#26126;&#36825;&#20010;&#24819;&#27861;&#12290;&#25968;&#25454;&#31354;&#38388;&#34987;&#35270;&#20026;&#19968;&#20010;&#37197;&#22791;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340; Fisher &#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#25289;&#22238;&#30340;&#65288;&#36864;&#21270;&#30340;&#65289;&#40654;&#26364;&#27969;&#24418;&#12290;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#35813;&#24230;&#37327;&#20165;&#20026;&#21322;&#27491;&#23450;&#65292;&#20854;&#20869;&#26680;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#23545;&#35937;&#12290;&#20174;&#35813;&#26680;&#20013;&#23548;&#20986;&#19968;&#20010;&#35268;&#33539;&#21494;&#38754;&#12290;&#27178;&#21521;&#21494;&#30340;&#26354;&#29575;&#32473;&#20986;&#20102;&#36866;&#24403;&#30340;&#20462;&#27491;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20004;&#27493;&#36817;&#20284;&#30340;&#27979;&#22320;&#32447;&#21644;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#25239;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#19968;&#20010; 2D &#29609;&#20855;&#31034;&#20363;&#20013;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are known to be vulnerable to adversarial attacks. Adversarial learning is therefore becoming a crucial task. We propose a new vision on neural network robustness using Riemannian geometry and foliation theory. The idea is illustrated by creating a new adversarial attack that takes into account the curvature of the data space. This new adversarial attack called the two-step spectral attack is a piece-wise linear approximation of a geodesic in the data space. The data space is treated as a (degenerate) Riemannian manifold equipped with the pullback of the Fisher Information Metric (FIM) of the neural network. In most cases, this metric is only semi-definite and its kernel becomes a central object to study. A canonical foliation is derived from this kernel. The curvature of transverse leaves gives the appropriate correction to get a two-step approximation of the geodesic and hence a new efficient adversarial attack. The method is first illustrated on a 2D toy example
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#21464;&#37327;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#22823;&#30456;&#20114;&#20449;&#24687;&#36755;&#20837;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#29289;&#27169;&#22411;&#30340;Fisher&#20449;&#24687;&#20998;&#26512;&#65292;&#35777;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#21462;&#24471;&#24040;&#22823;&#30340;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#24378;&#22823;&#30340;&#26102;&#38388;&#32467;&#26500;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35843;&#25972;&#20026;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#34701;&#20837;&#26102;&#24577;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.13426</link><description>&lt;p&gt;
&#31163;&#25955;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Active Learning for Discrete Latent Variable Models. (arXiv:2202.13426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#21464;&#37327;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#22823;&#30456;&#20114;&#20449;&#24687;&#36755;&#20837;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#29289;&#27169;&#22411;&#30340;Fisher&#20449;&#24687;&#20998;&#26512;&#65292;&#35777;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#21462;&#24471;&#24040;&#22823;&#30340;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#24378;&#22823;&#30340;&#26102;&#38388;&#32467;&#26500;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35843;&#25972;&#20026;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#34701;&#20837;&#26102;&#24577;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#25311;&#21512;&#27169;&#22411;&#21442;&#25968;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#22240;&#27492;&#25104;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#30340;&#20027;&#21160;&#23398;&#20064;&#30740;&#31350;&#24448;&#24448;&#24573;&#35270;&#20102;&#22312;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#23398;&#31185;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#21464;&#37327;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#22823;&#30456;&#20114;&#20449;&#24687;&#36755;&#20837;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31867;&#31216;&#20026;&#8220;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#29289;&#8221;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#30693;&#23545;&#20110;&#32447;&#24615;&#39640;&#26031;&#22238;&#24402;&#27169;&#22411;&#65292;&#20027;&#21160;&#23398;&#20064;&#24182;&#19981;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#20998;&#26512;&#65292;&#34920;&#26126;&#21363;&#20351;&#23545;&#20110;&#36825;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#20027;&#21160;&#23398;&#20064;&#20173;&#28982;&#21487;&#20197;&#21462;&#24471;&#24040;&#22823;&#30340;&#25910;&#30410;&#65292;&#24182;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23545;&#27492;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#24378;&#22823;&#30340;&#26102;&#38388;&#32467;&#26500;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35843;&#25972;&#20026;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#34701;&#20837;&#26102;&#24577;&#20381;&#36182;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20027;&#21160;&#23398;&#20064;&#22312;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31163;&#25955;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#21482;&#38656;&#36827;&#34892;&#36739;&#23569;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. We first apply our method to a class of models known as "mixtures of linear regressions" (MLR). While it is well known that active learning confers no advantage for linear-Gaussian regression models, we use Fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. We then consider a powerful class of temporally structured latent var
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#19981;&#33021;&#23436;&#20840;&#38450;&#27490;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65288;LIAs&#65289;&#65292;&#20294;&#21487;&#20197;&#38480;&#21046;LIAs&#23545;&#25163;&#30340;&#20248;&#21183;&#21644;&#35821;&#20041;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2202.12968</link><description>&lt;p&gt;
&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26159;&#21542;&#33021;&#22815;&#38450;&#27490;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Label Differential Privacy Prevent Label Inference Attacks?. (arXiv:2202.12968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12968
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#19981;&#33021;&#23436;&#20840;&#38450;&#27490;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65288;LIAs&#65289;&#65292;&#20294;&#21487;&#20197;&#38480;&#21046;LIAs&#23545;&#25163;&#30340;&#20248;&#21183;&#21644;&#35821;&#20041;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#65288;label-DP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#20844;&#20849;&#29305;&#24449;&#21644;&#25935;&#24863;&#31169;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31169;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20855;&#26377;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20294;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;label-DP&#24182;&#19981;&#33021;&#38450;&#27490;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65288;LIAs&#65289;&#65306;&#20351;&#29992;label-DP&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20844;&#20849;&#35757;&#32451;&#29305;&#24449;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#39640;&#31934;&#24230;&#22320;&#24674;&#22797;&#20854;&#26088;&#22312;&#20445;&#25252;&#30340;&#38750;&#24120;&#31169;&#20154;&#26631;&#31614;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#24182;&#19981;&#30683;&#30462;&#65292;&#24182;&#19988;label-DP&#26088;&#22312;&#38480;&#21046;LIA&#23545;&#25163;&#30340;&#20248;&#21183;&#65292;&#20197;&#20415;&#19982;&#20351;&#29992;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#39044;&#27979;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;label-DP $\epsilon=0$&#26102;&#65292;&#35813;&#20248;&#21183;&#20026;&#38646;&#65292;&#22240;&#27492;&#26368;&#20339;&#25915;&#20987;&#26159;&#26681;&#25454;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#26631;&#31614;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26174;&#31034;&#20102;label-DP&#25552;&#20379;&#30340;&#35821;&#20041;&#20445;&#25252;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#36873;&#25321; $\varepsilon$ &#20197;&#23558;LIAs&#30340;&#23041;&#32961;&#38480;&#21046;&#22312;&#26576;&#20010;&#27700;&#24179;&#20197;&#19979;&#30340;&#25351;&#21335;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Label differential privacy (label-DP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice label-DP does not preclude label inference attacks (LIAs): Models trained with label-DP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that label-DP is designed to limit the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At label-DP $\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Our bound shows the semantic protection conferred by label-DP and gives guidelines on how to choose $\varepsilon$ to limit the threat of LIAs below a certain level. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;MAML&#21644;ANIL&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#20849;&#21516;&#30340;&#25968;&#25454;&#34920;&#31034;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#36866;&#24212;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#26469;&#25913;&#21892;&#34920;&#31034;&#27861;&#65292;&#36825;&#20063;&#26159;&#23548;&#33268;&#20849;&#20139;&#34920;&#31034;&#27861;&#20986;&#29616;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2202.03483</link><description>&lt;p&gt;
MAML&#21644;ANIL&#34987;&#35777;&#26126;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAML and ANIL Provably Learn Representations. (arXiv:2202.03483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;MAML&#21644;ANIL&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#20849;&#21516;&#30340;&#25968;&#25454;&#34920;&#31034;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#36866;&#24212;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#26469;&#25913;&#21892;&#34920;&#31034;&#27861;&#65292;&#36825;&#20063;&#26159;&#23548;&#33268;&#20849;&#20139;&#34920;&#31034;&#27861;&#20986;&#29616;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32463;&#39564;&#35777;&#25454;&#35753;&#20154;&#20204;&#35748;&#20026;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#65288;GBML&#65289;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#20849;&#20139;&#30340;&#34920;&#36798;&#25968;&#25454;&#34920;&#31034;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;GBML&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;GBML&#26041;&#27861;&#65292;MAML&#21644;ANIL&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#19968;&#38454;&#36817;&#20284;&#37117;&#33021;&#22815;&#23398;&#20064;&#19968;&#32452;&#32473;&#23450;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#21516;&#34920;&#31034;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#33879;&#21517;&#30340;&#22810;&#20219;&#21153;&#32447;&#24615;&#34920;&#31034;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#33021;&#22815;&#20197;&#25351;&#25968;&#24555;&#30340;&#36895;&#24230;&#24674;&#22797;&#22320;&#38754;&#23454;&#20917;&#34920;&#31034;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#38416;&#26126;&#65292;&#39537;&#21160;MAML&#21644;ANIL&#24674;&#22797;&#28508;&#22312;&#34920;&#31034;&#27861;&#30340;&#21160;&#21147;&#26159;&#23427;&#20204;&#35843;&#25972;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#21033;&#29992;&#28508;&#22312;&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#26469;&#25913;&#21892;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#26041;&#21521;&#30340;&#34920;&#31034;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#37322;GBML&#26041;&#27861;&#23548;&#33268;&#20849;&#20139;&#34920;&#31034;&#27861;&#20986;&#29616;&#30340;&#29702;&#35770;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical evidence has driven conventional wisdom to believe that gradient-based meta-learning (GBML) methods perform well at few-shot learning because they learn an expressive data representation that is shared across tasks. However, the mechanics of GBML have remained largely mysterious from a theoretical perspective. In this paper, we prove that two well-known GBML methods, MAML and ANIL, as well as their first-order approximations, are capable of learning common representation among a set of given tasks. Specifically, in the well-known multi-task linear representation learning setting, they are able to recover the ground-truth representation at an exponentially fast rate. Moreover, our analysis illuminates that the driving force causing MAML and ANIL to recover the underlying representation is that they adapt the final layer of their model, which harnesses the underlying task diversity to improve the representation in all directions of interest. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#21363;&#21487;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#23545;Huggingface&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#19988;&#30456;&#20851;&#24615;&#24378;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2202.02842</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#35757;&#32451;&#25110;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#21363;&#21487;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#23545;Huggingface&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#19988;&#30456;&#20851;&#24615;&#24378;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#26500;&#21442;&#25968;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#20960;&#39033;&#23454;&#35777;&#30740;&#31350;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#30456;&#20851;&#20998;&#26512;&#65292;&#20197;&#23547;&#25214;&#26377;&#25928;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#20197;&#25351;&#23548;&#27169;&#22411;&#36873;&#25321;&#12290;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#39044;&#35745;&#19982;&#27979;&#35797;&#24615;&#33021;&#24378;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#30446;&#26631;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#20998;&#26512;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#27169;&#22411;&#36873;&#25321;&#30740;&#31350;&#65306;&#65288;i&#65289;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#65307;&#65288;ii&#65289;&#32771;&#34385;&#30452;&#25509;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#32780;&#38750;&#27867;&#21270;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#65307;&#65288;iii&#65289;&#25506;&#32034;&#19981;&#38656;&#35201;&#35775;&#38382;&#25968;&#25454;&#21363;&#21487;&#35745;&#31639;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20174;&#36825;&#20123;&#30446;&#26631;&#20986;&#21457;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#31532;&#19968;&#20010;&#20351;&#29992;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#23545;&#26469;&#33258;Huggingface&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#30340;&#32467;&#26524;&#65292;&#24182;&#27604;&#36739;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#19981;&#27490;&#19982;&#27979;&#35797;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#32780;&#19988;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting suitable architecture parameters and training hyperparameters is essential for enhancing machine learning (ML) model performance. Several recent empirical studies conduct large-scale correlational analysis on neural networks (NNs) to search for effective \emph{generalization metrics} that can guide this type of model selection. Effective metrics are typically expected to correlate strongly with test performance. In this paper, we expand on prior analyses by examining generalization-metric-based model selection with the following objectives: (i) focusing on natural language processing (NLP) tasks, as prior work primarily concentrates on computer vision (CV) tasks; (ii) considering metrics that directly predict \emph{test error} instead of the \emph{generalization gap}; (iii) exploring metrics that do not need access to data to compute. From these objectives, we are able to provide the first model selection results on large pretrained Transformers from Huggingface using general
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21457;&#29616;&#36719;&#23646;&#24615;&#30340;&#20010;&#24615;&#21270;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors. (arXiv:2202.02830v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02830
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#20351;&#29992;&#30340;&#21407;&#22987;&#29992;&#25143;&#21453;&#39304;&#30340;&#23616;&#38480;&#24615;&#65288;&#20363;&#22914;&#28857;&#20987;&#12289;&#39033;&#30446;&#28040;&#36153;&#12289;&#35780;&#20998;&#65289;&#12290;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#20197;&#26356;&#20016;&#23500;&#30340;&#26041;&#24335;&#34920;&#36798;&#24847;&#22270;&#12289;&#20559;&#22909;&#12289;&#32422;&#26463;&#21644;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;&#21253;&#25324;&#20998;&#31867;&#25628;&#32034;&#21644;&#23545;&#35805;&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#25214;&#21040;&#20351;&#29992;&#36825;&#20123;&#21453;&#39304;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#12290;&#19968;&#20010;&#25361;&#25112;&#26159;&#20174;&#32463;&#24120;&#29992;&#20110;&#25551;&#36848;&#25152;&#38656;&#39033;&#30446;&#30340;&#24320;&#25918;&#24335;&#26415;&#35821;&#25110;&#23646;&#24615;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#35821;&#20041;&#24847;&#22270;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#25913;&#36827;&#25512;&#33616;&#32467;&#26524;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#8212;&#8212;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23398;&#20064;&#19968;&#31181;&#34920;&#31034;&#65292;&#25429;&#25417;&#36825;&#20123;&#23646;&#24615;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#21151;&#33021;&#26159;&#23427;&#33021;&#22815;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interactive recommender systems have emerged as a promising paradigm to overcome the limitations of the primitive user feedback used by traditional recommender systems (e.g., clicks, item consumption, ratings). They allow users to express intent, preferences, constraints, and contexts in a richer fashion, often using natural language (including faceted search and dialogue). Yet more research is needed to find the most effective ways to use this feedback. One challenge is inferring a user's semantic intent from the open-ended terms or attributes often used to describe a desired item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [26], a recently developed approach for model interpretability in machine learning, we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in recommender systems. One novel feature of our approach is its ability to distinguis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24179;&#26041;&#26681;&#21644;&#36870;&#24179;&#26041;&#26681;&#36816;&#31639;&#12290;&#24403;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;&#30340;&#20302;&#31209;&#25200;&#21160;&#26102;&#65292;&#23384;&#22312;&#19968;&#31181;&#20302;&#31209;&#36817;&#20284;&#26657;&#27491;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20195;&#25968;Riccati&#26041;&#31243;&#30340;&#20302;&#31209;&#35299;&#35745;&#31639;&#65292;&#24182;&#21487;&#20197;&#22312;&#20004;&#20010;&#25968;&#20540;&#20363;&#23376;&#20013;&#24471;&#21040;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2201.13156</link><description>&lt;p&gt;
&#30697;&#38453;&#24179;&#26041;&#26681;&#30340;&#20302;&#31209;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Updates of Matrix Square Roots. (arXiv:2201.13156v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24179;&#26041;&#26681;&#21644;&#36870;&#24179;&#26041;&#26681;&#36816;&#31639;&#12290;&#24403;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;&#30340;&#20302;&#31209;&#25200;&#21160;&#26102;&#65292;&#23384;&#22312;&#19968;&#31181;&#20302;&#31209;&#36817;&#20284;&#26657;&#27491;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20195;&#25968;Riccati&#26041;&#31243;&#30340;&#20302;&#31209;&#35299;&#35745;&#31639;&#65292;&#24182;&#21487;&#20197;&#22312;&#20004;&#20010;&#25968;&#20540;&#20363;&#23376;&#20013;&#24471;&#21040;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#30697;&#38453;&#21152;&#19978;&#20302;&#31209;&#25200;&#21160;&#30340;&#32467;&#26500;&#26159;&#24120;&#35265;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#32791;&#26102;&#30340;&#30697;&#38453;&#35745;&#31639;&#65292;&#32463;&#24120;&#38656;&#35201;&#31639;&#27861;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#32500;&#25345;&#36825;&#31181;&#32467;&#26500;&#25191;&#34892;&#30697;&#38453;&#27714;&#36870;&#30340;Sherman-Morrison-Woodbury&#20844;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#30697;&#38453;&#24179;&#26041;&#26681;&#21644;&#36870;&#24179;&#26041;&#26681;&#36816;&#31639;&#12290;&#24403;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;&#30340;&#20302;&#31209;&#25200;&#21160;&#26102;&#65292;&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#19968;&#20010;&#20302;&#31209;&#36817;&#20284;&#26657;&#27491;(&#36870;)&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#23545;&#30495;&#27491;&#26657;&#27491;&#30340;&#29305;&#24449;&#20540;&#30340;&#20960;&#20309;&#34928;&#20943;&#30028;&#38480;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27492;&#26657;&#27491;&#26041;&#26696;&#20316;&#20026;&#20195;&#25968;Riccati&#26041;&#31243;&#30340;&#35299;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#35745;&#31639;&#27492;&#26041;&#31243;&#30340;&#20302;&#31209;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36890;&#36807;&#36817;&#20284;&#27714;&#35299;&#20195;&#25968;Riccati&#26041;&#31243;&#20135;&#29983;&#30340;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25968;&#20540;&#20363;&#23376;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models in which the covariance matrix has the structure of a sparse matrix plus a low rank perturbation are ubiquitous in data science applications. It is often desirable for algorithms to take advantage of such structures, avoiding costly matrix computations that often require cubic time and quadratic storage. This is often accomplished by performing operations that maintain such structures, e.g. matrix inversion via the Sherman-Morrison-Woodbury formula. In this paper we consider the matrix square root and inverse square root operations. Given a low rank perturbation to a matrix, we argue that a low-rank approximate correction to the (inverse) square root exists. We do so by establishing a geometric decay bound on the true correction's eigenvalues. We then proceed to frame the correction as the solution of an algebraic Riccati equation, and discuss how a low-rank solution to that equation can be computed. We analyze the approximation error incurred when approximately solving the alge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2201.11989</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#30340;&#23384;&#22312;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#65292;&#22914;&#19981;&#21516;&#30340;&#24658;&#23450;&#29575;&#25110;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#31561;&#65292;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#26377;&#21161;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#27492;&#22806;&#65292;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#20063;&#24456;&#37325;&#35201;&#65292;&#20004;&#32773;&#37117;&#24433;&#21709;&#20102;&#35757;&#32451;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#37327;&#12290;&#26412;&#25991;&#22522;&#20110;&#24658;&#23450;&#23398;&#20064;&#29575;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#19982;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;TTUR&#65292;&#20026;&#20102;&#25214;&#21040;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;&#25152;&#38656;&#27493;&#39588;&#25968;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Fr'echet Inception Distance&#65288;FID&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Eigenlearning&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#26680;&#22238;&#24402;&#22312;&#23398;&#20064;&#27491;&#20132;&#22522;&#20989;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#23432;&#24658;&#23450;&#24459;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20026;Nakkiran&#31561;&#20154;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#29616;&#35937;&#65292;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#19982;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#31995;&#32479;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2110.03922</link><description>&lt;p&gt;
Eigenlearning&#26694;&#26550;&#65306;&#26680;&#22238;&#24402;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#23432;&#24658;&#23450;&#24459;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks. (arXiv:2110.03922v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Eigenlearning&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#26680;&#22238;&#24402;&#22312;&#23398;&#20064;&#27491;&#20132;&#22522;&#20989;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#23432;&#24658;&#23450;&#24459;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20026;Nakkiran&#31561;&#20154;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#29616;&#35937;&#65292;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#19982;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#31995;&#32479;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#27979;&#35797;&#39118;&#38505;&#21644;&#20854;&#20182;&#27867;&#21270;&#25351;&#26631;&#23548;&#20986;&#20102;&#31616;&#21333;&#30340;&#38381;&#24335;&#20272;&#35745;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#25512;&#23548;&#22823;&#22823;&#31616;&#21270;&#65292;&#26368;&#32456;&#34920;&#36798;&#24335;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#36825;&#20123;&#25913;&#36827;&#24471;&#30410;&#20110;&#25105;&#20204;&#35782;&#21035;&#20986;&#30340;&#19968;&#20010;&#23574;&#38160;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#23427;&#38480;&#21046;&#20102;KRR&#23398;&#20064;&#20219;&#20309;&#27491;&#20132;&#22522;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#27979;&#35797;&#39118;&#38505;&#21644;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#21487;&#20197;&#36879;&#26126;&#22320;&#29992;&#20110;&#25105;&#20204;&#22312;&#26680;&#29305;&#24449;&#22522;&#20013;&#35780;&#20272;&#30340;&#23432;&#24658;&#37327;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#65306;i&#65289;&#20026;Nakkiran&#31561;&#20154;&#65288;2020&#65289;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#25552;&#20379;&#29702;&#35770;&#35299;&#37322;&#65292;ii&#65289;&#25512;&#24191;&#20808;&#21069;&#20851;&#20110;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#30340;&#32467;&#26524;&#65292;iii&#65289;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#65292;&#24182;iv&#65289;&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#21644;&#29087;&#30693;&#31995;&#32479;&#20043;&#38388;&#30340;&#20005;&#23494;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the "deep bootstrap" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Sinkhorn&#36317;&#31163;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#25512;&#23548;&#20986;&#26356;&#23481;&#26131;&#22788;&#29702;&#19988;&#22312;&#23454;&#38469;&#20013;&#26356;&#21512;&#29702;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.11926</link><description>&lt;p&gt;
Sinkhorn&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sinkhorn Distributionally Robust Optimization. (arXiv:2109.11926v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.11926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Sinkhorn&#36317;&#31163;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#25512;&#23548;&#20986;&#26356;&#23481;&#26131;&#22788;&#29702;&#19988;&#22312;&#23454;&#38469;&#20013;&#26356;&#21512;&#29702;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Sinkhorn&#36317;&#31163; -&#19968;&#31181;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;Wasserstein&#36317;&#31163;&#21464;&#20307;- &#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#12290;&#25105;&#20204;&#20026;&#19968;&#33324;&#21517;&#20041;&#20998;&#24067;&#25512;&#23548;&#20102;&#20984;&#35268;&#21010;&#23545;&#20598;&#37325;&#26500;&#12290;&#30456;&#27604;&#20110;Wasserstein DRO&#65292;&#23545;&#20110;&#26356;&#22823;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#35745;&#31639;&#19978;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#23427;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#23545;&#23454;&#38469;&#24212;&#29992;&#26356;&#21512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#20598;&#37325;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#20559;&#26799;&#24230;&#31070;&#32463;&#20803;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#20363;&#65292;&#20197;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust optimization (DRO) with Sinkhorn distance -a variant of Wasserstein distance based on entropic regularization. We derive convex programming dual reformulation for a general nominal distribution. Compared with Wasserstein DRO, it is computationally tractable for a larger class of loss functions, and its worst-case distribution is more reasonable for practical applications. To solve the dual reformulation, we develop a stochastic mirror descent algorithm using biased gradient oracles and analyze its convergence rate. Finally, we provide numerical examples using synthetic and real data to demonstrate its superior performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20195;&#29702;&#20803;&#34920;&#31034;&#65288;MRA&#65289;&#65292;&#33021;&#22815;&#36328;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#21270;&#30340;&#20154;&#21475;&#25968;&#37327;&#36827;&#34892;&#25512;&#24191;&#30340;&#20195;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#20195;&#29702;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2108.12988</link><description>&lt;p&gt;
&#23398;&#20064;&#20803;&#34920;&#31034;&#26469;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20195;&#29702;&#20803;&#34920;&#31034;&#65288;MRA&#65289;&#65292;&#33021;&#22815;&#36328;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#21270;&#30340;&#20154;&#21475;&#25968;&#37327;&#36827;&#34892;&#25512;&#24191;&#30340;&#20195;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#20195;&#29702;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#22312;&#21333;&#20010;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#30340;&#34892;&#20026;&#36890;&#24120;&#20165;&#38480;&#20110;&#32473;&#23450;&#30340;&#20195;&#29702;&#25968;&#37327;&#12290;&#30001;&#20154;&#21475;&#25968;&#37327;&#21464;&#21270;&#24341;&#36215;&#30340;&#27599;&#20010;&#21333;&#20010;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#26368;&#20339;&#32852;&#21512;&#31574;&#30053;&#21644;&#28216;&#25103;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#29616;&#20195;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#29420;&#31435;&#24314;&#27169;&#12290;&#26412;&#25991;&#38024;&#23545;&#21019;&#24314;&#21487;&#20197;&#36328;&#20154;&#21475;&#25968;&#37327;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#36827;&#34892;&#25512;&#24191;&#30340;&#20195;&#29702;&#12290;&#27599;&#20010;&#20195;&#29702;&#19981;&#20877;&#23398;&#20064;&#21333;&#19968;&#31574;&#30053;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#28216;&#25103;&#26377;&#25928;&#31574;&#30053;&#30340;&#31574;&#30053;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#29702;&#20803;&#34920;&#31034;&#65288;MRA&#65289;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#28216;&#25103;&#36890;&#29992;&#21644;&#29305;&#23450;&#30340;&#31574;&#30053;&#30693;&#35782;&#12290;&#36890;&#36807;&#29992;&#22810;&#27169;&#24577;&#28508;&#22312;&#31574;&#30053;&#34920;&#31034;&#31574;&#30053;&#38598;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#21457;&#29616;&#28216;&#25103;&#36890;&#29992;&#30340;&#25112;&#30053;&#30693;&#35782;&#21644;&#19981;&#21516;&#30340;&#25112;&#30053;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20851;&#38480;&#21046;&#30340;&#20114;&#20449;&#24687;&#21487;&#20197;&#33719;&#24471;&#20195;&#29702;&#30340;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number. Every single MG induced by varying the population may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent reinforcement learning algorithms. In this work, our focus is on creating agents that can generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set comprising effective strategies across a variety of games. To achieve this, we propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the game-common strategic knowledge and diverse strategic modes are discovered through an iterative optimization procedure. We prove that by approximately maximizing the resulting constrained mutual i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#30340;&#27969;&#24418;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#24494;&#20998;&#39033;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#24433;&#20687;&#37325;&#26500;&#21644;&#29699;&#22534;&#31215;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2108.06988</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#26799;&#24230;&#35745;&#31639;&#31639;&#27861;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A diffusion-map-based algorithm for gradient computation on manifolds and applications. (arXiv:2108.06988v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#30340;&#27969;&#24418;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#24494;&#20998;&#39033;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#24433;&#20687;&#37325;&#26500;&#21644;&#29699;&#22534;&#31215;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27969;&#24418;&#20869;&#37096;&#28857;&#19978;&#30340;&#20989;&#25968;&#26679;&#26412;&#35780;&#20272;&#20272;&#35745;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#23450;&#20041;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388; Riemannian &#23376;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#30340;&#40654;&#26364;&#26799;&#24230;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#65292;&#36991;&#20813;&#20102;&#24494;&#20998;&#39033;&#65292;&#24182;&#35777;&#26126;&#20102;&#40654;&#26364;&#26799;&#24230;&#25299;&#23637;&#30340;&#20998;&#26512;&#25910;&#25947;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#40654;&#26364;&#26799;&#24230;&#20272;&#35745;&#24212;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#39564;&#35777;&#65292;&#21253;&#25324;&#20174;&#26410;&#30693;&#38543;&#26426;&#35282;&#24230;&#20998;&#24067;&#20013;&#36827;&#34892;&#24433;&#20687;&#37325;&#26500;&#20197;&#21450;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#30340;&#29699;&#22534;&#31215;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We recover the Riemannian gradient of a given function defined on interior points of a Riemannian submanifold in the Euclidean space based on a sample of function evaluations at points in the submanifold. This approach is based on the estimates of the Laplace-Beltrami operator proposed in the diffusion-maps theory. The Riemannian gradient estimates do not involve differential terms. Analytical convergence results of the Riemannian gradient expansion are proved. We apply the Riemannian gradient estimate in a gradient-based algorithm providing a derivative-free optimization method. We test and validate several applications, including tomographic reconstruction from an unknown random angle distribution, and the sphere packing problem in dimensions 2 and 3.
&lt;/p&gt;</description></item><item><title>Sequoia&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#36830;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#21508;&#31181;&#35774;&#32622;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37325;&#22797;&#24037;&#20316;&#65292;&#40723;&#21169;&#36830;&#32493;&#23398;&#20064;&#30340;&#32479;&#19968;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2108.01005</link><description>&lt;p&gt;
Sequoia: &#19968;&#20010;&#32479;&#19968;&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequoia: A Software Framework to Unify Continual Learning Research. (arXiv:2108.01005v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01005
&lt;/p&gt;
&lt;p&gt;
Sequoia&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#36830;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#21508;&#31181;&#35774;&#32622;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37325;&#22797;&#24037;&#20316;&#65292;&#40723;&#21169;&#36830;&#32493;&#23398;&#20064;&#30340;&#32479;&#19968;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#39046;&#22495;&#26088;&#22312;&#36890;&#36807;&#19982;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#31215;&#32047;&#30693;&#35782;&#21644;&#25216;&#33021;&#65292;&#24182;&#24320;&#21457;&#31639;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#23384;&#22312;&#22823;&#37327;&#35780;&#20272;&#31243;&#24207;&#65288;&#35774;&#32622;&#65289;&#21644;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65288;&#26041;&#27861;&#65289;&#65292;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#37117;&#26377;&#20854;&#33258;&#24049;&#30340;&#20551;&#35774;&#38598;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;&#36830;&#32493;&#23398;&#20064;&#30340;&#36827;&#23637;&#38590;&#20197;&#34913;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35774;&#32622;&#30340;&#20998;&#31867;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#32622;&#37117;&#34987;&#25551;&#36848;&#20026;&#19968;&#32452;&#20551;&#35774;&#12290;&#27492;&#35270;&#22270;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#20986;&#29616;&#65292;&#20854;&#20013;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#25104;&#20026;&#20855;&#26377;&#26356;&#22810;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35774;&#32622;&#30340;&#29238;&#32423;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#32487;&#25215;&#26469;&#20849;&#20139;&#21644;&#37325;&#29992;&#30740;&#31350;&#65292;&#22240;&#20026;&#20026;&#32473;&#23450;&#35774;&#32622;&#24320;&#21457;&#26041;&#27861;&#20063;&#20351;&#20854;&#30452;&#25509;&#36866;&#29992;&#20110;&#20854;&#20219;&#20309;&#23376;&#20195;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Sequoia&#30340;&#20844;&#24320;&#21487;&#29992;&#36719;&#20214;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#26469;&#33258;&#36830;&#32493;&#30417;&#30563;&#23398;&#20064;&#65288;CSL&#65289;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#39046;&#22495;&#30340;&#21508;&#31181;&#35774;&#32622;&#65292;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#25193;&#23637;&#23450;&#21046;&#21644;&#25193;&#23637;&#12290;&#36890;&#36807;Sequoia&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#22312;&#23396;&#31435;&#22320;&#24320;&#21457;&#26041;&#27861;&#21644;&#36827;&#34892;&#23454;&#39564;&#25152;&#20135;&#29983;&#30340;&#37325;&#22797;&#21162;&#21147;&#65292;&#24182;&#40723;&#21169;&#22312;&#36830;&#32493;&#23398;&#20064;&#19978;&#36827;&#34892;&#32479;&#19968;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Continual Learning (CL) seeks to develop algorithms that accumulate knowledge and skills over time through interaction with non-stationary environments. In practice, a plethora of evaluation procedures (settings) and algorithmic solutions (methods) exist, each with their own potentially disjoint set of assumptions. This variety makes measuring progress in CL difficult. We propose a taxonomy of settings, where each setting is described as a set of assumptions. A tree-shaped hierarchy emerges from this view, where more general settings become the parents of those with more restrictive assumptions. This makes it possible to use inheritance to share and reuse research, as developing a method for a given setting also makes it directly applicable onto any of its children. We instantiate this idea as a publicly available software framework called Sequoia, which features a wide variety of settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#23545;&#26694;&#26550;&#65292;&#21487;&#22312;&#22122;&#22768;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#32467;&#26500;&#25104;&#23545;&#20132;&#20114;&#20316;&#20026;&#20027;&#35201;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#25552;&#20986;&#30340;PI-GNN&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#20214;&#65292;&#33258;&#36866;&#24212;&#20272;&#35745;PI&#26631;&#31614;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;PI&#20272;&#35745;&#27169;&#22411;&#21644;&#19968;&#31181;&#35299;&#32806;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.07451</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#21644;&#21033;&#29992;&#25104;&#23545;&#20132;&#20114;&#26469;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions. (arXiv:2106.07451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#23545;&#26694;&#26550;&#65292;&#21487;&#22312;&#22122;&#22768;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#32467;&#26500;&#25104;&#23545;&#20132;&#20114;&#20316;&#20026;&#20027;&#35201;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#25552;&#20986;&#30340;PI-GNN&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#20214;&#65292;&#33258;&#36866;&#24212;&#20272;&#35745;PI&#26631;&#31614;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;PI&#20272;&#35745;&#27169;&#22411;&#21644;&#19968;&#31181;&#35299;&#32806;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25945;&#25480;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20934;&#30830;&#20998;&#31867;&#20005;&#37325;&#22122;&#22768;&#26631;&#31614;&#30340;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#25506;&#32034;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22122;&#22768;&#22270;&#30340;&#25104;&#23545;&#26694;&#26550;&#65292;&#20854;&#20381;&#36182;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#25104;&#23545;&#20132;&#20114;&#65288;PI&#65289;&#20316;&#20026;&#20027;&#35201;&#30340;&#23398;&#20064;&#20195;&#29702;&#65292;&#38500;&#20102;&#20174;&#24102;&#22122;&#22768;&#33410;&#28857;&#31867;&#26631;&#31614;&#36827;&#34892;&#30340;&#36880;&#28857;&#23398;&#20064;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;PI-GNN&#26694;&#26550;&#36129;&#29486;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#20214;&#65306;&#65288;1&#65289;&#33258;&#36866;&#24212;&#20272;&#35745;PI&#26631;&#31614;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;PI&#20272;&#35745;&#27169;&#22411;&#65292;&#20854;&#34987;&#23450;&#20041;&#20026;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#20849;&#20139;&#30456;&#21516;&#30340;&#33410;&#28857;&#26631;&#31614;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#35299;&#32806;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching Graph Neural Networks (GNNs) to accurately classify nodes under severely noisy labels is an important problem in real-world graph learning applications, but is currently underexplored. Although pairwise training methods have demonstrated promise in supervised metric learning and unsupervised contrastive learning, they remain less studied on noisy graphs, where the structural pairwise interactions (PI) between nodes are abundant and thus might benefit label noise learning rather than the pointwise methods. This paper bridges the gap by proposing a pairwise framework for noisy node classification on graphs, which relies on the PI as a primary learning proxy in addition to the pointwise learning from the noisy node class labels. Our proposed framework PI-GNN contributes two novel components: (1) a confidence-aware PI estimation model that adaptively estimates the PI labels, which are defined as whether the two nodes share the same node labels, and (2) a decoupled training approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20132;&#20114;&#24335;&#31070;&#32463;&#36807;&#31243;(INP)&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#20197;&#21152;&#36895;&#38543;&#26426;&#27169;&#25311;&#36807;&#31243;&#65292;&#20854;&#20013;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#26102;&#38388;&#31070;&#32463;&#36807;&#31243;(STNP)&#23454;&#29616;&#27169;&#25311;&#22120;&#21160;&#24577;&#30340;&#27169;&#25311;&#65292;&#20197;&#21450;&#21033;&#29992;&#28508;&#22312;&#20449;&#24687;&#22686;&#30410;(LIG)&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#24335;&#26469;&#20943;&#23569;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2106.02770</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#21152;&#36895;&#38543;&#26426;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Bayesian Active Learning for Accelerating Stochastic Simulation. (arXiv:2106.02770v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20132;&#20114;&#24335;&#31070;&#32463;&#36807;&#31243;(INP)&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#20197;&#21152;&#36895;&#38543;&#26426;&#27169;&#25311;&#36807;&#31243;&#65292;&#20854;&#20013;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#26102;&#38388;&#31070;&#32463;&#36807;&#31243;(STNP)&#23454;&#29616;&#27169;&#25311;&#22120;&#21160;&#24577;&#30340;&#27169;&#25311;&#65292;&#20197;&#21450;&#21033;&#29992;&#28508;&#22312;&#20449;&#24687;&#22686;&#30410;(LIG)&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#24335;&#26469;&#20943;&#23569;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23454;&#29616;&#31934;&#32454;&#31890;&#24230;&#19979;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;&#26102;&#38388;&#24180;&#40836;&#32467;&#26500;&#27969;&#34892;&#30149;&#27169;&#22411;&#31561;&#38543;&#26426;&#27169;&#25311;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#20195;&#20215;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20132;&#20114;&#24335;&#31070;&#32463;&#36807;&#31243;(INP)&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#20197;&#21152;&#36895;&#38543;&#26426;&#27169;&#25311;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#24314;&#31435;&#22312;&#31070;&#32463;&#36807;&#31243;(NP)&#23478;&#26063;&#22522;&#30784;&#20043;&#19978;&#30340;&#31354;&#38388;&#26102;&#38388;&#20195;&#29702;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25910;&#36141;&#20989;&#25968;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;&#26102;&#38388;&#31070;&#32463;&#36807;&#31243;(STNP)&#26469;&#27169;&#25311;&#27169;&#25311;&#22120;&#21160;&#24577;&#65292;&#21516;&#26102;&#22312;NP&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25910;&#33719;&#20989;&#25968;&#8212;&#8212;&#28508;&#22312;&#20449;&#24687;&#22686;&#30410;(LIG)&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#36341;&#35777;&#26126;&#65292;LIG&#30456;&#23545;&#20110;&#39640;&#32500;&#38543;&#26426;&#25277;&#26679;&#21487;&#20197;&#38477;&#20302;&#27169;&#25311;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;NN&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#39537;&#21160;&#20934;&#21017;&#26469;&#35843;&#20248;&#26368;&#36817;&#37051;&#25968;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#20572;&#27490;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#35745;&#31639;&#21644;&#25913;&#21892;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#30740;&#31350;&#35777;&#26126;&#65292;&#24403;&#23376;&#26679;&#26412;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#35777;&#24212;&#29992;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2105.09788</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65306;&#31639;&#27861;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.09788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;NN&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#39537;&#21160;&#20934;&#21017;&#26469;&#35843;&#20248;&#26368;&#36817;&#37051;&#25968;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#20572;&#27490;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#35745;&#31639;&#21644;&#25913;&#21892;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#30740;&#31350;&#35777;&#26126;&#65292;&#24403;&#23376;&#26679;&#26412;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#35777;&#24212;&#29992;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#35268;&#27169;&#24322;&#24120;&#24222;&#22823;&#25110;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#19978;&#26102;&#65292;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#20998;&#31867;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;NN&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26368;&#36817;&#37051;&#25968;&#26159;&#30001;&#25968;&#25454;&#39537;&#21160;&#20934;&#21017;&#38543;&#26426;&#36873;&#25321;&#30340;&#35843;&#20248;&#21442;&#25968;&#12290;&#22312;&#23547;&#25214;&#26368;&#20248;&#35843;&#20248;&#21442;&#25968;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;&#20572;&#27490;&#35268;&#21017;&#65292;&#36825;&#19981;&#20165;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#23376;&#26679;&#26412;&#22823;&#23567;&#32452;&#21512;&#19979;&#65292;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;NN&#20998;&#31867;&#22120;&#30340;&#36229;&#39069;&#39118;&#38505;&#25910;&#25947;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#23376;&#26679;&#26412;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;&#25152;&#25552;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When data is of an extraordinarily large size or physically stored in different locations, the distributed nearest neighbor (NN) classifier is an attractive tool for classification. We propose a novel distributed adaptive NN classifier for which the number of nearest neighbors is a tuning parameter stochastically chosen by a data-driven criterion. An early stopping rule is proposed when searching for the optimal tuning parameter, which not only speeds up the computation but also improves the finite sample performance of the proposed Algorithm. Convergence rate of excess risk of the distributed adaptive NN classifier is investigated under various sub-sample size compositions. In particular, we show that when the sub-sample sizes are sufficiently large, the proposed classifier achieves the nearly optimal convergence rate. Effectiveness of the proposed approach is demonstrated through simulation studies as well as an empirical application to a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2006.10916</link><description>&lt;p&gt;
&#27010;&#29575;&#20844;&#24179;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Fair Clustering. (arXiv:2006.10916v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20013;&#22830;&#20915;&#31574;&#32773;&#34987;&#36171;&#20104;&#20102;&#19968;&#20010;&#39030;&#28857;&#30340;&#23436;&#25972;&#24230;&#37327;&#22270;&#65292;&#24182;&#19988;&#24517;&#39035;&#25552;&#20379;&#39030;&#28857;&#30340;&#32858;&#31867;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#23458;&#35266;&#20989;&#25968;&#12290;&#22312;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#65292;&#39030;&#28857;&#34987;&#36171;&#20104;&#20102;&#39068;&#33394;&#65288;&#20363;&#22914;&#65292;&#23646;&#20110;&#19968;&#20010;&#32452;&#30340;&#25104;&#21592;&#36164;&#26684;&#65289;&#65292;&#26377;&#25928;&#32858;&#31867;&#30340;&#29305;&#24449;&#20063;&#21487;&#33021;&#21253;&#25324;&#39068;&#33394;&#22312;&#35813;&#32858;&#31867;&#20013;&#30340;&#34920;&#31034;&#12290;&#20043;&#21069;&#30340;&#20844;&#24179;&#32858;&#31867;&#24037;&#20316;&#20551;&#35774;&#23436;&#20840;&#30693;&#36947;&#32452;&#25104;&#21592;&#36523;&#20221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20551;&#35774;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#26469;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#65292;&#23545;&#20197;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#25285;&#20445;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#8220;&#24230;&#37327;&#25104;&#21592;&#36523;&#20221;&#8221;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#32452;&#20855;&#26377;&#39034;&#24207;&#21644;&#36317;&#31163;&#30340;&#27010;&#24565;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20197;&#21450;&#22522;&#32447;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#22320;&#30693;&#36947;&#32452;&#25104;&#21592;&#36523;&#20221;&#26102;&#25581;&#31034;&#24494;&#22937;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clustering problems, a central decision-maker is given a complete metric graph over vertices and must provide a clustering of vertices that minimizes some objective function. In fair clustering problems, vertices are endowed with a color (e.g., membership in a group), and the features of a valid clustering might also include the representation of colors in that clustering. Prior work in fair clustering assumes complete knowledge of group membership. In this paper, we generalize prior work by assuming imperfect knowledge of group membership through probabilistic assignments. We present clustering algorithms in this more general setting with approximation ratio guarantees. We also address the problem of "metric membership", where different groups have a notion of order and distance. Experiments are conducted using our proposed algorithms as well as baselines to validate our approach and also surface nuanced concerns when group membership is not known deterministically.
&lt;/p&gt;</description></item><item><title>PReNet&#26159;&#19968;&#31181;&#28145;&#24230;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26082;&#26377;&#24050;&#30693;&#21448;&#26377;&#26410;&#30693;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#36890;&#36807;&#23398;&#20064;&#25104;&#23545;&#30340;&#20851;&#31995;&#29305;&#24449;&#21644;&#24322;&#24120;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;-&#24322;&#24120;&#12289;&#24322;&#24120;-&#27491;&#24120;&#21644;&#27491;&#24120;-&#27491;&#24120;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/1910.13601</link><description>&lt;p&gt;
&#28145;&#24230;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Weakly-supervised Anomaly Detection. (arXiv:1910.13601v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.13601
&lt;/p&gt;
&lt;p&gt;
PReNet&#26159;&#19968;&#31181;&#28145;&#24230;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26082;&#26377;&#24050;&#30693;&#21448;&#26377;&#26410;&#30693;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#36890;&#36807;&#23398;&#20064;&#25104;&#23545;&#30340;&#20851;&#31995;&#29305;&#24449;&#21644;&#24322;&#24120;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;-&#24322;&#24120;&#12289;&#24322;&#24120;-&#27491;&#24120;&#21644;&#27491;&#24120;-&#27491;&#24120;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;&#24322;&#24120;&#26679;&#26412;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65288;&#22823;&#22810;&#25968;&#20026;&#27491;&#24120;&#25968;&#25454;&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#26174;&#31034;&#22312;&#19982;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#20250;&#20851;&#27880;&#19982;&#32473;&#23450;&#24322;&#24120;&#26679;&#26412;&#23545;&#24212;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#27867;&#21270;&#21040;&#19981;&#23646;&#20110;&#27492;&#31867;&#24773;&#20917;&#30340;&#26032;&#31867;&#22411;/&#31867;&#21035;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#20026;&#20102;&#26816;&#27979;&#26082;&#26377;&#24050;&#30693;&#21448;&#26377;&#26410;&#30693;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#31216;&#20026;Pairwise Relation Prediction Network (PReNet)&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#25277;&#21462;&#30340;&#35757;&#32451;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25104;&#23545;&#30340;&#20851;&#31995;&#29305;&#24449;&#21644;&#24322;&#24120;&#20998;&#25968;&#65292;&#20854;&#20013;&#25104;&#23545;&#30340;&#20851;&#31995;&#21487;&#20197;&#26159;&#24322;&#24120;-&#24322;&#24120;&#65292;&#24322;&#24120;-&#26410;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;-&#26410;&#26631;&#35760;&#12290;&#30001;&#20110;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#22823;&#22810;&#26159;&#27491;&#24120;&#30340;&#65292;&#36825;&#31181;&#20851;&#31995;&#39044;&#27979;&#24378;&#21046;&#36827;&#34892;&#24322;&#24120;-&#24322;&#24120;&#12289;&#24322;&#24120;-&#27491;&#24120;&#21644;&#27491;&#24120;-&#27491;&#24120;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent semi-supervised anomaly detection methods that are trained using small labeled anomaly examples and large unlabeled data (mostly normal data) have shown largely improved performance over unsupervised methods. However, these methods often focus on fitting abnormalities illustrated by the given anomaly examples only (i.e.,, seen anomalies), and consequently they fail to generalize to those that are not, i.e., new types/classes of anomaly unseen during training. To detect both seen and unseen anomalies, we introduce a novel deep weakly-supervised approach, namely Pairwise Relation prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two randomly sampled training instances, in which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and normal-normal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;K&#22343;&#20540;&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31561;&#26041;&#27861;&#65292;&#39044;&#27979;&#32593;&#32422;&#36710;&#21483;&#36710;&#31995;&#32479;&#20013;&#20056;&#23458;&#34892;&#31243;&#30340;&#36215;&#28857;&#21644;&#32456;&#28857;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/1910.08145</link><description>&lt;p&gt;
&#22312;&#32593;&#32422;&#36710;&#21483;&#36710;&#31995;&#32479;&#20013;&#39044;&#27979;&#20056;&#23458;&#36215;&#32456;&#28857;&#30340;&#27169;&#22411;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Proposing a Model for Predicting Passenger Origin-Destination in Online Taxi-Hailing Systems. (arXiv:1910.08145v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.08145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;K&#22343;&#20540;&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31561;&#26041;&#27861;&#65292;&#39044;&#27979;&#32593;&#32422;&#36710;&#21483;&#36710;&#31995;&#32479;&#20013;&#20056;&#23458;&#34892;&#31243;&#30340;&#36215;&#28857;&#21644;&#32456;&#28857;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20056;&#23458;&#30340;&#36215;&#32456;&#28857;&#23545;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#20013;&#30340;&#20132;&#36890;&#35268;&#21010;&#12289;&#20132;&#36890;&#31649;&#29702;&#21644;&#35843;&#24230;&#20248;&#21270;&#26377;&#30528;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#39044;&#27979;&#34892;&#31243;&#30340;&#36215;&#28857;&#21644;&#32456;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;K&#22343;&#20540;&#32858;&#31867;&#22312;&#22235;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#35774;&#32622;&#20102;&#36215;&#28857;&#21644;&#32456;&#28857;&#21306;&#22495;&#30340;&#26368;&#22823;&#32858;&#31867;&#22823;&#23567;&#32422;&#26463;&#26469;&#30830;&#23450;&#26377;&#25928;&#30340;&#20986;&#34892;&#27969;&#12290;&#30001;&#20110;&#38598;&#32676;&#25968;&#37327;&#24222;&#22823;&#65292;&#25105;&#20204;&#37319;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38477;&#20302;&#20986;&#34892;&#38598;&#32676;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#19968;&#31181;&#22534;&#21472;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#20010;&#38598;&#32676;&#20013;&#30340;&#20986;&#34892;&#27425;&#25968;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32467;&#26524;&#27604;&#36739;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;1&#23567;&#26102;&#21644;30&#20998;&#38047;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#23454;&#29616;&#20102;5-7\%&#21644;14\%&#30340;&#36739;&#20302;&#22343;&#26041;&#32477;&#23545;&#35823;&#24046;&#65288;MAPE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the significance of transportation planning, traffic management, and dispatch optimization, predicting passenger origin-destination has emerged as a crucial requirement for intelligent transportation systems management. In this study, we present a model designed to forecast the origin and destination of travels within a specified time window. To derive meaningful travel flows, we employ K-means clustering in a four-dimensional space with a maximum cluster size constraint for origin and destination zones. Given the large number of clusters, we utilize non-negative matrix factorization to reduce the number of travel clusters. Furthermore, we implement a stacked recurrent neural network model to predict the travel count in each cluster. A comparison of our results with existing models reveals that our proposed model achieves a 5-7\% lower mean absolute percentage error (MAPE) for 1-hour time windows and a 14\% lower MAPE for 30-minute time windows.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#21152;&#26435;&#31232;&#30095;&#37319;&#26679;&#65288;POWSS&#65289;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#31354;&#38388;&#30340;POMDPs&#20013;&#20934;&#30830;&#20272;&#35745;Q&#20540;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/1910.04332</link><description>&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#31354;&#38388;&#30340;POMDPs&#20013;&#30340;&#31232;&#30095;&#26641;&#25628;&#32034;&#26368;&#20248;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sparse tree search optimality guarantees in POMDPs with continuous observation spaces. (arXiv:1910.04332v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.04332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#21152;&#26435;&#31232;&#30095;&#37319;&#26679;&#65288;POWSS&#65289;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#31354;&#38388;&#30340;POMDPs&#20013;&#20934;&#30830;&#20272;&#35745;Q&#20540;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20855;&#26377;&#34920;&#31034;&#23454;&#38469;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#65292;&#20294;&#35299;&#20915;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#35266;&#27979;&#26435;&#37325;&#30340;&#22312;&#32447;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#31354;&#38388;&#30340;&#39046;&#22495;&#20013;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#26377;&#27491;&#24335;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#19968;&#31181;&#31616;&#21270;&#26041;&#27861;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#21152;&#26435;&#31232;&#30095;&#37319;&#26679;&#65288;POWSS&#65289;&#65292;&#23558;&#27491;&#30830;&#22320;&#20272;&#35745;Q&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#33021;&#21147;&#26469;&#20351;&#20854;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) with continuous state and observation spaces have powerful flexibility for representing real-world decision and control problems but are notoriously difficult to solve. Recent online sampling-based algorithms that use observation likelihood weighting have shown unprecedented effectiveness in domains with continuous observation spaces. However there has been no formal theoretical justification for this technique. This work offers such a justification, proving that a simplified algorithm, partially observable weighted sparse sampling (POWSS), will estimate Q-values accurately with high probability and can be made to perform arbitrarily near the optimal solution by increasing computational power.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26465;&#20214;&#38543;&#26426;&#22330;&#22312;AST&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20195;&#30721;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;Java&#31243;&#24207;&#30340;&#20462;&#22797;&#36716;&#25442;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#20363;&#21270;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#26032;&#30340;&#21644;&#26377;&#29992;&#30340;&#20462;&#22797;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/1907.09282</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#21270;&#39044;&#27979;&#31639;&#27861;&#23398;&#20064;&#20195;&#30721;&#29305;&#24449;&#19982;&#20195;&#30721;&#36716;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning the Relation between Code Features and Code Transforms with Structured Prediction. (arXiv:1907.09282v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.09282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26465;&#20214;&#38543;&#26426;&#22330;&#22312;AST&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20195;&#30721;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;Java&#31243;&#24207;&#30340;&#20462;&#22797;&#36716;&#25442;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#20363;&#21270;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#26032;&#30340;&#21644;&#26377;&#29992;&#30340;&#20462;&#22797;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#22312;AST&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20195;&#30721;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25351;&#23548;&#33258;&#21160;&#20195;&#30721;&#28436;&#21464;&#25216;&#26415;&#20013;&#20195;&#30721;&#36716;&#25442;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#31163;&#32447;&#23398;&#20064;&#27010;&#29575;&#27169;&#22411;&#65292;&#25429;&#25417;&#29305;&#23450;&#20195;&#30721;&#36716;&#25442;&#24212;&#29992;&#20110;&#29305;&#23450;AST&#33410;&#28857;&#30340;&#26041;&#24335;&#65292;&#28982;&#21518;&#21033;&#29992;&#23398;&#20064;&#30340;&#27169;&#22411;&#20026;&#20219;&#24847;&#26032;&#30340;&#12289;&#30475;&#19981;&#35265;&#30340;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;Java&#31243;&#24207;&#30340;&#20462;&#22797;&#36716;&#25442;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#20363;&#21270;&#20102;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#32452;&#31934;&#24515;&#35774;&#35745;&#30340;&#20195;&#30721;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#28085;&#30422;&#19981;&#21516;&#35821;&#27861;&#34920;&#31034;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#20197;&#20415;&#36328;&#20195;&#30721;&#24212;&#29992;&#20462;&#22797;&#36716;&#25442;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#21551;&#21457;&#24335;&#35268;&#21017;&#25110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20197;&#24448;&#24037;&#20316;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#12289;&#39640;&#25928;&#22320;&#21457;&#29616;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#26032;&#30340;&#21644;&#26377;&#29992;&#30340;&#20462;&#22797;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this paper the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. {Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction}. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises tran
&lt;/p&gt;</description></item></channel></rss>