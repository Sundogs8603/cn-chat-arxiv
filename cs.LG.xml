<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01344</link><description>&lt;p&gt;
&#21333;&#35843;&#12289;Bi-Lipschitz&#21644;Polyak-\L{}ojasiewicz&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01344
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BiLipNet&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#36870;&#30340;\emph{Bi-Lipschitz}&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#25511;&#21046;&#20854;\emph{Lipschitzness}&#65288;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#36755;&#20986;&#25935;&#24863;&#24615;&#65289;&#21644;\emph{inverse Lipschitzness}&#65288;&#19981;&#21516;&#36755;&#20986;&#30340;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#65289;&#30340;&#33021;&#21147;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#20855;&#26377;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#20197;&#26500;&#24314;Bi-Lipschitz&#32593;&#32476;&#12290;&#35748;&#35777;&#26159;&#22522;&#20110;&#22686;&#37327;&#20108;&#27425;&#32422;&#26463;&#30340;&#65292;&#19982;&#35889;&#24402;&#19968;&#21270;&#30456;&#27604;&#65292;&#23427;&#33021;&#23454;&#29616;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21453;&#21521;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#19977;&#31639;&#23376;&#20998;&#35010;&#38382;&#39064;&#65292;&#24050;&#30693;&#23384;&#22312;&#24555;&#36895;&#31639;&#27861;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;Bi-Lipschitz&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#37327;&#36755;&#20986;&#32593;&#32476;&#65292;&#21363;PLNet&#65292;&#23427;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#30340;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#65292;&#20363;&#22914;&#29420;&#29305;&#24615;&#21644;&#39640;&#25928;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new \emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \emph{Lipschitzness} (output sensitivity to input perturbations) and \emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2311.04843</link><description>&lt;p&gt;
&#36328;&#36234;&#32500;&#24230;&#65306;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#21487;&#20449;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Dimensions: Confident Reachability for High-Dimensional Controllers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#29616;&#12290;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#36890;&#36807;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#24863;&#30693;&#27169;&#24335;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#25191;&#34892;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#36825;&#31181;&#25511;&#21046;&#22120;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#22312;&#22788;&#29702;&#20855;&#26377;&#25968;&#21315;&#20010;&#32500;&#24230;&#30340;&#36755;&#20837;&#26102;&#26080;&#27861;&#25193;&#23637;&#65292;&#29305;&#21035;&#26159;&#24403;&#21508;&#20010;&#36755;&#20837;&#65288;&#22914;&#20687;&#32032;&#65289;&#32570;&#20047;&#26126;&#30830;&#30340;&#29289;&#29702;&#24847;&#20041;&#26102;&#12290;&#26412;&#25991;&#22312;&#36830;&#25509;&#35814;&#23613;&#30340;&#38381;&#29615;&#39564;&#35777;&#19982;&#39640;&#32500;&#25511;&#21046;&#22120;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20960;&#20010;&#20302;&#32500;&#25511;&#21046;&#22120;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#24179;&#34913;&#20302;&#32500;&#25511;&#21046;&#22120;&#30340;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#39564;&#35777;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#21518;&#65292;&#22914;&#26524;&#20302;&#32500;&#21487;&#36798;&#24615;&#32467;&#26524;&#24050;&#32463;&#24471;&#21040;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions -- especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are infl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15989</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided Machine Learning: Current Trends and Future Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15989
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#23398;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#24314;&#27169;&#20013;&#30340;&#20114;&#34917;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26032;&#20852;&#39046;&#22495;&#31185;&#23398;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26088;&#22312;&#21033;&#29992;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#31867;&#22411;&#12289;&#25506;&#35752;&#30340;&#30693;&#35782;-ML&#38598;&#25104;&#24418;&#24335;&#20197;&#21450;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#30340;&#26041;&#27861;&#31561;&#26041;&#38754;&#35752;&#35770;&#20102;KGML&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#29615;&#22659;&#31185;&#23398;&#20013;&#21457;&#23637;&#30340;KGML&#26041;&#27861;&#30340;&#19968;&#20123;&#24120;&#35265;&#29992;&#20363;&#31867;&#21035;&#65292;&#20197;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20363;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.15527</link><description>&lt;p&gt;
&#20381;&#20174;&#22312;&#32447;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Conformal online model aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#20174;&#39044;&#27979;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20570;&#20986;&#24378;&#28872;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23558;&#28857;&#39044;&#27979;&#36716;&#25442;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#30340;&#38598;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20381;&#20174;&#39044;&#27979;&#21482;&#22312;&#20107;&#20808;&#30830;&#23450;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36215;&#20316;&#29992;&#12290;&#20381;&#20174;&#39044;&#27979;&#20013;&#30456;&#23545;&#36739;&#23569;&#28041;&#21450;&#30340;&#38382;&#39064;&#26159;&#27169;&#22411;&#36873;&#25321;&#21644;/&#25110;&#32858;&#21512;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#24212;&#35813;&#22914;&#20309;&#20381;&#20174;&#21270;&#20247;&#22810;&#39044;&#27979;&#26041;&#27861;&#65288;&#38543;&#26426;&#26862;&#26519;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#27491;&#21017;&#21270;&#32447;&#24615;&#27169;&#22411;&#31561;&#65289;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#26469;&#33258;&#22810;&#20010;&#31639;&#27861;&#30340;&#39044;&#27979;&#38598;&#36827;&#34892;&#25237;&#31080;&#65292;&#20854;&#20013;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#19978;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;</title><link>https://arxiv.org/abs/2403.12237</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#30340;&#39640;&#25928;&#22522;&#20110;Transformer&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#36807;&#31243;&#23545;&#20110;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;HPO&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#20197;&#20854;&#21487;&#35266;&#30340;&#35745;&#31639;&#21344;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#32780;&#38395;&#21517;&#65307;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;TRL-HPO&#37197;&#22791;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#21270;&#21644;&#28176;&#36827;&#29983;&#25104;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;TRL-HPO&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;CNN&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30456;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;TRL-HPO&#30340;&#20998;&#31867;&#32467;&#26524;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;6.8%&#65292;&#35777;&#26126;&#20102;TRL-HPO&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.</title><link>https://arxiv.org/abs/2403.04764</link><description>&lt;p&gt;
&#23558;Thompson&#25277;&#26679;&#36951;&#25022;&#19982;Sigma&#27604;&#29575;&#65288;TS-RSR&#65289;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32463;&#36807;&#35777;&#26126;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#20854;&#20013;&#25277;&#26679;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#26041;&#27861;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33021;&#22815;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#20013;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#20197;&#26368;&#23567;&#21270;&#28857;&#20043;&#38388;&#30340;&#20887;&#20313;&#65292;&#21516;&#26102;&#20851;&#27880;&#20855;&#26377;&#39640;&#39044;&#27979;&#22343;&#20540;&#25110;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#36951;&#25022;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#20174;&#25968;&#23383;&#19978;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24179;&#22343;&#20540;&#19978;&#27604;&#20960;&#20010;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#25209;&#37327;BO&#31639;&#27861;&#34920;&#29616;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#25191;&#34892;&#35302;&#21457;&#22120;&#21487;&#20197;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#34920;&#29616;&#20986;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#30340;&#22266;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03792</link><description>&lt;p&gt;
&#31070;&#32463;&#25191;&#34892;&#65306;&#23398;&#20064;&#25191;&#34892;&#35302;&#21457;&#22120;&#29992;&#20110;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03792
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#25191;&#34892;&#35302;&#21457;&#22120;&#21487;&#20197;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#34920;&#29616;&#20986;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#30340;&#22266;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#25191;&#34892;&#65288;Neural Exec&#65289;&#30340;&#26032;&#22411;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#23383;&#31526;&#20018;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#20808;&#21069;&#30340;&#25351;&#20196;&#24182;...&#8221;&#65289;&#30340;&#24050;&#30693;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#21019;&#24314;&#25191;&#34892;&#35302;&#21457;&#22120;&#27010;&#24565;&#21270;&#20026;&#21487;&#24494;&#20998;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#25191;&#34892;&#35302;&#21457;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31215;&#26497;&#36827;&#21462;&#30340;&#23545;&#25163;&#21487;&#20197;&#20266;&#36896;&#20986;&#19981;&#20165;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#34920;&#29616;&#20986;&#22266;&#26377;&#28789;&#27963;&#24615;&#30340;&#35302;&#21457;&#22120;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#35774;&#35745;&#21644;&#29983;&#25104;&#33021;&#22815;&#22312;&#32463;&#21382;&#22810;&#38454;&#27573;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#30340;&#24773;&#20917;&#19979;&#25345;&#20037;&#23384;&#22312;&#30340;&#31070;&#32463;&#25191;&#34892;&#65288;Neural Exec&#65289;&#65292;&#20363;&#22914;&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#24212;&#29992;&#20013;&#12290;&#26356;&#20026;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#29983;&#25104;&#19982;&#20219;&#20309;&#24050;&#30693;&#25915;&#20987;&#26126;&#26174;&#19981;&#21516;&#30340;&#24418;&#24335;&#21644;&#24418;&#29366;&#30340;&#35302;&#21457;&#22120;&#65292;&#32469;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03792v1 Announce Type: cross  Abstract: We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous instructions and..."), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestep
&lt;/p&gt;</description></item><item><title>Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00877</link><description>&lt;p&gt;
Disaggregated Multi-Tower: &#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#25512;&#33616;&#24314;&#27169;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00877
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#25153;&#24179;&#26550;&#26500;&#12289;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#24335;&#21644;&#20998;&#23618;&#25968;&#25454;&#20013;&#24515;&#25299;&#25169;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#30456;&#20851;&#30340;&#20302;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disaggregated Multi-Tower&#65288;DMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24314;&#27169;&#25216;&#26415;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35821;&#20041;&#20445;&#30041;&#30340;Tower Transform&#65288;SPTT&#65289;&#65292;&#19968;&#20010;&#23558;&#21333;&#29255;&#20840;&#23616;&#23884;&#20837;&#26597;&#25214;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#22612;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#20301;&#32622;&#20851;&#31995;&#30340;&#26032;&#22411;&#35757;&#32451;&#27169;&#24335;&#65307;&#65288;2&#65289;Tower Module&#65288;TM&#65289;&#65292;&#19968;&#20010;&#38468;&#21152;&#21040;&#27599;&#20010;&#22612;&#30340;&#21327;&#21516;&#31264;&#23494;&#32452;&#20214;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#20132;&#20114;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36890;&#20449;&#37327;&#65307;&#21644;&#65288;3&#65289;Tower Partitioner&#65288;TP&#65289;&#65292;&#19968;&#20010;&#29305;&#24449;&#20998;&#21306;&#22120;&#65292;&#31995;&#32479;&#22320;&#21019;&#24314;&#20855;&#26377;&#26377;&#24847;&#20041;&#29305;&#24449;&#20132;&#20114;&#21644;&#36127;&#36733;&#24179;&#34913;&#20998;&#37197;&#30340;&#22612;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#26469;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DMT&#30456;&#27604;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;1.9&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.13505</link><description>&lt;p&gt;
SimPro&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#26694;&#26550;&#23454;&#29616;&#36924;&#30495;&#30340;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13505
&lt;/p&gt;
&lt;p&gt;
SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#35299;&#20915;&#19968;&#20010;&#26356;&#20026;&#36924;&#30495;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21516;&#26102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31867;&#21035;&#20998;&#24067;&#26082;&#26410;&#30693;&#21448;&#21487;&#33021;&#19981;&#21305;&#37197;&#12290;&#24403;&#21069;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#24448;&#24448;&#39044;&#35774;&#20102;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#31867;&#21035;&#20998;&#24067;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#20165;&#36866;&#24212;&#20110;&#26576;&#20123;&#20998;&#24067;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#24230;&#36866;&#24212;&#24615;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;SimPro&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#21019;&#26032;&#22320;&#25913;&#36827;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#36825;&#31181;&#20998;&#31163;&#20419;&#36827;&#20102;&#22312;&#26368;&#22823;&#21270;&#36807;&#31243;&#20013;&#23545;&#31867;&#21035;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12812</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#20010;&#24615;&#21270;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Decentralized Algorithms for Online Personalized Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#24403;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#20250;&#24341;&#20837;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#65292;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#20195;&#29702;&#38543;&#26102;&#38388;&#20174;&#23454;&#20540;&#20998;&#24067;&#20013;&#25910;&#38598;&#26679;&#26412;&#26469;&#20272;&#35745;&#20854;&#22343;&#20540;&#12290;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#20195;&#29702;&#25968;&#37327;A&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20195;&#29702;&#33258;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#20351;&#24471;&#27599;&#20010;&#20195;&#29702;&#21482;&#33021;&#19982;&#36873;&#23450;&#25968;&#37327;&#30340;&#23545;&#31561;&#20307;r&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#21327;&#20316;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#20449;&#24565;&#20256;&#25773;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.07031</link><description>&lt;p&gt;
&#23454;&#20363;&#32423;&#21035;&#30340;&#23433;&#20840;&#24863;&#30693;&#19982;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#21450;&#20854;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#26657;&#20934;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#22609;&#36896;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#21462;&#20195;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#20851;&#27880;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#36229;&#36234;&#32431;&#35270;&#35273;&#36755;&#20837;&#29305;&#24449;&#30340;&#22235;&#31181;&#23454;&#20363;&#32423;&#21035;&#36136;&#37327;&#65292;&#26088;&#22312;&#20351;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38382;&#39064;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20943;&#23569;&#30001;&#22522;&#20110;DNN&#30340;&#32452;&#20214;&#35782;&#21035;&#20986;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35843;&#20248;&#21487;&#20197;&#22686;&#24378;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20013;&#23433;&#20840;&#20851;&#38190;&#38169;&#35823;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#22122;&#22768;&#27169;&#22411;&#21644;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#26469;&#32771;&#34385;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#26799;&#24230;&#27969;&#21160;&#21644;&#37325;&#26500;&#39057;&#35889;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.04930</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#34013;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Blue noise for diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#22122;&#22768;&#27169;&#22411;&#21644;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#26469;&#32771;&#34385;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#26799;&#24230;&#27969;&#21160;&#21644;&#37325;&#26500;&#39057;&#35889;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#65292;&#20294;&#36825;&#21487;&#33021;&#26080;&#27861;&#26368;&#20248;&#22320;&#32771;&#34385;&#21435;&#22122;&#32593;&#32476;&#37325;&#26500;&#30340;&#39057;&#35889;&#20869;&#23481;&#12290;&#23613;&#31649;&#30456;&#20851;&#22122;&#22768;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#20854;&#22312;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#31867;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#21464;&#22122;&#22768;&#27169;&#22411;&#26469;&#23558;&#30456;&#20851;&#22122;&#22768;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#25104;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#30830;&#23450;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#30456;&#27604;&#20165;&#20351;&#29992;&#39640;&#26031;&#30333;&#22122;&#22768;&#65288;&#38543;&#26426;&#22122;&#22768;&#65289;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20801;&#35768;&#22312;&#21333;&#20010;&#23567;&#25209;&#37327;&#20013;&#24341;&#20837;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#26799;&#24230;&#27969;&#21160;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and qua
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>RetDream&#26159;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02972</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Score Distillation for Text-to-3D Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02972
&lt;/p&gt;
&lt;p&gt;
RetDream&#26159;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;3D&#29983;&#25104;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19981;&#36275;&#30340;3D&#20808;&#39564;&#30693;&#35782;&#20063;&#23548;&#33268;&#20102;3D&#20960;&#20309;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#35299;&#20915;3D&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;2D&#25968;&#25454;&#30456;&#27604;&#65292;3D&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26377;&#38480;&#65292;&#36825;&#23548;&#33268;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22238;&#36991;&#36825;&#20123;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#38024;&#23545;&#24471;&#20998;&#33976;&#39311;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;RetDream&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#36136;&#37327;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#36164;&#28304;&#26469;&#34701;&#20837;&#20854;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.15502</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bayesian Tests. (arXiv:2401.15502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#26426;&#23494;&#25968;&#25454;&#36827;&#34892;&#31185;&#23398;&#20551;&#35774;&#26816;&#39564;&#30340;&#39046;&#22495;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#22522;&#30707;&#12290;&#22312;&#25253;&#21578;&#31185;&#23398;&#21457;&#29616;&#26102;&#65292;&#24191;&#27867;&#37319;&#29992;&#36125;&#21494;&#26031;&#26816;&#39564;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;P&#20540;&#30340;&#20027;&#35201;&#25209;&#35780;&#65292;&#21363;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#26080;&#27861;&#37327;&#21270;&#23545;&#31454;&#20105;&#20551;&#35774;&#30340;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22522;&#20110;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#22522;&#30784;&#19978;&#33258;&#28982;&#20135;&#29983;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25512;&#26029;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#22240;&#23376;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24314;&#27169;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#23454;&#36136;&#24615;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#22312;&#25152;&#25552;&#26694;&#26550;&#19979;&#30830;&#31435;&#36125;&#21494;&#26031;&#22240;&#23376;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#20215;&#20540;&#26356;&#26032;&#21644;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;$\tilde{O}(T^{-1})$&#12290;</title><link>http://arxiv.org/abs/2401.15240</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games. (arXiv:2401.15240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#20215;&#20540;&#26356;&#26032;&#21644;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;$\tilde{O}(T^{-1})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#22810;&#20154;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#20043;&#21069;&#30340;&#32467;&#26524;&#23454;&#29616;&#20102;$O(T^{-1/2})$&#25910;&#25947;&#36895;&#24230;&#21040;&#21327;&#26041;&#24046;&#22343;&#34913;&#21644;$O(T^{-3/4})$&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21040;&#36739;&#24369;&#30340;&#30095;&#26494;&#21327;&#26041;&#24046;&#22343;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#38750;&#32806;&#21512;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#20351;&#20854;&#36798;&#21040;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;$\tilde{O}(T^{-1})$&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#65288;i&#65289;&#24179;&#28369;&#30340;&#20215;&#20540;&#26356;&#26032;&#21644;&#65288;ii&#65289;&#20855;&#26377;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#30340;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#26500;&#24314;&#32780;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study policy optimization algorithms for computing correlated equilibria in multi-player general-sum Markov Games. Previous results achieve $O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated $O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated equilibrium. In this paper, we improve both results significantly by providing an uncoupled policy optimization algorithm that attains a near-optimal $\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium. Our algorithm is constructed by combining two main elements (i) smooth value updates and (ii) the optimistic-follow-the-regularized-leader algorithm with the log barrier regularizer.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;</title><link>http://arxiv.org/abs/2401.01404</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#32593;&#32476;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37325;&#24314;&#26159;&#25351;&#22312;&#21482;&#26377;&#20851;&#20110;&#26465;&#20214;&#20598;&#32852;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;N&#20010;&#33410;&#28857;&#20043;&#38388;&#26410;&#35266;&#27979;&#21040;&#30340;&#25104;&#23545;&#32806;&#21512;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20284;&#20046;&#26080;&#27861;&#36991;&#20813;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;O(N^2)&#65292;&#21363;&#35201;&#32771;&#34385;&#27599;&#31181;&#21487;&#33021;&#30340;&#25104;&#23545;&#32806;&#21512;&#33267;&#23569;&#19968;&#27425;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#32593;&#32476;&#37117;&#26159;&#31232;&#30095;&#30340;&#65292;&#38750;&#38646;&#32806;&#21512;&#30340;&#25968;&#37327;&#21482;&#26377;O(N)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#37325;&#24314;&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#22312;&#23376;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#20854;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24230;&#23485;&#26494;&#19978;&#30028;&#20026;O(N^(3/2)logN)&#65292;&#20294;&#20855;&#26377;&#26356;&#20856;&#22411;&#30340;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;O(Nlog^2 N)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#65292;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
&lt;/p&gt;</description></item><item><title>MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2312.06786</link><description>&lt;p&gt;
&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06786
&lt;/p&gt;
&lt;p&gt;
MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(LTSF)&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#36807;&#21435;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#20540;&#12290;&#24403;&#21069;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;(SOTA)&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#30001;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20855;&#26377;&#32447;&#24615;&#26144;&#23556;&#23618;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#39118;&#26684;&#30340;&#22686;&#24378;&#32447;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;(MoLE)&#12290;MoLE&#19981;&#26159;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#35757;&#32451;&#22810;&#20010;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;(&#21363;&#19987;&#23478;)&#21644;&#19968;&#20010;&#26435;&#34913;&#21644;&#28151;&#21512;&#20854;&#36755;&#20986;&#30340;&#36335;&#30001;&#27169;&#22411;&#12290;&#34429;&#28982;&#25972;&#20010;&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#20294;&#27599;&#20010;&#19987;&#23478;&#37117;&#23398;&#20250;&#19987;&#38376;&#22788;&#29702;&#29305;&#23450;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#32780;&#36335;&#30001;&#27169;&#22411;&#21017;&#23398;&#20250;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#19987;&#23478;&#20204;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MoLE&#38477;&#20302;&#20102;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;(DLinear&#65292;RLinear&#21644;RMLP)&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.04469</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Watermarks for Language Models. (arXiv:2312.04469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#27700;&#21360;&#21487;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#32479;&#35745;&#26816;&#27979;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#20013;&#12290;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#36890;&#36807;&#25913;&#21464;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#26469;&#25805;&#20316;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#23558;&#23545;&#27700;&#21360;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#27700;&#21360;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#33021;&#33258;&#28982;&#29983;&#25104;&#24102;&#27700;&#21360;&#25991;&#26412;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#20351;&#24320;&#25918;&#27169;&#22411;&#20063;&#33021;&#20174;&#27700;&#21360;&#20013;&#21463;&#30410;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#27700;&#21360;&#29992;&#20110;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20266;&#36896;&#27700;&#21360;&#24182;&#29983;&#25104;&#26377;&#23475;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26469;&#25439;&#23475;&#21463;&#23475;&#27169;&#22411;&#30340;&#22768;&#35465;&#12290;&#20026;&#20102;&#30740;&#31350;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#34892;&#20026;&#31867;&#20284;&#20110;&#20351;&#29992;&#22522;&#20110;&#35299;&#30721;&#30340;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2310.05921</link><description>&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;: &#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions. (arXiv:2310.05921v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05921
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19981;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#36825;&#31181;&#20915;&#31574;&#30340;&#20363;&#23376;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20174;&#20381;&#36182;&#20110;&#34892;&#20154;&#39044;&#27979;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#31639;&#27861;&#65292;&#21040;&#26657;&#20934;&#33258;&#21160;&#21270;&#21046;&#36896;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#38169;&#35823;&#29575;&#65292;&#20877;&#21040;&#22312;&#36816;&#34892;&#26102;&#36873;&#25321;&#20449;&#20219;&#21517;&#20041;&#31574;&#30053;&#36824;&#26159;&#20999;&#25442;&#21040;&#23433;&#20840;&#22791;&#20221;&#31574;&#30053;&#12290;&#25105;&#20204;&#31639;&#27861;&#20135;&#29983;&#30340;&#20915;&#31574;&#22312;&#32479;&#35745;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#26159;&#23433;&#20840;&#30340;&#65292;&#26080;&#38656;&#23545;&#19990;&#30028;&#27169;&#22411;&#20316;&#20986;&#20219;&#20309;&#20551;&#35774;&#65307;&#35266;&#27979;&#25968;&#25454;&#21487;&#20197;&#19981;&#28385;&#36275;&#29420;&#31435;&#21516;&#20998;&#24067;(I.I.D.)&#30340;&#26465;&#20214;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#23545;&#25239;&#24615;&#30340;&#12290;&#35813;&#29702;&#35770;&#23558;&#31526;&#21512;&#39044;&#27979;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#30452;&#25509;&#26657;&#20934;&#20915;&#31574;&#65292;&#32780;&#19981;&#38656;&#35201;&#26500;&#24314;&#39044;&#27979;&#38598;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22260;&#32469;&#20154;&#31867;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#12289;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#21644;&#26426;&#22120;&#20154;&#21046;&#36896;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturin
&lt;/p&gt;</description></item><item><title>"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"</title><link>http://arxiv.org/abs/2310.04420</link><description>&lt;p&gt;
"BrainSCUBA: &#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;"
&lt;/p&gt;
&lt;p&gt;
BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. (arXiv:2310.04420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04420
&lt;/p&gt;
&lt;p&gt;
"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#29702;&#35299;&#39640;&#32423;&#35270;&#35273;&#30382;&#23618;&#30340;&#21151;&#33021;&#32452;&#32455;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#25163;&#21160;&#36873;&#25321;&#30340;&#21050;&#28608;&#26469;&#26144;&#23556;&#31070;&#32463;&#32676;&#20307;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#36873;&#25321;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35270;&#35273;&#30382;&#23618;&#21151;&#33021;&#30340;&#39044;&#35774;&#20551;&#35774;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;- &#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23398;&#21040;&#30340;&#20016;&#23500;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#38454;&#35270;&#35273;&#21306;&#22495;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#20307;&#32032;&#32423;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#21512;&#25104;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#22270;&#20687;&#22312;&#35821;&#20041;&#19978;&#26159;&#36830;&#36143;&#30340;&#24182;&#19988;&#20855;&#26377;&#39640;&#30340;&#36136;&#37327;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#23545;FLAN-T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23545;&#20854;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.03150</link><description>&lt;p&gt;
&#22312;&#38750;&#24120;&#36793;&#32536;&#19978;&#23545;LLMs&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;&#65306;&#22909;&#12289;&#22351;&#21644;&#19985;(arXiv:2310.03150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#23545;FLAN-T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23545;&#20854;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22522;&#30784;&#27169;&#22411;&#22240;&#20026;&#25552;&#20379;&#20102;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#19982;&#25968;&#25454;&#20132;&#20114;&#21644;&#24555;&#36895;&#26816;&#32034;&#20449;&#24687;&#30340;&#26032;&#26426;&#20250;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;LLMs&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#30001;&#20110;&#27861;&#24459;&#25110;&#25216;&#26415;&#38480;&#21046;&#21487;&#33021;&#38590;&#20197;&#35775;&#38382;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#31169;&#26377;&#35745;&#31639;&#36164;&#28304;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25193;&#22823;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25968;&#25454;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#37319;&#29992;&#30828;&#20214;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;LLMs&#24341;&#20837;&#29616;&#20195;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;FLAN-T5&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#24494;&#35843;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;80M&#21040;3B&#65292;&#24182;&#24212;&#29992;&#20110;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24494;&#35266;&#27700;&#24179;&#30340;&#30828;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#27169;&#22411;&#30340;FLOP&#21033;&#29992;&#29575;&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20013;&#24515;GPU&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#32593;&#32476;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20855;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.  This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#21442;&#25968;&#65292;&#21487;&#20197;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.02246</link><description>&lt;p&gt;
&#23398;&#20064;&#25918;&#26494;&#65306;&#22312;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances. (arXiv:2310.02246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#21442;&#25968;&#65292;&#21487;&#20197;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;$Ax=b$&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#31185;&#23398;&#35745;&#31639;&#21407;&#29702;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#27714;&#35299;&#22120;&#21644;&#39044;&#22788;&#29702;&#22120;&#12290;&#23427;&#20204;&#24102;&#26377;&#21442;&#25968;&#65292;&#20854;&#26368;&#20339;&#20540;&#21462;&#20915;&#20110;&#35201;&#35299;&#20915;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#30830;&#23450;&#65307;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#27425;&#20248;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#38656;&#35201;&#35299;&#20915;&#35768;&#22810;&#30456;&#20851;&#32447;&#24615;&#31995;&#32479;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#22312;&#21333;&#20010;&#25968;&#20540;&#27169;&#25311;&#26399;&#38388;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#39034;&#24207;&#36873;&#25321;&#21442;&#25968;&#65292;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#65311;&#23545;&#20110;&#36807;&#24230;&#36731;&#26494;&#65288;SOR&#65289;&#36825;&#31181;&#26631;&#20934;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#22238;&#31572;&#32943;&#23450;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#20165;&#36845;&#20195;&#27425;&#25968;&#20316;&#20026;&#21453;&#39304;&#30340;&#36172;&#24466;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36873;&#25321;&#24207;&#21015;&#23454;&#20363;&#30340;&#21442;&#25968;&#65292;&#20351;&#24471;&#24635;&#25104;&#26412;&#25509;&#36817;&#26368;&#20339;&#22266;&#23450;&#30340;&#969;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving a linear system $Ax=b$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter $\omega$ has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm -- using only the number of iterations as feedback -- can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed $\omega$ as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.08793</link><description>&lt;p&gt;
Fin-Fact:&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08793
&lt;/p&gt;
&lt;p&gt;
Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#20107;&#23454;&#26680;&#26597;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fin-Fact&#65292;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21253;&#25324;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#25552;&#20379;&#19987;&#19994;&#30693;&#35782;&#21644;&#21487;&#20449;&#24230;&#12290;&#30001;&#20110;&#20854;&#22810;&#27169;&#24577;&#24615;&#36136;&#28085;&#30422;&#20102;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#65292;Fin-Fact&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#28304;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#37329;&#34701;&#39046;&#22495;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#22312;&#36130;&#21153;&#25253;&#21578;&#21644;&#26032;&#38395;&#20256;&#25773;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#28145;&#24230;&#30340;&#35299;&#37322;&#65292;Fin-Fact&#20351;&#29992;&#25143;&#65292;&#21253;&#25324;&#39046;&#22495;&#19987;&#23478;&#21644;&#32456;&#31471;&#29992;&#25143;&#65292;&#33021;&#22815;&#29702;&#35299;&#20107;&#23454;&#26680;&#26597;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#39564;&#35777;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#20419;&#36827;&#23545;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#20449;&#20219;&#12290;Fin-Fact&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;https://github.com/IIT-DM/Fin-Fact/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30740;&#31350;&#20102;&#22996;&#25176;&#30340;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#22865;&#32422;&#35299;&#20915;&#20102;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01837</link><description>&lt;p&gt;
&#22996;&#25176;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
Delegating Data Collection in Decentralized Machine Learning. (arXiv:2309.01837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30740;&#31350;&#20102;&#22996;&#25176;&#30340;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#22865;&#32422;&#35299;&#20915;&#20102;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20986;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#22996;&#25176;&#38382;&#39064;&#12290;&#20197;&#22865;&#32422;&#29702;&#35770;&#20026;&#20986;&#21457;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35299;&#20915;&#20004;&#20010;&#22522;&#26412;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#30340;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#22865;&#32422;&#65306;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#20219;&#20309;&#27169;&#22411;&#26368;&#20248;&#24615;&#33021;&#30340;&#32570;&#20047;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#32447;&#24615;&#22865;&#32422;&#21487;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#20351;&#22996;&#25176;&#20154;&#21482;&#26377;&#19968;&#20010;&#23567;&#30340;&#27979;&#35797;&#38598;&#65292;&#20063;&#33021;&#23454;&#29616;1-1/e&#30340;&#19968;&#31561;&#25928;&#29992;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22996;&#25176;&#20154;&#27979;&#35797;&#38598;&#22823;&#23567;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21487;&#20197;&#36798;&#21040;&#23545;&#26368;&#20248;&#25928;&#29992;&#30340;&#36924;&#36817;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#22865;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1-1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal's test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00733</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#35270;&#35273;&#29305;&#24449;&#21040;&#25991;&#26412;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;TExplain&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;LLMs&#20043;&#38388;&#24314;&#31435;&#36830;&#25509;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#30340;&#21477;&#23376;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#21477;&#23376;&#28982;&#21518;&#29992;&#20110;&#25552;&#21462;&#26368;&#39057;&#32321;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#20998;&#31867;&#22120;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#21644;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#19982;&#35270;&#35273;&#34920;&#31034;&#23545;&#24212;&#30340;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#26469;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16471</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38544;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31574;&#30053;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25509;&#35302;&#21644;&#30896;&#25758;&#65292;&#31574;&#30053;&#21442;&#25968;&#30340;&#23567;&#25913;&#21464;&#21487;&#33021;&#23548;&#33268;&#26497;&#20854;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#20363;&#22914;&#65292;&#22312;&#36275;&#29699;&#20013;&#65292;&#36890;&#36807;&#31245;&#24494;&#25913;&#21464;&#36386;&#29699;&#20301;&#32622;&#25110;&#26045;&#21152;&#29699;&#30340;&#21147;&#25110;&#32773;&#29699;&#30340;&#25705;&#25830;&#21147;&#21457;&#29983;&#21464;&#21270;&#65292;&#29699;&#21487;&#20197;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#21521;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#24819;&#35937;&#22312;&#19981;&#21516;&#30340;&#26041;&#21521;&#19978;&#22836;&#29699;&#38656;&#35201;&#23436;&#20840;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#36866;&#24212;&#30446;&#26631;&#25110;&#29615;&#22659;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#25110;&#29615;&#22659;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#33050;&#26426;&#22120;&#20154;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#20301;&#32622;&#30340;&#38544;&#24335;&#21464;&#21270;&#25110;&#29699;&#30340;&#24674;&#22797;&#31995;&#25968;&#30340;&#21464;&#21270;&#65292;&#32780;&#26631;&#20934;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#21017;&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2308.02001</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02001
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20855;&#26377;m&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#32500;&#25968;d&#65288;&#21363;md+m&#20010;&#35757;&#32451;&#21442;&#25968;&#65289;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#21363;&#32593;&#32476;&#33021;&#22815;&#35760;&#24518;&#30340;&#19968;&#33324;&#25968;&#25454;&#30340;&#26368;&#22823;&#23610;&#23544;&#65292;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#23545;&#20110;&#38750;&#22810;&#39033;&#24335;&#23454;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;sigmoid&#21644;&#24179;&#28369;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;&#24179;&#28369;ReLU&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;md/2&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#32422;&#20026;2&#20493;&#12290;&#31867;&#20284;&#30340;&#20808;&#21069;&#32467;&#26524;&#20165;&#38480;&#20110;&#38454;&#36291;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#32467;&#26524;&#21463;&#21040;&#23545;&#25968;&#22240;&#23376;&#21644;&#38543;&#26426;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#28041;&#21450;Hadamard&#24130;&#21644;Khati-Rao&#31215;&#30340;&#30697;&#38453;&#30340;&#31209;&#26469;&#32771;&#23519;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#25193;&#23637;&#20102;&#20851;&#20110;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20043;&#21069;&#20851;&#20110;&#23384;&#20648;&#23481;&#37327;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#24182;&#26377;&#24076;&#26395;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#27493;&#38271;&#35268;&#21017;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.11201</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#24102;&#26377;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning with Auto-Tuned Clients. (arXiv:2306.11201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#27493;&#38271;&#35268;&#21017;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22810;&#27425;&#21327;&#20316;&#27493;&#39588;&#35757;&#32451;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#20998;&#24067;&#12289;&#21442;&#19982;&#29575;&#21644;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#21487;&#33021;&#22823;&#22823;&#21464;&#21270;&#65292;&#20294;&#36825;&#31181;&#28789;&#27963;&#24615;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#26032;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23458;&#25143;&#31471;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\Delta$-SGD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#27493;&#38271;&#35268;&#21017;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#36890;&#36807;&#36866;&#24212;&#35813;&#23458;&#25143;&#31471;&#27491;&#22312;&#20248;&#21270;&#30340;&#20989;&#25968;&#30340;&#23616;&#37096;&#24179;&#28369;&#24615;&#26469;&#20351;&#29992;&#33258;&#24049;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#23458;&#25143;&#31471;&#36866;&#24212;&#24615;&#22312;&#21508;&#31181;FL&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning framework where the global model of a central server is trained via multiple collaborative steps by participating clients without sharing their data. While being a flexible framework, where the distribution of local data, participation rate, and computing power of each client can greatly vary, such flexibility gives rise to many new challenges, especially in the hyperparameter tuning on the client side. We propose $\Delta$-SGD, a simple step size rule for SGD that enables each client to use its own step size by adapting to the local smoothness of the function each client is optimizing. We provide theoretical and empirical results where the benefit of the client adaptivity is shown in various FL scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#21644;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#20197;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.10835</link><description>&lt;p&gt;
&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Dynamic Submodular Optimization. (arXiv:2306.10835v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#21644;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#20197;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28385;&#36275;&#19968;&#33324;&#32422;&#26463;&#26465;&#20214;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#20026;&#23376;&#27169;&#35268;&#21010;&#30340;&#38382;&#39064;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#27714;&#35299;&#20808;&#21069;&#36718;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#26469;&#36991;&#20813;&#21407;&#38382;&#39064;&#30340;NP-&#22256;&#38590;&#24615;&#12290;&#25105;&#20204;&#23558;OSGA&#25193;&#23637;&#20026;&#36890;&#29992;&#30340;&#36817;&#20284;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;OSGA&#22312;&#26102;&#38388;&#38271;&#24230;&#21644;&#32047;&#31215;&#36718;&#27425;&#26368;&#20248;&#21464;&#21270;&#26041;&#38754;&#20855;&#26377;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#26368;&#20005;&#26684;&#36793;&#30028;&#30456;&#20284;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;&#23545;&#20110;&#27809;&#26377;&#36817;&#20284;&#35299;&#25110;&#38656;&#35201;&#26356;&#31616;&#21333;&#30340;&#23454;&#29616;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;Lova\'sz&#25193;&#23637;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#30340;&#36951;&#25022;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30005;&#21147;&#31995;&#32479;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose new algorithms with provable performance for online binary optimization subject to general constraints and in dynamic settings. We consider the subset of problems in which the objective function is submodular. We propose the online submodular greedy algorithm (OSGA) which solves to optimality an approximation of the previous round loss function to avoid the NP-hardness of the original problem. We extend OSGA to a generic approximation function. We show that OSGA has a dynamic regret bound similar to the tightest bounds in online convex optimization with respect to the time horizon and the cumulative round optimum variation. For instances where no approximation exists or a computationally simpler implementation is desired, we design the online submodular projected gradient descent (OSPGD) by leveraging the Lova\'sz extension. We obtain a regret bound that is akin to the conventional online gradient descent (OGD). Finally, we numerically test our algorithms in two power system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICLE&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#24555;&#36895;&#35745;&#31639;&#27599;&#20010;&#32452;&#21512;&#30340;&#36866;&#24212;&#24230;&#26469;&#21152;&#36895;&#25628;&#32034;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#36716;&#31227;&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06545</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26694;&#26550;&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Framework for Modular Continual Learning. (arXiv:2306.06545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICLE&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#24555;&#36895;&#35745;&#31639;&#27599;&#20010;&#32452;&#21512;&#30340;&#36866;&#24212;&#24230;&#26469;&#21152;&#36895;&#25628;&#32034;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#36716;&#31227;&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#26041;&#27861;&#26159;&#22686;&#37327;&#23398;&#20064;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#65292;&#27599;&#20010;&#38382;&#39064;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22359;&#32452;&#21512;&#19988;&#36991;&#20813;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#25628;&#32034;&#21487;&#33021;&#30340;&#27169;&#22359;&#32452;&#21512;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#35780;&#20272;&#32452;&#21512;&#24615;&#33021;&#38656;&#35201;&#19968;&#36718;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#21517;&#20026;PICLE&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#26469;&#24555;&#36895;&#35745;&#31639;&#27599;&#20010;&#32452;&#21512;&#30340;&#36866;&#24212;&#24230;&#26469;&#21152;&#36895;&#25628;&#32034;&#12290;&#27169;&#22411;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#33391;&#22909;&#27169;&#22359;&#32452;&#21512;&#30340;&#30693;&#35782;&#19982;&#25968;&#25454;&#38598;&#29305;&#23450;&#20449;&#24687;&#12290;&#23427;&#30340;&#20351;&#29992;&#34987;&#20998;&#20026;&#24863;&#30693;&#21644;&#28508;&#22312;&#23376;&#38598;&#31561;&#23376;&#38598;&#30340;&#25628;&#32034;&#31354;&#38388;&#21152;&#20197;&#34917;&#20805;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PICLE&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#36716;&#31227;&#30340;&#27169;&#22359;&#21270;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#33021;&#25193;&#23637;&#21040;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#22871;&#20214;&#19978;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#22871;&#20214;&#26088;&#22312;&#25429;&#25417;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#21516;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular approaches, which use a different composition of modules for each problem and avoid forgetting by design, have been shown to be a promising direction in continual learning (CL). However, searching through the large, discrete space of possible module compositions is a challenge because evaluating a composition's performance requires a round of neural network training. To address this challenge, we develop a modular CL framework, called PICLE, that accelerates search by using a probabilistic model to cheaply compute the fitness of each composition. The model combines prior knowledge about good module compositions with dataset-specific information. Its use is complemented by splitting up the search space into subsets, such as perceptual and latent subsets. We show that PICLE is the first modular CL algorithm to achieve different types of transfer while scaling to large search spaces. We evaluate it on two benchmark suites designed to capture different desiderata of CL techniques. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20855;&#26377;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;&#19988;&#19982;&#23481;&#24046;&#27700;&#24179;&#30340;&#20851;&#32852;&#24615;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.19001</link><description>&lt;p&gt;
&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31574;&#30053;&#35780;&#20272;&#30340;&#39640;&#27010;&#29575;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sharp high-probability sample complexities for policy evaluation with linear function approximation. (arXiv:2305.19001v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20855;&#26377;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;&#19988;&#19982;&#23481;&#24046;&#27700;&#24179;&#30340;&#20851;&#32852;&#24615;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28041;&#21450;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22312;&#26080;&#38480;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#35780;&#20272;&#31639;&#27861;&#65288;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#21644;&#24102;&#26377;&#26799;&#24230;&#26657;&#27491;&#30340;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#32447;&#24615;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65289;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#20445;&#35777;&#26368;&#20339;&#32447;&#24615;&#31995;&#25968;&#30340;&#39044;&#23450;&#20041;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;&#31574;&#30053;&#35774;&#32622;&#21644;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#36798;&#21040;&#20102;&#19982;&#23481;&#24046;&#27700;&#24179;&#30340;&#26368;&#20339;&#20851;&#32852;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#37327;&#26126;&#30830;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#31574;&#30053;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19978;&#38480;&#30028;&#38480;&#19982;&#20851;&#38190;&#38382;&#39064;&#21442;&#25968;&#19978;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#38480;&#30028;&#38480;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05281</link><description>&lt;p&gt;
&#24102;&#26377;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#65306;&#19968;&#31181;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Unobserved Variables: A Proxy Variable Approach. (arXiv:2305.05281v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26410;&#35266;&#23519;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#28151;&#26434;&#25110;&#20013;&#20171;&#65289;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#22240;&#26524;&#35782;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#31471;&#22240;&#26524;&#25506;&#32034;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#35843;&#25972;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#30340;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;&#24341;&#21457;&#30340;&#32447;&#24615;&#36829;&#35268;&#26469;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#26377;&#20005;&#26684;&#32423;&#21035;&#32422;&#26463;&#30340;&#31163;&#25955;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#36817;&#31471;&#20551;&#35774;&#26816;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#36866;&#29992;&#20110;&#30001;&#36830;&#32493;&#21464;&#37327;&#32452;&#25104;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#25552;&#20986;&#32473;&#23450;&#38544;&#34255;&#22240;&#23376;&#30340;&#35266;&#27979;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#27491;&#21017;&#26465;&#20214;&#65292;&#20351;&#24471;&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#35266;&#23519;&#20195;&#29702;&#20197;&#36275;&#22815;&#30340;&#26377;&#38480;&#32454;&#26684;&#31163;&#25955;&#21270;&#65292;&#21017;&#28041;&#21450;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#21487;&#20197;&#26377;&#25928;&#22320;&#21463;&#21040;&#25511;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#20855;&#26377;&#19968;&#33324;&#20808;&#39564;&#38480;&#21046;&#30340;&#26032;&#36817;&#31471;&#22240;&#26524;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations from observational data is important. The existence of unobserved variables (e.g. latent confounding or mediation) can mislead the causal identification. To overcome this problem, proximal causal discovery methods attempted to adjust for the bias via the proxy of the unobserved variable. Particularly, hypothesis test-based methods proposed to identify the causal edge by testing the induced violation of linearity. However, these methods only apply to discrete data with strict level constraints, which limits their practice in the real world. In this paper, we fix this problem by extending the proximal hypothesis test to cases where the system consists of continuous variables. Our strategy is to present regularity conditions on the conditional distributions of the observed variables given the hidden factor, such that if we discretize its observed proxy with sufficiently fine, finite bins, the involved discretization error can be effectively controlled. Based o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06027</link><description>&lt;p&gt;
&#25345;&#32493;&#25193;&#25955;&#65306;&#20351;&#29992;C-LoRA&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#25345;&#32493;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21482;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#23450;&#20041;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22810;&#20010;&#32454;&#31890;&#24230;&#27010;&#24565;&#20197;&#36830;&#32493;&#26041;&#24335;&#65288;&#21363;&#25345;&#32493;&#24615;&#22320;&#65289;&#33258;&#23450;&#20041;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#25216;&#26415;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;C-LoRA&#65292;&#37319;&#29992;&#27969;&#34892;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36328;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#36830;&#32493;&#33258;&#25105;&#27491;&#21017;&#21270;&#20302;&#31209;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21253;&#25324;&#33258;&#23450;&#20041;&#23545;&#35937;&#30340;&#21333;&#35789;&#65288;&#21363;&#8220;&#20154;&#8221;&#29992;&#20110;&#20154;&#33080;&#25968;&#25454;&#38598;&#65289;&#24182;&#21021;&#22987;&#21270;&#20026;&#23436;&#20840;&#38543;&#26426;&#23884;&#20837;&#30340;&#23450;&#21046;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#24341;&#20837;&#20102;&#24494;&#23567;&#30340;&#39069;&#22806;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#39592;&#26550;&#26469;&#25429;&#25417;&#28508;&#22312;&#30340;&#27969;&#24418;&#20960;&#20309;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#36816;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#65292;&#38500;&#20102;&#20855;&#26377;&#38750;&#21442;&#25968;&#20248;&#28857;&#20043;&#22806;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#25968;&#25454;&#65292;&#22024;&#26434;&#35266;&#23519;&#26102;&#20063;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11786</link><description>&lt;p&gt;
Skeleton Regression&#65306;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#32467;&#26500;&#20272;&#35745;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure. (arXiv:2303.11786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#39592;&#26550;&#26469;&#25429;&#25417;&#28508;&#22312;&#30340;&#27969;&#24418;&#20960;&#20309;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#36816;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#65292;&#38500;&#20102;&#20855;&#26377;&#38750;&#21442;&#25968;&#20248;&#28857;&#20043;&#22806;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#25968;&#25454;&#65292;&#22024;&#26434;&#35266;&#23519;&#26102;&#20063;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#26088;&#22312;&#22788;&#29702;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#30340;&#22797;&#26434;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#65292;&#31216;&#20026;&#39592;&#26550;&#65292;&#20197;&#25429;&#33719;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#39592;&#26550;&#22270;&#19978;&#23450;&#20041;&#25351;&#26631;&#65292;&#24212;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#29305;&#24449;&#36716;&#25442;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#12290;&#38500;&#20102;&#21253;&#25324;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#38750;&#21442;&#25968;&#22238;&#24402;&#22120;&#22312;&#39592;&#26550;&#22270;&#31561;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#22238;&#24402;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#24320;&#32500;&#24230;&#28798;&#38590;&#65292;&#20855;&#26377;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#30340;&#24182;&#38598;&#24182;&#19988;&#40065;&#26834;&#24615;&#33021;&#24212;&#23545;&#21152;&#24615;&#22122;&#22768;&#21644;&#22024;&#26434;&#35266;&#23519;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. In addition to the included nonparametric methods, we also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework allows us to bypass the curse of dimensionality and provides additional advantages that it can handle the union of multiple manifolds and is robust to additive noise and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2301.10226</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20449;&#21495;&#65292;&#21363;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#21487;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#12290;&#27700;&#21360;&#21487;&#20197;&#23884;&#20837;&#21040;&#25991;&#26412;&#20013;&#65292;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#22312;&#19981;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;API&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#21333;&#35789;&#20043;&#21069;&#36873;&#25321;&#19968;&#32452;&#38543;&#26426;&#30340;&#8220;&#32511;&#33394;&#8221;&#26631;&#35760;&#65292;&#28982;&#21518;&#22312;&#25277;&#26679;&#36807;&#31243;&#20013;&#36719;&#24615;&#22320;&#25512;&#24191;&#20351;&#29992;&#36825;&#20123;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;P&#20540;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27700;&#21360;&#25216;&#26415;&#65292; &#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#27700;&#21360;&#25216;&#26415;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;Open Pretrained Transformer&#65288;OPT&#65289;&#23478;&#26063;&#30340;&#19968;&#20010;&#25968;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#26469;&#27979;&#35797;&#27700;&#21360;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#22312;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#26816;&#39564;&#20855;&#26377;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26816;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09201</link><description>&lt;p&gt;
&#20855;&#26377;&#35889;&#27491;&#21017;&#21270;&#30340;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Spectral Regularized Kernel Two-Sample Tests. (arXiv:2212.09201v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#22312;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#26816;&#39564;&#20855;&#26377;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26816;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#19968;&#31181;&#22312;&#38750;&#21442;&#25968;&#26816;&#39564;&#38382;&#39064;&#20013;&#24191;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#23884;&#20837;&#30340;&#27010;&#24565;&#26469;&#22788;&#29702;&#19968;&#33324;&#65288;&#21363;&#38750;&#27431;&#20960;&#37324;&#24471;&#65289;&#22495;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#29702;&#35299;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#26500;&#24314;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21452;&#26679;&#26412;&#26816;&#39564;&#22312;Hellinger&#36317;&#31163;&#19979;&#30340;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;MMD&#26816;&#39564;&#20462;&#25913;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21327;&#26041;&#24046;&#20449;&#24687;&#65288;MMD&#26816;&#39564;&#26080;&#27861;&#25429;&#33719;&#65289;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#20855;&#26377;&#27604;MMD&#26816;&#39564;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#36848;&#26816;&#39564;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26469;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#36866;&#24212;&#26816;&#39564;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, an approach that has gained a lot of popularity to tackle non-parametric testing problems on general (i.e., non-Euclidean) domains is based on the notion of reproducing kernel Hilbert space (RKHS) embedding of probability distributions. The main goal of our work is to understand the optimality of two-sample tests constructed based on this approach. First, we show that the popular MMD (maximum mean discrepancy) two-sample test is not optimal in terms of the separation boundary measured in Hellinger distance. Second, we propose a modification to the MMD test based on spectral regularization by taking into account the covariance information (which is not captured by the MMD test) and prove the proposed test to be minimax optimal with a smaller separation boundary than that achieved by the MMD test. Third, we propose an adaptive version of the above test which involves a data-driven strategy to choose the regularization parameter and show the adaptive test to be almos
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2211.09325</link><description>&lt;p&gt;
TAX-Pose&#65306;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22522;&#20110;&#31034;&#33539;&#36716;&#31227;&#30456;&#20851;&#25216;&#33021;&#65311;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#25110;&#26410;&#35265;&#36807;&#30340;&#37197;&#32622;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#20114;&#23545;&#35937;&#30456;&#20851;&#37096;&#20998;&#30340;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#20851;&#31995;&#26159;&#19968;&#31181;&#21487;&#20197;&#36716;&#31227;&#21040;&#21516;&#19968;&#31867;&#21035;&#26032;&#29289;&#20307;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#27010;&#24565;&#65307;&#20363;&#22914;&#65292;&#24179;&#24213;&#38149;&#30456;&#23545;&#20110;&#28900;&#31665;&#30340;&#23039;&#21183;&#20851;&#31995;&#25110;&#32773;&#26479;&#23376;&#30456;&#23545;&#20110;&#26479;&#26550;&#30340;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#20026;&#8220;&#36328;&#23039;&#21183;&#8221;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#27010;&#24565;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#23398;&#20064;&#30340;&#23545;&#35937;&#38388;&#23545;&#24212;&#20851;&#31995;&#26469;&#23398;&#20064;&#20272;&#35745;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#12290;&#28982;&#21518;&#65292;&#20272;&#35745;&#30340;&#36328;&#23039;&#21183;&#29992;&#20110;&#24341;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#23545;&#35937;&#25805;&#32437;&#21040;&#25152;&#38656;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#20986;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#30340;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#19982;&#31070;&#32463;&#32593;&#32476;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#26159;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2205.11787</link><description>&lt;p&gt;
&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#30340;&#20108;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quadratic models for understanding neural network dynamics. (arXiv:2205.11787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11787
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#20986;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#30340;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#19982;&#31070;&#32463;&#32593;&#32476;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#26159;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#22686;&#21152;&#26102;&#65292;&#21487;&#20197;&#29992;&#32447;&#24615;&#27169;&#22411;&#26469;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#29305;&#24615;&#19981;&#33021;&#34987;&#32447;&#24615;&#27169;&#22411;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;[Lewkowycz&#31561;&#20154;&#65292;2020]&#65292;&#24403;&#20351;&#29992;&#22823;&#23398;&#20064;&#29575;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#26102;&#20250;&#20986;&#29616;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#23588;&#20854;&#26159;&#22312;&#24377;&#24339;&#38454;&#27573;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the "catapult phase" [Lewkowycz et al. 2020] that arises when training such models with large learning rates. We then empirically show that the behaviour of neural quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models can be an effective tool for analysis of neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;</title><link>http://arxiv.org/abs/1511.05240</link><description>&lt;p&gt;
McDiarmid&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
An extension of McDiarmid's inequality. (arXiv:1511.05240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1511.05240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#25512;&#24191;&#35770;&#35777;&#25512;&#24191;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#39640;&#27010;&#29575;&#38598;&#21512;&#12290;&#36825;&#20123;&#20989;&#25968;&#38598;&#20013;&#20110;&#23427;&#20204;&#30340;&#26465;&#20214;&#26399;&#26395;&#21608;&#22260;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize McDiarmid's inequality for functions with bounded differences on a high probability set, using an extension argument. Those functions concentrate around their conditional expectations. We further extend the results to concentration in general metric spaces.
&lt;/p&gt;</description></item></channel></rss>